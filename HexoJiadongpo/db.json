{"meta":{"version":1,"warehouse":"2.2.0"},"models":{"Asset":[{"_id":"source/CNAME","path":"CNAME","modified":1,"renderable":0},{"_id":"source/images/cenrise10.jpg","path":"images/cenrise10.jpg","modified":1,"renderable":0},{"_id":"source/images/douban.jpg","path":"images/douban.jpg","modified":1,"renderable":0},{"_id":"source/images/github.png","path":"images/github.png","modified":1,"renderable":0},{"_id":"source/images/meizhuang.png","path":"images/meizhuang.png","modified":1,"renderable":0},{"_id":"source/images/quora.jpeg","path":"images/quora.jpeg","modified":1,"renderable":0},{"_id":"source/images/taobao.png","path":"images/taobao.png","modified":1,"renderable":0},{"_id":"source/images/favicon.ico","path":"images/favicon.ico","modified":1,"renderable":0},{"_id":"source/images/twitter.png","path":"images/twitter.png","modified":1,"renderable":0},{"_id":"source/images/weibo.jpg","path":"images/weibo.jpg","modified":1,"renderable":0},{"_id":"source/images/zhihu.png","path":"images/zhihu.png","modified":1,"renderable":0},{"_id":"source/images/facebook.png","path":"images/facebook.png","modified":1,"renderable":0},{"_id":"source/images/kettle/sorted merger1.jpg","path":"images/kettle/sorted merger1.jpg","modified":1,"renderable":0},{"_id":"source/images/kettle/开源ETL工具-kettle_ETL是什么.png","path":"images/kettle/开源ETL工具-kettle_ETL是什么.png","modified":1,"renderable":0},{"_id":"source/images/kettle/开源ETL工具-kettle_Jobs（工作）4.png","path":"images/kettle/开源ETL工具-kettle_Jobs（工作）4.png","modified":1,"renderable":0},{"_id":"source/images/kettle/开源ETL工具-kettle_Jobs（工作）5.png","path":"images/kettle/开源ETL工具-kettle_Jobs（工作）5.png","modified":1,"renderable":0},{"_id":"source/images/kettle/开源ETL工具-kettle_Jobs（工作）6.png","path":"images/kettle/开源ETL工具-kettle_Jobs（工作）6.png","modified":1,"renderable":0},{"_id":"source/images/kettle/开源ETL工具-kettle_Jobs（工作）9.png","path":"images/kettle/开源ETL工具-kettle_Jobs（工作）9.png","modified":1,"renderable":0},{"_id":"source/images/kettle/开源ETL工具-kettle_Transformation(转换).jpg","path":"images/kettle/开源ETL工具-kettle_Transformation(转换).jpg","modified":1,"renderable":0},{"_id":"source/images/kettle/开源ETL工具-kettle_Step（步骤）.png","path":"images/kettle/开源ETL工具-kettle_Step（步骤）.png","modified":1,"renderable":0},{"_id":"themes/next/source/css/main.styl","path":"css/main.styl","modified":1,"renderable":1},{"_id":"themes/next/source/images/algolia_logo.svg","path":"images/algolia_logo.svg","modified":1,"renderable":1},{"_id":"themes/next/source/images/apple-touch-icon-next.png","path":"images/apple-touch-icon-next.png","modified":1,"renderable":1},{"_id":"themes/next/source/images/avatar.gif","path":"images/avatar.gif","modified":1,"renderable":1},{"_id":"themes/next/source/images/cc-by-nc-nd.svg","path":"images/cc-by-nc-nd.svg","modified":1,"renderable":1},{"_id":"themes/next/source/images/cc-by-nc-sa.svg","path":"images/cc-by-nc-sa.svg","modified":1,"renderable":1},{"_id":"themes/next/source/images/cc-by-nc.svg","path":"images/cc-by-nc.svg","modified":1,"renderable":1},{"_id":"themes/next/source/images/cc-by-nd.svg","path":"images/cc-by-nd.svg","modified":1,"renderable":1},{"_id":"themes/next/source/images/cc-by-sa.svg","path":"images/cc-by-sa.svg","modified":1,"renderable":1},{"_id":"themes/next/source/images/cc-by.svg","path":"images/cc-by.svg","modified":1,"renderable":1},{"_id":"themes/next/source/images/cc-zero.svg","path":"images/cc-zero.svg","modified":1,"renderable":1},{"_id":"themes/next/source/images/favicon-16x16-next.png","path":"images/favicon-16x16-next.png","modified":1,"renderable":1},{"_id":"themes/next/source/images/favicon-32x32-next.png","path":"images/favicon-32x32-next.png","modified":1,"renderable":1},{"_id":"themes/next/source/images/favicon.ico","path":"images/favicon.ico","modified":1,"renderable":1},{"_id":"themes/next/source/images/loading.gif","path":"images/loading.gif","modified":1,"renderable":1},{"_id":"themes/next/source/images/logo.svg","path":"images/logo.svg","modified":1,"renderable":1},{"_id":"themes/next/source/images/placeholder.gif","path":"images/placeholder.gif","modified":1,"renderable":1},{"_id":"themes/next/source/images/quote-l.svg","path":"images/quote-l.svg","modified":1,"renderable":1},{"_id":"themes/next/source/images/quote-r.svg","path":"images/quote-r.svg","modified":1,"renderable":1},{"_id":"themes/next/source/images/searchicon.png","path":"images/searchicon.png","modified":1,"renderable":1},{"_id":"source/images/kettle/Kettle插件架构001.jpg","path":"images/kettle/Kettle插件架构001.jpg","modified":1,"renderable":0},{"_id":"source/images/avatar.jpg","path":"images/avatar.jpg","modified":1,"renderable":0},{"_id":"source/images/kettle/开源ETL工具-kettle_Jobs（工作）3.png","path":"images/kettle/开源ETL工具-kettle_Jobs（工作）3.png","modified":1,"renderable":0},{"_id":"source/images/kettle/开源ETL工具-kettle_Jobs（工作）7.png","path":"images/kettle/开源ETL工具-kettle_Jobs（工作）7.png","modified":1,"renderable":0},{"_id":"source/images/kettle/开源ETL工具-kettle_Jobs（工作）8.png","path":"images/kettle/开源ETL工具-kettle_Jobs（工作）8.png","modified":1,"renderable":0},{"_id":"source/images/kettle/开源ETL工具-kettle_Kettle构成.jpg","path":"images/kettle/开源ETL工具-kettle_Kettle构成.jpg","modified":1,"renderable":0},{"_id":"source/images/kettle/开源ETL工具-kettle_Jobs（工作）.png","path":"images/kettle/开源ETL工具-kettle_Jobs（工作）.png","modified":1,"renderable":0},{"_id":"themes/next/source/lib/font-awesome/HELP-US-OUT.txt","path":"lib/font-awesome/HELP-US-OUT.txt","modified":1,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/bower.json","path":"lib/font-awesome/bower.json","modified":1,"renderable":1},{"_id":"themes/next/source/lib/velocity/velocity.min.js","path":"lib/velocity/velocity.min.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/velocity/velocity.ui.js","path":"lib/velocity/velocity.ui.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/velocity/velocity.ui.min.js","path":"lib/velocity/velocity.ui.min.js","modified":1,"renderable":1},{"_id":"themes/next/source/js/src/affix.js","path":"js/src/affix.js","modified":1,"renderable":1},{"_id":"themes/next/source/js/src/algolia-search.js","path":"js/src/algolia-search.js","modified":1,"renderable":1},{"_id":"themes/next/source/js/src/bootstrap.js","path":"js/src/bootstrap.js","modified":1,"renderable":1},{"_id":"themes/next/source/js/src/exturl.js","path":"js/src/exturl.js","modified":1,"renderable":1},{"_id":"themes/next/source/js/src/js.cookie.js","path":"js/src/js.cookie.js","modified":1,"renderable":1},{"_id":"themes/next/source/js/src/motion.js","path":"js/src/motion.js","modified":1,"renderable":1},{"_id":"themes/next/source/js/src/post-details.js","path":"js/src/post-details.js","modified":1,"renderable":1},{"_id":"themes/next/source/js/src/scroll-cookie.js","path":"js/src/scroll-cookie.js","modified":1,"renderable":1},{"_id":"themes/next/source/js/src/scrollspy.js","path":"js/src/scrollspy.js","modified":1,"renderable":1},{"_id":"themes/next/source/js/src/utils.js","path":"js/src/utils.js","modified":1,"renderable":1},{"_id":"themes/next/source/images/avatar.jpg","path":"images/avatar.jpg","modified":1,"renderable":1},{"_id":"themes/next/source/lib/jquery/index.js","path":"lib/jquery/index.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/css/font-awesome.css","path":"lib/font-awesome/css/font-awesome.css","modified":1,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/css/font-awesome.css.map","path":"lib/font-awesome/css/font-awesome.css.map","modified":1,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/css/font-awesome.min.css","path":"lib/font-awesome/css/font-awesome.min.css","modified":1,"renderable":1},{"_id":"themes/next/source/lib/ua-parser-js/dist/ua-parser.min.js","path":"lib/ua-parser-js/dist/ua-parser.min.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/ua-parser-js/dist/ua-parser.pack.js","path":"lib/ua-parser-js/dist/ua-parser.pack.js","modified":1,"renderable":1},{"_id":"themes/next/source/js/src/schemes/pisces.js","path":"js/src/schemes/pisces.js","modified":1,"renderable":1},{"_id":"source/images/hadoop/kylin/Kylin的技术架构.jpg","path":"images/hadoop/kylin/Kylin的技术架构.jpg","modified":1,"renderable":0},{"_id":"source/images/hadoop/kylin/一个四维Cube的例子.jpg","path":"images/hadoop/kylin/一个四维Cube的例子.jpg","modified":1,"renderable":0},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.woff","path":"lib/font-awesome/fonts/fontawesome-webfont.woff","modified":1,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.woff2","path":"lib/font-awesome/fonts/fontawesome-webfont.woff2","modified":1,"renderable":1},{"_id":"themes/next/source/lib/velocity/velocity.js","path":"lib/velocity/velocity.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.eot","path":"lib/font-awesome/fonts/fontawesome-webfont.eot","modified":1,"renderable":1},{"_id":"source/images/hadoop/kylin/维度和度量的例子.jpg","path":"images/hadoop/kylin/维度和度量的例子.jpg","modified":1,"renderable":0}],"Cache":[{"_id":"source/CNAME","hash":"3cf76298bb7b4f7f21570d8949949e869edb09b5","modified":1503249010000},{"_id":"source/README.md","hash":"a7cc04175aea18a29094012b0d3a73ce3cfa8d99","modified":1503249010000},{"_id":"source/baidu_verify_iea4FlOhCY.html","hash":"3e15a1400f4178f4535b05a5360442d961a82cc5","modified":1503249010000},{"_id":"themes/next/.editorconfig","hash":"792fd2bd8174ece1a75d5fd24ab16594886f3a7f","modified":1526368421362},{"_id":"themes/next/.bowerrc","hash":"3228a58ed0ece9f85e1e3136352094080b8dece1","modified":1526368421361},{"_id":"themes/next/.eslintrc.json","hash":"cc5f297f0322672fe3f684f823bc4659e4a54c41","modified":1526368421362},{"_id":"themes/next/.gitattributes","hash":"44bd4729c74ccb88110804f41746fec07bf487d4","modified":1526368421362},{"_id":"themes/next/.gitignore","hash":"a18c2e83bb20991b899b58e6aeadcb87dd8aa16e","modified":1526368421365},{"_id":"themes/next/.stylintrc","hash":"b28e24704a5d8de08346c45286574c8e76cc109f","modified":1526368421365},{"_id":"themes/next/.travis.yml","hash":"3d1dc928c4a97933e64379cfde749dedf62f252c","modified":1526368421365},{"_id":"themes/next/.stickler.yml","hash":"b7939095038cbdc4883fc10950e163a60a643b43","modified":1526368421365},{"_id":"themes/next/LICENSE.md","hash":"fc7227c508af3351120181cbf2f9b99dc41f063e","modified":1526368421365},{"_id":"themes/next/_config.yml","hash":"aadd340be4902669e063f2aa0e021f20700f48e6","modified":1526385764934},{"_id":"themes/next/bower.json","hash":"29515e8a97ae28e07a934e38d0fc79af695d14fc","modified":1526368421367},{"_id":"themes/next/crowdin.yml","hash":"e026078448c77dcdd9ef50256bb6635a8f83dca6","modified":1526368421367},{"_id":"themes/next/README.md","hash":"807c28ad6473b221101251d244aa08e2a61b0d60","modified":1526368421366},{"_id":"themes/next/package.json","hash":"d2eccdf5f241a3b42f2c9a873e3a063e54316ea9","modified":1526368421416},{"_id":"themes/next/gulpfile.coffee","hash":"48d2f9fa88a4210308fc41cc7d3f6d53989f71b7","modified":1526368421375},{"_id":"source/_posts/Markdown基础入门.md","hash":"ebffa095d8e09d35bb30fefad0d7f93720f8d6a6","modified":1503020675000},{"_id":"source/about/index.md","hash":"d274f8bc8626b7e8a0826bbb34a65a122ee67b62","modified":1526385655886},{"_id":"source/images/cenrise10.jpg","hash":"004afe6e4189ddb75ea1f4289ed2f5a0fc63ae44","modified":1503249010000},{"_id":"source/images/douban.jpg","hash":"7bd59f85ca708d96a65dddd6981086cbb639cd81","modified":1503249010000},{"_id":"source/images/github.png","hash":"ec237c5368083111c4952dbd80148e8e375e206e","modified":1503249010000},{"_id":"source/images/meizhuang.png","hash":"422b2633c2a77683e03387c91c8e0e72638b427b","modified":1503249010000},{"_id":"source/images/quora.jpeg","hash":"576783adddddee7ab97f15fcf3eba4565f4027dd","modified":1503249010000},{"_id":"source/images/taobao.png","hash":"c84ef7815c69fd97edaeb4b84772dfb89bc6ef52","modified":1503249010000},{"_id":"source/images/favicon.ico","hash":"6ed407cb30e21a406a45d0076a4a3226d2633bf0","modified":1503249010000},{"_id":"source/images/twitter.png","hash":"8b678142eae17a91d47d6fbe6260ae5cd9781c9f","modified":1503249010000},{"_id":"source/images/weibo.jpg","hash":"7f3fcab888eb49a6b21e9fb1b5a300bc9f7e1860","modified":1503249010000},{"_id":"source/images/zhihu.png","hash":"46f6e7bc2f6fb30b7bdbda83f9678efa003a68d5","modified":1503249010000},{"_id":"source/categories/index.md","hash":"0fa7c5b3e1e66035e222ae40db8da30dc6b0d86b","modified":1526385667149},{"_id":"source/tags/index.md","hash":"281b4a8c5aadb520edb7df2276833512630b3a08","modified":1526385677440},{"_id":"themes/next/.git/HEAD","hash":"acbaef275e46a7f14c1ef456fff2c8bbe8c84724","modified":1526368421354},{"_id":"themes/next/.git/config","hash":"e2ca9fa6f115d4406d24bf0df53fc26ce13e0c9b","modified":1526368421356},{"_id":"themes/next/.git/index","hash":"ce312c45b95aa5a7eabef9fc4d2b844ff0d8138d","modified":1526382648448},{"_id":"themes/next/.git/description","hash":"9635f1b7e12c045212819dd934d809ef07efa2f4","modified":1526368410490},{"_id":"themes/next/.git/packed-refs","hash":"acc0900604a227f71d02fef59211dfb60dc224e7","modified":1526368421352},{"_id":"themes/next/.github/CONTRIBUTING.md","hash":"f846118d7fc68c053df47b24e1f661241645373f","modified":1526368421363},{"_id":"themes/next/.github/CODE_OF_CONDUCT.md","hash":"b63696d41f022525e40d7e7870c3785b6bc7536b","modified":1526368421363},{"_id":"themes/next/.github/ISSUE_TEMPLATE.md","hash":"00c25366764e6b9ccb40b877c60dc13b2916bbf7","modified":1526368421363},{"_id":"themes/next/.github/PULL_REQUEST_TEMPLATE.md","hash":"7abbb4c8a29b2c14e576a00f53dbc0b4f5669c13","modified":1526368421364},{"_id":"themes/next/.github/stale.yml","hash":"fd0856f6745db8bd0228079ccb92a662830cc4fb","modified":1526368421364},{"_id":"themes/next/.github/browserstack_logo.png","hash":"a6c43887f64a7f48a2814e3714eaa1215e542037","modified":1526368421364},{"_id":"themes/next/docs/ALGOLIA-SEARCH.md","hash":"141e989844d0b5ae2e09fb162a280715afb39b0d","modified":1526368421368},{"_id":"themes/next/docs/AUTHORS.md","hash":"7b24be2891167bdedb9284a682c2344ec63e50b5","modified":1526368421368},{"_id":"themes/next/docs/DATA-FILES.md","hash":"8e1962dd3e1b700169b3ae5bba43992f100651ce","modified":1526368421369},{"_id":"themes/next/docs/AGPL3.md","hash":"0d2b8c5fa8a614723be0767cc3bca39c49578036","modified":1526368421367},{"_id":"themes/next/docs/INSTALLATION.md","hash":"2bbdd6c1751b2b42ce9b9335da420c6026a483e9","modified":1526368421369},{"_id":"themes/next/docs/LEANCLOUD-COUNTER-SECURITY.md","hash":"120750c03ec30ccaa470b113bbe39f3d423c67f0","modified":1526368421369},{"_id":"themes/next/docs/LICENSE","hash":"fe607fe22fc9308f6434b892a7f2d2c5514b8f0d","modified":1526368421369},{"_id":"themes/next/docs/MATH.md","hash":"0ae4258950de01a457ea8123a8d13ec6db496e53","modified":1526368421369},{"_id":"themes/next/docs/UPDATE-FROM-5.1.X.md","hash":"ad57c168d12ba01cf144a1ea0627b2ffd1847d3e","modified":1526368421370},{"_id":"themes/next/languages/de.yml","hash":"fb478c5040a4e58a4c1ad5fb52a91e5983d65a3a","modified":1526368421379},{"_id":"themes/next/languages/default.yml","hash":"c540c3a0d7db2d4239293c8783881962640b6c34","modified":1526368421380},{"_id":"themes/next/languages/en.yml","hash":"c540c3a0d7db2d4239293c8783881962640b6c34","modified":1526368421380},{"_id":"themes/next/languages/fr.yml","hash":"0162a85ae4175e66882a9ead1249fedb89200467","modified":1526368421381},{"_id":"themes/next/languages/it.yml","hash":"62ef41d0a9a3816939cb4d93a524e6930ab9c517","modified":1526368421382},{"_id":"themes/next/languages/id.yml","hash":"e7fb582e117a0785036dcdbb853a6551263d6aa6","modified":1526368421382},{"_id":"themes/next/languages/ja.yml","hash":"5f8e54c666393d1ca2e257f6b1e3b4116f6657d8","modified":1526368421382},{"_id":"themes/next/languages/ko.yml","hash":"fae155018ae0efdf68669b2c7dd3f959c2e45cc9","modified":1526368421383},{"_id":"themes/next/languages/nl.yml","hash":"bb9ce8adfa5ee94bc6b5fac6ad24ba4605d180d3","modified":1526368421383},{"_id":"themes/next/languages/pt-BR.yml","hash":"bfc80c8a363fa2e8dde38ea2bc85cd19e15ab653","modified":1526368421383},{"_id":"themes/next/languages/ru.yml","hash":"db0644e738d2306ac38567aa183ca3e859a3980f","modified":1526368421384},{"_id":"themes/next/languages/pt.yml","hash":"3cb51937d13ff12fcce747f972ccb664840a9ef3","modified":1526368421384},{"_id":"themes/next/languages/tr.yml","hash":"c5f0c20743b1dd52ccb256050b1397d023e6bcd9","modified":1526368421385},{"_id":"themes/next/languages/vi.yml","hash":"8da921dd8335dd676efce31bf75fdd4af7ce6448","modified":1526368421386},{"_id":"themes/next/languages/zh-CN.yml","hash":"fbbf3a0b664ae8e927c700b0a813692b94345156","modified":1526368421386},{"_id":"themes/next/languages/zh-HK.yml","hash":"7903b96912c605e630fb695534012501b2fad805","modified":1526368421387},{"_id":"themes/next/languages/zh-TW.yml","hash":"6e6d2cd8f4244cb1b349b94904cb4770935acefd","modified":1526368421387},{"_id":"themes/next/layout/_layout.swig","hash":"09e8a6bfe5aa901c66d314601c872e57f05509e8","modified":1526368421388},{"_id":"themes/next/layout/archive.swig","hash":"2b6450c6b6d2bcbcd123ad9f59922a5e323d77a5","modified":1526368421413},{"_id":"themes/next/layout/category.swig","hash":"5d955284a42f802a48560b4452c80906a5d1da02","modified":1526368421414},{"_id":"themes/next/layout/index.swig","hash":"53300ca42c00cba050bc98b0a3f2d888d71829b1","modified":1526368421414},{"_id":"themes/next/layout/page.swig","hash":"79040bae5ec14291441b33eea341a24a7c0e9f93","modified":1526368421414},{"_id":"themes/next/layout/post.swig","hash":"e7458f896ac33086d9427979f0f963475b43338e","modified":1526368421415},{"_id":"themes/next/layout/schedule.swig","hash":"3e9cba5313bf3b98a38ccb6ef78b56ffa11d66ee","modified":1526368421415},{"_id":"themes/next/layout/tag.swig","hash":"ba402ce8fd55e80b240e019e8d8c48949b194373","modified":1526368421415},{"_id":"themes/next/scripts/helpers.js","hash":"392cda207757d4c055b53492a98f81386379fc4f","modified":1526368421416},{"_id":"themes/next/scripts/merge-configs.js","hash":"33afe97284d34542015d358a720823feeebef120","modified":1526368421417},{"_id":"themes/next/scripts/merge.js","hash":"9130dabe6a674c54b535f322b17d75fe6081472f","modified":1526368421418},{"_id":"themes/next/test/.jshintrc","hash":"19f93d13d1689fe033c82eb2d5f3ce30b6543cc0","modified":1526368421470},{"_id":"themes/next/test/intern.js","hash":"11fa8a4f5c3b4119a179ae0a2584c8187f907a73","modified":1526368421471},{"_id":"themes/next/test/helpers.js","hash":"a1f5de25154c3724ffc24a91ddc576cdbd60864f","modified":1526368421470},{"_id":"source/images/facebook.png","hash":"a404e32587f618a63375fcbae73d3c99ee4b590b","modified":1503249010000},{"_id":"themes/next/source/fonts/.gitkeep","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1526368421450},{"_id":"source/_posts/java/JDK源码分析之集合框架HashMap.md","hash":"d54ba7b5b5c97defa6690d5d484c524e30323ddf","modified":1503020675000},{"_id":"source/_posts/java/Java基础之转型.md","hash":"e6fadf077f2a7155d64a909586c0bc34dc50bd0b","modified":1503020675000},{"_id":"source/_posts/java/java中Object转String.md","hash":"567ebf1de7e7b3e2c43cb29e1bbcf692bb691294","modified":1503020675000},{"_id":"source/_posts/java/jdk环境变量配置.md","hash":"1d16258177970a40fac7213af28e1c4ac2b282f8","modified":1503020675000},{"_id":"source/_posts/java/消息传送基础.md","hash":"a39188ee09c9cc48fbdef73dd266866b11ce73f0","modified":1503020675000},{"_id":"source/_posts/linux/Linux固定IP上网方式.md","hash":"cdca6298ad6f2897112ff1d0f86776d93a71f0f0","modified":1503020675000},{"_id":"source/_posts/linux/Linux常用命令及操作.md","hash":"ed12c1455ad7c66c057be9dfe79ef7fe73eef9a4","modified":1503020675000},{"_id":"source/_posts/linux/SSH用户等效性配置.md","hash":"95202030963d4ee9585f5b95f7c95d333ff012cd","modified":1503020675000},{"_id":"source/_posts/kettle/Kettle源码构建过程.md","hash":"7dc1dc61c2026ce5c12c27789283de7af7e7ce60","modified":1503020675000},{"_id":"source/_posts/kettle/Kettle插件架构.md","hash":"2a45db5a783f878fb62a15703c8c1c1b44e5df49","modified":1503020675000},{"_id":"source/_posts/kettle/Sorted Merge组件.md","hash":"eebb03b33d7d2dff4d2ed81e3a6a4c88f1c1c7a5","modified":1503020675000},{"_id":"source/_posts/kettle/开源ETL工具-kettle.md","hash":"b8b6ecbf4897395e22ef7ff97fa2e5956b663feb","modified":1503020675000},{"_id":"source/_posts/hadoop/Apache Spark与Apache Hadoop的关系.md","hash":"e4bab854f301ed13632448874e069c185218250e","modified":1503020675000},{"_id":"source/_posts/hadoop/HBase入门概念.md","hash":"23365bd2407d35989a40023981bcf88cb1677db9","modified":1503020675000},{"_id":"source/_posts/hadoop/HDFS入门概念.md","hash":"bb981c816721c5e0335eb09022289718db517dbf","modified":1503020675000},{"_id":"source/_posts/hadoop/Hadoop之分布式存储.md","hash":"c693b49467f47f8210f1299984cdf6b92f070cb6","modified":1503020675000},{"_id":"source/_posts/hadoop/Hadoop之分布式计算 .md","hash":"69b817e1953087ae80f25e1fac79f3ace79cffc7","modified":1503020675000},{"_id":"source/_posts/hadoop/Hadoop之实时分析.md","hash":"35a7152458b73dd346f9b6fac2f055164933b4f3","modified":1503020675000},{"_id":"source/_posts/hadoop/Hadoop之数据采集.md","hash":"928744f402f88fb98e5fe92bd8ad90c6dcb148f2","modified":1503020675000},{"_id":"source/_posts/hadoop/Hadoop之流式计算.md","hash":"689f8c488fe46a3ad97681d7c62c7aa75eb19100","modified":1503020675000},{"_id":"source/_posts/hadoop/Hadoop知识点.md","hash":"3715102cebfb17842888473ac84b8c6ccafb697e","modified":1503020675000},{"_id":"source/_posts/hadoop/Hive入门概念.md","hash":"52afb0ded34a21420ddbb289c83501361b61f03d","modified":1503020675000},{"_id":"source/_posts/hadoop/Kylin入门概念.md","hash":"24ec9922dd5134345d4ee8e37fb9c9cd1370dff2","modified":1503020675000},{"_id":"source/_posts/hadoop/数据分析软件分类.md","hash":"79bbca6f498423f533448281158e4082a8e6f434","modified":1503020675000},{"_id":"source/_posts/oracle/Oracle SQL优化.md","hash":"6b584fe42d496b9351d5525d419d299577798664","modified":1503020675000},{"_id":"source/_posts/oracle/Oracle数据库系统架构.md","hash":"15d44c295d3c7f4ba2ce31f1fa9378e8fa716450","modified":1503020675000},{"_id":"source/_posts/oracle/Oracle网络和数据库连接.md","hash":"ed5e5ac1a8e2098629993ae0007c22ab021e326f","modified":1503020675000},{"_id":"source/_posts/oracle/RAW类型.md","hash":"106fa1b1506606f968d392d40860959eb5606e5a","modified":1503020675000},{"_id":"source/_posts/spark/TODO-Spark体系概述.md","hash":"ff573c8338b6cc703215d9c6207b5346c1b09209","modified":1503020675000},{"_id":"source/_posts/spring/Spring源码分析之环境准备.md","hash":"22dce836f39e578e11294d1a5375db5402021928","modified":1503020675000},{"_id":"source/_posts/异常/JasperListener类找不到.md","hash":"f2c19527ec44a0e4078275841b6f39f909ae8c2c","modified":1503250560000},{"_id":"source/_posts/异常/Java_heap_space_OutOfMemoryError.md","hash":"3ae2384822c8c24f98f4d55bded3fd362d469c72","modified":1526363826605},{"_id":"source/_posts/异常/connection_holder_is_null.md","hash":"cd39cd6cee89ce1222c34e0b77fd5801f0898560","modified":1503250560000},{"_id":"source/_posts/异常/permGen_space_OutOfMemoryError.md","hash":"11ba61b4f873fdf798dc9c29fee208510cad1f18","modified":1503250560000},{"_id":"source/_posts/服务/和田市卫浴安装家具安装.md","hash":"2f395f27e892cd4a0c4c4553aabe5b3fd2352e23","modified":1503020675000},{"_id":"source/_posts/网络安全/企业安全组.md","hash":"5ac378a0c5f496885fb611ffbdef9b6dd550f2c1","modified":1503020675000},{"_id":"source/_posts/网络安全/安全组第一次会议提出的问题整理.md","hash":"e3c1644ae45762b835cde78efe7576bcb5a3ba1f","modified":1503020675000},{"_id":"source/_posts/算法/常用算法概述.md","hash":"f4835c00aef52fa1463e118612013ebcffb9e865","modified":1503020675000},{"_id":"source/_posts/设计模式/主要设计模式及简要介绍.md","hash":"25621291936921e3d41ba09aafd2d1fa2e046c0f","modified":1503020675000},{"_id":"source/images/kettle/sorted merger1.jpg","hash":"ba3ed99f8f4684e81e064d41780f5bc1a87e79fb","modified":1503249010000},{"_id":"source/images/kettle/开源ETL工具-kettle_ETL是什么.png","hash":"8da47a6bc4a7cccc87476040b8abf530dbd2aca1","modified":1503249010000},{"_id":"source/images/kettle/开源ETL工具-kettle_Jobs（工作）4.png","hash":"3a58daf90045124022e3c5cd0271f090c3ac3fd6","modified":1503249010000},{"_id":"source/images/kettle/开源ETL工具-kettle_Jobs（工作）5.png","hash":"c7ca49eac6954c3caf7a278b2e57b811d7ccddc3","modified":1503249010000},{"_id":"source/images/kettle/开源ETL工具-kettle_Jobs（工作）6.png","hash":"6c0c4f78e4a29da901770ff00cc0dadee2f1f1c0","modified":1503249010000},{"_id":"source/images/kettle/开源ETL工具-kettle_Jobs（工作）9.png","hash":"2e16235c079e3911348061430614bd6f790f21cd","modified":1503249010000},{"_id":"source/images/kettle/开源ETL工具-kettle_Transformation(转换).jpg","hash":"60a026f89af1642da224c52a001a1e215366bfb2","modified":1503249010000},{"_id":"source/images/kettle/开源ETL工具-kettle_Step（步骤）.png","hash":"f26ad03a18d856fd198a3dfc0c898cc30b5d77a4","modified":1503249010000},{"_id":"themes/next/.git/logs/HEAD","hash":"6a8952dd2db8ea48c2f7f63d92aa247fad2992fb","modified":1526368421355},{"_id":"themes/next/.git/info/exclude","hash":"c879df015d97615050afa7b9641e3352a1e701ac","modified":1526368410484},{"_id":"themes/next/.git/hooks/applypatch-msg.sample","hash":"4de88eb95a5e93fd27e78b5fb3b5231a8d8917dd","modified":1526368410491},{"_id":"themes/next/.git/hooks/commit-msg.sample","hash":"ee1ed5aad98a435f2020b6de35c173b75d9affac","modified":1526368410490},{"_id":"themes/next/.git/hooks/post-update.sample","hash":"b614c2f63da7dca9f1db2e7ade61ef30448fc96c","modified":1526368410493},{"_id":"themes/next/.git/hooks/pre-applypatch.sample","hash":"f208287c1a92525de9f5462e905a9d31de1e2d75","modified":1526368410493},{"_id":"themes/next/.git/hooks/pre-commit.sample","hash":"36aed8976dcc08b5076844f0ec645b18bc37758f","modified":1526368410491},{"_id":"themes/next/.git/hooks/pre-receive.sample","hash":"705a17d259e7896f0082fe2e9f2c0c3b127be5ac","modified":1526368410492},{"_id":"themes/next/.git/hooks/pre-push.sample","hash":"5c8518bfd1d1d3d2c1a7194994c0a16d8a313a41","modified":1526368410493},{"_id":"themes/next/.git/hooks/pre-rebase.sample","hash":"288efdc0027db4cfd8b7c47c4aeddba09b6ded12","modified":1526368410491},{"_id":"themes/next/.git/hooks/prepare-commit-msg.sample","hash":"2584806ba147152ae005cb675aa4f01d5d068456","modified":1526368410492},{"_id":"themes/next/.git/hooks/update.sample","hash":"e729cd61b27c128951d139de8e7c63d1a3758dde","modified":1526368410493},{"_id":"themes/next/docs/ru/UPDATE-FROM-5.1.X.md","hash":"b1dd18d9b890b21718883ea1832e7e02a773104a","modified":1526368421371},{"_id":"themes/next/docs/ru/DATA-FILES.md","hash":"d6d20f60f77a76c77f8e65d0c9adbd79d0274557","modified":1526368421370},{"_id":"themes/next/docs/ru/README.md","hash":"712d9a9a557c54dd6638adfb0e1d2bb345b60756","modified":1526368421371},{"_id":"themes/next/docs/ru/INSTALLATION.md","hash":"6c5d69e94961c793da156217ecf1179e868d7ba1","modified":1526368421370},{"_id":"themes/next/docs/zh-CN/DATA-FILES.md","hash":"f3eec572a7d83542e2710a7404082014aaa1a5e7","modified":1526368421373},{"_id":"themes/next/docs/zh-CN/ALGOLIA-SEARCH.md","hash":"6855402e2ef59aae307e8bd2a990647d3a605eb8","modified":1526368421371},{"_id":"themes/next/docs/zh-CN/CODE_OF_CONDUCT.md","hash":"a45a791b49954331390d548ac34169d573ea5922","modified":1526368421371},{"_id":"themes/next/docs/zh-CN/CONTRIBUTING.md","hash":"44e4fb7ce2eca20dfa98cdd1700b50d6def4086f","modified":1526368421372},{"_id":"themes/next/docs/zh-CN/README.md","hash":"84d349fda6b9973c81a9ad4677db9d9ee1828506","modified":1526368421375},{"_id":"themes/next/docs/zh-CN/LEANCLOUD-COUNTER-SECURITY.md","hash":"24cf2618d164440b047bb9396263de83bee5b993","modified":1526368421374},{"_id":"themes/next/docs/zh-CN/MATH.md","hash":"e03607b608db4aa7d46f6726827c51ac16623339","modified":1526368421374},{"_id":"themes/next/docs/zh-CN/INSTALLATION.md","hash":"b19a6e0ae96eb7c756fb5b1ba03934c7f9cbb3c3","modified":1526368421373},{"_id":"themes/next/docs/zh-CN/UPDATE-FROM-5.1.X.md","hash":"c1ba919f70efe87a39e6217883e1625af0b2c23c","modified":1526368421375},{"_id":"themes/next/layout/_custom/head.swig","hash":"9e1b9666efa77f4cf8d8261bcfa445a9ac608e53","modified":1526368421388},{"_id":"themes/next/layout/_custom/header.swig","hash":"adc83b19e793491b1c6ea0fd8b46cd9f32e592fc","modified":1526368421388},{"_id":"themes/next/layout/_custom/sidebar.swig","hash":"adc83b19e793491b1c6ea0fd8b46cd9f32e592fc","modified":1526368421388},{"_id":"themes/next/layout/_macro/post-collapse.swig","hash":"31322a7f57936cf2dc62e824af5490da5354cf02","modified":1526368421389},{"_id":"themes/next/layout/_macro/post-copyright.swig","hash":"05e67c50a4f3a20fad879ed61b890de8ca6ba4ea","modified":1526368421389},{"_id":"themes/next/layout/_macro/post-related.swig","hash":"08fe30ce8909b920540231e36c97e28cfbce62b6","modified":1526368421389},{"_id":"themes/next/layout/_macro/post.swig","hash":"686e60ede86547bdd7bc34c3629e4c9dbd134a21","modified":1526368421390},{"_id":"themes/next/layout/_macro/reward.swig","hash":"bd5778d509c51f4b1d8da3a2bc35462929f08c75","modified":1526368421390},{"_id":"themes/next/layout/_macro/sidebar.swig","hash":"1f3121ef66a4698fd78f34bf2594ef79a407c92c","modified":1526368421390},{"_id":"themes/next/layout/_macro/wechat-subscriber.swig","hash":"a9e1346b83cf99e06bed59a53fc069279751e52a","modified":1526368421390},{"_id":"themes/next/layout/_partials/breadcrumb.swig","hash":"6994d891e064f10607bce23f6e2997db7994010e","modified":1526368421391},{"_id":"themes/next/layout/_partials/comments.swig","hash":"5df32b286a8265ba82a4ef5e1439ff34751545ad","modified":1526368421391},{"_id":"themes/next/layout/_partials/footer.swig","hash":"129c018522275592affa73817d47cedba8ab4079","modified":1526379787966},{"_id":"themes/next/layout/_partials/page-header.swig","hash":"1aaf32bed57b976c4c1913fd801be34d4838cc72","modified":1526368421393},{"_id":"themes/next/layout/_partials/pagination.swig","hash":"dbe321bcf3cf45917cc11a3e3f50d8572bac2c70","modified":1526368421394},{"_id":"themes/next/layout/_scripts/boostrap.swig","hash":"0a0129e926c27fffc6e7ef87fe370016bc7a4564","modified":1526368421396},{"_id":"themes/next/layout/_scripts/commons.swig","hash":"6fc63d5da49cb6157b8792f39c7305b55a0d1593","modified":1526368421396},{"_id":"themes/next/layout/_scripts/noscript.swig","hash":"ac3ad2c0eccdf16edaa48816d111aaf51200a54b","modified":1526368421396},{"_id":"themes/next/layout/_scripts/vendors.swig","hash":"e0bdc723d1dc858b41fd66e44e2786e6519f259f","modified":1526368421398},{"_id":"themes/next/layout/_third-party/bookmark.swig","hash":"60001c8e08b21bf3a7afaf029839e1455340e95d","modified":1526368421407},{"_id":"themes/next/layout/_third-party/github-banner.swig","hash":"cabd9640dc3027a0b3ac06f5ebce777e50754065","modified":1526368421409},{"_id":"themes/next/layout/_third-party/copy-code.swig","hash":"a8ab2035654dd06d94faf11a35750529e922d719","modified":1526368421409},{"_id":"themes/next/layout/_third-party/exturl.swig","hash":"f532ce257fca6108e84b8f35329c53f272c2ce84","modified":1526368421409},{"_id":"themes/next/layout/_third-party/needsharebutton.swig","hash":"927f19160ae14e7030df306fc7114ba777476282","modified":1526368421410},{"_id":"themes/next/layout/_third-party/pangu.swig","hash":"6b75c5fd76ae7cf0a7b04024510bd5221607eab3","modified":1526368421410},{"_id":"themes/next/layout/_third-party/rating.swig","hash":"fc93b1a7e6aed0dddb1f3910142b48d8ab61174e","modified":1526368421411},{"_id":"themes/next/layout/_third-party/schedule.swig","hash":"22369026c87fc23893c35a7f250b42f3bb1b60f1","modified":1526368421411},{"_id":"themes/next/layout/_third-party/scroll-cookie.swig","hash":"b0ca46e0d1ff4c08cb0a3a8c1994f20d0260cef9","modified":1526368421411},{"_id":"themes/next/scripts/tags/button.js","hash":"5a61c2da25970a4981fbd65f4a57c5e85db4dcda","modified":1526368421418},{"_id":"themes/next/scripts/tags/center-quote.js","hash":"db70a841e7c1708f95ca97b44413b526b267fa9b","modified":1526368421419},{"_id":"themes/next/scripts/tags/exturl.js","hash":"2b3a4dc15dea33972c0b6d46a1483dabbf06fb5b","modified":1526368421419},{"_id":"themes/next/scripts/tags/full-image.js","hash":"a98fc19a90924f2368e1982f8c449cbc09df8439","modified":1526368421420},{"_id":"themes/next/scripts/tags/group-pictures.js","hash":"1b97b1b5364945b8ab3e50813bef84273055234f","modified":1526368421420},{"_id":"themes/next/scripts/tags/include-raw.js","hash":"b7600f6b868d8f4f7032126242d9738cd1e6ad71","modified":1526368421420},{"_id":"themes/next/scripts/tags/label.js","hash":"621004f2836040b12c4e8fef77e62cf22c561297","modified":1526368421421},{"_id":"themes/next/scripts/tags/lazy-image.js","hash":"460e5e1f305847dcd4bcab9da2038a85f0a1c273","modified":1526368421421},{"_id":"themes/next/scripts/tags/note.js","hash":"4975d4433e11161b2e9a5744b7287c2d667b3c76","modified":1526368421421},{"_id":"themes/next/scripts/tags/tabs.js","hash":"5786545d51c38e8ca38d1bfc7dd9e946fc70a316","modified":1526368421421},{"_id":"themes/next/source/css/main.styl","hash":"c26ca6e7b5bd910b9046d6722c8e00be672890e0","modified":1526368421449},{"_id":"themes/next/source/images/algolia_logo.svg","hash":"ec119560b382b2624e00144ae01c137186e91621","modified":1526368421450},{"_id":"themes/next/source/images/apple-touch-icon-next.png","hash":"2959dbc97f31c80283e67104fe0854e2369e40aa","modified":1526368421450},{"_id":"themes/next/source/images/avatar.gif","hash":"264082bb3a1af70d5499c7d22b0902cb454b6d12","modified":1526368421451},{"_id":"themes/next/source/images/cc-by-nc-nd.svg","hash":"c6524ece3f8039a5f612feaf865d21ec8a794564","modified":1526368421451},{"_id":"themes/next/source/images/cc-by-nc-sa.svg","hash":"3031be41e8753c70508aa88e84ed8f4f653f157e","modified":1526368421452},{"_id":"themes/next/source/images/cc-by-nc.svg","hash":"8d39b39d88f8501c0d27f8df9aae47136ebc59b7","modified":1526368421452},{"_id":"themes/next/source/images/cc-by-nd.svg","hash":"c563508ce9ced1e66948024ba1153400ac0e0621","modified":1526368421452},{"_id":"themes/next/source/images/cc-by-sa.svg","hash":"aa4742d733c8af8d38d4c183b8adbdcab045872e","modified":1526368421453},{"_id":"themes/next/source/images/cc-by.svg","hash":"28a0a4fe355a974a5e42f68031652b76798d4f7e","modified":1526368421453},{"_id":"themes/next/source/images/cc-zero.svg","hash":"87669bf8ac268a91d027a0a4802c92a1473e9030","modified":1526368421453},{"_id":"themes/next/source/images/favicon-16x16-next.png","hash":"943a0d67a9cdf8c198109b28f9dbd42f761d11c3","modified":1526368421453},{"_id":"themes/next/source/images/favicon-32x32-next.png","hash":"0749d7b24b0d2fae1c8eb7f671ad4646ee1894b1","modified":1526368421454},{"_id":"themes/next/source/images/favicon.ico","hash":"6ed407cb30e21a406a45d0076a4a3226d2633bf0","modified":1503249010000},{"_id":"themes/next/source/images/loading.gif","hash":"5fbd472222feb8a22cf5b8aa5dc5b8e13af88e2b","modified":1526368421454},{"_id":"themes/next/source/images/logo.svg","hash":"d29cacbae1bdc4bbccb542107ee0524fe55ad6de","modified":1526368421454},{"_id":"themes/next/source/images/placeholder.gif","hash":"5fbd472222feb8a22cf5b8aa5dc5b8e13af88e2b","modified":1526368421454},{"_id":"themes/next/source/images/quote-l.svg","hash":"94e870b4c8c48da61d09522196d4dd40e277a98f","modified":1526368421454},{"_id":"themes/next/source/images/quote-r.svg","hash":"e60ae504f9d99b712c793c3740c6b100d057d4ec","modified":1526368421455},{"_id":"themes/next/source/images/searchicon.png","hash":"67727a6a969be0b2659b908518fa6706eed307b8","modified":1526368421455},{"_id":"source/_posts/oracle/Oracle SQL基础知识.md","hash":"6bc1bff15467512b1163eadda15ae0d1f1f101f3","modified":1503020675000},{"_id":"source/images/kettle/Kettle插件架构001.jpg","hash":"142c232a45620ca45831388a094cacd8137c0671","modified":1503249010000},{"_id":"source/images/avatar.jpg","hash":"e02f499056e9d6fde4dd9f4cd468a652359d6fd7","modified":1503249010000},{"_id":"source/images/kettle/开源ETL工具-kettle_Jobs（工作）3.png","hash":"8fbfeefaa2346dbcff8a735077c003b3731d720e","modified":1503249010000},{"_id":"source/images/kettle/开源ETL工具-kettle_Jobs（工作）7.png","hash":"fd96587bd686fa59031e9d41b69c0d917f061b3a","modified":1503249010000},{"_id":"source/images/kettle/开源ETL工具-kettle_Jobs（工作）8.png","hash":"50c05f7fa4fa0e9bf0295e784c57e6c53bf2f660","modified":1503249010000},{"_id":"source/images/kettle/开源ETL工具-kettle_Kettle构成.jpg","hash":"0f34cfda2c040aaf3617352325d8c2776454cd43","modified":1503249010000},{"_id":"themes/next/layout/_scripts/schemes/mist.swig","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1526368421397},{"_id":"themes/next/layout/_scripts/schemes/muse.swig","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1526368421397},{"_id":"themes/next/source/css/_mixins/Mist.styl","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1526368421441},{"_id":"themes/next/source/css/_mixins/Muse.styl","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1526368421442},{"_id":"themes/next/source/css/_mixins/custom.styl","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1526368421442},{"_id":"themes/next/source/css/_variables/Muse.styl","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1526368421449},{"_id":"themes/next/source/css/_variables/custom.styl","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1526368421449},{"_id":"source/_posts/tools/emacs/TODO-emacs入门.md","hash":"ef244d28f6aea7dcea09b25633b5301319a0e358","modified":1503020675000},{"_id":"source/_posts/tools/vim/TODO-vim入门实践.md","hash":"007bb54b8746f7c5a7d2d8454d36ee022b142e2b","modified":1503020675000},{"_id":"source/_posts/前端/angular.js/TODO-angular.js入门之HelloWorld.md","hash":"8a47edcf4d77a4668b0dca29a3cf24d7771aa543","modified":1503020675000},{"_id":"source/_posts/前端/react.js/TODO-react.js入门之HelloWorld.md","hash":"7cf7728c9c84b5b1b9ba4a696694a14eff42d6f5","modified":1503020675000},{"_id":"source/_posts/前端/vue.js/TODO-vue.js入门之HelloWorld.md","hash":"6ecc7fc6f31b1e30d5b6535d9a7efe7e6e6ece6a","modified":1503020675000},{"_id":"source/images/kettle/开源ETL工具-kettle_Jobs（工作）.png","hash":"420f30fb3d264b426b0d466106384dbabb0dc8df","modified":1503249010000},{"_id":"themes/next/.git/objects/b5/6cd0c08b7063577b87f2c6bc17b621d693aa70","hash":"3cc0963a79f6c296eb6ca9df8e37423c51a14e00","modified":1526382648446},{"_id":"themes/next/.git/refs/heads/master","hash":"6b4527a1b6f98c1f6bbf8cba4c00acd150c54dbf","modified":1526368421355},{"_id":"themes/next/layout/_macro/menu/menu-badge.swig","hash":"65c5e585982dae7ae1542cada71858b4ea1f73d6","modified":1526368421389},{"_id":"themes/next/layout/_macro/menu/menu-item.swig","hash":"d1b73c926109145e52605929b75914cc8b60fb89","modified":1526368421389},{"_id":"themes/next/layout/_partials/head/external-fonts.swig","hash":"7ce76358411184482bb0934e70037949dd0da8ca","modified":1526368421392},{"_id":"themes/next/layout/_partials/head/head-unique.swig","hash":"a7e376b087ae77f2e2a61ba6af81cde5af693174","modified":1526368421392},{"_id":"themes/next/layout/_partials/header/brand.swig","hash":"fd780171713aada5eb4f4ffed8e714617c8ae6be","modified":1526368421393},{"_id":"themes/next/layout/_partials/head/head.swig","hash":"00bf33b3c557b8f7e9faf49b226ea6ff7df5cda0","modified":1526368421392},{"_id":"themes/next/layout/_partials/header/index.swig","hash":"2082f5077551123e695e8afec471c9c44b436acb","modified":1526368421393},{"_id":"themes/next/layout/_partials/header/menu.swig","hash":"3db735d0cd2d449edf2674310ac1e7c0043cb357","modified":1526368421393},{"_id":"themes/next/layout/_partials/header/sub-menu.swig","hash":"88b4b6051592d26bff59788acb76346ce4e398c2","modified":1526368421393},{"_id":"themes/next/layout/_partials/search/index.swig","hash":"a33b29ccbdc2248aedff23b04e0627f435824406","modified":1526368421394},{"_id":"themes/next/layout/_partials/search/localsearch.swig","hash":"957701729b85fb0c5bfcf2fb99c19d54582f91ed","modified":1526368421394},{"_id":"themes/next/layout/_partials/search/swiftype.swig","hash":"959b7e04a96a5596056e4009b73b6489c117597e","modified":1526368421394},{"_id":"themes/next/layout/_partials/search/tinysou.swig","hash":"eefe2388ff3d424694045eda21346989b123977c","modified":1526368421394},{"_id":"themes/next/layout/_partials/share/add-this.swig","hash":"23e23dc0f76ef3c631f24c65277adf7ea517b383","modified":1526368421395},{"_id":"themes/next/layout/_partials/share/baidushare.swig","hash":"1f1107468aaf03f7d0dcd7eb2b653e2813a675b4","modified":1526368421395},{"_id":"themes/next/layout/_partials/share/duoshuo_share.swig","hash":"89c5a5240ecb223acfe1d12377df5562a943fd5d","modified":1526368421395},{"_id":"themes/next/layout/_partials/share/jiathis.swig","hash":"048fd5e98149469f8c28c21ba3561a7a67952c9b","modified":1526368421396},{"_id":"themes/next/layout/_scripts/pages/post-details.swig","hash":"cc865af4a3cb6d25a0be171b7fc919ade306bb50","modified":1526368421397},{"_id":"themes/next/layout/_scripts/schemes/gemini.swig","hash":"ea03fe9c98ddcfcc0ecfdbe5a2b622f9cde3b3a1","modified":1526368421397},{"_id":"themes/next/layout/_scripts/schemes/pisces.swig","hash":"ea03fe9c98ddcfcc0ecfdbe5a2b622f9cde3b3a1","modified":1526368421397},{"_id":"themes/next/layout/_third-party/comments/changyan.swig","hash":"0e3378f7c39b2b0f69638290873ede6b6b6825c0","modified":1526368421407},{"_id":"themes/next/layout/_third-party/comments/disqus.swig","hash":"8878241797f8494a70968756c57cacdfc77b61c7","modified":1526368421407},{"_id":"themes/next/layout/_third-party/comments/hypercomments.swig","hash":"17a54796f6e03fc834880a58efca45c286e40e40","modified":1526368421408},{"_id":"themes/next/layout/_third-party/comments/gitment.swig","hash":"fe8177e4698df764e470354b6acde8292a3515e0","modified":1526368421407},{"_id":"themes/next/layout/_third-party/comments/index.swig","hash":"40e3cacbd5fa5f2948d0179eff6dd88053e8648e","modified":1526368421408},{"_id":"themes/next/layout/_third-party/comments/livere.swig","hash":"6f340d122a9816ccdf4b45b662880a4b2d087671","modified":1526368421408},{"_id":"themes/next/layout/_third-party/comments/valine.swig","hash":"c0eb6123464d745ac5324ce6deac8ded601f432f","modified":1526368421408},{"_id":"themes/next/layout/_third-party/comments/youyan.swig","hash":"42f62695029834d45934705c619035733762309e","modified":1526368421409},{"_id":"themes/next/layout/_third-party/analytics/analytics-with-widget.swig","hash":"98df9d72e37dd071e882f2d5623c9d817815b139","modified":1526368421399},{"_id":"themes/next/layout/_third-party/analytics/application-insights.swig","hash":"60426bf73f8a89ba61fb1be2df3ad5398e32c4ef","modified":1526368421399},{"_id":"themes/next/layout/_third-party/analytics/baidu-analytics.swig","hash":"deda6a814ed48debc694c4e0c466f06c127163d0","modified":1526368421399},{"_id":"themes/next/layout/_third-party/analytics/busuanzi-counter.swig","hash":"67f0cb55e6702c492e99a9f697827629da036a0c","modified":1526368421399},{"_id":"themes/next/layout/_third-party/analytics/cnzz-analytics.swig","hash":"8160b27bee0aa372c7dc7c8476c05bae57f58d0f","modified":1526368421400},{"_id":"themes/next/layout/_third-party/analytics/facebook-sdk.swig","hash":"a234c5cd1f75ca5731e814d0dbb92fdcf9240d1b","modified":1526368421400},{"_id":"themes/next/layout/_third-party/analytics/firestore.swig","hash":"94b26dfbcd1cf2eb87dd9752d58213338926af27","modified":1526368421400},{"_id":"themes/next/layout/_third-party/analytics/google-analytics.swig","hash":"beb53371c035b62e1a2c7bb76c63afbb595fe6e5","modified":1526368421401},{"_id":"themes/next/layout/_third-party/analytics/index.swig","hash":"5e9bb24c750b49513d9a65799e832f07410002ac","modified":1526368421402},{"_id":"themes/next/layout/_third-party/analytics/lean-analytics.swig","hash":"cee047575ae324398025423696b760db64d04e6f","modified":1526368421403},{"_id":"themes/next/layout/_third-party/analytics/tencent-analytics.swig","hash":"3658414379e0e8a34c45c40feadc3edc8dc55f88","modified":1526368421404},{"_id":"themes/next/layout/_third-party/analytics/tencent-mta.swig","hash":"0ddc94ed4ba0c19627765fdf1abc4d8efbe53d5a","modified":1526368421406},{"_id":"themes/next/layout/_third-party/analytics/vkontakte-api.swig","hash":"c3971fd154d781088e1cc665035f8561a4098f4c","modified":1526368421406},{"_id":"themes/next/layout/_third-party/math/index.swig","hash":"a6fc00ec7f5642aabd66aa1cf51c6acc5b10e012","modified":1526368421410},{"_id":"themes/next/layout/_third-party/math/katex.swig","hash":"97dbc2035bcb5aa7eafb80a4202dc827cce34983","modified":1526368421410},{"_id":"themes/next/layout/_third-party/math/mathjax.swig","hash":"9b9ff4cc6d5474ab03f09835a2be80e0dba9fe89","modified":1526368421410},{"_id":"themes/next/layout/_third-party/search/index.swig","hash":"c747fb5c6b1f500e8f0c583e44195878b66e4e29","modified":1526368421412},{"_id":"themes/next/layout/_third-party/search/localsearch.swig","hash":"b15e10abe85b4270860a56c970b559baa258b2a8","modified":1526368421412},{"_id":"themes/next/layout/_third-party/search/tinysou.swig","hash":"cb3a5d36dbe1630bab84e03a52733a46df7c219b","modified":1526368421413},{"_id":"themes/next/layout/_third-party/seo/baidu-push.swig","hash":"c057b17f79e8261680fbae8dc4e81317a127c799","modified":1526368421413},{"_id":"themes/next/source/css/_custom/custom.styl","hash":"328d9a9696cc2ccf59c67d3c26000d569f46344c","modified":1526368421441},{"_id":"themes/next/source/css/_mixins/base.styl","hash":"81ca13d6d0beff8b1a4b542a51e3b0fb68f08efd","modified":1526368421442},{"_id":"themes/next/source/css/_mixins/Gemini.styl","hash":"2aa5b7166a85a8aa34b17792ae4f58a5a96df6cc","modified":1526368421441},{"_id":"themes/next/source/css/_mixins/Pisces.styl","hash":"2640a54fa63bdd4c547eab7ce2fc1192cf0ccec8","modified":1526368421442},{"_id":"themes/next/source/css/_variables/Mist.styl","hash":"be087dcc060e8179f7e7f60ab4feb65817bd3d9f","modified":1526368421448},{"_id":"themes/next/source/css/_variables/base.styl","hash":"cfb03ec629f13883509eac66e561e9dba562333f","modified":1526368421449},{"_id":"themes/next/source/css/_variables/Pisces.styl","hash":"32392d213f5d05bc26b2dc452f2fc6fea9d44f6d","modified":1526368421449},{"_id":"themes/next/source/css/_variables/Gemini.styl","hash":"7a2706304465b9e673d5561b715e7c72a238437c","modified":1526368421448},{"_id":"themes/next/source/lib/font-awesome/HELP-US-OUT.txt","hash":"4f7bf961f1bed448f6ba99aeb9219fabf930ba96","modified":1526368421459},{"_id":"themes/next/source/lib/font-awesome/bower.json","hash":"279a8a718ab6c930a67c41237f0aac166c1b9440","modified":1526368421460},{"_id":"themes/next/source/lib/font-awesome/.bower.json","hash":"a2aaaf12378db56bd10596ba3daae30950eac051","modified":1526368421459},{"_id":"themes/next/source/lib/font-awesome/.gitignore","hash":"69d152fa46b517141ec3b1114dd6134724494d83","modified":1526368421459},{"_id":"themes/next/source/lib/font-awesome/.npmignore","hash":"dcf470ab3a358103bb896a539cc03caeda10fa8b","modified":1526368421459},{"_id":"themes/next/source/lib/velocity/velocity.min.js","hash":"2f1afadc12e4cf59ef3b405308d21baa97e739c6","modified":1526368421469},{"_id":"themes/next/source/lib/velocity/velocity.ui.js","hash":"6a1d101eab3de87527bb54fcc8c7b36b79d8f0df","modified":1526368421469},{"_id":"themes/next/source/lib/velocity/velocity.ui.min.js","hash":"ed5e534cd680a25d8d14429af824f38a2c7d9908","modified":1526368421470},{"_id":"themes/next/source/js/src/affix.js","hash":"978e0422b5bf1b560236d8d10ebc1adcf66392e3","modified":1526368421455},{"_id":"themes/next/source/js/src/algolia-search.js","hash":"b172f697ed339a24b1e80261075232978d164c35","modified":1526368421455},{"_id":"themes/next/source/js/src/bootstrap.js","hash":"40de94fd18fcbd67a327d63b0d1e242a08aa5404","modified":1526368421456},{"_id":"themes/next/source/js/src/exturl.js","hash":"e42e2aaab7bf4c19a0c8e779140e079c6aa5c0b1","modified":1526368421456},{"_id":"themes/next/source/js/src/js.cookie.js","hash":"9b37973a90fd50e71ea91682265715e45ae82c75","modified":1526368421456},{"_id":"themes/next/source/js/src/motion.js","hash":"50e57f8acb6924c6999cdcc664ddd3f0730d2061","modified":1526368421456},{"_id":"themes/next/source/js/src/post-details.js","hash":"d1333fb588d4521b4d1e9c69aef06e0ad1bf0b12","modified":1526368421457},{"_id":"themes/next/source/js/src/scroll-cookie.js","hash":"09dc828cbf5f31158ff6250d2bf7c3cde6365c67","modified":1526368421457},{"_id":"themes/next/source/js/src/scrollspy.js","hash":"fe4da1b9fe73518226446f5f27d2831e4426fc35","modified":1526368421458},{"_id":"themes/next/source/js/src/utils.js","hash":"4284c67ea1435de2acd523f6d48c0d073fd1ad03","modified":1526368421458},{"_id":"themes/next/.git/objects/pack/pack-dc27a68dafb9ad03581d6f941c3feaf9f927bfc3.idx","hash":"ed0d055ffa2a43d71fdde6be4108cd88f5014c57","modified":1526368421336},{"_id":"themes/next/source/images/avatar.jpg","hash":"e02f499056e9d6fde4dd9f4cd468a652359d6fd7","modified":1503249010000},{"_id":"themes/next/source/lib/jquery/index.js","hash":"41b4bfbaa96be6d1440db6e78004ade1c134e276","modified":1526368421466},{"_id":"themes/next/.git/logs/refs/heads/master","hash":"6a8952dd2db8ea48c2f7f63d92aa247fad2992fb","modified":1526368421355},{"_id":"themes/next/.git/refs/remotes/origin/HEAD","hash":"d9427cda09aba1cdde5c69c2b13c905bddb0bc51","modified":1526368421354},{"_id":"themes/next/layout/_third-party/search/algolia-search/assets.swig","hash":"6958a97fde63e03983ec2394a4f8e408860fb42b","modified":1526368421412},{"_id":"themes/next/layout/_third-party/search/algolia-search/dom.swig","hash":"ba698f49dd3a868c95b240d802f5b1b24ff287e4","modified":1526368421412},{"_id":"themes/next/source/css/_common/components/back-to-top.styl","hash":"31050fc7a25784805b4843550151c93bfa55c9c8","modified":1526368421422},{"_id":"themes/next/source/css/_common/components/buttons.styl","hash":"7e509c7c28c59f905b847304dd3d14d94b6f3b8e","modified":1526368421422},{"_id":"themes/next/source/css/_common/components/comments.styl","hash":"471f1627891aca5c0e1973e09fbcb01e1510d193","modified":1526368421422},{"_id":"themes/next/source/css/_common/components/components.styl","hash":"a6bb5256be6195e76addbda12f4ed7c662d65e7a","modified":1526368421423},{"_id":"themes/next/source/css/_common/components/back-to-top-sidebar.styl","hash":"4719ce717962663c5c33ef97b1119a0b3a4ecdc3","modified":1526368421422},{"_id":"themes/next/source/css/_common/components/pagination.styl","hash":"c5d48863f332ff8ce7c88dec2c893f709d7331d3","modified":1526368421427},{"_id":"themes/next/source/css/_common/components/tag-cloud.styl","hash":"dd8a3b22fc2f222ac6e6c05bd8a773fb039169c0","modified":1526368421435},{"_id":"themes/next/source/css/_common/outline/outline.styl","hash":"2186be20e317505cd31886f1291429cc21f76703","modified":1526368421439},{"_id":"themes/next/source/css/_common/scaffolding/base.styl","hash":"18309b68ff33163a6f76a39437e618bb6ed411f8","modified":1526368421440},{"_id":"themes/next/source/css/_common/scaffolding/helpers.styl","hash":"9c25c75311e1bd4d68df031d3f2ae6d141a90766","modified":1526368421440},{"_id":"themes/next/source/css/_common/scaffolding/normalize.styl","hash":"ece571f38180febaf02ace8187ead8318a300ea7","modified":1526368421441},{"_id":"themes/next/source/css/_common/scaffolding/scaffolding.styl","hash":"a280a583b7615e939aaddbf778f5c108ef8a2a6c","modified":1526368421441},{"_id":"themes/next/source/css/_common/scaffolding/tables.styl","hash":"0810e7c43d6c8adc8434a8fa66eabe0436ab8178","modified":1526368421441},{"_id":"themes/next/source/css/_common/scaffolding/mobile.styl","hash":"47a46583a1f3731157a3f53f80ed1ed5e2753e8e","modified":1526368421440},{"_id":"themes/next/source/css/_schemes/Mist/_base.styl","hash":"0bef9f0dc134215bc4d0984ba3a16a1a0b6f87ec","modified":1526368421443},{"_id":"themes/next/source/css/_schemes/Mist/_header.styl","hash":"5ae7906dc7c1d9468c7f4b4a6feddddc555797a1","modified":1526368421443},{"_id":"themes/next/source/css/_schemes/Mist/_logo.styl","hash":"38e5df90c8689a71c978fd83ba74af3d4e4e5386","modified":1526368421443},{"_id":"themes/next/source/css/_schemes/Mist/_menu.styl","hash":"f43c821ea272f80703862260b140932fe4aa0e1f","modified":1526368421443},{"_id":"themes/next/source/css/_schemes/Gemini/index.styl","hash":"f362fbc791dafb378807cabbc58abf03e097af6d","modified":1526368421442},{"_id":"themes/next/source/css/_schemes/Mist/_posts-expanded.styl","hash":"2212511ae14258d93bec57993c0385e5ffbb382b","modified":1526368421443},{"_id":"themes/next/source/css/_schemes/Mist/_search.styl","hash":"1452cbe674cc1d008e1e9640eb4283841058fc64","modified":1526368421444},{"_id":"themes/next/source/css/_schemes/Mist/index.styl","hash":"5e12572b18846250e016a872a738026478ceef37","modified":1526368421444},{"_id":"themes/next/source/css/_schemes/Muse/_logo.styl","hash":"8829bc556ca38bfec4add4f15a2f028092ac6d46","modified":1526368421445},{"_id":"themes/next/source/css/_schemes/Muse/_menu.styl","hash":"35f093fe4c1861661ac1542d6e8ea5a9bbfeb659","modified":1526368421445},{"_id":"themes/next/source/css/_schemes/Muse/_layout.styl","hash":"0efa036a15c18f5abb058b7c0fad1dd9ac5eed4c","modified":1526368421445},{"_id":"themes/next/source/css/_schemes/Muse/_search.styl","hash":"1452cbe674cc1d008e1e9640eb4283841058fc64","modified":1526368421446},{"_id":"themes/next/source/css/_schemes/Pisces/_brand.styl","hash":"c4ed249798296f60bda02351fe6404fb3ef2126f","modified":1526368421447},{"_id":"themes/next/source/css/_schemes/Muse/index.styl","hash":"d5e8ea6336bc2e237d501ed0d5bbcbbfe296c832","modified":1526368421446},{"_id":"themes/next/source/css/_schemes/Pisces/_menu.styl","hash":"05a5abf02e84ba8f639b6f9533418359f0ae4ecb","modified":1526368421447},{"_id":"themes/next/source/css/_schemes/Pisces/_layout.styl","hash":"ba1842dbeb97e46c6c4d2ae0e7a2ca6d610ada67","modified":1526368421447},{"_id":"themes/next/source/css/_schemes/Pisces/_posts.styl","hash":"2f878213cb24c5ddc18877f6d15ec5c5f57745ac","modified":1526368421447},{"_id":"themes/next/source/css/_schemes/Pisces/index.styl","hash":"5779cc8086b1cfde9bc4f1afdd85223bdc45f0a0","modified":1526368421448},{"_id":"themes/next/source/css/_schemes/Pisces/_sidebar.styl","hash":"41f9cdafa00e256561c50ae0b97ab7fcd7c1d6a2","modified":1526368421448},{"_id":"themes/next/source/css/_schemes/Pisces/_sub-menu.styl","hash":"ffa870c3fa37a48b01dc6f967e66f5df508d02bf","modified":1526368421448},{"_id":"themes/next/source/lib/font-awesome/css/font-awesome.css","hash":"0140952c64e3f2b74ef64e050f2fe86eab6624c8","modified":1526368421460},{"_id":"themes/next/source/lib/font-awesome/css/font-awesome.css.map","hash":"0189d278706509412bac4745f96c83984e1d59f4","modified":1526368421461},{"_id":"themes/next/source/lib/font-awesome/css/font-awesome.min.css","hash":"512c7d79033e3028a9be61b540cf1a6870c896f8","modified":1526368421461},{"_id":"themes/next/source/lib/ua-parser-js/dist/ua-parser.min.js","hash":"38628e75e4412cc6f11074e03e1c6d257aae495b","modified":1526368421466},{"_id":"themes/next/source/lib/ua-parser-js/dist/ua-parser.pack.js","hash":"214dad442a92d36af77ed0ca1d9092b16687f02f","modified":1526368421467},{"_id":"themes/next/source/js/src/schemes/pisces.js","hash":"8050a5b2683d1d77238c5762b6bd89c543daed6e","modified":1526368421457},{"_id":"source/images/hadoop/kylin/Kylin的技术架构.jpg","hash":"bb0ae5d716d913bcbffa15fd1b8cc14307a1aa36","modified":1503249010000},{"_id":"source/images/hadoop/kylin/一个四维Cube的例子.jpg","hash":"69401999f9322bbe99d7537ea438a431a479118c","modified":1503249010000},{"_id":"themes/next/.git/objects/07/14eaa6e0ac8607fd134c6b1e93c16c7ede8225","hash":"342cb43ba1c85481a52daf397c5adfb1460f7ad8","modified":1526381654522},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.woff","hash":"28b782240b3e76db824e12c02754a9731a167527","modified":1526368421464},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.woff2","hash":"d6f48cba7d076fb6f2fd6ba993a75b9dc1ecbf0c","modified":1526368421465},{"_id":"themes/next/source/lib/velocity/velocity.js","hash":"9f08181baea0cc0e906703b7e5df9111b9ef3373","modified":1526368421468},{"_id":"themes/next/.git/logs/refs/remotes/origin/HEAD","hash":"6a8952dd2db8ea48c2f7f63d92aa247fad2992fb","modified":1526368421354},{"_id":"themes/next/source/css/_common/components/footer/footer.styl","hash":"39dee82d481dd9d44e33658960ec63e47cd0a715","modified":1526368421423},{"_id":"themes/next/source/css/_common/components/header/header.styl","hash":"7cc3f36222494c9a1325c5347d7eb9ae53755a32","modified":1526368421423},{"_id":"themes/next/source/css/_common/components/header/headerband.styl","hash":"d27448f199fc2f9980b601bc22b87f08b5d64dd1","modified":1526368421424},{"_id":"themes/next/source/css/_common/components/header/menu.styl","hash":"8a2421cb9005352905fae9d41a847ae56957247e","modified":1526368421424},{"_id":"themes/next/source/css/_common/components/header/site-meta.styl","hash":"6c00f6e0978f4d8f9a846a15579963728aaa6a17","modified":1526368421424},{"_id":"themes/next/source/css/_common/components/header/site-nav.styl","hash":"49c2b2c14a1e7fcc810c6be4b632975d0204c281","modified":1526368421424},{"_id":"themes/next/source/css/_common/components/highlight/diff.styl","hash":"96f32ea6c3265a3889e6abe57587f6e2a2a40dfb","modified":1526368421425},{"_id":"themes/next/source/css/_common/components/highlight/highlight.styl","hash":"17b95828f9db7f131ec0361a8c0e89b0b5c9bff5","modified":1526368421425},{"_id":"themes/next/source/css/_common/components/highlight/theme.styl","hash":"b76387934fb6bb75212b23c1a194486892cc495e","modified":1526368421425},{"_id":"themes/next/source/css/_common/components/pages/archive.styl","hash":"f5aa2ba3bfffc15475e7e72a55b5c9d18609fdf5","modified":1526368421425},{"_id":"themes/next/source/css/_common/components/pages/breadcrumb.styl","hash":"7dd9a0378ccff3e4a2003f486b1a34e74c20dac6","modified":1526368421425},{"_id":"themes/next/source/css/_common/components/pages/categories.styl","hash":"4eff5b252d7b614e500fc7d52c97ce325e57d3ab","modified":1526368421426},{"_id":"themes/next/source/css/_common/components/pages/pages.styl","hash":"fb451dc4cc0355b57849c27d3eb110c73562f794","modified":1526368421426},{"_id":"themes/next/source/css/_common/components/pages/post-detail.styl","hash":"9bf4362a4d0ae151ada84b219d39fbe5bb8c790e","modified":1526368421426},{"_id":"themes/next/source/css/_common/components/pages/schedule.styl","hash":"a82afbb72d83ee394aedc7b37ac0008a9823b4f4","modified":1526368421427},{"_id":"themes/next/source/css/_common/components/post/post-button.styl","hash":"e72a89e0f421444453e149ba32c77a64bd8e44e8","modified":1526368421427},{"_id":"themes/next/source/css/_common/components/post/post-collapse.styl","hash":"0f7f522cc6bfb3401d5afd62b0fcdf48bb2d604b","modified":1526368421428},{"_id":"themes/next/source/css/_common/components/post/post-eof.styl","hash":"2cdc094ecf907a02fce25ad4a607cd5c40da0f2b","modified":1526368421428},{"_id":"themes/next/source/css/_common/components/post/post-expand.styl","hash":"ca89b167d368eac50a4f808fa53ba67e69cbef94","modified":1526368421428},{"_id":"themes/next/source/css/_common/components/post/post-gallery.styl","hash":"387ce23bba52b22a586b2dfb4ec618fe1ffd3926","modified":1526368421429},{"_id":"themes/next/source/css/_common/components/post/post-meta.styl","hash":"417f05ff12a2aaca6ceeac8b7e7eb26e9440c4c3","modified":1526368421429},{"_id":"themes/next/source/css/_common/components/post/post-nav.styl","hash":"a5d8617a24d7cb6c5ad91ea621183ca2c0917331","modified":1526368421429},{"_id":"themes/next/source/css/_common/components/post/post-copyright.styl","hash":"f54367c0feda6986c030cc4d15a0ca6ceea14bcb","modified":1526368421428},{"_id":"themes/next/source/css/_common/components/header/github-banner.styl","hash":"ee37e6c465b9b2a7e39175fccfcbed14f2db039b","modified":1526368421423},{"_id":"themes/next/source/css/_common/components/post/post-reading_progress.styl","hash":"f4e9f870baa56eae423a123062f00e24cc780be1","modified":1526368421430},{"_id":"themes/next/source/css/_common/components/post/post-rtl.styl","hash":"017074ef58166e2d69c53bb7590a0e7a8947a1ed","modified":1526368421430},{"_id":"themes/next/source/css/_common/components/post/post-reward.styl","hash":"36332c8a91f089f545f3c3e8ea90d08aa4d6e60c","modified":1526368421430},{"_id":"themes/next/source/css/_common/components/post/post-tags.styl","hash":"a352ae5b1f8857393bf770d2e638bf15f0c9585d","modified":1526368421430},{"_id":"themes/next/source/css/_common/components/post/post-title.styl","hash":"c0ac49fadd33ca4a9a0a04d5ff2ac6560d0ecd9e","modified":1526368421430},{"_id":"themes/next/source/css/_common/components/post/post-type.styl","hash":"10251257aceecb117233c9554dcf8ecfef8e2104","modified":1526368421431},{"_id":"themes/next/source/css/_common/components/post/post.styl","hash":"8bf095377d28881f63a30bd7db97526829103bf2","modified":1526368421431},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar-author-links.styl","hash":"35c0350096921dd8e2222ec41b6c17a4ea6b44f2","modified":1526368421432},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar-author.styl","hash":"bbe0d111f6451fc04e52719fd538bd0753ec17f9","modified":1526368421432},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar-blogroll.styl","hash":"89dd4f8b1f1cce3ad46cf2256038472712387d02","modified":1526368421432},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar-feed-link.styl","hash":"9486ddd2cb255227db102d09a7df4cae0fabad72","modified":1526368421433},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar-toc.styl","hash":"4427ed3250483ed5b7baad74fa93474bd1eda729","modified":1526368421434},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar-nav.styl","hash":"45fa7193435a8eae9960267438750b4c9fa9587f","modified":1526368421433},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar-toggle.styl","hash":"f7784aba0c1cd20d824c918c120012d57a5eaa2a","modified":1526368421434},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar.styl","hash":"43bc58daa8d35d5d515dc787ceb21dd77633fe49","modified":1526368421434},{"_id":"themes/next/source/css/_common/components/sidebar/site-state.styl","hash":"3623e7fa4324ec1307370f33d8f287a9e20a5578","modified":1526368421435},{"_id":"themes/next/source/css/_common/components/tags/blockquote-center.styl","hash":"c2abe4d87148e23e15d49ee225bc650de60baf46","modified":1526368421435},{"_id":"themes/next/source/css/_common/components/tags/full-image.styl","hash":"5d15cc8bbefe44c77a9b9f96bf04a6033a4b35b8","modified":1526368421436},{"_id":"themes/next/source/css/_common/components/tags/group-pictures.styl","hash":"4851b981020c5cbc354a1af9b831a2dcb3cf9d39","modified":1526368421436},{"_id":"themes/next/source/css/_common/components/tags/label.styl","hash":"4a457d265d62f287c63d48764ce45d9bcfc9ec5a","modified":1526368421436},{"_id":"themes/next/source/css/_common/components/post/post-widgets.styl","hash":"e4055a0d2cd2b0ad9dc55928e2f3e7bd4e499da3","modified":1526368421431},{"_id":"themes/next/source/css/_common/components/tags/note-modern.styl","hash":"ee7528900578ef4753effe05b346381c40de5499","modified":1526368421436},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar-dimmer.styl","hash":"efa5e5022e205b52786ce495d4879f5e7b8f84b2","modified":1526368421433},{"_id":"themes/next/source/css/_common/components/tags/exturl.styl","hash":"1b3cc9f4e5a7f6e05b4100e9990b37b20d4a2005","modified":1526368421435},{"_id":"themes/next/source/css/_common/components/tags/tabs.styl","hash":"4ab5deed8c3b0c338212380f678f8382672e1bcb","modified":1526368421437},{"_id":"themes/next/source/css/_common/components/tags/note.styl","hash":"32c9156bea5bac9e9ad0b4c08ffbca8b3d9aac4b","modified":1526368421437},{"_id":"themes/next/source/css/_common/components/tags/tags.styl","hash":"ead0d0f2321dc71505788c7f689f92257cf14947","modified":1526368421437},{"_id":"themes/next/source/css/_common/components/third-party/busuanzi-counter.styl","hash":"d4e6d8d7b34dc69994593c208f875ae8f7e8a3ae","modified":1526368421438},{"_id":"themes/next/source/css/_common/components/third-party/han.styl","hash":"cce6772e2cdb4db85d35486ae4c6c59367fbdd40","modified":1526368421438},{"_id":"themes/next/source/css/_common/components/third-party/jiathis.styl","hash":"327b5f63d55ec26f7663185c1a778440588d9803","modified":1526368421438},{"_id":"themes/next/source/css/_common/components/third-party/localsearch.styl","hash":"d89c4b562b528e4746696b2ad8935764d133bdae","modified":1526368421439},{"_id":"themes/next/source/css/_common/components/third-party/baidushare.styl","hash":"93b08815c4d17e2b96fef8530ec1f1064dede6ef","modified":1526368421438},{"_id":"themes/next/source/css/_common/components/third-party/gitment.styl","hash":"34935b40237c074be5f5e8818c14ccfd802b7439","modified":1526368421438},{"_id":"themes/next/source/css/_common/components/third-party/needsharebutton.styl","hash":"a5e3e6b4b4b814a9fe40b34d784fed67d6d977fa","modified":1526368421439},{"_id":"themes/next/source/css/_common/components/third-party/third-party.styl","hash":"1c06be422bc41fd35e5c7948cdea2c09961207f6","modified":1526368421439},{"_id":"themes/next/source/css/_common/components/third-party/algolia-search.styl","hash":"10e9bb3392826a5a8f4cabfc14c6d81645f33fe6","modified":1526368421437},{"_id":"themes/next/source/css/_common/components/third-party/related-posts.styl","hash":"76937db9702053d772f6758d9cea4088c2a6e2a3","modified":1526368421439},{"_id":"themes/next/source/css/_schemes/Mist/outline/outline.styl","hash":"5dc4859c66305f871e56cba78f64bfe3bf1b5f01","modified":1526368421444},{"_id":"themes/next/source/css/_schemes/Mist/sidebar/sidebar-blogroll.styl","hash":"817587e46df49e819858c8ecbafa08b53d5ff040","modified":1526368421445},{"_id":"themes/next/source/css/_schemes/Muse/sidebar/sidebar-blogroll.styl","hash":"817587e46df49e819858c8ecbafa08b53d5ff040","modified":1526368421446},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.eot","hash":"d980c2ce873dc43af460d4d572d441304499f400","modified":1526368421463},{"_id":"source/images/hadoop/kylin/维度和度量的例子.jpg","hash":"6c6dfb29f569da77258ef8247073f63383214f47","modified":1503249010000},{"_id":"themes/next/.git/objects/pack/pack-dc27a68dafb9ad03581d6f941c3feaf9f927bfc3.pack","hash":"79dcd2540e094616455afc031dc97ec0747dfaf1","modified":1526368421335},{"_id":"public/README.html","hash":"6787fd4101aa71ef1496b63f421a2210ff2e88ae","modified":1526390828926},{"_id":"public/baidu_verify_iea4FlOhCY.html","hash":"9085f3259c17bd53cb7be18a8c62502bf03b080d","modified":1526390828926},{"_id":"public/archives/2016/09/index.html","hash":"e912111a8ca6e2b965bcecc24eeb9ae66d26798d","modified":1526390828927},{"_id":"public/archives/2017/01/index.html","hash":"141c6aeda9948dfc24dbe6de58ddc752bbb616c3","modified":1526390828927},{"_id":"public/archives/2017/06/index.html","hash":"04d4a4796e3b80ba79bb8273d7c157e75cb705ed","modified":1526390828927},{"_id":"public/categories/markdown/index.html","hash":"7aa32278818eb1841f8a4163f34eeaca1d228679","modified":1526390828927},{"_id":"public/categories/java/jdk源码/index.html","hash":"c041eb68e808e2430a71d868969a19afc7f59453","modified":1526390828927},{"_id":"public/categories/java/jms/index.html","hash":"8f914a66b2dc6282a0b0298bf05ba33e8fcf44d2","modified":1526390828927},{"_id":"public/categories/linux/index.html","hash":"e115c7d7d9a5d5510846cb21e049ee40a2c97386","modified":1526390828927},{"_id":"public/categories/java/java基础/index.html","hash":"43963ef8c2193315cbfd3d2a7aa1c64e1fcfee6f","modified":1526390828927},{"_id":"public/categories/分布式/index.html","hash":"cae544995a8384936dd0a7567e16c98af6d2f782","modified":1526390828927},{"_id":"public/categories/linux/网络/index.html","hash":"15e83d35ffcd3ce284cec38037d6dfff60b6bc33","modified":1526390828927},{"_id":"public/categories/linux/linux基本配置/index.html","hash":"41be94d0f2080aba5f196991aae0c17725a48325","modified":1526390828928},{"_id":"public/categories/开源项目/kettle/index.html","hash":"9ae71753b7518f62db39179446f0b859edcaade0","modified":1526390828928},{"_id":"public/categories/开源项目/spring/index.html","hash":"c48a19b8901fcf7def354a4755913f786a15b3d4","modified":1526390828928},{"_id":"public/categories/异常/index.html","hash":"a28436eb44bd74cedbf3b60ef7437c8e44d91c40","modified":1526390828928},{"_id":"public/categories/大数据/hadoop/index.html","hash":"91a5ce4fdcebc348aa810d81b62fd7fe875bfb53","modified":1526390828928},{"_id":"public/categories/服务/index.html","hash":"ae637436afa64cc11aa3bdd846fd495575197512","modified":1526390828928},{"_id":"public/categories/网络安全/index.html","hash":"034810459645c5f60c2c144fe7be7f28efda9a32","modified":1526390828928},{"_id":"public/categories/分布式/hbase/index.html","hash":"0be494ae4203cde6d4ad2f46913467829c63e283","modified":1526390828928},{"_id":"public/categories/软件工程/index.html","hash":"5332a5c996da24f26f2aaca216db217d2b71ef3c","modified":1526390828928},{"_id":"public/categories/大数据/hdfs/index.html","hash":"39e2e10c2c12a7e96050616b6aa62581f3a1cea4","modified":1526390828928},{"_id":"public/categories/分布式/分布式存储/index.html","hash":"d2c8e21b302f69ca54b481d896b9a30e1050b27b","modified":1526390828928},{"_id":"public/categories/分布式/分布式计算/index.html","hash":"35e3420e232bdbafaec368b849b9bb48b0208f24","modified":1526390828928},{"_id":"public/categories/大数据/实时分析/index.html","hash":"b999cc32f59ba3360a90cecdc6f08a76ed7172ca","modified":1526390828928},{"_id":"public/categories/大数据/数据仓库/index.html","hash":"d57a999d606d89e6b96c8d78f072871216c7856a","modified":1526390828928},{"_id":"public/categories/大数据/流式计算/index.html","hash":"b5ab90a2a39d3b3b78b2265362eafeb57ff2b921","modified":1526390828929},{"_id":"public/categories/大数据/hive/index.html","hash":"d89d1e59f5da0868f253dbdd56000395df3f9141","modified":1526390828929},{"_id":"public/categories/大数据/kylin/index.html","hash":"e3f38163c823fff65dd67ffaa6608b4991de1794","modified":1526390828929},{"_id":"public/categories/大数据/数据分析/index.html","hash":"2386c23f70e2ebb374cc2daf09421a6be2ce057b","modified":1526390828929},{"_id":"public/categories/大数据/spark/index.html","hash":"8b760937e163af5bd687b2a8f6e33125d2c4517b","modified":1526390828929},{"_id":"public/categories/异常/tomcat/index.html","hash":"a5b5b49c73e1c21f88117e769fad8015663674a4","modified":1526390828929},{"_id":"public/categories/网络安全/安全小组/index.html","hash":"9c59821645834145ba801d6f52f503352b89a067","modified":1526390828929},{"_id":"public/categories/软件工程/算法/index.html","hash":"ab8b232442b3c3bf06664331f24ba8ddd7584f23","modified":1526390828929},{"_id":"public/categories/软件工程/设计模式/index.html","hash":"1a0b8593ec20487c1bac584c0263ff8ad47308c2","modified":1526390828929},{"_id":"public/categories/tools/index.html","hash":"3f9597efc5a4e0733dd73abbd41eb839a95af06e","modified":1526390828929},{"_id":"public/categories/前端/index.html","hash":"6a23664c9161fce9992a8b7aba10c82b07266017","modified":1526390828929},{"_id":"public/categories/tools/emacs/index.html","hash":"0dcbc1566648512bb2258d61a078003c31aab83c","modified":1526390828929},{"_id":"public/categories/tools/vim/index.html","hash":"97caaba207f8cd92ff7f438bb0debe68b8d764f2","modified":1526390828929},{"_id":"public/categories/前端/angular-js/index.html","hash":"c49c73e53c3d083a2f75f3c7a237bd3317b522a2","modified":1526390828929},{"_id":"public/categories/前端/react-js/index.html","hash":"0d7558b4c8be20357381a550f0775f087dcc6e77","modified":1526390828929},{"_id":"public/tags/markdown/index.html","hash":"2c59eda80b83db1538e39565ef5300f4eb10ddf4","modified":1526390828930},{"_id":"public/tags/jdk源码/index.html","hash":"27d49afaac8f59ac9f530134f9dcdf505457a651","modified":1526390828930},{"_id":"public/tags/java基础/index.html","hash":"7ad59f94bf049d4188b46f149446a9972be5f50a","modified":1526390828930},{"_id":"public/tags/jms/index.html","hash":"6a7150ede648dca247b49af677db3a5ec18da7c5","modified":1526390828930},{"_id":"public/tags/linux/index.html","hash":"bb3d735de0d273ffa136bb0731911ec44d60e6ee","modified":1526390828930},{"_id":"public/tags/网络/index.html","hash":"27aca502bc75fd3482f6a642132804f2e8dc5e53","modified":1526390828930},{"_id":"public/tags/开源项目/index.html","hash":"a9dadb457c945eba471023a263fdc34ca367074f","modified":1526390828930},{"_id":"public/tags/kettle/index.html","hash":"7f621d12144bb47003d706a05ed46b001a808170","modified":1526390828930},{"_id":"public/tags/spark/index.html","hash":"045e2f688fd7c0a87a0c89bb6767cc1934fa66a7","modified":1526390828930},{"_id":"public/tags/hadoop/index.html","hash":"252fb35b091b1904ab40fefd4c1f7e84eec37ac3","modified":1526390828930},{"_id":"public/tags/分布式/index.html","hash":"6f1a96323c5076a7e2ae76e454f05e1f7ac4cd37","modified":1526390828930},{"_id":"public/tags/hbase/index.html","hash":"f9e1892d2f4727a9ba873ed97f3f1db67a6c7459","modified":1526390828930},{"_id":"public/tags/分布式存储/index.html","hash":"b6a5de06f0dffc9997cc0e6af48c2dad20dac09b","modified":1526390828930},{"_id":"public/tags/分布式计算/index.html","hash":"47ce15fb9438e0594ee6dc78eb21978716e57b04","modified":1526390828930},{"_id":"public/tags/实时分析/index.html","hash":"acd238168b0f8979925a4f9b528d5939f88a12ed","modified":1526390828931},{"_id":"public/tags/数据仓库/index.html","hash":"181c1b72a2eb82513d5b9e4784bbe43efa0e021b","modified":1526390828931},{"_id":"public/tags/数据采集/index.html","hash":"f2d30480b8567474e72ed35072c26e6ad3aff77f","modified":1526390828931},{"_id":"public/tags/流式计算/index.html","hash":"bf745b4af628d3bb9225205a3650083572b110ad","modified":1526390828931},{"_id":"public/tags/hive/index.html","hash":"e9045f4a7257e7a0ec71ab15808d85a4d18437f4","modified":1526390828931},{"_id":"public/tags/kylin/index.html","hash":"77a6d23e62ce7c3d4e7dae3d923c0c28d86e4ce1","modified":1526390828931},{"_id":"public/tags/数据分析/index.html","hash":"e31fb5a169826b03e6091aca666829feda1427d6","modified":1526390828931},{"_id":"public/tags/spring/index.html","hash":"577f2bb567be2bf060527d47b4f7fcb3947e0965","modified":1526390828931},{"_id":"public/tags/异常/index.html","hash":"333a0ee5d8f49dbd5c016ba1a5a26c0bd1cef817","modified":1526390828931},{"_id":"public/tags/tomcat/index.html","hash":"0efe3c2ae0b583755a77981f190e0c46466f7986","modified":1526390828931},{"_id":"public/tags/服务/index.html","hash":"40cee3945c6ae8de40e2a5c64eab0dd4600d4f1a","modified":1526390828931},{"_id":"public/tags/网络安全/index.html","hash":"87145dbc3f5aa452ed101bcb12a8c1eb034762f4","modified":1526390828931},{"_id":"public/tags/安全小组/index.html","hash":"57c6501819e18a5e428d5f8b765580d368255c64","modified":1526390828931},{"_id":"public/tags/软件工程/index.html","hash":"392b39ef436e282e5fb69d1917c11594c1b0c936","modified":1526390828931},{"_id":"public/tags/算法/index.html","hash":"ae71b1708ccefa8af65e09ae2b1081dbc2ad3b27","modified":1526390828932},{"_id":"public/tags/设计模式/index.html","hash":"c8e406684eb6d7539c002f6ebed2dfeb88cdfbb2","modified":1526390828932},{"_id":"public/tags/tools-emacs/index.html","hash":"c65484cef460e3f90148ec127c14dea93da8a3fa","modified":1526390828932},{"_id":"public/tags/tools-vim/index.html","hash":"9abfbd1d45a3fd8af2337d3ec2fbe8a6be3f1324","modified":1526390828932},{"_id":"public/tags/angular-js/index.html","hash":"837bf85800eddbe53a4553d3588052b5144e026d","modified":1526390828932},{"_id":"public/tags/react-js/index.html","hash":"11e6907f74897c21dcf814ea5e4849cf6452d697","modified":1526390828932},{"_id":"public/categories/index.html","hash":"3d9572ae422ec60145d89f0358d62aea3624b095","modified":1526390828932},{"_id":"public/tags/index.html","hash":"fe1be37ffa85af31f955b6502c87cb92008c56ac","modified":1526390828932},{"_id":"public/about/index.html","hash":"d32cf109a37ae050a8373ec78f3115f144729916","modified":1526390828932},{"_id":"public/2017/08/18/异常/Java_heap_space_OutOfMemoryError/index.html","hash":"ce3df4372d6ba000f5fe3d0d6fe97a98d0c39f3e","modified":1526390828932},{"_id":"public/2017/08/18/异常/JasperListener类找不到/index.html","hash":"957290b49e2853f6cde57ff0656536075a9a9acf","modified":1526390828932},{"_id":"public/2017/08/18/异常/connection_holder_is_null/index.html","hash":"8c9353619d8932d792b1c7901da2d261a04f1b7e","modified":1526390828932},{"_id":"public/2017/08/18/异常/permGen_space_OutOfMemoryError/index.html","hash":"9ca53bddebe37b9305e179a52ec821e74aec210e","modified":1526390828932},{"_id":"public/2017/06/09/服务/和田市卫浴安装家具安装/index.html","hash":"880124e3af6cff1cfaf3bc9dc7b31f2c93b1fa89","modified":1526390828932},{"_id":"public/2017/05/16/网络安全/安全组第一次会议提出的问题整理/index.html","hash":"dc3593e4a78accac75859b542c64f0e0a2ffa865","modified":1526390828933},{"_id":"public/2017/05/08/hadoop/Hadoop知识点/index.html","hash":"57bab0c4278a8ff1dfd55691b133f55d19b5323f","modified":1526390828933},{"_id":"public/2017/05/04/tools/emacs/TODO-emacs入门/index.html","hash":"b833c9de943ba3b7c3623be7f481e2e65cfa0526","modified":1526390828933},{"_id":"public/2017/05/04/tools/vim/TODO-vim入门实践/index.html","hash":"94da440fe23233c3b651a4ca1eefa73e18769b87","modified":1526390828933},{"_id":"public/2017/05/02/设计模式/主要设计模式及简要介绍/index.html","hash":"77ef4477cfe08a0515c93641bfede0926b865070","modified":1526390828933},{"_id":"public/2017/05/02/linux/Linux常用命令及操作/index.html","hash":"0c7ed5b23ba19cef46aad8b7ee953da6f58d15bf","modified":1526390828933},{"_id":"public/2017/05/02/hadoop/HBase入门概念/index.html","hash":"62c288cbe89abcc76a22cf4ba69af6424ffc1e93","modified":1526390828933},{"_id":"public/2017/05/02/hadoop/Hive入门概念/index.html","hash":"1de46c26cb304cb0c6ecf6af1bdfadada9d0ac7d","modified":1526390828933},{"_id":"public/2017/04/27/前端/react.js/TODO-react.js入门之HelloWorld/index.html","hash":"a6eb1ec34244206c023aa6d68b4e22e7ffebcf36","modified":1526390828933},{"_id":"public/2017/04/27/前端/angular.js/TODO-angular.js入门之HelloWorld/index.html","hash":"86f02a2176dd73f6567562e258886fbe4af70380","modified":1526390828933},{"_id":"public/2017/04/27/前端/vue.js/TODO-vue.js入门之HelloWorld/index.html","hash":"8d000b76b8f4090ed7d92c625aaef135f2e8153d","modified":1526390828933},{"_id":"public/2017/04/18/java/jdk环境变量配置/index.html","hash":"880c81fbf4d7cceeb9e97db8761fea76caa75d81","modified":1526390828933},{"_id":"public/2017/04/18/Markdown基础入门/index.html","hash":"2b1a6bb1c2199813f73cd079c8be4ac90a4b71ba","modified":1526390828933},{"_id":"public/2017/04/18/java/JDK源码分析之集合框架HashMap/index.html","hash":"b9473326ac53c95f82cf9bd84e2894a85498d159","modified":1526390828933},{"_id":"public/2017/04/18/spring/Spring源码分析之环境准备/index.html","hash":"db4c28895de4799d0071fd18634f85f776ac6137","modified":1526390828933},{"_id":"public/2017/04/16/hadoop/Apache Spark与Apache Hadoop的关系/index.html","hash":"e3ec5959063abbd07f94b5be653b81dc44d0ea15","modified":1526390828933},{"_id":"public/2017/04/16/hadoop/Hadoop之分布式计算 /index.html","hash":"5e06030e38f50480885fd5ea1cc9cfa82ee03300","modified":1526390828934},{"_id":"public/2017/04/16/hadoop/Hadoop之分布式存储/index.html","hash":"94b60f907937716cb106b1f5e08fbf8722e92bf6","modified":1526390828934},{"_id":"public/2017/04/16/hadoop/Hadoop之实时分析/index.html","hash":"ef799c6eb3b054562fceac80c364bd2376d3c184","modified":1526390828934},{"_id":"public/2017/04/16/hadoop/数据分析软件分类/index.html","hash":"0226ae930f49ccdc68004fb776213ec3be15c830","modified":1526390828934},{"_id":"public/2017/04/16/hadoop/Hadoop之数据采集/index.html","hash":"b0a97813409722ca1d6efb0b2d81ab44e8f1c53d","modified":1526390828934},{"_id":"public/2017/04/16/hadoop/HDFS入门概念/index.html","hash":"04bfb78edf7cde0d28ccd45f72a77252bb9408b7","modified":1526390828934},{"_id":"public/2017/04/16/hadoop/Hadoop之流式计算/index.html","hash":"7fbbbde8ab0c6b2626e3f2e5d8773ca116ee976d","modified":1526390828934},{"_id":"public/2017/04/16/spark/TODO-Spark体系概述/index.html","hash":"d550f8b8e05b8003c6e854208cf757f56b5623ae","modified":1526390828934},{"_id":"public/2017/04/16/hadoop/Kylin入门概念/index.html","hash":"16ec3a8a622121d943d24cbf0f14841eb0b67712","modified":1526390828934},{"_id":"public/2017/04/15/kettle/Kettle插件架构/index.html","hash":"e5ff3b0ad8d5777f41831221695cc66c67a1d659","modified":1526390828934},{"_id":"public/2017/04/15/kettle/开源ETL工具-kettle/index.html","hash":"f47a69c27b8e9b226f98d0e19b3ec9b2885990e9","modified":1526390828934},{"_id":"public/2017/04/01/oracle/Oracle网络和数据库连接/index.html","hash":"94115373fbbe7834cbd2fe0bef46de31b3ac6886","modified":1526390828934},{"_id":"public/2017/01/01/oracle/Oracle数据库系统架构/index.html","hash":"eb09ee05abaaa78f136153848bbd2252d7f6dc97","modified":1526390828934},{"_id":"public/2017/01/01/oracle/Oracle SQL优化/index.html","hash":"751d4301c5c8072df96628c9410f7661167ec7a1","modified":1526390828934},{"_id":"public/2017/01/01/网络安全/企业安全组/index.html","hash":"1322d7eed58ab836a3c1d7e0813c05dc9aac75d5","modified":1526390828934},{"_id":"public/2017/01/01/算法/常用算法概述/index.html","hash":"39f3825c1e06b1e44b48108763224139b44c8e76","modified":1526390828935},{"_id":"public/2016/09/20/linux/Linux固定IP上网方式/index.html","hash":"47362dbb5bf7c6d164afeefae2f08c26375fa94a","modified":1526390828935},{"_id":"public/2016/05/22/kettle/Kettle源码构建过程/index.html","hash":"850657529c52b2e612fee207d6cc07ae2505774b","modified":1526390828935},{"_id":"public/2016/05/22/kettle/Sorted Merge组件/index.html","hash":"51e387ae66b84aa3707dae1c3222f9ee643fbc61","modified":1526390828935},{"_id":"public/2016/05/22/oracle/Oracle SQL基础知识/index.html","hash":"dc421bad0c8b7f0a1b9e61cdb84f055c88481417","modified":1526390828935},{"_id":"public/2016/05/21/java/消息传送基础/index.html","hash":"d53f6d31a2f0b5e4540c9918a94ee1f8b652c6fc","modified":1526390828935},{"_id":"public/2016/05/21/oracle/RAW类型/index.html","hash":"61e7d41e0a8a84aa2fcef88b58b7c9a82c83cfbf","modified":1526390828935},{"_id":"public/2016/05/20/linux/SSH用户等效性配置/index.html","hash":"773e0c4b309ba0933e8b433c584d8a989de4d565","modified":1526390828935},{"_id":"public/2016/05/20/java/Java基础之转型/index.html","hash":"5d942b41de2a8dbfeef006e3f621d7edbaf2ce52","modified":1526390828935},{"_id":"public/2016/05/20/java/java中Object转String/index.html","hash":"4553ae3e8fecb9a07113006b7194476928cd7daf","modified":1526390828935},{"_id":"public/archives/index.html","hash":"1fe0a42ef4db8ccebd2fd5ac9f08fae81f0ed54f","modified":1526390828935},{"_id":"public/archives/page/2/index.html","hash":"3c68b01d47f444bd09f2391417d3692679f5e40e","modified":1526390828935},{"_id":"public/archives/page/3/index.html","hash":"faa38cd1fd2f7a4b6cd89fabfe9553b90e796d2e","modified":1526390828935},{"_id":"public/archives/page/4/index.html","hash":"503290fe9e11d9017eb9b0cf53b5971835e5f348","modified":1526390828935},{"_id":"public/archives/page/5/index.html","hash":"1f048eb7c4377083196fdd0b8cbd2d5f5c9e91b1","modified":1526390828935},{"_id":"public/archives/2016/index.html","hash":"9f79b9417cded5a4e68d0cb8e11e47084d0a8855","modified":1526390828936},{"_id":"public/archives/2016/05/index.html","hash":"ecb4faa21329c06bb9ce412f0a471d720715d0b3","modified":1526390828936},{"_id":"public/archives/2017/index.html","hash":"cd0c94a944081c2ab70577558592f253fff0d7f6","modified":1526390828936},{"_id":"public/archives/2017/page/2/index.html","hash":"9fc0fdb934dfc3795113b75bd1ebaa9012365efc","modified":1526390828936},{"_id":"public/archives/2017/page/3/index.html","hash":"f3928106fb8194ed4f7cc4f0aaad984748179a67","modified":1526390828936},{"_id":"public/archives/2017/page/4/index.html","hash":"040108acbecfc74b73c09e436bea59878fc5cfd0","modified":1526390828936},{"_id":"public/archives/2017/04/index.html","hash":"59cbd4dcc241a074f72f9a9a7e09df82a2cdfaab","modified":1526390828936},{"_id":"public/archives/2017/04/page/2/index.html","hash":"94a03eb3d7962e8b96456b6644231060bbd91a31","modified":1526390828936},{"_id":"public/archives/2017/05/index.html","hash":"95707b83c0c10bb0cb34dbdf66983c61ab8ca74a","modified":1526390828936},{"_id":"public/archives/2017/08/index.html","hash":"830c276ad39589a409e00fd752090ad9cefa8ddd","modified":1526390828936},{"_id":"public/categories/java/index.html","hash":"5e28e742690f4dc6c79b4f8aa9193c680fb6526f","modified":1526390828936},{"_id":"public/categories/开源项目/index.html","hash":"b49036a447f70b76b893c031f99ef6c90b119cdc","modified":1526390828936},{"_id":"public/categories/大数据/index.html","hash":"d6b7bf7d3988800bab1dd2564020b9d617b0c416","modified":1526390828936},{"_id":"public/categories/数据库/index.html","hash":"a1b3c657271385109fe92220516c381216bb3300","modified":1526390828936},{"_id":"public/categories/数据库/oracle/index.html","hash":"815cc0071d76ea00f87d30964f1347cf82c7a28e","modified":1526390828936},{"_id":"public/index.html","hash":"d306d9207273dc98aae960aae12f3a10733c88c6","modified":1526390828937},{"_id":"public/page/2/index.html","hash":"8a575e078eaf7feb6de91782282068d91e1b62ad","modified":1526390828937},{"_id":"public/page/3/index.html","hash":"cb18f6fd030848667b1a5f2b439d43b9250b878e","modified":1526390828937},{"_id":"public/page/4/index.html","hash":"3b8fcf912094c5416647f1b1c55c02b6a4d6b45a","modified":1526390828937},{"_id":"public/page/5/index.html","hash":"c2768dacd44619e42ca6e68cc734d4d72f1242a1","modified":1526390828937},{"_id":"public/tags/java/index.html","hash":"8f72e22f9bbfcc83790b9c77bd18031b605c0019","modified":1526390828937},{"_id":"public/tags/大数据/index.html","hash":"6781246dbaf325891553bbda3658ee9725b83018","modified":1526390828937},{"_id":"public/tags/oracle/index.html","hash":"6c650dc32ea36f0b4aa1352cd88c189321638c39","modified":1526390828937},{"_id":"public/tags/数据库/index.html","hash":"ddee707da3f40b82fbe8c1a35094b0296bc2f9be","modified":1526390828937},{"_id":"public/CNAME","hash":"3cf76298bb7b4f7f21570d8949949e869edb09b5","modified":1526390828946},{"_id":"public/images/cenrise10.jpg","hash":"004afe6e4189ddb75ea1f4289ed2f5a0fc63ae44","modified":1526390828946},{"_id":"public/images/douban.jpg","hash":"7bd59f85ca708d96a65dddd6981086cbb639cd81","modified":1526390828946},{"_id":"public/images/github.png","hash":"ec237c5368083111c4952dbd80148e8e375e206e","modified":1526390828946},{"_id":"public/images/meizhuang.png","hash":"422b2633c2a77683e03387c91c8e0e72638b427b","modified":1526390828946},{"_id":"public/images/quora.jpeg","hash":"576783adddddee7ab97f15fcf3eba4565f4027dd","modified":1526390828946},{"_id":"public/images/taobao.png","hash":"c84ef7815c69fd97edaeb4b84772dfb89bc6ef52","modified":1526390828946},{"_id":"public/images/favicon.ico","hash":"6ed407cb30e21a406a45d0076a4a3226d2633bf0","modified":1526390828946},{"_id":"public/images/twitter.png","hash":"8b678142eae17a91d47d6fbe6260ae5cd9781c9f","modified":1526390828946},{"_id":"public/images/weibo.jpg","hash":"7f3fcab888eb49a6b21e9fb1b5a300bc9f7e1860","modified":1526390828946},{"_id":"public/images/zhihu.png","hash":"46f6e7bc2f6fb30b7bdbda83f9678efa003a68d5","modified":1526390828946},{"_id":"public/images/kettle/sorted merger1.jpg","hash":"ba3ed99f8f4684e81e064d41780f5bc1a87e79fb","modified":1526390828946},{"_id":"public/images/kettle/开源ETL工具-kettle_Jobs（工作）4.png","hash":"3a58daf90045124022e3c5cd0271f090c3ac3fd6","modified":1526390828946},{"_id":"public/images/kettle/开源ETL工具-kettle_ETL是什么.png","hash":"8da47a6bc4a7cccc87476040b8abf530dbd2aca1","modified":1526390828947},{"_id":"public/images/kettle/开源ETL工具-kettle_Jobs（工作）5.png","hash":"c7ca49eac6954c3caf7a278b2e57b811d7ccddc3","modified":1526390828947},{"_id":"public/images/kettle/开源ETL工具-kettle_Jobs（工作）6.png","hash":"6c0c4f78e4a29da901770ff00cc0dadee2f1f1c0","modified":1526390828947},{"_id":"public/images/kettle/开源ETL工具-kettle_Transformation(转换).jpg","hash":"60a026f89af1642da224c52a001a1e215366bfb2","modified":1526390828958},{"_id":"public/images/kettle/开源ETL工具-kettle_Jobs（工作）9.png","hash":"2e16235c079e3911348061430614bd6f790f21cd","modified":1526390828958},{"_id":"public/images/kettle/开源ETL工具-kettle_Step（步骤）.png","hash":"f26ad03a18d856fd198a3dfc0c898cc30b5d77a4","modified":1526390828958},{"_id":"public/images/algolia_logo.svg","hash":"ec119560b382b2624e00144ae01c137186e91621","modified":1526390828958},{"_id":"public/images/apple-touch-icon-next.png","hash":"2959dbc97f31c80283e67104fe0854e2369e40aa","modified":1526390828958},{"_id":"public/images/avatar.gif","hash":"264082bb3a1af70d5499c7d22b0902cb454b6d12","modified":1526390828958},{"_id":"public/images/cc-by-nc-nd.svg","hash":"c6524ece3f8039a5f612feaf865d21ec8a794564","modified":1526390828958},{"_id":"public/images/cc-by-nc-sa.svg","hash":"3031be41e8753c70508aa88e84ed8f4f653f157e","modified":1526390828958},{"_id":"public/images/cc-by-nc.svg","hash":"8d39b39d88f8501c0d27f8df9aae47136ebc59b7","modified":1526390828959},{"_id":"public/images/cc-by-nd.svg","hash":"c563508ce9ced1e66948024ba1153400ac0e0621","modified":1526390828959},{"_id":"public/images/cc-by-sa.svg","hash":"aa4742d733c8af8d38d4c183b8adbdcab045872e","modified":1526390828959},{"_id":"public/images/cc-by.svg","hash":"28a0a4fe355a974a5e42f68031652b76798d4f7e","modified":1526390828959},{"_id":"public/images/cc-zero.svg","hash":"87669bf8ac268a91d027a0a4802c92a1473e9030","modified":1526390828959},{"_id":"public/images/favicon-16x16-next.png","hash":"943a0d67a9cdf8c198109b28f9dbd42f761d11c3","modified":1526390828959},{"_id":"public/images/favicon-32x32-next.png","hash":"0749d7b24b0d2fae1c8eb7f671ad4646ee1894b1","modified":1526390828959},{"_id":"public/images/loading.gif","hash":"5fbd472222feb8a22cf5b8aa5dc5b8e13af88e2b","modified":1526390828959},{"_id":"public/images/logo.svg","hash":"d29cacbae1bdc4bbccb542107ee0524fe55ad6de","modified":1526390828959},{"_id":"public/images/placeholder.gif","hash":"5fbd472222feb8a22cf5b8aa5dc5b8e13af88e2b","modified":1526390828959},{"_id":"public/images/quote-l.svg","hash":"94e870b4c8c48da61d09522196d4dd40e277a98f","modified":1526390828959},{"_id":"public/images/quote-r.svg","hash":"e60ae504f9d99b712c793c3740c6b100d057d4ec","modified":1526390828959},{"_id":"public/images/searchicon.png","hash":"67727a6a969be0b2659b908518fa6706eed307b8","modified":1526390828959},{"_id":"public/lib/font-awesome/HELP-US-OUT.txt","hash":"4f7bf961f1bed448f6ba99aeb9219fabf930ba96","modified":1526390828959},{"_id":"public/lib/font-awesome/css/font-awesome.css.map","hash":"0189d278706509412bac4745f96c83984e1d59f4","modified":1526390828959},{"_id":"public/images/facebook.png","hash":"a404e32587f618a63375fcbae73d3c99ee4b590b","modified":1526390829552},{"_id":"public/images/kettle/Kettle插件架构001.jpg","hash":"142c232a45620ca45831388a094cacd8137c0671","modified":1526390829552},{"_id":"public/images/kettle/开源ETL工具-kettle_Jobs（工作）3.png","hash":"8fbfeefaa2346dbcff8a735077c003b3731d720e","modified":1526390829555},{"_id":"public/images/kettle/开源ETL工具-kettle_Jobs（工作）8.png","hash":"50c05f7fa4fa0e9bf0295e784c57e6c53bf2f660","modified":1526390829555},{"_id":"public/images/kettle/开源ETL工具-kettle_Jobs（工作）7.png","hash":"fd96587bd686fa59031e9d41b69c0d917f061b3a","modified":1526390829555},{"_id":"public/images/kettle/开源ETL工具-kettle_Kettle构成.jpg","hash":"0f34cfda2c040aaf3617352325d8c2776454cd43","modified":1526390829556},{"_id":"public/lib/font-awesome/fonts/fontawesome-webfont.woff2","hash":"d6f48cba7d076fb6f2fd6ba993a75b9dc1ecbf0c","modified":1526390829556},{"_id":"public/lib/font-awesome/fonts/fontawesome-webfont.woff","hash":"28b782240b3e76db824e12c02754a9731a167527","modified":1526390829556},{"_id":"public/lib/font-awesome/bower.json","hash":"64394a2a9aa00f8e321d8daa5e51a420f0e96dad","modified":1526390829561},{"_id":"public/lib/velocity/velocity.ui.min.js","hash":"ed5e534cd680a25d8d14429af824f38a2c7d9908","modified":1526390829561},{"_id":"public/js/src/algolia-search.js","hash":"b172f697ed339a24b1e80261075232978d164c35","modified":1526390829561},{"_id":"public/js/src/affix.js","hash":"978e0422b5bf1b560236d8d10ebc1adcf66392e3","modified":1526390829561},{"_id":"public/js/src/bootstrap.js","hash":"40de94fd18fcbd67a327d63b0d1e242a08aa5404","modified":1526390829562},{"_id":"public/js/src/exturl.js","hash":"e42e2aaab7bf4c19a0c8e779140e079c6aa5c0b1","modified":1526390829562},{"_id":"public/js/src/js.cookie.js","hash":"9b37973a90fd50e71ea91682265715e45ae82c75","modified":1526390829562},{"_id":"public/js/src/post-details.js","hash":"d1333fb588d4521b4d1e9c69aef06e0ad1bf0b12","modified":1526390829562},{"_id":"public/js/src/scrollspy.js","hash":"fe4da1b9fe73518226446f5f27d2831e4426fc35","modified":1526390829563},{"_id":"public/js/src/motion.js","hash":"50e57f8acb6924c6999cdcc664ddd3f0730d2061","modified":1526390829563},{"_id":"public/js/src/scroll-cookie.js","hash":"09dc828cbf5f31158ff6250d2bf7c3cde6365c67","modified":1526390829563},{"_id":"public/js/src/utils.js","hash":"4284c67ea1435de2acd523f6d48c0d073fd1ad03","modified":1526390829563},{"_id":"public/lib/ua-parser-js/dist/ua-parser.min.js","hash":"38628e75e4412cc6f11074e03e1c6d257aae495b","modified":1526390829563},{"_id":"public/lib/ua-parser-js/dist/ua-parser.pack.js","hash":"214dad442a92d36af77ed0ca1d9092b16687f02f","modified":1526390829563},{"_id":"public/js/src/schemes/pisces.js","hash":"8050a5b2683d1d77238c5762b6bd89c543daed6e","modified":1526390829564},{"_id":"public/css/main.css","hash":"a5dc8423a0197dbe76277113ef721b4c5d04aa99","modified":1526390829564},{"_id":"public/lib/velocity/velocity.min.js","hash":"2f1afadc12e4cf59ef3b405308d21baa97e739c6","modified":1526390829564},{"_id":"public/lib/velocity/velocity.ui.js","hash":"6a1d101eab3de87527bb54fcc8c7b36b79d8f0df","modified":1526390829564},{"_id":"public/lib/jquery/index.js","hash":"41b4bfbaa96be6d1440db6e78004ade1c134e276","modified":1526390829564},{"_id":"public/lib/font-awesome/css/font-awesome.css","hash":"0140952c64e3f2b74ef64e050f2fe86eab6624c8","modified":1526390829564},{"_id":"public/lib/font-awesome/css/font-awesome.min.css","hash":"512c7d79033e3028a9be61b540cf1a6870c896f8","modified":1526390829564},{"_id":"public/lib/velocity/velocity.js","hash":"9f08181baea0cc0e906703b7e5df9111b9ef3373","modified":1526390829564},{"_id":"public/images/kettle/开源ETL工具-kettle_Jobs（工作）.png","hash":"420f30fb3d264b426b0d466106384dbabb0dc8df","modified":1526390829564},{"_id":"public/lib/font-awesome/fonts/fontawesome-webfont.eot","hash":"d980c2ce873dc43af460d4d572d441304499f400","modified":1526390829564},{"_id":"public/images/avatar.jpg","hash":"e02f499056e9d6fde4dd9f4cd468a652359d6fd7","modified":1526390829572},{"_id":"public/images/hadoop/kylin/Kylin的技术架构.jpg","hash":"bb0ae5d716d913bcbffa15fd1b8cc14307a1aa36","modified":1526390829572},{"_id":"public/images/hadoop/kylin/一个四维Cube的例子.jpg","hash":"69401999f9322bbe99d7537ea438a431a479118c","modified":1526390829573},{"_id":"public/images/hadoop/kylin/维度和度量的例子.jpg","hash":"6c6dfb29f569da77258ef8247073f63383214f47","modified":1526390829586}],"Category":[{"name":"markdown","_id":"cjh7poll90005bp7rxt1nwozv"},{"name":"java","_id":"cjh7pols6000cbp7ryeuun3lv"},{"name":"jdk源码","parent":"cjh7pols6000cbp7ryeuun3lv","_id":"cjh7polsj000sbp7r4yjwomkw"},{"name":"jms","parent":"cjh7pols6000cbp7ryeuun3lv","_id":"cjh7polsm000xbp7rjq3k79t1"},{"name":"linux","_id":"cjh7polsv0014bp7rpz8qoyr4"},{"name":"java基础","parent":"cjh7pols6000cbp7ryeuun3lv","_id":"cjh7polsy001bbp7r3sd6urtf"},{"name":"开源项目","_id":"cjh7poltc001zbp7r2d4y9l04"},{"name":"大数据","_id":"cjh7poltm002lbp7ryqy9w02p"},{"name":"分布式","_id":"cjh7polto002qbp7r7juajtly"},{"name":"网络","parent":"cjh7polsv0014bp7rpz8qoyr4","_id":"cjh7polts0030bp7rkfrj3tfx"},{"name":"linux基本配置","parent":"cjh7polsv0014bp7rpz8qoyr4","_id":"cjh7poltx003hbp7rlgw4l1su"},{"name":"数据库","_id":"cjh7polu30047bp7r5njiq3wc"},{"name":"kettle","parent":"cjh7poltc001zbp7r2d4y9l04","_id":"cjh7polu4004abp7rq7o4qa7e"},{"name":"spring","parent":"cjh7poltc001zbp7r2d4y9l04","_id":"cjh7polu9004ybp7r9cfd8gvb"},{"name":"异常","_id":"cjh7polua0052bp7rbqbdjo6s"},{"name":"hadoop","parent":"cjh7poltm002lbp7ryqy9w02p","_id":"cjh7poluf005jbp7r9fyat32p"},{"name":"服务","_id":"cjh7polug005pbp7r2vdv9rv6"},{"name":"网络安全","_id":"cjh7poluh005vbp7rk20sasre"},{"name":"hbase","parent":"cjh7polto002qbp7r7juajtly","_id":"cjh7polui005ybp7ro7cuycse"},{"name":"软件工程","_id":"cjh7poluj0064bp7rpvo1wr6f"},{"name":"hdfs","parent":"cjh7poltm002lbp7ryqy9w02p","_id":"cjh7poluk006abp7rybp7ua5v"},{"name":"分布式存储","parent":"cjh7polto002qbp7r7juajtly","_id":"cjh7polun006gbp7r2r837lh2"},{"name":"分布式计算","parent":"cjh7polto002qbp7r7juajtly","_id":"cjh7poluo006lbp7r1lo9247x"},{"name":"实时分析","parent":"cjh7poltm002lbp7ryqy9w02p","_id":"cjh7polup006pbp7rrjozbhfp"},{"name":"数据仓库","parent":"cjh7poltm002lbp7ryqy9w02p","_id":"cjh7poluq006ubp7rx6wuq1a1"},{"name":"流式计算","parent":"cjh7poltm002lbp7ryqy9w02p","_id":"cjh7polur006zbp7ris9z2spt"},{"name":"hive","parent":"cjh7poltm002lbp7ryqy9w02p","_id":"cjh7polut007abp7rrczaotlm"},{"name":"kylin","parent":"cjh7poltm002lbp7ryqy9w02p","_id":"cjh7poluu007fbp7r74k3czoq"},{"name":"数据分析","parent":"cjh7poltm002lbp7ryqy9w02p","_id":"cjh7poluw007lbp7rwn922k3h"},{"name":"oracle","parent":"cjh7polu30047bp7r5njiq3wc","_id":"cjh7polux007qbp7rcx3n5bri"},{"name":"spark","parent":"cjh7poltm002lbp7ryqy9w02p","_id":"cjh7polv40089bp7r5san0ok9"},{"name":"tomcat","parent":"cjh7polua0052bp7rbqbdjo6s","_id":"cjh7polv5008gbp7ralzji6q5"},{"name":"安全小组","parent":"cjh7poluh005vbp7rk20sasre","_id":"cjh7polva008ybp7rd44wfp73"},{"name":"算法","parent":"cjh7poluj0064bp7rpvo1wr6f","_id":"cjh7polvc0097bp7r8s8r17xv"},{"name":"设计模式","parent":"cjh7poluj0064bp7rpvo1wr6f","_id":"cjh7polvd009fbp7r0misroi6"},{"name":"tools","_id":"cjh7polxd009ubp7r1x1tlpqf"},{"name":"前端","_id":"cjh7polxh00a2bp7ryyr7x6ek"},{"name":"emacs","parent":"cjh7polxd009ubp7r1x1tlpqf","_id":"cjh7polxi00a8bp7re4uubhui"},{"name":"vim","parent":"cjh7polxd009ubp7r1x1tlpqf","_id":"cjh7polxk00adbp7r41y11fto"},{"name":"angular.js","parent":"cjh7polxh00a2bp7ryyr7x6ek","_id":"cjh7polxk00agbp7rg8sbvikn"},{"name":"react.js","parent":"cjh7polxh00a2bp7ryyr7x6ek","_id":"cjh7polxk00aibp7rrhqfnfm0"}],"Data":[],"Page":[{"_content":"# jiadongpo.github.io\ncenrise.com\n","source":"README.md","raw":"# jiadongpo.github.io\ncenrise.com\n","date":"2018-05-15T07:22:21.040Z","updated":"2017-08-20T17:10:10.000Z","path":"README.html","title":"","comments":1,"layout":"page","_id":"cjh7poljf0000bp7r8itx5cn4","content":"<h1 id=\"jiadongpo-github-io\"><a href=\"#jiadongpo-github-io\" class=\"headerlink\" title=\"jiadongpo.github.io\"></a>jiadongpo.github.io</h1><p>cenrise.com</p>\n","site":{"data":{}},"excerpt":"","more":"<h1 id=\"jiadongpo-github-io\"><a href=\"#jiadongpo-github-io\" class=\"headerlink\" title=\"jiadongpo.github.io\"></a>jiadongpo.github.io</h1><p>cenrise.com</p>\n"},{"_content":"iea4FlOhCY","source":"baidu_verify_iea4FlOhCY.html","raw":"iea4FlOhCY","date":"2018-05-15T07:22:21.041Z","updated":"2017-08-20T17:10:10.000Z","path":"baidu_verify_iea4FlOhCY.html","title":"","comments":1,"layout":"page","_id":"cjh7poljg0001bp7r4avtpzfp","content":"iea4FlOhCY","site":{"data":{}},"excerpt":"","more":"iea4FlOhCY"},{"title":"分类","date":"2016-05-20T07:30:28.000Z","type":"categories","comments":0,"_content":"","source":"categories/index.md","raw":"---\ntitle: 分类\ndate: 2016-05-20 15:30:28\ntype: \"categories\"\ncomments: false\n---\n","updated":"2018-05-15T12:01:07.149Z","path":"categories/index.html","layout":"page","_id":"cjh7poll60003bp7rh8yyk9sw","content":"","site":{"data":{}},"excerpt":"","more":""},{"title":"标签","date":"2016-05-20T07:30:01.000Z","type":"tags","comments":0,"_content":"","source":"tags/index.md","raw":"---\ntitle: 标签\ndate: 2016-05-20 15:30:01\ntype: \"tags\"\ncomments: false\n---\n","updated":"2018-05-15T12:01:17.440Z","path":"tags/index.html","layout":"page","_id":"cjh7poll80004bp7rnw03sp0o","content":"","site":{"data":{}},"excerpt":"","more":""},{"title":"关于","date":"2016-05-20T09:50:35.000Z","type":"about","comments":0,"_content":"\n推介：\n----------\n贾东坡，河南商丘人，目前工作地在北京，是一名奔走于研发一线的普通程序员。  \n\n技术背景  \n&emsp;&ensp;编程语言：Java  \n&emsp;&ensp;相关技术：Kettle、Spring  \n&emsp;&ensp;另外，在大数据技术上也有少许的经验，我也比较喜欢相关的工作。\n\n行业背景：金融、航空\n\n- 微博：http://weibo.com/zhichengchina  \n\n\n- 邮箱：dongpo.jia@cenrise.com  \n\n\n起因\n----------\n该站起于2015年10月22号，当时无意中发现了Github Pages，于是就试着搭建了这个博客，后来出于对写博客的重要性认知不足，导致此站一度荒废，后来也有意无意更新过部分内容。  \n\ncenrise.com，域名寓意“中原崛起”，愿做“中原崛起的一颗凡星”，名曰：中起之星。希望自己的家乡河南有更好的发展吧。\n\n本站意图是用于激励自己更好的成长、积极的分享。知行合一，行远致胜!\n\n \n友链\n----------\n**商丘**：http://shangqiu.me/","source":"about/index.md","raw":"---\ntitle: 关于\ndate: 2016-05-20 17:50:35\ntype: \"about\"\ncomments: false\n---\n\n推介：\n----------\n贾东坡，河南商丘人，目前工作地在北京，是一名奔走于研发一线的普通程序员。  \n\n技术背景  \n&emsp;&ensp;编程语言：Java  \n&emsp;&ensp;相关技术：Kettle、Spring  \n&emsp;&ensp;另外，在大数据技术上也有少许的经验，我也比较喜欢相关的工作。\n\n行业背景：金融、航空\n\n- 微博：http://weibo.com/zhichengchina  \n\n\n- 邮箱：dongpo.jia@cenrise.com  \n\n\n起因\n----------\n该站起于2015年10月22号，当时无意中发现了Github Pages，于是就试着搭建了这个博客，后来出于对写博客的重要性认知不足，导致此站一度荒废，后来也有意无意更新过部分内容。  \n\ncenrise.com，域名寓意“中原崛起”，愿做“中原崛起的一颗凡星”，名曰：中起之星。希望自己的家乡河南有更好的发展吧。\n\n本站意图是用于激励自己更好的成长、积极的分享。知行合一，行远致胜!\n\n \n友链\n----------\n**商丘**：http://shangqiu.me/","updated":"2018-05-15T12:00:55.886Z","path":"about/index.html","layout":"page","_id":"cjh7pollb0007bp7roz44fsd9","content":"<h2 id=\"推介：\"><a href=\"#推介：\" class=\"headerlink\" title=\"推介：\"></a>推介：</h2><p>贾东坡，河南商丘人，目前工作地在北京，是一名奔走于研发一线的普通程序员。  </p>\n<p>技术背景<br>&emsp;&ensp;编程语言：Java<br>&emsp;&ensp;相关技术：Kettle、Spring<br>&emsp;&ensp;另外，在大数据技术上也有少许的经验，我也比较喜欢相关的工作。</p>\n<p>行业背景：金融、航空</p>\n<ul>\n<li>微博：<a href=\"http://weibo.com/zhichengchina\" target=\"_blank\" rel=\"noopener\">http://weibo.com/zhichengchina</a>  </li>\n</ul>\n<ul>\n<li>邮箱：<a href=\"mailto:dongpo.jia@cenrise.com\">dongpo.jia@cenrise.com</a>  </li>\n</ul>\n<h2 id=\"起因\"><a href=\"#起因\" class=\"headerlink\" title=\"起因\"></a>起因</h2><p>该站起于2015年10月22号，当时无意中发现了Github Pages，于是就试着搭建了这个博客，后来出于对写博客的重要性认知不足，导致此站一度荒废，后来也有意无意更新过部分内容。  </p>\n<p>cenrise.com，域名寓意“中原崛起”，愿做“中原崛起的一颗凡星”，名曰：中起之星。希望自己的家乡河南有更好的发展吧。</p>\n<p>本站意图是用于激励自己更好的成长、积极的分享。知行合一，行远致胜!</p>\n<h2 id=\"友链\"><a href=\"#友链\" class=\"headerlink\" title=\"友链\"></a>友链</h2><p><strong>商丘</strong>：<a href=\"http://shangqiu.me/\" target=\"_blank\" rel=\"noopener\">http://shangqiu.me/</a></p>\n","site":{"data":{}},"excerpt":"","more":"<h2 id=\"推介：\"><a href=\"#推介：\" class=\"headerlink\" title=\"推介：\"></a>推介：</h2><p>贾东坡，河南商丘人，目前工作地在北京，是一名奔走于研发一线的普通程序员。  </p>\n<p>技术背景<br>&emsp;&ensp;编程语言：Java<br>&emsp;&ensp;相关技术：Kettle、Spring<br>&emsp;&ensp;另外，在大数据技术上也有少许的经验，我也比较喜欢相关的工作。</p>\n<p>行业背景：金融、航空</p>\n<ul>\n<li>微博：<a href=\"http://weibo.com/zhichengchina\" target=\"_blank\" rel=\"noopener\">http://weibo.com/zhichengchina</a>  </li>\n</ul>\n<ul>\n<li>邮箱：<a href=\"mailto:dongpo.jia@cenrise.com\">dongpo.jia@cenrise.com</a>  </li>\n</ul>\n<h2 id=\"起因\"><a href=\"#起因\" class=\"headerlink\" title=\"起因\"></a>起因</h2><p>该站起于2015年10月22号，当时无意中发现了Github Pages，于是就试着搭建了这个博客，后来出于对写博客的重要性认知不足，导致此站一度荒废，后来也有意无意更新过部分内容。  </p>\n<p>cenrise.com，域名寓意“中原崛起”，愿做“中原崛起的一颗凡星”，名曰：中起之星。希望自己的家乡河南有更好的发展吧。</p>\n<p>本站意图是用于激励自己更好的成长、积极的分享。知行合一，行远致胜!</p>\n<h2 id=\"友链\"><a href=\"#友链\" class=\"headerlink\" title=\"友链\"></a>友链</h2><p><strong>商丘</strong>：<a href=\"http://shangqiu.me/\" target=\"_blank\" rel=\"noopener\">http://shangqiu.me/</a></p>\n"}],"Post":[{"title":"Markdown基础入门","date":"2017-04-18T07:00:00.000Z","_content":"\n简介\n----------\nMarkdown 的目标是实现「易读易写」。\n\n\n表格\n----------\n实现表格的两种方式  \n方式一：当某项过长时，表格可能如下显示，不好看。  \n具体使用方式请看示例。  \n•\t------: 为右对齐。  \n•\t:------ 为左对齐。  \n•\t:------: 为居中对齐。  \n•\t------- 为使用默认居中对齐。  \n1.9.2 示例  \n|         属性项               |                    属性说明    \n|    ------: |    :-------:    |    :---------   |    ------    |\n|    组件名称    |    步骤的名字，这个名字在一个转换中必须是唯一的。    | \n|    字段    |    指定字段名和排序方向(升序/降序);点击获取字段检索列表字段从输入流    | \n|字段|指定排序的字段名。|\n|升序|排序原则：升序或降序。如果选择升序，排序顺序将是：数字->英文->汉字，汉字是按照拼音排序的，也同样会按照声调排序。如果是多音字，只会取一个读音，无法根据语境判断其的读音。|\n\n显示如下：  \n \n注意  \n1.\t每个Markdown解析器都不一样，可能左右居中对齐方式的表示方式不一样。  \n\n方式二：表格形式（推荐）    \n\n`<table>\n    <tr>  \n        <th>属性项</th>  \n        <th>属性说明</th>  \n    </tr>\n    <tr>\n        <td>组件名称</td>\n        <td>步骤的名字，这个名字在一个转换中必须是唯一的。</td>\n    </tr>\n    <tr>\n        <td>字段</td>\n        <td>指定字段名和排序方向(升序/降序);点击获取字段检索列表字段从输入流。</td>\n    </tr>\n    <tr>\n        <td>字段</td>\n        <td>指定排序的字段名。</td>\n    </tr>\n    <tr>\n        <td>升序</td>\n        <td>排序原则：升序或降序。如果选择升序，    \n        排序顺序将是：数字->英文->汉字，汉字是按照拼音排序的，    \n        也同样会按照声调排序。如果是多音字，只会取一个读音，    \n        无法根据语境判断其的读音。</td>\n    </tr>\n</table>`\n输出如下：  \n<table>\n    <tr>  \n        <th>属性项</th>  \n        <th>属性说明</th>  \n    </tr>\n    <tr>\n        <td>组件名称</td>\n        <td>步骤的名字，这个名字在一个转换中必须是唯一的。</td>\n    </tr>\n    <tr>\n        <td>字段</td>\n        <td>指定字段名和排序方向(升序/降序);</td>\n    </tr>\n    <tr>\n        <td>字段</td>\n        <td>指定排序的字段名。</td>\n    </tr>\n    <tr>\n        <td>升序</td>\n        <td>排序原则：升序或降序。如果选择升\n    </tr>\n</table>\n\n\n\n\n首行缩进\n----------\n由于markdown语法主要考虑的是英文，所以对于中文的首行缩进并不太友好，两种方法都可以完美解决这个问题。\n\n- 把输入法由半角改为全角。 两次空格之后就能够有两个汉字的缩进。  \n\n\n- 在开头的时候，先输入下面的代码，然后紧跟着输入文本即可。分号也不要掉。   \n\n直接写  \n半方大的空白```&ensp;```或```&#8194;```   \n全方大的空白```&emsp;```或```&#8195;```  \n不断行的空白格```&nbsp;```或```&#160;```\n做为显示时这几个转义不能单独写，要在前后添加```\n\n","source":"_posts/Markdown基础入门.md","raw":"---\ntitle: Markdown基础入门\ndate: 2017-04-18 15:00:00\ntags: [markdown]\ncategories: [markdown]\n---\n\n简介\n----------\nMarkdown 的目标是实现「易读易写」。\n\n\n表格\n----------\n实现表格的两种方式  \n方式一：当某项过长时，表格可能如下显示，不好看。  \n具体使用方式请看示例。  \n•\t------: 为右对齐。  \n•\t:------ 为左对齐。  \n•\t:------: 为居中对齐。  \n•\t------- 为使用默认居中对齐。  \n1.9.2 示例  \n|         属性项               |                    属性说明    \n|    ------: |    :-------:    |    :---------   |    ------    |\n|    组件名称    |    步骤的名字，这个名字在一个转换中必须是唯一的。    | \n|    字段    |    指定字段名和排序方向(升序/降序);点击获取字段检索列表字段从输入流    | \n|字段|指定排序的字段名。|\n|升序|排序原则：升序或降序。如果选择升序，排序顺序将是：数字->英文->汉字，汉字是按照拼音排序的，也同样会按照声调排序。如果是多音字，只会取一个读音，无法根据语境判断其的读音。|\n\n显示如下：  \n \n注意  \n1.\t每个Markdown解析器都不一样，可能左右居中对齐方式的表示方式不一样。  \n\n方式二：表格形式（推荐）    \n\n`<table>\n    <tr>  \n        <th>属性项</th>  \n        <th>属性说明</th>  \n    </tr>\n    <tr>\n        <td>组件名称</td>\n        <td>步骤的名字，这个名字在一个转换中必须是唯一的。</td>\n    </tr>\n    <tr>\n        <td>字段</td>\n        <td>指定字段名和排序方向(升序/降序);点击获取字段检索列表字段从输入流。</td>\n    </tr>\n    <tr>\n        <td>字段</td>\n        <td>指定排序的字段名。</td>\n    </tr>\n    <tr>\n        <td>升序</td>\n        <td>排序原则：升序或降序。如果选择升序，    \n        排序顺序将是：数字->英文->汉字，汉字是按照拼音排序的，    \n        也同样会按照声调排序。如果是多音字，只会取一个读音，    \n        无法根据语境判断其的读音。</td>\n    </tr>\n</table>`\n输出如下：  \n<table>\n    <tr>  \n        <th>属性项</th>  \n        <th>属性说明</th>  \n    </tr>\n    <tr>\n        <td>组件名称</td>\n        <td>步骤的名字，这个名字在一个转换中必须是唯一的。</td>\n    </tr>\n    <tr>\n        <td>字段</td>\n        <td>指定字段名和排序方向(升序/降序);</td>\n    </tr>\n    <tr>\n        <td>字段</td>\n        <td>指定排序的字段名。</td>\n    </tr>\n    <tr>\n        <td>升序</td>\n        <td>排序原则：升序或降序。如果选择升\n    </tr>\n</table>\n\n\n\n\n首行缩进\n----------\n由于markdown语法主要考虑的是英文，所以对于中文的首行缩进并不太友好，两种方法都可以完美解决这个问题。\n\n- 把输入法由半角改为全角。 两次空格之后就能够有两个汉字的缩进。  \n\n\n- 在开头的时候，先输入下面的代码，然后紧跟着输入文本即可。分号也不要掉。   \n\n直接写  \n半方大的空白```&ensp;```或```&#8194;```   \n全方大的空白```&emsp;```或```&#8195;```  \n不断行的空白格```&nbsp;```或```&#160;```\n做为显示时这几个转义不能单独写，要在前后添加```\n\n","slug":"Markdown基础入门","published":1,"updated":"2017-08-18T01:44:35.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjh7poll30002bp7rj3ei2anb","content":"<h2 id=\"简介\"><a href=\"#简介\" class=\"headerlink\" title=\"简介\"></a>简介</h2><p>Markdown 的目标是实现「易读易写」。</p>\n<h2 id=\"表格\"><a href=\"#表格\" class=\"headerlink\" title=\"表格\"></a>表格</h2><p>实现表格的两种方式<br>方式一：当某项过长时，表格可能如下显示，不好看。<br>具体使用方式请看示例。<br>•    ——: 为右对齐。<br>•    :—— 为左对齐。<br>•    :——: 为居中对齐。<br>•    ——- 为使用默认居中对齐。<br>1.9.2 示例<br>|         属性项               |                    属性说明<br>|    ——: |    :——-:    |    :———   |    ——    |<br>|    组件名称    |    步骤的名字，这个名字在一个转换中必须是唯一的。    |<br>|    字段    |    指定字段名和排序方向(升序/降序);点击获取字段检索列表字段从输入流    |<br>|字段|指定排序的字段名。|<br>|升序|排序原则：升序或降序。如果选择升序，排序顺序将是：数字-&gt;英文-&gt;汉字，汉字是按照拼音排序的，也同样会按照声调排序。如果是多音字，只会取一个读音，无法根据语境判断其的读音。|</p>\n<p>显示如下：  </p>\n<p>注意  </p>\n<ol>\n<li>每个Markdown解析器都不一样，可能左右居中对齐方式的表示方式不一样。  </li>\n</ol>\n<p>方式二：表格形式（推荐）    </p>\n<p><code>&lt;table&gt;\n    &lt;tr&gt;  \n        &lt;th&gt;属性项&lt;/th&gt;  \n        &lt;th&gt;属性说明&lt;/th&gt;  \n    &lt;/tr&gt;\n    &lt;tr&gt;\n        &lt;td&gt;组件名称&lt;/td&gt;\n        &lt;td&gt;步骤的名字，这个名字在一个转换中必须是唯一的。&lt;/td&gt;\n    &lt;/tr&gt;\n    &lt;tr&gt;\n        &lt;td&gt;字段&lt;/td&gt;\n        &lt;td&gt;指定字段名和排序方向(升序/降序);点击获取字段检索列表字段从输入流。&lt;/td&gt;\n    &lt;/tr&gt;\n    &lt;tr&gt;\n        &lt;td&gt;字段&lt;/td&gt;\n        &lt;td&gt;指定排序的字段名。&lt;/td&gt;\n    &lt;/tr&gt;\n    &lt;tr&gt;\n        &lt;td&gt;升序&lt;/td&gt;\n        &lt;td&gt;排序原则：升序或降序。如果选择升序，    \n        排序顺序将是：数字-&gt;英文-&gt;汉字，汉字是按照拼音排序的，    \n        也同样会按照声调排序。如果是多音字，只会取一个读音，    \n        无法根据语境判断其的读音。&lt;/td&gt;\n    &lt;/tr&gt;\n&lt;/table&gt;</code><br>输出如下：  </p>\n<table><br>    <tr><br>        <th>属性项</th><br>        <th>属性说明</th><br>    </tr><br>    <tr><br>        <td>组件名称</td><br>        <td>步骤的名字，这个名字在一个转换中必须是唯一的。</td><br>    </tr><br>    <tr><br>        <td>字段</td><br>        <td>指定字段名和排序方向(升序/降序);</td><br>    </tr><br>    <tr><br>        <td>字段</td><br>        <td>指定排序的字段名。</td><br>    </tr><br>    <tr><br>        <td>升序</td><br>        <td>排序原则：升序或降序。如果选择升<br>    </td></tr><br></table>\n\n\n\n\n<h2 id=\"首行缩进\"><a href=\"#首行缩进\" class=\"headerlink\" title=\"首行缩进\"></a>首行缩进</h2><p>由于markdown语法主要考虑的是英文，所以对于中文的首行缩进并不太友好，两种方法都可以完美解决这个问题。</p>\n<ul>\n<li>把输入法由半角改为全角。 两次空格之后就能够有两个汉字的缩进。  </li>\n</ul>\n<ul>\n<li>在开头的时候，先输入下面的代码，然后紧跟着输入文本即可。分号也不要掉。   </li>\n</ul>\n<p>直接写<br>半方大的空白<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">全方大的空白```&amp;emsp;```或```&amp;#8195;```  </span><br><span class=\"line\">不断行的空白格```&amp;nbsp;```或```&amp;#160;</span><br></pre></td></tr></table></figure></p>\n<p>做为显示时这几个转义不能单独写，要在前后添加<code>`</code></p>\n","site":{"data":{}},"excerpt":"","more":"<h2 id=\"简介\"><a href=\"#简介\" class=\"headerlink\" title=\"简介\"></a>简介</h2><p>Markdown 的目标是实现「易读易写」。</p>\n<h2 id=\"表格\"><a href=\"#表格\" class=\"headerlink\" title=\"表格\"></a>表格</h2><p>实现表格的两种方式<br>方式一：当某项过长时，表格可能如下显示，不好看。<br>具体使用方式请看示例。<br>•    ——: 为右对齐。<br>•    :—— 为左对齐。<br>•    :——: 为居中对齐。<br>•    ——- 为使用默认居中对齐。<br>1.9.2 示例<br>|         属性项               |                    属性说明<br>|    ——: |    :——-:    |    :———   |    ——    |<br>|    组件名称    |    步骤的名字，这个名字在一个转换中必须是唯一的。    |<br>|    字段    |    指定字段名和排序方向(升序/降序);点击获取字段检索列表字段从输入流    |<br>|字段|指定排序的字段名。|<br>|升序|排序原则：升序或降序。如果选择升序，排序顺序将是：数字-&gt;英文-&gt;汉字，汉字是按照拼音排序的，也同样会按照声调排序。如果是多音字，只会取一个读音，无法根据语境判断其的读音。|</p>\n<p>显示如下：  </p>\n<p>注意  </p>\n<ol>\n<li>每个Markdown解析器都不一样，可能左右居中对齐方式的表示方式不一样。  </li>\n</ol>\n<p>方式二：表格形式（推荐）    </p>\n<p><code>&lt;table&gt;\n    &lt;tr&gt;  \n        &lt;th&gt;属性项&lt;/th&gt;  \n        &lt;th&gt;属性说明&lt;/th&gt;  \n    &lt;/tr&gt;\n    &lt;tr&gt;\n        &lt;td&gt;组件名称&lt;/td&gt;\n        &lt;td&gt;步骤的名字，这个名字在一个转换中必须是唯一的。&lt;/td&gt;\n    &lt;/tr&gt;\n    &lt;tr&gt;\n        &lt;td&gt;字段&lt;/td&gt;\n        &lt;td&gt;指定字段名和排序方向(升序/降序);点击获取字段检索列表字段从输入流。&lt;/td&gt;\n    &lt;/tr&gt;\n    &lt;tr&gt;\n        &lt;td&gt;字段&lt;/td&gt;\n        &lt;td&gt;指定排序的字段名。&lt;/td&gt;\n    &lt;/tr&gt;\n    &lt;tr&gt;\n        &lt;td&gt;升序&lt;/td&gt;\n        &lt;td&gt;排序原则：升序或降序。如果选择升序，    \n        排序顺序将是：数字-&gt;英文-&gt;汉字，汉字是按照拼音排序的，    \n        也同样会按照声调排序。如果是多音字，只会取一个读音，    \n        无法根据语境判断其的读音。&lt;/td&gt;\n    &lt;/tr&gt;\n&lt;/table&gt;</code><br>输出如下：  </p>\n<table><br>    <tr><br>        <th>属性项</th><br>        <th>属性说明</th><br>    </tr><br>    <tr><br>        <td>组件名称</td><br>        <td>步骤的名字，这个名字在一个转换中必须是唯一的。</td><br>    </tr><br>    <tr><br>        <td>字段</td><br>        <td>指定字段名和排序方向(升序/降序);</td><br>    </tr><br>    <tr><br>        <td>字段</td><br>        <td>指定排序的字段名。</td><br>    </tr><br>    <tr><br>        <td>升序</td><br>        <td>排序原则：升序或降序。如果选择升<br>    </td></tr><br></table>\n\n\n\n\n<h2 id=\"首行缩进\"><a href=\"#首行缩进\" class=\"headerlink\" title=\"首行缩进\"></a>首行缩进</h2><p>由于markdown语法主要考虑的是英文，所以对于中文的首行缩进并不太友好，两种方法都可以完美解决这个问题。</p>\n<ul>\n<li>把输入法由半角改为全角。 两次空格之后就能够有两个汉字的缩进。  </li>\n</ul>\n<ul>\n<li>在开头的时候，先输入下面的代码，然后紧跟着输入文本即可。分号也不要掉。   </li>\n</ul>\n<p>直接写<br>半方大的空白<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">全方大的空白```&amp;emsp;```或```&amp;#8195;```  </span><br><span class=\"line\">不断行的空白格```&amp;nbsp;```或```&amp;#160;</span><br></pre></td></tr></table></figure></p>\n<p>做为显示时这几个转义不能单独写，要在前后添加<code>`</code></p>\n"},{"title":"JDK源码分析之集合框架HashMap","date":"2017-04-18T02:00:00.000Z","_content":"#JDK源码分析之集合框架HashMap\n","source":"_posts/java/JDK源码分析之集合框架HashMap.md","raw":"---\ntitle: JDK源码分析之集合框架HashMap\ndate: 2017-04-18 10:00:00\ntags: [java,jdk源码]\ncategories: [java,jdk源码]\n---\n#JDK源码分析之集合框架HashMap\n","slug":"java/JDK源码分析之集合框架HashMap","published":1,"updated":"2017-08-18T01:44:35.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjh7pols2000abp7runsnvvva","content":"<p>#JDK源码分析之集合框架HashMap</p>\n","site":{"data":{}},"excerpt":"","more":"<p>#JDK源码分析之集合框架HashMap</p>\n"},{"title":"Java基础之转型","date":"2016-05-20T06:43:49.000Z","_content":"上转型对象\n父类的声明指向子类对象（最无争议的说法）\n父类 对象名 = new 子类构造方法();\n(1)该对象可以调用子类重写父类的方法(本质)\n(2)该对象不能调用子类独有的方法\n(3)上转型对象可以强制转化成子类对象 (进而访问子类独有的方法)\n父类 :     Person\n子类 :Teacher    Student\nPerson person = new Teacher();\nStudnet student= (Student)person;\n在企业开发的时候,当别人给你传递一个对象的时候,如果对象的类型,不是很确定,要先测试一下，instanceof :java中的一个关键字,专门用来进行对象 类型的测试,跟强制类型转化,经常结合使用\n    if(person instanceof Student2){\n      Student2 p2 =(Student2) person;\n","source":"_posts/java/Java基础之转型.md","raw":"---\ntitle: Java基础之转型\ndate: 2016-05-20 14:43:49\ntags: [java,java基础]\ncategories: [java,java基础]\n---\n上转型对象\n父类的声明指向子类对象（最无争议的说法）\n父类 对象名 = new 子类构造方法();\n(1)该对象可以调用子类重写父类的方法(本质)\n(2)该对象不能调用子类独有的方法\n(3)上转型对象可以强制转化成子类对象 (进而访问子类独有的方法)\n父类 :     Person\n子类 :Teacher    Student\nPerson person = new Teacher();\nStudnet student= (Student)person;\n在企业开发的时候,当别人给你传递一个对象的时候,如果对象的类型,不是很确定,要先测试一下，instanceof :java中的一个关键字,专门用来进行对象 类型的测试,跟强制类型转化,经常结合使用\n    if(person instanceof Student2){\n      Student2 p2 =(Student2) person;\n","slug":"java/Java基础之转型","published":1,"updated":"2017-08-18T01:44:35.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjh7pols3000bbp7r95m8b3q2","content":"<p>上转型对象<br>父类的声明指向子类对象（最无争议的说法）<br>父类 对象名 = new 子类构造方法();<br>(1)该对象可以调用子类重写父类的方法(本质)<br>(2)该对象不能调用子类独有的方法<br>(3)上转型对象可以强制转化成子类对象 (进而访问子类独有的方法)<br>父类 :     Person<br>子类 :Teacher    Student<br>Person person = new Teacher();<br>Studnet student= (Student)person;<br>在企业开发的时候,当别人给你传递一个对象的时候,如果对象的类型,不是很确定,要先测试一下，instanceof :java中的一个关键字,专门用来进行对象 类型的测试,跟强制类型转化,经常结合使用<br>    if(person instanceof Student2){<br>      Student2 p2 =(Student2) person;</p>\n","site":{"data":{}},"excerpt":"","more":"<p>上转型对象<br>父类的声明指向子类对象（最无争议的说法）<br>父类 对象名 = new 子类构造方法();<br>(1)该对象可以调用子类重写父类的方法(本质)<br>(2)该对象不能调用子类独有的方法<br>(3)上转型对象可以强制转化成子类对象 (进而访问子类独有的方法)<br>父类 :     Person<br>子类 :Teacher    Student<br>Person person = new Teacher();<br>Studnet student= (Student)person;<br>在企业开发的时候,当别人给你传递一个对象的时候,如果对象的类型,不是很确定,要先测试一下，instanceof :java中的一个关键字,专门用来进行对象 类型的测试,跟强制类型转化,经常结合使用<br>    if(person instanceof Student2){<br>      Student2 p2 =(Student2) person;</p>\n"},{"title":"java中Object转String","date":"2016-05-20T06:41:37.000Z","_content":"Object转为String的几种形式\n 在java项目的实际开发和应用中，常常需要用到将对象转为String这一基本功能。本文将对常用的转换方法进行一个总结。常用的方法有Object.toString()，(String)要转换的对象，String.valueOf(Object)等。下面对这些方法一一进行分析。\n方法1：采用 Object.toString()方法\n请看下面的例子：\nObject object = getObject();\nSystem.out.println(object.toString());\n在这种使用方法中，因为java.lang.Object类里已有public方法.toString()，所以对任何严格意义上的java对象都可以调用此方法。但在使用时要注意，必须保证object不是null值，否则将抛出NullPointerException异常。采用这种方法时，通常派生类会覆盖Object里的toString()方法。\n方法2：采用类型转换(String)object方法\n这是标准的类型转换，将object转成String类型的值。使用这种方法时，需要注意的是类型必须能转成String类型。因此最好用instanceof做个类型检查，以判断是否可以转换。否则容易抛出CalssCastException异常。此外，需特别小心的是因定义为Object 类型的对象在转成String时语法检查并不会报错，这将可能导致潜在的错误存在。这时要格外小心。如：\nObject obj = new Integer(100);\nString　strVal = (String)obj;\n在运行时将会出错，因为将Integer类型强制转换为String类型，无法通过。但是，\nInteger obj = new Integer(100);\nString　strVal = (String)obj;\n如是格式代码，将会报语法错误。\n此外，因null值可以强制转换为任何java类类型，(String)null也是合法的。\n方法3：采用String.valueOf(Object)\nString.valueOf(Object)的基础是Object.toString()。但它与Object.toString()又有所不同。在前面方法1的分析中提到，使用第一种时需保证不为null。但采用第三种方法时，将不用担心object是否为null值这一问题。为了便于说明问题，我们来分析一下相关的源代码。Jdk里String.valueOf(Object)源码如下：\n/**\n* Returns the string representation of the Object argument.\n*\n* @param　 obj　 an Object.\n* @return　if the argument is null, then a string equal to\n*　　　　　\"null\"; otherwise, the value of\n*　　　　　obj.toString() is returned.\n* @see　　 java.lang.Object.toString()\n*/\npublic static String valueOf(Object obj) {\nreturn (obj == null) ? \"null\" : obj.toString();\n}\n从上面的源码可以很清晰的看出null值不用担心的理由。但是，这也恰恰给了我们隐患。我们应当注意到，当object为null时，String.valueOf(object)的值是字符串\"null\"，而不是null!在使用过程中切记要注意。试想一下，如果我们用 \n  if(String.valueOf(object)==null)\n{\n  System.out.println(“传入的值是null!\");\n}\n这样的语句将可能会发生什么问题。再想一下，向控制台输出时，在视觉上如下语句在执行的结果上有什么不同：\nSystem.out.println(String.valueOf(null));\nSystem.out.println(null);\n我们看到的输出将是一模一样的东西：null，但它们意义相同吗？\n判断一个字符串为空\n s为一个字符串，判断它为空的方法：\nif   (null==s ||\"\".equals(s))   {  \n......\n  }   \n注意：这里的null==s和\"\".equals(s)不要写成s==null和s.equals(s)，因为\"\"这个值是已经确定的，预知的，而s是未知的，所以用得不小心的时候s.equals(\"\")就会出现nullpoint异常。在这里虽然不会,因为前面有if(null==s),但是习惯跟在那里使用没有关系的。不一定的equals方法，包括其它很多处理，如果用确定的值处理问题会比未确定的处理少很多bug。\n\n来自 <http://www.cnblogs.com/sp2012/archive/2012/02/21/2465693.html> \n","source":"_posts/java/java中Object转String.md","raw":"---\ntitle: java中Object转String\ndate: 2016-05-20 14:41:37\ntags: [java,java基础]\ncategories: [java,java基础]\n---\nObject转为String的几种形式\n 在java项目的实际开发和应用中，常常需要用到将对象转为String这一基本功能。本文将对常用的转换方法进行一个总结。常用的方法有Object.toString()，(String)要转换的对象，String.valueOf(Object)等。下面对这些方法一一进行分析。\n方法1：采用 Object.toString()方法\n请看下面的例子：\nObject object = getObject();\nSystem.out.println(object.toString());\n在这种使用方法中，因为java.lang.Object类里已有public方法.toString()，所以对任何严格意义上的java对象都可以调用此方法。但在使用时要注意，必须保证object不是null值，否则将抛出NullPointerException异常。采用这种方法时，通常派生类会覆盖Object里的toString()方法。\n方法2：采用类型转换(String)object方法\n这是标准的类型转换，将object转成String类型的值。使用这种方法时，需要注意的是类型必须能转成String类型。因此最好用instanceof做个类型检查，以判断是否可以转换。否则容易抛出CalssCastException异常。此外，需特别小心的是因定义为Object 类型的对象在转成String时语法检查并不会报错，这将可能导致潜在的错误存在。这时要格外小心。如：\nObject obj = new Integer(100);\nString　strVal = (String)obj;\n在运行时将会出错，因为将Integer类型强制转换为String类型，无法通过。但是，\nInteger obj = new Integer(100);\nString　strVal = (String)obj;\n如是格式代码，将会报语法错误。\n此外，因null值可以强制转换为任何java类类型，(String)null也是合法的。\n方法3：采用String.valueOf(Object)\nString.valueOf(Object)的基础是Object.toString()。但它与Object.toString()又有所不同。在前面方法1的分析中提到，使用第一种时需保证不为null。但采用第三种方法时，将不用担心object是否为null值这一问题。为了便于说明问题，我们来分析一下相关的源代码。Jdk里String.valueOf(Object)源码如下：\n/**\n* Returns the string representation of the Object argument.\n*\n* @param　 obj　 an Object.\n* @return　if the argument is null, then a string equal to\n*　　　　　\"null\"; otherwise, the value of\n*　　　　　obj.toString() is returned.\n* @see　　 java.lang.Object.toString()\n*/\npublic static String valueOf(Object obj) {\nreturn (obj == null) ? \"null\" : obj.toString();\n}\n从上面的源码可以很清晰的看出null值不用担心的理由。但是，这也恰恰给了我们隐患。我们应当注意到，当object为null时，String.valueOf(object)的值是字符串\"null\"，而不是null!在使用过程中切记要注意。试想一下，如果我们用 \n  if(String.valueOf(object)==null)\n{\n  System.out.println(“传入的值是null!\");\n}\n这样的语句将可能会发生什么问题。再想一下，向控制台输出时，在视觉上如下语句在执行的结果上有什么不同：\nSystem.out.println(String.valueOf(null));\nSystem.out.println(null);\n我们看到的输出将是一模一样的东西：null，但它们意义相同吗？\n判断一个字符串为空\n s为一个字符串，判断它为空的方法：\nif   (null==s ||\"\".equals(s))   {  \n......\n  }   \n注意：这里的null==s和\"\".equals(s)不要写成s==null和s.equals(s)，因为\"\"这个值是已经确定的，预知的，而s是未知的，所以用得不小心的时候s.equals(\"\")就会出现nullpoint异常。在这里虽然不会,因为前面有if(null==s),但是习惯跟在那里使用没有关系的。不一定的equals方法，包括其它很多处理，如果用确定的值处理问题会比未确定的处理少很多bug。\n\n来自 <http://www.cnblogs.com/sp2012/archive/2012/02/21/2465693.html> \n","slug":"java/java中Object转String","published":1,"updated":"2017-08-18T01:44:35.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjh7pols6000ebp7rkfdhe3ml","content":"<p>Object转为String的几种形式<br> 在java项目的实际开发和应用中，常常需要用到将对象转为String这一基本功能。本文将对常用的转换方法进行一个总结。常用的方法有Object.toString()，(String)要转换的对象，String.valueOf(Object)等。下面对这些方法一一进行分析。<br>方法1：采用 Object.toString()方法<br>请看下面的例子：<br>Object object = getObject();<br>System.out.println(object.toString());<br>在这种使用方法中，因为java.lang.Object类里已有public方法.toString()，所以对任何严格意义上的java对象都可以调用此方法。但在使用时要注意，必须保证object不是null值，否则将抛出NullPointerException异常。采用这种方法时，通常派生类会覆盖Object里的toString()方法。<br>方法2：采用类型转换(String)object方法<br>这是标准的类型转换，将object转成String类型的值。使用这种方法时，需要注意的是类型必须能转成String类型。因此最好用instanceof做个类型检查，以判断是否可以转换。否则容易抛出CalssCastException异常。此外，需特别小心的是因定义为Object 类型的对象在转成String时语法检查并不会报错，这将可能导致潜在的错误存在。这时要格外小心。如：<br>Object obj = new Integer(100);<br>String　strVal = (String)obj;<br>在运行时将会出错，因为将Integer类型强制转换为String类型，无法通过。但是，<br>Integer obj = new Integer(100);<br>String　strVal = (String)obj;<br>如是格式代码，将会报语法错误。<br>此外，因null值可以强制转换为任何java类类型，(String)null也是合法的。<br>方法3：采用String.valueOf(Object)<br>String.valueOf(Object)的基础是Object.toString()。但它与Object.toString()又有所不同。在前面方法1的分析中提到，使用第一种时需保证不为null。但采用第三种方法时，将不用担心object是否为null值这一问题。为了便于说明问题，我们来分析一下相关的源代码。Jdk里String.valueOf(Object)源码如下：<br>/**</p>\n<ul>\n<li>Returns the string representation of the Object argument.<br>*</li>\n<li>@param　 obj　 an Object.</li>\n<li>@return　if the argument is null, then a string equal to<br><em>　　　　　“null”; otherwise, the value of\n</em>　　　　　obj.toString() is returned.</li>\n<li>@see　　 java.lang.Object.toString()<br>*/<br>public static String valueOf(Object obj) {<br>return (obj == null) ? “null” : obj.toString();<br>}<br>从上面的源码可以很清晰的看出null值不用担心的理由。但是，这也恰恰给了我们隐患。我们应当注意到，当object为null时，String.valueOf(object)的值是字符串”null”，而不是null!在使用过程中切记要注意。试想一下，如果我们用<br>if(String.valueOf(object)==null)<br>{<br>System.out.println(“传入的值是null!”);<br>}<br>这样的语句将可能会发生什么问题。再想一下，向控制台输出时，在视觉上如下语句在执行的结果上有什么不同：<br>System.out.println(String.valueOf(null));<br>System.out.println(null);<br>我们看到的输出将是一模一样的东西：null，但它们意义相同吗？<br>判断一个字符串为空<br>s为一个字符串，判断它为空的方法：<br>if   (null==s ||””.equals(s))   {<br>……<br>}<br>注意：这里的null==s和””.equals(s)不要写成s==null和s.equals(s)，因为””这个值是已经确定的，预知的，而s是未知的，所以用得不小心的时候s.equals(“”)就会出现nullpoint异常。在这里虽然不会,因为前面有if(null==s),但是习惯跟在那里使用没有关系的。不一定的equals方法，包括其它很多处理，如果用确定的值处理问题会比未确定的处理少很多bug。</li>\n</ul>\n<p>来自 <a href=\"http://www.cnblogs.com/sp2012/archive/2012/02/21/2465693.html\" target=\"_blank\" rel=\"noopener\">http://www.cnblogs.com/sp2012/archive/2012/02/21/2465693.html</a> </p>\n","site":{"data":{}},"excerpt":"","more":"<p>Object转为String的几种形式<br> 在java项目的实际开发和应用中，常常需要用到将对象转为String这一基本功能。本文将对常用的转换方法进行一个总结。常用的方法有Object.toString()，(String)要转换的对象，String.valueOf(Object)等。下面对这些方法一一进行分析。<br>方法1：采用 Object.toString()方法<br>请看下面的例子：<br>Object object = getObject();<br>System.out.println(object.toString());<br>在这种使用方法中，因为java.lang.Object类里已有public方法.toString()，所以对任何严格意义上的java对象都可以调用此方法。但在使用时要注意，必须保证object不是null值，否则将抛出NullPointerException异常。采用这种方法时，通常派生类会覆盖Object里的toString()方法。<br>方法2：采用类型转换(String)object方法<br>这是标准的类型转换，将object转成String类型的值。使用这种方法时，需要注意的是类型必须能转成String类型。因此最好用instanceof做个类型检查，以判断是否可以转换。否则容易抛出CalssCastException异常。此外，需特别小心的是因定义为Object 类型的对象在转成String时语法检查并不会报错，这将可能导致潜在的错误存在。这时要格外小心。如：<br>Object obj = new Integer(100);<br>String　strVal = (String)obj;<br>在运行时将会出错，因为将Integer类型强制转换为String类型，无法通过。但是，<br>Integer obj = new Integer(100);<br>String　strVal = (String)obj;<br>如是格式代码，将会报语法错误。<br>此外，因null值可以强制转换为任何java类类型，(String)null也是合法的。<br>方法3：采用String.valueOf(Object)<br>String.valueOf(Object)的基础是Object.toString()。但它与Object.toString()又有所不同。在前面方法1的分析中提到，使用第一种时需保证不为null。但采用第三种方法时，将不用担心object是否为null值这一问题。为了便于说明问题，我们来分析一下相关的源代码。Jdk里String.valueOf(Object)源码如下：<br>/**</p>\n<ul>\n<li>Returns the string representation of the Object argument.<br>*</li>\n<li>@param　 obj　 an Object.</li>\n<li>@return　if the argument is null, then a string equal to<br><em>　　　　　“null”; otherwise, the value of\n</em>　　　　　obj.toString() is returned.</li>\n<li>@see　　 java.lang.Object.toString()<br>*/<br>public static String valueOf(Object obj) {<br>return (obj == null) ? “null” : obj.toString();<br>}<br>从上面的源码可以很清晰的看出null值不用担心的理由。但是，这也恰恰给了我们隐患。我们应当注意到，当object为null时，String.valueOf(object)的值是字符串”null”，而不是null!在使用过程中切记要注意。试想一下，如果我们用<br>if(String.valueOf(object)==null)<br>{<br>System.out.println(“传入的值是null!”);<br>}<br>这样的语句将可能会发生什么问题。再想一下，向控制台输出时，在视觉上如下语句在执行的结果上有什么不同：<br>System.out.println(String.valueOf(null));<br>System.out.println(null);<br>我们看到的输出将是一模一样的东西：null，但它们意义相同吗？<br>判断一个字符串为空<br>s为一个字符串，判断它为空的方法：<br>if   (null==s ||””.equals(s))   {<br>……<br>}<br>注意：这里的null==s和””.equals(s)不要写成s==null和s.equals(s)，因为””这个值是已经确定的，预知的，而s是未知的，所以用得不小心的时候s.equals(“”)就会出现nullpoint异常。在这里虽然不会,因为前面有if(null==s),但是习惯跟在那里使用没有关系的。不一定的equals方法，包括其它很多处理，如果用确定的值处理问题会比未确定的处理少很多bug。</li>\n</ul>\n<p>来自 <a href=\"http://www.cnblogs.com/sp2012/archive/2012/02/21/2465693.html\" target=\"_blank\" rel=\"noopener\">http://www.cnblogs.com/sp2012/archive/2012/02/21/2465693.html</a> </p>\n"},{"title":"jdk环境变量配置","date":"2017-04-18T08:43:49.000Z","_content":"# jdk环境变量配置 \n\njdk环境变量配置 \n进行java开发，首先要安装jdk，安装了jdk后还要进行环境变量配置：\n1、下载jdk（http://java.sun.com/javase/downloads/index.jsp），我下载的版本是：jdk-6u14-windows-i586.exe\n2、安装jdk-6u14-windows-i586.exe\n3、配置环境变量：右击“我的电脑”-->\"高级\"-->\"环境变量\"\n1）在系统变量里新建JAVA_HOME变量，变量值为：C:\\Program Files\\Java\\jdk1.6.0_14（根据自己的安装路径填写）\n2）新建classpath变量，变量值为：.;%JAVA_HOME%\\lib;%JAVA_HOME%\\lib\\tools.jar\n3）在path变量（已存在不用新建）添加变量值：%JAVA_HOME%\\bin;%JAVA_HOME%\\jre\\bin（注意变量值之间用“;”隔开）\n4、“开始”-->“运行”-->输入“javac”-->\"Enter\"，如果能正常打印用法说明配置成功！\n补充环境变量的解析:\nJAVA_HOME:jdk的安装路径\nclasspath:java加载类路径，只有类在classpath中java命令才能识别，在路径前加了个\".\"表示当前路径。\npath：系统在任何路径下都可以识别java,javac命令。\n\n\n\n","source":"_posts/java/jdk环境变量配置.md","raw":"---\ntitle: jdk环境变量配置 \ndate: 2017-04-18 16:43:49\ntags: [java]\ncategories: [java]\n---\n# jdk环境变量配置 \n\njdk环境变量配置 \n进行java开发，首先要安装jdk，安装了jdk后还要进行环境变量配置：\n1、下载jdk（http://java.sun.com/javase/downloads/index.jsp），我下载的版本是：jdk-6u14-windows-i586.exe\n2、安装jdk-6u14-windows-i586.exe\n3、配置环境变量：右击“我的电脑”-->\"高级\"-->\"环境变量\"\n1）在系统变量里新建JAVA_HOME变量，变量值为：C:\\Program Files\\Java\\jdk1.6.0_14（根据自己的安装路径填写）\n2）新建classpath变量，变量值为：.;%JAVA_HOME%\\lib;%JAVA_HOME%\\lib\\tools.jar\n3）在path变量（已存在不用新建）添加变量值：%JAVA_HOME%\\bin;%JAVA_HOME%\\jre\\bin（注意变量值之间用“;”隔开）\n4、“开始”-->“运行”-->输入“javac”-->\"Enter\"，如果能正常打印用法说明配置成功！\n补充环境变量的解析:\nJAVA_HOME:jdk的安装路径\nclasspath:java加载类路径，只有类在classpath中java命令才能识别，在路径前加了个\".\"表示当前路径。\npath：系统在任何路径下都可以识别java,javac命令。\n\n\n\n","slug":"java/jdk环境变量配置","published":1,"updated":"2017-08-18T01:44:35.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjh7pols7000fbp7rfu1fboid","content":"<h1 id=\"jdk环境变量配置\"><a href=\"#jdk环境变量配置\" class=\"headerlink\" title=\"jdk环境变量配置\"></a>jdk环境变量配置</h1><p>jdk环境变量配置<br>进行java开发，首先要安装jdk，安装了jdk后还要进行环境变量配置：<br>1、下载jdk（<a href=\"http://java.sun.com/javase/downloads/index.jsp），我下载的版本是：jdk-6u14-windows-i586.exe\" target=\"_blank\" rel=\"noopener\">http://java.sun.com/javase/downloads/index.jsp），我下载的版本是：jdk-6u14-windows-i586.exe</a><br>2、安装jdk-6u14-windows-i586.exe<br>3、配置环境变量：右击“我的电脑”–&gt;”高级”–&gt;”环境变量”<br>1）在系统变量里新建JAVA_HOME变量，变量值为：C:\\Program Files\\Java\\jdk1.6.0_14（根据自己的安装路径填写）<br>2）新建classpath变量，变量值为：.;%JAVA_HOME%\\lib;%JAVA_HOME%\\lib\\tools.jar<br>3）在path变量（已存在不用新建）添加变量值：%JAVA_HOME%\\bin;%JAVA_HOME%\\jre\\bin（注意变量值之间用“;”隔开）<br>4、“开始”–&gt;“运行”–&gt;输入“javac”–&gt;”Enter”，如果能正常打印用法说明配置成功！<br>补充环境变量的解析:<br>JAVA_HOME:jdk的安装路径<br>classpath:java加载类路径，只有类在classpath中java命令才能识别，在路径前加了个”.”表示当前路径。<br>path：系统在任何路径下都可以识别java,javac命令。</p>\n","site":{"data":{}},"excerpt":"","more":"<h1 id=\"jdk环境变量配置\"><a href=\"#jdk环境变量配置\" class=\"headerlink\" title=\"jdk环境变量配置\"></a>jdk环境变量配置</h1><p>jdk环境变量配置<br>进行java开发，首先要安装jdk，安装了jdk后还要进行环境变量配置：<br>1、下载jdk（<a href=\"http://java.sun.com/javase/downloads/index.jsp），我下载的版本是：jdk-6u14-windows-i586.exe\" target=\"_blank\" rel=\"noopener\">http://java.sun.com/javase/downloads/index.jsp），我下载的版本是：jdk-6u14-windows-i586.exe</a><br>2、安装jdk-6u14-windows-i586.exe<br>3、配置环境变量：右击“我的电脑”–&gt;”高级”–&gt;”环境变量”<br>1）在系统变量里新建JAVA_HOME变量，变量值为：C:\\Program Files\\Java\\jdk1.6.0_14（根据自己的安装路径填写）<br>2）新建classpath变量，变量值为：.;%JAVA_HOME%\\lib;%JAVA_HOME%\\lib\\tools.jar<br>3）在path变量（已存在不用新建）添加变量值：%JAVA_HOME%\\bin;%JAVA_HOME%\\jre\\bin（注意变量值之间用“;”隔开）<br>4、“开始”–&gt;“运行”–&gt;输入“javac”–&gt;”Enter”，如果能正常打印用法说明配置成功！<br>补充环境变量的解析:<br>JAVA_HOME:jdk的安装路径<br>classpath:java加载类路径，只有类在classpath中java命令才能识别，在路径前加了个”.”表示当前路径。<br>path：系统在任何路径下都可以识别java,javac命令。</p>\n"},{"title":"消息传送基础","date":"2016-05-21T15:43:49.000Z","_content":"# 消息传送基础\n\n## 消息传送模型\n\tJMS支持两种消息传送模型：点对点模型和发布/订阅模型。有时候，又称这些消息传送模型为消息传送域。点对点消息传送模型和发布/订阅消息传送模型经常分别缩写为p2p和Pub/Sub。\n\n\t从JMS的视角来看，消息传送客户端称为JMS客户端(JMS Client)，而消息传送系统则称为JMS提供者（JMS provider）。一个JMS应用程序是由多个JMS客户端和（通常是）一个JMS提供者所组成的业务系统。\n\t此外，生产消息的JMS客户端称为消息生产者（message produceer），而接收消息的JMS客户端则称为消息消费者(message consumer)。一个JMS客户端可以既是消息生产者又是消息消费者。\n### 点对点模型\n\t点对点消息传送模型允许JMS客户端通过队列（queue）这个虚拟通道来同步和异步发送、接收消息。在点对点模型中，消息生产者称为发送者（Sender），而消息消费者则称为接收者（receiver）。传统上，点对点模型是一个基于拉取(Pull)或基于轮询(polling)的消息传送模型，这种模型从队列中请求消息，而不是自动地将消息推送到客户端。点对点消息传递模型的一个突出特点就是：发送到队列的消息被一个而且仅仅一个接收者所接收，即使可能有多个接收者在一个队列中侦听同一消息时，也是如此。\n\t点对点消息传送模型既支持异步“即发即弃（fire and forget）”消息传送方式，又支持同步请求/应答消息传送方式。点对点消息传送模型比发布订阅模型具有更强的耦合性，发磅者通常会知道消息初如何使用，而且也会知道谁将接收该消息。举例来说，发送者可能会向一个队列发送一个证券交易订单并等待响应，响应中应包含一个交易确认码。这样一来，消息发送者就会知道消息接收者将来处理交易订单。另一个例子就是一个生成长时间运行报告的异步请求。发送者发出报告请求，而当该报告准备就绪时，就会给发送者发送一条通知消息。在这种情况下，发送者就会知道消息接收者将要处理该消息并创建报告。\n\t点对点模型支持负载均衡，它允许多个接收者侦听同一个队列，并以此来分配负载。JMS规范没有规定在多个接收者中间分发消息的规则，尽管某些JMS厂商已经选择实现些规则来提升负载均衡能力。点对点模型还具有其他优点，比如说，队列浏览器允许客户端在消费龅牙消息之前查看队列内容—--在发布订阅模型中，并没有这样浏览器的概念。\n### 发布/订阅模型\n\t在发布订阅模型中，消息会被发布到一个名为主题（topic）的虚拟通道中。消息生产者称为发布者(publisher)，而消息消费者则称为订阅者（subscriber）。与点对点模型不同，使用发布订阅模型发布一个主题的消息，能够由多个订阅者所接收。有时候，也称这项技术为广播(broadcasting)消息。每个订阅者都会接收到每条消息的一个副本。总地来说，发布订阅消息传送模型基本上是一个基于推送(push)的模型，其中消息自动地向消费者广播，它们无须请求或轮询主题来获得新消息。\n\t发布订阅模型的去耦能力要比p2p模型更强，消息发布者通常不会意识到有多少订阅者或那些订阅者如何处理这些消息。举例来说，假定每次在Java应用程序发生异常时，向一个主题发布一条消息。发布者的责任仅仅是广播发生了一个异常。该发布者不会知道或者说通常也不关心如何使用该消息。例如，有可能是订阅者根据该异常向开发人员或支持人员发送一封电子邮件，也有可能是订阅者收集不同类型的异常数目用于生成报告，甚至是订阅者根据异常的类型，使用这个信息来通知随叫随到(on-call)的技术人员。\n\t在发布订阅消息传送模型内部，有多种不同类型的订阅者。非持久订阅者是临时订阅类型，它们只是在主动侦听主题时才接收消息。而另一方面，持久订阅者将接收发布的每条消息的一个副本，即便在发布消息，它们处于“离线’状态时也是如此。另外还有动态持久订阅者和受管的持久订阅者等类型。\n\n## Rpc和异步消息传送\n\tRPC(Remote Procedure Call，远程过程调用)是通常用于描述分布式计算模型的术语。现在Java和.Net这两种平台都在使用这个术语。基于组件的体系结构，比如企业级JavaBean（Enterprise JavaBeans，EJB），就是建立在这个模型基础之上的。对于许多应用程序来说，基于RPC的技术已经是，并且将继续是切实可行的解决方案。不过，企业消息传送模型在特定类型的分布式应用程序中表现更为出色。在本节中，我们将讨论每种模型的优缺点。\n### 紧密耦合的rpc\n\t紧密耦合的RPC模型最为成功的一个领域就是构建3层或n层应用程序。在这个模型中，表示层（第1层）使用RPC和中间层（第2层）的业务逻辑进行通信，访问位于后端（第3层）的数据。Sun Microsystems公司的J2EE平台和Microsoft公司的.NET平台是这种体系结构最为先进的范例。\n\t使用J2EE、JSP和Servlet技术的表示层，而企业级JavaBean（EJB）则是中间层。抛开平台不论，这些系统使用的核心技术是基于成为定义通信范例的RPC的中间件。\n\tRPC试图模仿在一个进程中运行的某个系统的行为。在调用一个远程过程时，调用者将被阻塞，直到该过程完成并将控制权返回给调用者。从开发者的角度看，这种同步模型使得该系统就好像运行在一个进程当中。这些工作会依次完成，同时确保以顺序完成。RPC同步的本质特性，将客户端（进行调用的软件）和服务器（为该调用服务的软件）二者紧密耦合在一起。因为客户端已被阻塞，所以它无法继续进行工作。直到服务器做出响应为止。\n\tRPC紧密耦合的本质特性导致出现了相互高度依赖的系统，其中一个系统的失效会对其它系统产生立竿见影的弱化影响。例如，在J2EE中，如果期望使用企业级bean的servlet顺利工作，EJB服务器就必须正常地发挥功能。\n\t虽然RPC在许多场景中表现优秀，但是在系统对系统的处理过程当中，它的同步、紧密耦合等本质却是一个严重的缺陷，因为”系统对系统“有很多垂直的应用程序集成在一起。在系统对系统场景中，垂直系统之间的通信线程不仅数量众多、而且方向也是错综复杂，如下图：\n \n\t让我们设想一下使用紧密救命的RPC机制实现这种基础设施所面临的挑战。这些系统之间的连接管理是多对多的问题。当您身混合系统中加入另一个应用程序时，您不得不回过头来让其余所有的系统都知道它，而且，这些系统也会崩溃(crash)。它们仍然需要预定停工时间，而且对象的接口也需要升级。\n\t当该系统的一部分中断运行时，一切都停步。当您向一个订单输入系统添加订单时，它要对其他系统逐个进行同步调用。这会导致订单输入系统发生阻塞，并一直等待，直到每个系统都处理完成订单时为止。\n\t\n\t正是的PC系统的同步、紧密耦合、相互依赖等本质特性，便得子系统中出现的故障最终会导致整个系统的失效。就像在”系统对系统“场景中那样，当RPC紧密耦合的本质特性不再适用时，消息传送机制为此提供了另一种选择方案。\n### 企业消息传送\n\t各个子系统在可用性方面存在的问题，并不是使用面向消息的中间件所事业来的后果。消息传送机制的一个基本思想就是：规定应用程序之间的通信应该采用异步方式。将各部分连接在一些的代码会假定这是一条单身消息，它不需要立即从另一个应用程序那里得到响应。换句话说，它不必等待对这条消息的响应。这是RPC和异步消息传送之间的主要区别，而且，它对于 理解消息传送系统的优点来说至关重要。\n\t在一个异步消息传送系统当中，每个子系统（收款、存货等）都不存在和其他系统的耦合。它们通过消息传送服务顺进行通信，因此，某个子系统出现故障，并不会妨碍其他子系统的运行。如下图：\n\n\t在网络化系统中会出现局部故障，这是一个不可避免的事实。其中的一个系统，可能会在其连接运行期间的某个时刻，发生不可预测的故障，或者需要停机。这种现象可能会由于内部系统和合作系统地理上的分散而被进一步放大。考虑到这个因素，JMS提供了保证传送（guaranteed delivery）方式，它可以确保即便发生了局部故障，预定消费者最终也会接收到这条消息。\n\t保证传送使用的是一种”保存并转发（store-and-forward）”的机制，这就意味着，如果预定消费者当前并不可用，底层消息服务器就会将输入的消息写到一个持久存储器（persistent store）之中。随后，当该接收应用程序变为可用时，“保存并转发”机制会把预定消费者在不可用时错过的所有消息传送给它们。\n\n\t概括来说，JMS不仅仅是另外一种事件服务。它的设计涵盖了范围极广的企业应用程序，包括EAI、B2B和摄像头模型等。通过异步处理、“保存并转发”及“保证传送”机制，它为保持业务应用程序连续运行并实现不间断服务提供了很高的可用性。它还通过发布订阅功能和点对点功能，提供了集成灵活性。通过位置透明和管理控制，它提供了一种健壮的、基于服务的体系结构。而且，最重要的是，它非常易于学习和使用。\n","source":"_posts/java/消息传送基础.md","raw":"---\ntitle: 消息传送基础\ndate: 2016-05-21 23:43:49\ntags: [java,jms]\ncategories: [java,jms]\n---\n# 消息传送基础\n\n## 消息传送模型\n\tJMS支持两种消息传送模型：点对点模型和发布/订阅模型。有时候，又称这些消息传送模型为消息传送域。点对点消息传送模型和发布/订阅消息传送模型经常分别缩写为p2p和Pub/Sub。\n\n\t从JMS的视角来看，消息传送客户端称为JMS客户端(JMS Client)，而消息传送系统则称为JMS提供者（JMS provider）。一个JMS应用程序是由多个JMS客户端和（通常是）一个JMS提供者所组成的业务系统。\n\t此外，生产消息的JMS客户端称为消息生产者（message produceer），而接收消息的JMS客户端则称为消息消费者(message consumer)。一个JMS客户端可以既是消息生产者又是消息消费者。\n### 点对点模型\n\t点对点消息传送模型允许JMS客户端通过队列（queue）这个虚拟通道来同步和异步发送、接收消息。在点对点模型中，消息生产者称为发送者（Sender），而消息消费者则称为接收者（receiver）。传统上，点对点模型是一个基于拉取(Pull)或基于轮询(polling)的消息传送模型，这种模型从队列中请求消息，而不是自动地将消息推送到客户端。点对点消息传递模型的一个突出特点就是：发送到队列的消息被一个而且仅仅一个接收者所接收，即使可能有多个接收者在一个队列中侦听同一消息时，也是如此。\n\t点对点消息传送模型既支持异步“即发即弃（fire and forget）”消息传送方式，又支持同步请求/应答消息传送方式。点对点消息传送模型比发布订阅模型具有更强的耦合性，发磅者通常会知道消息初如何使用，而且也会知道谁将接收该消息。举例来说，发送者可能会向一个队列发送一个证券交易订单并等待响应，响应中应包含一个交易确认码。这样一来，消息发送者就会知道消息接收者将来处理交易订单。另一个例子就是一个生成长时间运行报告的异步请求。发送者发出报告请求，而当该报告准备就绪时，就会给发送者发送一条通知消息。在这种情况下，发送者就会知道消息接收者将要处理该消息并创建报告。\n\t点对点模型支持负载均衡，它允许多个接收者侦听同一个队列，并以此来分配负载。JMS规范没有规定在多个接收者中间分发消息的规则，尽管某些JMS厂商已经选择实现些规则来提升负载均衡能力。点对点模型还具有其他优点，比如说，队列浏览器允许客户端在消费龅牙消息之前查看队列内容—--在发布订阅模型中，并没有这样浏览器的概念。\n### 发布/订阅模型\n\t在发布订阅模型中，消息会被发布到一个名为主题（topic）的虚拟通道中。消息生产者称为发布者(publisher)，而消息消费者则称为订阅者（subscriber）。与点对点模型不同，使用发布订阅模型发布一个主题的消息，能够由多个订阅者所接收。有时候，也称这项技术为广播(broadcasting)消息。每个订阅者都会接收到每条消息的一个副本。总地来说，发布订阅消息传送模型基本上是一个基于推送(push)的模型，其中消息自动地向消费者广播，它们无须请求或轮询主题来获得新消息。\n\t发布订阅模型的去耦能力要比p2p模型更强，消息发布者通常不会意识到有多少订阅者或那些订阅者如何处理这些消息。举例来说，假定每次在Java应用程序发生异常时，向一个主题发布一条消息。发布者的责任仅仅是广播发生了一个异常。该发布者不会知道或者说通常也不关心如何使用该消息。例如，有可能是订阅者根据该异常向开发人员或支持人员发送一封电子邮件，也有可能是订阅者收集不同类型的异常数目用于生成报告，甚至是订阅者根据异常的类型，使用这个信息来通知随叫随到(on-call)的技术人员。\n\t在发布订阅消息传送模型内部，有多种不同类型的订阅者。非持久订阅者是临时订阅类型，它们只是在主动侦听主题时才接收消息。而另一方面，持久订阅者将接收发布的每条消息的一个副本，即便在发布消息，它们处于“离线’状态时也是如此。另外还有动态持久订阅者和受管的持久订阅者等类型。\n\n## Rpc和异步消息传送\n\tRPC(Remote Procedure Call，远程过程调用)是通常用于描述分布式计算模型的术语。现在Java和.Net这两种平台都在使用这个术语。基于组件的体系结构，比如企业级JavaBean（Enterprise JavaBeans，EJB），就是建立在这个模型基础之上的。对于许多应用程序来说，基于RPC的技术已经是，并且将继续是切实可行的解决方案。不过，企业消息传送模型在特定类型的分布式应用程序中表现更为出色。在本节中，我们将讨论每种模型的优缺点。\n### 紧密耦合的rpc\n\t紧密耦合的RPC模型最为成功的一个领域就是构建3层或n层应用程序。在这个模型中，表示层（第1层）使用RPC和中间层（第2层）的业务逻辑进行通信，访问位于后端（第3层）的数据。Sun Microsystems公司的J2EE平台和Microsoft公司的.NET平台是这种体系结构最为先进的范例。\n\t使用J2EE、JSP和Servlet技术的表示层，而企业级JavaBean（EJB）则是中间层。抛开平台不论，这些系统使用的核心技术是基于成为定义通信范例的RPC的中间件。\n\tRPC试图模仿在一个进程中运行的某个系统的行为。在调用一个远程过程时，调用者将被阻塞，直到该过程完成并将控制权返回给调用者。从开发者的角度看，这种同步模型使得该系统就好像运行在一个进程当中。这些工作会依次完成，同时确保以顺序完成。RPC同步的本质特性，将客户端（进行调用的软件）和服务器（为该调用服务的软件）二者紧密耦合在一起。因为客户端已被阻塞，所以它无法继续进行工作。直到服务器做出响应为止。\n\tRPC紧密耦合的本质特性导致出现了相互高度依赖的系统，其中一个系统的失效会对其它系统产生立竿见影的弱化影响。例如，在J2EE中，如果期望使用企业级bean的servlet顺利工作，EJB服务器就必须正常地发挥功能。\n\t虽然RPC在许多场景中表现优秀，但是在系统对系统的处理过程当中，它的同步、紧密耦合等本质却是一个严重的缺陷，因为”系统对系统“有很多垂直的应用程序集成在一起。在系统对系统场景中，垂直系统之间的通信线程不仅数量众多、而且方向也是错综复杂，如下图：\n \n\t让我们设想一下使用紧密救命的RPC机制实现这种基础设施所面临的挑战。这些系统之间的连接管理是多对多的问题。当您身混合系统中加入另一个应用程序时，您不得不回过头来让其余所有的系统都知道它，而且，这些系统也会崩溃(crash)。它们仍然需要预定停工时间，而且对象的接口也需要升级。\n\t当该系统的一部分中断运行时，一切都停步。当您向一个订单输入系统添加订单时，它要对其他系统逐个进行同步调用。这会导致订单输入系统发生阻塞，并一直等待，直到每个系统都处理完成订单时为止。\n\t\n\t正是的PC系统的同步、紧密耦合、相互依赖等本质特性，便得子系统中出现的故障最终会导致整个系统的失效。就像在”系统对系统“场景中那样，当RPC紧密耦合的本质特性不再适用时，消息传送机制为此提供了另一种选择方案。\n### 企业消息传送\n\t各个子系统在可用性方面存在的问题，并不是使用面向消息的中间件所事业来的后果。消息传送机制的一个基本思想就是：规定应用程序之间的通信应该采用异步方式。将各部分连接在一些的代码会假定这是一条单身消息，它不需要立即从另一个应用程序那里得到响应。换句话说，它不必等待对这条消息的响应。这是RPC和异步消息传送之间的主要区别，而且，它对于 理解消息传送系统的优点来说至关重要。\n\t在一个异步消息传送系统当中，每个子系统（收款、存货等）都不存在和其他系统的耦合。它们通过消息传送服务顺进行通信，因此，某个子系统出现故障，并不会妨碍其他子系统的运行。如下图：\n\n\t在网络化系统中会出现局部故障，这是一个不可避免的事实。其中的一个系统，可能会在其连接运行期间的某个时刻，发生不可预测的故障，或者需要停机。这种现象可能会由于内部系统和合作系统地理上的分散而被进一步放大。考虑到这个因素，JMS提供了保证传送（guaranteed delivery）方式，它可以确保即便发生了局部故障，预定消费者最终也会接收到这条消息。\n\t保证传送使用的是一种”保存并转发（store-and-forward）”的机制，这就意味着，如果预定消费者当前并不可用，底层消息服务器就会将输入的消息写到一个持久存储器（persistent store）之中。随后，当该接收应用程序变为可用时，“保存并转发”机制会把预定消费者在不可用时错过的所有消息传送给它们。\n\n\t概括来说，JMS不仅仅是另外一种事件服务。它的设计涵盖了范围极广的企业应用程序，包括EAI、B2B和摄像头模型等。通过异步处理、“保存并转发”及“保证传送”机制，它为保持业务应用程序连续运行并实现不间断服务提供了很高的可用性。它还通过发布订阅功能和点对点功能，提供了集成灵活性。通过位置透明和管理控制，它提供了一种健壮的、基于服务的体系结构。而且，最重要的是，它非常易于学习和使用。\n","slug":"java/消息传送基础","published":1,"updated":"2017-08-18T01:44:35.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjh7pols8000gbp7rx7qi8t2t","content":"<h1 id=\"消息传送基础\"><a href=\"#消息传送基础\" class=\"headerlink\" title=\"消息传送基础\"></a>消息传送基础</h1><h2 id=\"消息传送模型\"><a href=\"#消息传送模型\" class=\"headerlink\" title=\"消息传送模型\"></a>消息传送模型</h2><pre><code>JMS支持两种消息传送模型：点对点模型和发布/订阅模型。有时候，又称这些消息传送模型为消息传送域。点对点消息传送模型和发布/订阅消息传送模型经常分别缩写为p2p和Pub/Sub。\n\n从JMS的视角来看，消息传送客户端称为JMS客户端(JMS Client)，而消息传送系统则称为JMS提供者（JMS provider）。一个JMS应用程序是由多个JMS客户端和（通常是）一个JMS提供者所组成的业务系统。\n此外，生产消息的JMS客户端称为消息生产者（message produceer），而接收消息的JMS客户端则称为消息消费者(message consumer)。一个JMS客户端可以既是消息生产者又是消息消费者。\n</code></pre><h3 id=\"点对点模型\"><a href=\"#点对点模型\" class=\"headerlink\" title=\"点对点模型\"></a>点对点模型</h3><pre><code>点对点消息传送模型允许JMS客户端通过队列（queue）这个虚拟通道来同步和异步发送、接收消息。在点对点模型中，消息生产者称为发送者（Sender），而消息消费者则称为接收者（receiver）。传统上，点对点模型是一个基于拉取(Pull)或基于轮询(polling)的消息传送模型，这种模型从队列中请求消息，而不是自动地将消息推送到客户端。点对点消息传递模型的一个突出特点就是：发送到队列的消息被一个而且仅仅一个接收者所接收，即使可能有多个接收者在一个队列中侦听同一消息时，也是如此。\n点对点消息传送模型既支持异步“即发即弃（fire and forget）”消息传送方式，又支持同步请求/应答消息传送方式。点对点消息传送模型比发布订阅模型具有更强的耦合性，发磅者通常会知道消息初如何使用，而且也会知道谁将接收该消息。举例来说，发送者可能会向一个队列发送一个证券交易订单并等待响应，响应中应包含一个交易确认码。这样一来，消息发送者就会知道消息接收者将来处理交易订单。另一个例子就是一个生成长时间运行报告的异步请求。发送者发出报告请求，而当该报告准备就绪时，就会给发送者发送一条通知消息。在这种情况下，发送者就会知道消息接收者将要处理该消息并创建报告。\n点对点模型支持负载均衡，它允许多个接收者侦听同一个队列，并以此来分配负载。JMS规范没有规定在多个接收者中间分发消息的规则，尽管某些JMS厂商已经选择实现些规则来提升负载均衡能力。点对点模型还具有其他优点，比如说，队列浏览器允许客户端在消费龅牙消息之前查看队列内容—--在发布订阅模型中，并没有这样浏览器的概念。\n</code></pre><h3 id=\"发布-订阅模型\"><a href=\"#发布-订阅模型\" class=\"headerlink\" title=\"发布/订阅模型\"></a>发布/订阅模型</h3><pre><code>在发布订阅模型中，消息会被发布到一个名为主题（topic）的虚拟通道中。消息生产者称为发布者(publisher)，而消息消费者则称为订阅者（subscriber）。与点对点模型不同，使用发布订阅模型发布一个主题的消息，能够由多个订阅者所接收。有时候，也称这项技术为广播(broadcasting)消息。每个订阅者都会接收到每条消息的一个副本。总地来说，发布订阅消息传送模型基本上是一个基于推送(push)的模型，其中消息自动地向消费者广播，它们无须请求或轮询主题来获得新消息。\n发布订阅模型的去耦能力要比p2p模型更强，消息发布者通常不会意识到有多少订阅者或那些订阅者如何处理这些消息。举例来说，假定每次在Java应用程序发生异常时，向一个主题发布一条消息。发布者的责任仅仅是广播发生了一个异常。该发布者不会知道或者说通常也不关心如何使用该消息。例如，有可能是订阅者根据该异常向开发人员或支持人员发送一封电子邮件，也有可能是订阅者收集不同类型的异常数目用于生成报告，甚至是订阅者根据异常的类型，使用这个信息来通知随叫随到(on-call)的技术人员。\n在发布订阅消息传送模型内部，有多种不同类型的订阅者。非持久订阅者是临时订阅类型，它们只是在主动侦听主题时才接收消息。而另一方面，持久订阅者将接收发布的每条消息的一个副本，即便在发布消息，它们处于“离线’状态时也是如此。另外还有动态持久订阅者和受管的持久订阅者等类型。\n</code></pre><h2 id=\"Rpc和异步消息传送\"><a href=\"#Rpc和异步消息传送\" class=\"headerlink\" title=\"Rpc和异步消息传送\"></a>Rpc和异步消息传送</h2><pre><code>RPC(Remote Procedure Call，远程过程调用)是通常用于描述分布式计算模型的术语。现在Java和.Net这两种平台都在使用这个术语。基于组件的体系结构，比如企业级JavaBean（Enterprise JavaBeans，EJB），就是建立在这个模型基础之上的。对于许多应用程序来说，基于RPC的技术已经是，并且将继续是切实可行的解决方案。不过，企业消息传送模型在特定类型的分布式应用程序中表现更为出色。在本节中，我们将讨论每种模型的优缺点。\n</code></pre><h3 id=\"紧密耦合的rpc\"><a href=\"#紧密耦合的rpc\" class=\"headerlink\" title=\"紧密耦合的rpc\"></a>紧密耦合的rpc</h3><pre><code>紧密耦合的RPC模型最为成功的一个领域就是构建3层或n层应用程序。在这个模型中，表示层（第1层）使用RPC和中间层（第2层）的业务逻辑进行通信，访问位于后端（第3层）的数据。Sun Microsystems公司的J2EE平台和Microsoft公司的.NET平台是这种体系结构最为先进的范例。\n使用J2EE、JSP和Servlet技术的表示层，而企业级JavaBean（EJB）则是中间层。抛开平台不论，这些系统使用的核心技术是基于成为定义通信范例的RPC的中间件。\nRPC试图模仿在一个进程中运行的某个系统的行为。在调用一个远程过程时，调用者将被阻塞，直到该过程完成并将控制权返回给调用者。从开发者的角度看，这种同步模型使得该系统就好像运行在一个进程当中。这些工作会依次完成，同时确保以顺序完成。RPC同步的本质特性，将客户端（进行调用的软件）和服务器（为该调用服务的软件）二者紧密耦合在一起。因为客户端已被阻塞，所以它无法继续进行工作。直到服务器做出响应为止。\nRPC紧密耦合的本质特性导致出现了相互高度依赖的系统，其中一个系统的失效会对其它系统产生立竿见影的弱化影响。例如，在J2EE中，如果期望使用企业级bean的servlet顺利工作，EJB服务器就必须正常地发挥功能。\n虽然RPC在许多场景中表现优秀，但是在系统对系统的处理过程当中，它的同步、紧密耦合等本质却是一个严重的缺陷，因为”系统对系统“有很多垂直的应用程序集成在一起。在系统对系统场景中，垂直系统之间的通信线程不仅数量众多、而且方向也是错综复杂，如下图：\n\n让我们设想一下使用紧密救命的RPC机制实现这种基础设施所面临的挑战。这些系统之间的连接管理是多对多的问题。当您身混合系统中加入另一个应用程序时，您不得不回过头来让其余所有的系统都知道它，而且，这些系统也会崩溃(crash)。它们仍然需要预定停工时间，而且对象的接口也需要升级。\n当该系统的一部分中断运行时，一切都停步。当您向一个订单输入系统添加订单时，它要对其他系统逐个进行同步调用。这会导致订单输入系统发生阻塞，并一直等待，直到每个系统都处理完成订单时为止。\n\n正是的PC系统的同步、紧密耦合、相互依赖等本质特性，便得子系统中出现的故障最终会导致整个系统的失效。就像在”系统对系统“场景中那样，当RPC紧密耦合的本质特性不再适用时，消息传送机制为此提供了另一种选择方案。\n</code></pre><h3 id=\"企业消息传送\"><a href=\"#企业消息传送\" class=\"headerlink\" title=\"企业消息传送\"></a>企业消息传送</h3><pre><code>各个子系统在可用性方面存在的问题，并不是使用面向消息的中间件所事业来的后果。消息传送机制的一个基本思想就是：规定应用程序之间的通信应该采用异步方式。将各部分连接在一些的代码会假定这是一条单身消息，它不需要立即从另一个应用程序那里得到响应。换句话说，它不必等待对这条消息的响应。这是RPC和异步消息传送之间的主要区别，而且，它对于 理解消息传送系统的优点来说至关重要。\n在一个异步消息传送系统当中，每个子系统（收款、存货等）都不存在和其他系统的耦合。它们通过消息传送服务顺进行通信，因此，某个子系统出现故障，并不会妨碍其他子系统的运行。如下图：\n\n在网络化系统中会出现局部故障，这是一个不可避免的事实。其中的一个系统，可能会在其连接运行期间的某个时刻，发生不可预测的故障，或者需要停机。这种现象可能会由于内部系统和合作系统地理上的分散而被进一步放大。考虑到这个因素，JMS提供了保证传送（guaranteed delivery）方式，它可以确保即便发生了局部故障，预定消费者最终也会接收到这条消息。\n保证传送使用的是一种”保存并转发（store-and-forward）”的机制，这就意味着，如果预定消费者当前并不可用，底层消息服务器就会将输入的消息写到一个持久存储器（persistent store）之中。随后，当该接收应用程序变为可用时，“保存并转发”机制会把预定消费者在不可用时错过的所有消息传送给它们。\n\n概括来说，JMS不仅仅是另外一种事件服务。它的设计涵盖了范围极广的企业应用程序，包括EAI、B2B和摄像头模型等。通过异步处理、“保存并转发”及“保证传送”机制，它为保持业务应用程序连续运行并实现不间断服务提供了很高的可用性。它还通过发布订阅功能和点对点功能，提供了集成灵活性。通过位置透明和管理控制，它提供了一种健壮的、基于服务的体系结构。而且，最重要的是，它非常易于学习和使用。\n</code></pre>","site":{"data":{}},"excerpt":"","more":"<h1 id=\"消息传送基础\"><a href=\"#消息传送基础\" class=\"headerlink\" title=\"消息传送基础\"></a>消息传送基础</h1><h2 id=\"消息传送模型\"><a href=\"#消息传送模型\" class=\"headerlink\" title=\"消息传送模型\"></a>消息传送模型</h2><pre><code>JMS支持两种消息传送模型：点对点模型和发布/订阅模型。有时候，又称这些消息传送模型为消息传送域。点对点消息传送模型和发布/订阅消息传送模型经常分别缩写为p2p和Pub/Sub。\n\n从JMS的视角来看，消息传送客户端称为JMS客户端(JMS Client)，而消息传送系统则称为JMS提供者（JMS provider）。一个JMS应用程序是由多个JMS客户端和（通常是）一个JMS提供者所组成的业务系统。\n此外，生产消息的JMS客户端称为消息生产者（message produceer），而接收消息的JMS客户端则称为消息消费者(message consumer)。一个JMS客户端可以既是消息生产者又是消息消费者。\n</code></pre><h3 id=\"点对点模型\"><a href=\"#点对点模型\" class=\"headerlink\" title=\"点对点模型\"></a>点对点模型</h3><pre><code>点对点消息传送模型允许JMS客户端通过队列（queue）这个虚拟通道来同步和异步发送、接收消息。在点对点模型中，消息生产者称为发送者（Sender），而消息消费者则称为接收者（receiver）。传统上，点对点模型是一个基于拉取(Pull)或基于轮询(polling)的消息传送模型，这种模型从队列中请求消息，而不是自动地将消息推送到客户端。点对点消息传递模型的一个突出特点就是：发送到队列的消息被一个而且仅仅一个接收者所接收，即使可能有多个接收者在一个队列中侦听同一消息时，也是如此。\n点对点消息传送模型既支持异步“即发即弃（fire and forget）”消息传送方式，又支持同步请求/应答消息传送方式。点对点消息传送模型比发布订阅模型具有更强的耦合性，发磅者通常会知道消息初如何使用，而且也会知道谁将接收该消息。举例来说，发送者可能会向一个队列发送一个证券交易订单并等待响应，响应中应包含一个交易确认码。这样一来，消息发送者就会知道消息接收者将来处理交易订单。另一个例子就是一个生成长时间运行报告的异步请求。发送者发出报告请求，而当该报告准备就绪时，就会给发送者发送一条通知消息。在这种情况下，发送者就会知道消息接收者将要处理该消息并创建报告。\n点对点模型支持负载均衡，它允许多个接收者侦听同一个队列，并以此来分配负载。JMS规范没有规定在多个接收者中间分发消息的规则，尽管某些JMS厂商已经选择实现些规则来提升负载均衡能力。点对点模型还具有其他优点，比如说，队列浏览器允许客户端在消费龅牙消息之前查看队列内容—--在发布订阅模型中，并没有这样浏览器的概念。\n</code></pre><h3 id=\"发布-订阅模型\"><a href=\"#发布-订阅模型\" class=\"headerlink\" title=\"发布/订阅模型\"></a>发布/订阅模型</h3><pre><code>在发布订阅模型中，消息会被发布到一个名为主题（topic）的虚拟通道中。消息生产者称为发布者(publisher)，而消息消费者则称为订阅者（subscriber）。与点对点模型不同，使用发布订阅模型发布一个主题的消息，能够由多个订阅者所接收。有时候，也称这项技术为广播(broadcasting)消息。每个订阅者都会接收到每条消息的一个副本。总地来说，发布订阅消息传送模型基本上是一个基于推送(push)的模型，其中消息自动地向消费者广播，它们无须请求或轮询主题来获得新消息。\n发布订阅模型的去耦能力要比p2p模型更强，消息发布者通常不会意识到有多少订阅者或那些订阅者如何处理这些消息。举例来说，假定每次在Java应用程序发生异常时，向一个主题发布一条消息。发布者的责任仅仅是广播发生了一个异常。该发布者不会知道或者说通常也不关心如何使用该消息。例如，有可能是订阅者根据该异常向开发人员或支持人员发送一封电子邮件，也有可能是订阅者收集不同类型的异常数目用于生成报告，甚至是订阅者根据异常的类型，使用这个信息来通知随叫随到(on-call)的技术人员。\n在发布订阅消息传送模型内部，有多种不同类型的订阅者。非持久订阅者是临时订阅类型，它们只是在主动侦听主题时才接收消息。而另一方面，持久订阅者将接收发布的每条消息的一个副本，即便在发布消息，它们处于“离线’状态时也是如此。另外还有动态持久订阅者和受管的持久订阅者等类型。\n</code></pre><h2 id=\"Rpc和异步消息传送\"><a href=\"#Rpc和异步消息传送\" class=\"headerlink\" title=\"Rpc和异步消息传送\"></a>Rpc和异步消息传送</h2><pre><code>RPC(Remote Procedure Call，远程过程调用)是通常用于描述分布式计算模型的术语。现在Java和.Net这两种平台都在使用这个术语。基于组件的体系结构，比如企业级JavaBean（Enterprise JavaBeans，EJB），就是建立在这个模型基础之上的。对于许多应用程序来说，基于RPC的技术已经是，并且将继续是切实可行的解决方案。不过，企业消息传送模型在特定类型的分布式应用程序中表现更为出色。在本节中，我们将讨论每种模型的优缺点。\n</code></pre><h3 id=\"紧密耦合的rpc\"><a href=\"#紧密耦合的rpc\" class=\"headerlink\" title=\"紧密耦合的rpc\"></a>紧密耦合的rpc</h3><pre><code>紧密耦合的RPC模型最为成功的一个领域就是构建3层或n层应用程序。在这个模型中，表示层（第1层）使用RPC和中间层（第2层）的业务逻辑进行通信，访问位于后端（第3层）的数据。Sun Microsystems公司的J2EE平台和Microsoft公司的.NET平台是这种体系结构最为先进的范例。\n使用J2EE、JSP和Servlet技术的表示层，而企业级JavaBean（EJB）则是中间层。抛开平台不论，这些系统使用的核心技术是基于成为定义通信范例的RPC的中间件。\nRPC试图模仿在一个进程中运行的某个系统的行为。在调用一个远程过程时，调用者将被阻塞，直到该过程完成并将控制权返回给调用者。从开发者的角度看，这种同步模型使得该系统就好像运行在一个进程当中。这些工作会依次完成，同时确保以顺序完成。RPC同步的本质特性，将客户端（进行调用的软件）和服务器（为该调用服务的软件）二者紧密耦合在一起。因为客户端已被阻塞，所以它无法继续进行工作。直到服务器做出响应为止。\nRPC紧密耦合的本质特性导致出现了相互高度依赖的系统，其中一个系统的失效会对其它系统产生立竿见影的弱化影响。例如，在J2EE中，如果期望使用企业级bean的servlet顺利工作，EJB服务器就必须正常地发挥功能。\n虽然RPC在许多场景中表现优秀，但是在系统对系统的处理过程当中，它的同步、紧密耦合等本质却是一个严重的缺陷，因为”系统对系统“有很多垂直的应用程序集成在一起。在系统对系统场景中，垂直系统之间的通信线程不仅数量众多、而且方向也是错综复杂，如下图：\n\n让我们设想一下使用紧密救命的RPC机制实现这种基础设施所面临的挑战。这些系统之间的连接管理是多对多的问题。当您身混合系统中加入另一个应用程序时，您不得不回过头来让其余所有的系统都知道它，而且，这些系统也会崩溃(crash)。它们仍然需要预定停工时间，而且对象的接口也需要升级。\n当该系统的一部分中断运行时，一切都停步。当您向一个订单输入系统添加订单时，它要对其他系统逐个进行同步调用。这会导致订单输入系统发生阻塞，并一直等待，直到每个系统都处理完成订单时为止。\n\n正是的PC系统的同步、紧密耦合、相互依赖等本质特性，便得子系统中出现的故障最终会导致整个系统的失效。就像在”系统对系统“场景中那样，当RPC紧密耦合的本质特性不再适用时，消息传送机制为此提供了另一种选择方案。\n</code></pre><h3 id=\"企业消息传送\"><a href=\"#企业消息传送\" class=\"headerlink\" title=\"企业消息传送\"></a>企业消息传送</h3><pre><code>各个子系统在可用性方面存在的问题，并不是使用面向消息的中间件所事业来的后果。消息传送机制的一个基本思想就是：规定应用程序之间的通信应该采用异步方式。将各部分连接在一些的代码会假定这是一条单身消息，它不需要立即从另一个应用程序那里得到响应。换句话说，它不必等待对这条消息的响应。这是RPC和异步消息传送之间的主要区别，而且，它对于 理解消息传送系统的优点来说至关重要。\n在一个异步消息传送系统当中，每个子系统（收款、存货等）都不存在和其他系统的耦合。它们通过消息传送服务顺进行通信，因此，某个子系统出现故障，并不会妨碍其他子系统的运行。如下图：\n\n在网络化系统中会出现局部故障，这是一个不可避免的事实。其中的一个系统，可能会在其连接运行期间的某个时刻，发生不可预测的故障，或者需要停机。这种现象可能会由于内部系统和合作系统地理上的分散而被进一步放大。考虑到这个因素，JMS提供了保证传送（guaranteed delivery）方式，它可以确保即便发生了局部故障，预定消费者最终也会接收到这条消息。\n保证传送使用的是一种”保存并转发（store-and-forward）”的机制，这就意味着，如果预定消费者当前并不可用，底层消息服务器就会将输入的消息写到一个持久存储器（persistent store）之中。随后，当该接收应用程序变为可用时，“保存并转发”机制会把预定消费者在不可用时错过的所有消息传送给它们。\n\n概括来说，JMS不仅仅是另外一种事件服务。它的设计涵盖了范围极广的企业应用程序，包括EAI、B2B和摄像头模型等。通过异步处理、“保存并转发”及“保证传送”机制，它为保持业务应用程序连续运行并实现不间断服务提供了很高的可用性。它还通过发布订阅功能和点对点功能，提供了集成灵活性。通过位置透明和管理控制，它提供了一种健壮的、基于服务的体系结构。而且，最重要的是，它非常易于学习和使用。\n</code></pre>"},{"title":"固定IP上网方式","date":"2016-09-20T01:43:49.000Z","_content":"#固定IP上网方式#\n\n**1. 修改主机名称：/etc/sysconfig/network**  \nNETWORKING=yes\nHOSTNAME=centos.dm.tsai\n\n**2. 设置网络参数：/etc/sysconfig/network-scripts/ifcfg-eth0**  \n请记得，这个ifcfg-eth0需与文件内的DEVICE名称设置相同，并且，在这个文件内的所有设置，基本上就是bash的变量设置规则\n\n**[root@linux ~]# vi /etc/sysconfig/network-scripts/ifcfg-eth0**   \nDEVICE=eth0                    网卡代号，需要ifcfg-eth0相对应   \nBOOTPROTO=static\t\t\t   开机协议，有dhcp及static,这里是static   \nBROADCAST=192.168.1.255        广播地址  \nHWADDR=00:40:D0:13:C3:46       网卡地址  \nIPADDR=192.168.1.13            IP  \nNETMASK=255.255.255.0          子屏蔽网络  \nNETWORK=192.168.1.0           网段，该网段的第一个IP  \nGATEWAY=192.168.1.2           默认路由  \nONBOOT=yes                  是否开机启动  \nMTU=1500                    最大传输单元的设置值  \nGATEWAYDEV=eth0            主要路由的设备，通常不用设置  \n\n&emsp;&emsp;请注意每个变量（左边的英文）都应该要大写。否则我们的script会误判。关于IP的4个参数（IPADDR、NETMASK、NETWORK、BROADCAST），下面谈谈以下几个重要的设置值.  \n&emsp;&emsp;DEVICE: 这个设置后面接的是设备代号必须与文件名（ifcfg-eht0）的设备代号相同，否则会显示找不到设备名称。\n&emsp;&emsp;BOOTPROTO：启动该网络接口时，使用何种协议？如果是手动设置IP的环境，请输入static或none，如果是自动取得IP的情况，请输入dhcp。  \n&emsp;&emsp;GATEWAY：代表的是整个主机系统的Default Gateway，所以，设置这个项目时，**请特别留意。不要有重复设置的情况发生。**也就是说，当您有ifcfg-eth0、Ifcfg-eht1等多个文件时，只要在其中一个文件里设置GATEWAY即可。  \n&emsp;&emsp;GATEWAYDEV：如果您不是使用固定的IP作为Gateway，而是使用网络设备作为Gateway（通常Route最常有这样的设置），那也可以使用GATEWAYDEV来设置通信网关设备。不过这个设置项目很少使用。  \n&emsp;&emsp;HWADDR：这是网卡的卡号。记得以前常常在讲，如果有两块一模一样的网卡存在，例如在一台主机上安装两张RealTek网卡，由于是相同的芯片，所以/etc/modprobe.conf内无法指定出明确的eth0与eth1的对应（因为模块使用相同），那么哪一个才是eth0?利用HWADDR指定网卡的卡号，就能够清楚定义出不同网卡的代号了。  \n&emsp;&emsp;事实上，如果想了解每个变量的项目意义时，建议参考/sbin/ifup这个script的内容，script很清楚地记录了每个项目的应用。\n\n**3. 启动与关闭网卡：ifup/ifdown**\n启动与关闭网卡的方式有两种，下面分别介绍：\n[root@linux~]#ifup eth0  \n[root@linux~]#ifdown eth0 \n\n上面的做法是针对eth0来进行启动(ifup)与关闭(ifdown)\n[root@linux~]# /etc/init.d/network restart\n\n针对这台主机的所有网络接口(包含lo)与通信闸进行重新启动所以网络会停止再连接\n\n[root@linux~]#service network restart;\n\n**4. 设置DNS的IP： /etc/resolv.conf**\n这个文件会影响到您是否可以查询到主机名称与IP的对应。通常进行如果设置就可以了。\nnameServer 168.95.1.1\n\n","source":"_posts/linux/Linux固定IP上网方式.md","raw":"---\ntitle: 固定IP上网方式\ndate: 2016-09-20 09:43:49\ntags: [linux,网络]\ncategories: [linux,网络]\n---\n#固定IP上网方式#\n\n**1. 修改主机名称：/etc/sysconfig/network**  \nNETWORKING=yes\nHOSTNAME=centos.dm.tsai\n\n**2. 设置网络参数：/etc/sysconfig/network-scripts/ifcfg-eth0**  \n请记得，这个ifcfg-eth0需与文件内的DEVICE名称设置相同，并且，在这个文件内的所有设置，基本上就是bash的变量设置规则\n\n**[root@linux ~]# vi /etc/sysconfig/network-scripts/ifcfg-eth0**   \nDEVICE=eth0                    网卡代号，需要ifcfg-eth0相对应   \nBOOTPROTO=static\t\t\t   开机协议，有dhcp及static,这里是static   \nBROADCAST=192.168.1.255        广播地址  \nHWADDR=00:40:D0:13:C3:46       网卡地址  \nIPADDR=192.168.1.13            IP  \nNETMASK=255.255.255.0          子屏蔽网络  \nNETWORK=192.168.1.0           网段，该网段的第一个IP  \nGATEWAY=192.168.1.2           默认路由  \nONBOOT=yes                  是否开机启动  \nMTU=1500                    最大传输单元的设置值  \nGATEWAYDEV=eth0            主要路由的设备，通常不用设置  \n\n&emsp;&emsp;请注意每个变量（左边的英文）都应该要大写。否则我们的script会误判。关于IP的4个参数（IPADDR、NETMASK、NETWORK、BROADCAST），下面谈谈以下几个重要的设置值.  \n&emsp;&emsp;DEVICE: 这个设置后面接的是设备代号必须与文件名（ifcfg-eht0）的设备代号相同，否则会显示找不到设备名称。\n&emsp;&emsp;BOOTPROTO：启动该网络接口时，使用何种协议？如果是手动设置IP的环境，请输入static或none，如果是自动取得IP的情况，请输入dhcp。  \n&emsp;&emsp;GATEWAY：代表的是整个主机系统的Default Gateway，所以，设置这个项目时，**请特别留意。不要有重复设置的情况发生。**也就是说，当您有ifcfg-eth0、Ifcfg-eht1等多个文件时，只要在其中一个文件里设置GATEWAY即可。  \n&emsp;&emsp;GATEWAYDEV：如果您不是使用固定的IP作为Gateway，而是使用网络设备作为Gateway（通常Route最常有这样的设置），那也可以使用GATEWAYDEV来设置通信网关设备。不过这个设置项目很少使用。  \n&emsp;&emsp;HWADDR：这是网卡的卡号。记得以前常常在讲，如果有两块一模一样的网卡存在，例如在一台主机上安装两张RealTek网卡，由于是相同的芯片，所以/etc/modprobe.conf内无法指定出明确的eth0与eth1的对应（因为模块使用相同），那么哪一个才是eth0?利用HWADDR指定网卡的卡号，就能够清楚定义出不同网卡的代号了。  \n&emsp;&emsp;事实上，如果想了解每个变量的项目意义时，建议参考/sbin/ifup这个script的内容，script很清楚地记录了每个项目的应用。\n\n**3. 启动与关闭网卡：ifup/ifdown**\n启动与关闭网卡的方式有两种，下面分别介绍：\n[root@linux~]#ifup eth0  \n[root@linux~]#ifdown eth0 \n\n上面的做法是针对eth0来进行启动(ifup)与关闭(ifdown)\n[root@linux~]# /etc/init.d/network restart\n\n针对这台主机的所有网络接口(包含lo)与通信闸进行重新启动所以网络会停止再连接\n\n[root@linux~]#service network restart;\n\n**4. 设置DNS的IP： /etc/resolv.conf**\n这个文件会影响到您是否可以查询到主机名称与IP的对应。通常进行如果设置就可以了。\nnameServer 168.95.1.1\n\n","slug":"linux/Linux固定IP上网方式","published":1,"updated":"2017-08-18T01:44:35.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjh7polsd000kbp7ri19gu1bp","content":"<p>#固定IP上网方式#</p>\n<p><strong>1. 修改主机名称：/etc/sysconfig/network</strong><br>NETWORKING=yes<br>HOSTNAME=centos.dm.tsai</p>\n<p><strong>2. 设置网络参数：/etc/sysconfig/network-scripts/ifcfg-eth0</strong><br>请记得，这个ifcfg-eth0需与文件内的DEVICE名称设置相同，并且，在这个文件内的所有设置，基本上就是bash的变量设置规则</p>\n<p><strong>[root@linux ~]# vi /etc/sysconfig/network-scripts/ifcfg-eth0</strong><br>DEVICE=eth0                    网卡代号，需要ifcfg-eth0相对应<br>BOOTPROTO=static               开机协议，有dhcp及static,这里是static<br>BROADCAST=192.168.1.255        广播地址<br>HWADDR=00:40:D0:13:C3:46       网卡地址<br>IPADDR=192.168.1.13            IP<br>NETMASK=255.255.255.0          子屏蔽网络<br>NETWORK=192.168.1.0           网段，该网段的第一个IP<br>GATEWAY=192.168.1.2           默认路由<br>ONBOOT=yes                  是否开机启动<br>MTU=1500                    最大传输单元的设置值<br>GATEWAYDEV=eth0            主要路由的设备，通常不用设置  </p>\n<p>&emsp;&emsp;请注意每个变量（左边的英文）都应该要大写。否则我们的script会误判。关于IP的4个参数（IPADDR、NETMASK、NETWORK、BROADCAST），下面谈谈以下几个重要的设置值.<br>&emsp;&emsp;DEVICE: 这个设置后面接的是设备代号必须与文件名（ifcfg-eht0）的设备代号相同，否则会显示找不到设备名称。<br>&emsp;&emsp;BOOTPROTO：启动该网络接口时，使用何种协议？如果是手动设置IP的环境，请输入static或none，如果是自动取得IP的情况，请输入dhcp。<br>&emsp;&emsp;GATEWAY：代表的是整个主机系统的Default Gateway，所以，设置这个项目时，<strong>请特别留意。不要有重复设置的情况发生。</strong>也就是说，当您有ifcfg-eth0、Ifcfg-eht1等多个文件时，只要在其中一个文件里设置GATEWAY即可。<br>&emsp;&emsp;GATEWAYDEV：如果您不是使用固定的IP作为Gateway，而是使用网络设备作为Gateway（通常Route最常有这样的设置），那也可以使用GATEWAYDEV来设置通信网关设备。不过这个设置项目很少使用。<br>&emsp;&emsp;HWADDR：这是网卡的卡号。记得以前常常在讲，如果有两块一模一样的网卡存在，例如在一台主机上安装两张RealTek网卡，由于是相同的芯片，所以/etc/modprobe.conf内无法指定出明确的eth0与eth1的对应（因为模块使用相同），那么哪一个才是eth0?利用HWADDR指定网卡的卡号，就能够清楚定义出不同网卡的代号了。<br>&emsp;&emsp;事实上，如果想了解每个变量的项目意义时，建议参考/sbin/ifup这个script的内容，script很清楚地记录了每个项目的应用。</p>\n<p><strong>3. 启动与关闭网卡：ifup/ifdown</strong><br>启动与关闭网卡的方式有两种，下面分别介绍：<br>[root@linux~]#ifup eth0<br>[root@linux~]#ifdown eth0 </p>\n<p>上面的做法是针对eth0来进行启动(ifup)与关闭(ifdown)<br>[root@linux~]# /etc/init.d/network restart</p>\n<p>针对这台主机的所有网络接口(包含lo)与通信闸进行重新启动所以网络会停止再连接</p>\n<p>[root@linux~]#service network restart;</p>\n<p><strong>4. 设置DNS的IP： /etc/resolv.conf</strong><br>这个文件会影响到您是否可以查询到主机名称与IP的对应。通常进行如果设置就可以了。<br>nameServer 168.95.1.1</p>\n","site":{"data":{}},"excerpt":"","more":"<p>#固定IP上网方式#</p>\n<p><strong>1. 修改主机名称：/etc/sysconfig/network</strong><br>NETWORKING=yes<br>HOSTNAME=centos.dm.tsai</p>\n<p><strong>2. 设置网络参数：/etc/sysconfig/network-scripts/ifcfg-eth0</strong><br>请记得，这个ifcfg-eth0需与文件内的DEVICE名称设置相同，并且，在这个文件内的所有设置，基本上就是bash的变量设置规则</p>\n<p><strong>[root@linux ~]# vi /etc/sysconfig/network-scripts/ifcfg-eth0</strong><br>DEVICE=eth0                    网卡代号，需要ifcfg-eth0相对应<br>BOOTPROTO=static               开机协议，有dhcp及static,这里是static<br>BROADCAST=192.168.1.255        广播地址<br>HWADDR=00:40:D0:13:C3:46       网卡地址<br>IPADDR=192.168.1.13            IP<br>NETMASK=255.255.255.0          子屏蔽网络<br>NETWORK=192.168.1.0           网段，该网段的第一个IP<br>GATEWAY=192.168.1.2           默认路由<br>ONBOOT=yes                  是否开机启动<br>MTU=1500                    最大传输单元的设置值<br>GATEWAYDEV=eth0            主要路由的设备，通常不用设置  </p>\n<p>&emsp;&emsp;请注意每个变量（左边的英文）都应该要大写。否则我们的script会误判。关于IP的4个参数（IPADDR、NETMASK、NETWORK、BROADCAST），下面谈谈以下几个重要的设置值.<br>&emsp;&emsp;DEVICE: 这个设置后面接的是设备代号必须与文件名（ifcfg-eht0）的设备代号相同，否则会显示找不到设备名称。<br>&emsp;&emsp;BOOTPROTO：启动该网络接口时，使用何种协议？如果是手动设置IP的环境，请输入static或none，如果是自动取得IP的情况，请输入dhcp。<br>&emsp;&emsp;GATEWAY：代表的是整个主机系统的Default Gateway，所以，设置这个项目时，<strong>请特别留意。不要有重复设置的情况发生。</strong>也就是说，当您有ifcfg-eth0、Ifcfg-eht1等多个文件时，只要在其中一个文件里设置GATEWAY即可。<br>&emsp;&emsp;GATEWAYDEV：如果您不是使用固定的IP作为Gateway，而是使用网络设备作为Gateway（通常Route最常有这样的设置），那也可以使用GATEWAYDEV来设置通信网关设备。不过这个设置项目很少使用。<br>&emsp;&emsp;HWADDR：这是网卡的卡号。记得以前常常在讲，如果有两块一模一样的网卡存在，例如在一台主机上安装两张RealTek网卡，由于是相同的芯片，所以/etc/modprobe.conf内无法指定出明确的eth0与eth1的对应（因为模块使用相同），那么哪一个才是eth0?利用HWADDR指定网卡的卡号，就能够清楚定义出不同网卡的代号了。<br>&emsp;&emsp;事实上，如果想了解每个变量的项目意义时，建议参考/sbin/ifup这个script的内容，script很清楚地记录了每个项目的应用。</p>\n<p><strong>3. 启动与关闭网卡：ifup/ifdown</strong><br>启动与关闭网卡的方式有两种，下面分别介绍：<br>[root@linux~]#ifup eth0<br>[root@linux~]#ifdown eth0 </p>\n<p>上面的做法是针对eth0来进行启动(ifup)与关闭(ifdown)<br>[root@linux~]# /etc/init.d/network restart</p>\n<p>针对这台主机的所有网络接口(包含lo)与通信闸进行重新启动所以网络会停止再连接</p>\n<p>[root@linux~]#service network restart;</p>\n<p><strong>4. 设置DNS的IP： /etc/resolv.conf</strong><br>这个文件会影响到您是否可以查询到主机名称与IP的对应。通常进行如果设置就可以了。<br>nameServer 168.95.1.1</p>\n"},{"title":"Linux常用命令及操作","date":"2017-05-02T02:54:00.000Z","_content":"下面整理一些常用的命令，内容来自于网络整合，会把自己常用的收集在这里  \n\n#CURL命令\n下载单个文件，默认将输出打印到标准输出中(STDOUT)中  \ncurl http://www.centos.org  \n通过-o/-O选项保存下载的文件到指定的文件中：  \n-o：将文件保存为命令行中指定的文件名的文件中  \n-O：使用URL中默认的文件名保存文件到本地  \n# 将文件下载到本地并命名为mygettext.html  \ncurl -o mygettext.html http://www.gnu.org/software/gettext/manual/gettext.html  \n# 将文件保存到本地并命名为gettext.html  \ncurl -O http://www.gnu.org/software/gettext/manual/gettext.html  \n同样可以使用转向字符\">\"对输出进行转向输出  \n##同时获取多个文件  \ncurl -O URL1 -O URL2  \n\n\n#ECHO命令\n在终端下打印变量value的时候也是常常用到的, 因此有必要了解下echo的用法  \necho命令的功能是在显示器上显示一段文字，一般起到一个提示的作用。  \n该命令的一般格式为： echo [ -n ] 字符串  \n其中选项n表示输出文字后不换行；字符串能加引号，也能不加引号。用echo命令输出加引号的字符串时，将字符串原样输出；用echo命令输出不加引号的字符串时，将字符串中的各个单词作为字符串输出，各字符串之间用一个空格分割。  \n功能说明：显示文字。  \n语 　 法：echo [-ne][字符串]或 echo [--help][--version]\n补充说明：echo会将输入的字符串送往标准输出。输出的字符串间以空白字符隔开, 并在最后加上换行号。  \n参　　 数：-n 不要在最后自动换行  \n-e 若字符串中出现以下字符，则特别加以处理，而不会将它当成一般  \n文字输出：  \n   \\a 发出警告声；  \n   \\b 删除前一个字符；  \n   \\c 最后不加上换行符号；  \n   \\f 换行但光标仍旧停留在原来的位置；  \n   \\n 换行且光标移至行首；  \n   \\r 光标移至行首，但不换行；  \n   \\t 插入tab；  \n   \\v 与\\f相同；  \n   \\\\ 插入\\字符；  \n   \\nnn 插入nnn（八进制）所代表的ASCII字符；  \n–help 显示帮助  \n–version 显示版本信息  \n  \n##其它功能\nECHO命令是大家都熟悉的DOS批处理命令的一条子命令，但它的一些功能和用法也许你并不是全都知道，不信你瞧：\n1． 作为控制批处理命令在执行时是否显示命令行自身的开关 格式：ECHO [ON|OFF] 如果想关闭“ECHO OFF”命令行自身的显示，则需要在该命令行前加上“@”。  \n2． 显示当前ECHO设置状态 格式：ECHO  \n3． 输出提示信息 格式：ECHO信息内容 上述是ECHO命令常见的三种用法，也是大家熟悉和会用的，但作为DOS命令淘金者你还应该知道下面的技巧：  \n4． 关闭DOS命令提示符 在DOS提示符状态下键入ECHO OFF，能够关闭DOS提示符的显示使屏幕只留下光标，直至键入ECHO ON，提示符才会重新出现。  \n5． 输出空行，即相当于输入一个回车 格式：ECHO． 值得注意的是命令行中的“．”要紧跟在ECHO后面中间不能有空格，否则“．”将被当作提示信息输出到屏幕。另外“．”可以用，：；”／[/]＋等任一符号替代。 在下面的例子中ECHO．输出的回车，经DOS管道转向作为TIME命令的输入，即相当于在TIME命令执行后给出一个回车。所以执行时系统会在显示当前时间后，自动返回到DOS提示符状态： C:〉ECHO.|TIME ECHO命令输出空行的另一个应用实例是：将ECHO．加在自动批处理文件中，使原本在屏幕下方显示的提示画面，出现在屏幕上方。  \n6． 答复命令中的提问 格式：ECHO答复语|命令文件名 上述格式可以用于简化一些需要人机对话的命令（如：CHKDSK／F；FORMAT Drive:；del *.*）的操作，它是通过DOS管道命令把ECHO命令输出的预置答复语作为人机对话命令的输入。下面的例子就相当于在调用的命令出现人机对话时输入“Y”回车： C:〉ECHO Y|CHKDSK/F C:〉ECHO Y|DEL A :*.*  \n7． 建立新文件或增加文件内容 格式：ECHO 文件内容＞文件名 ECHO 文件内容＞＞文件名 例如：C:〉ECHO @ECHO OFF〉AUTOEXEC.BAT建立自动批处理文件 C:〉ECHO C:/CPAV/BOOTSAFE〉〉AUTOEXEC.BAT向自动批处理文件中追加内容 C:TYPE AUTOEXEC.BAT显示该自动批处理文件 @ECHO OFF C:/CPAV/BOOTSAFE\n    可用于设置环境变量，如：  \n    $sudo echo \"export HIVE_HOME=$PWD/hive-0.9.0\" > /etc/profile.d/hive.sh  \n    $sudo echo \"PATH=$PATH:$HIVE_HOME/bin\" >> /etc/profile.d/hive.sh  \n8． 向打印机输出打印内容或打印控制码 格式：ECHO 打印机控制码＞PRN ECHO 打印内容＞PRN 下面的例子是向M－1724打印机输入打印控制码。＜Alt＞156是按住Alt键在小键盘键入156，类似情况依此类推： C:〉ECHO 〈Alt〉+156〈Alt〉+42〈Alt〉+116〉PRN（输入下划线命令FS＊t） C:〉ECHO 〈Alt〉+155@〉PRN（输入初始化命令ESC@） C:〉ECHO.〉PRN（换行）  \n9． 使喇叭鸣响 C:〉ECHO ^G “^G”是用Ctrl＋G或Alt＋007输入，输入多个^G可以产生多声鸣响。使用方法是直接将其加入批处理文件中或做成批处理文件调用。  \n10．执行ESC控制序列修改屏幕和键盘设置 我们知道DOS的设备驱动程序ANSI.SYS提供了一套用来修改屏幕和键盘设置的ESC控制序列。如执行下述内容的批处理程序可以把功能键F12定义为DOS命令“DIR／W”，并把屏幕颜色修改为白色字符蓝色背景。 @ECHO”←[0;134;”DIR/W”;13p @ECHO”←[1;37;44m （注：批处理文件中“←”字符的输入方法是在编辑状态下按Alt中小键盘上的27） DOS命令是接触计算机的人首先要学到的，对许多人来说是太熟悉太简单了，其实不然，在这些命令中蕴藏着丰富的内容，仍有待于我们进一步去理解去开发，如果你是一个有心人就一定会从这些自以为熟知的命令中发现新的闪光点，淘得真金。  \n\n#>和>>的区别,<号使用\nLinux中经常会用到将内容输出到某文件当中，只需要在执行命令后面加上>或者>>号即可进入操作。  \n大于号：将一条命令执行结果（标准输出，或者错误输出，本来都要打印到屏幕上面的）重定向其它输出设备（文件，打开文件操作符，或打印机等等）  \n小于号：命令默认从键盘获得的输入，改成从文件，或者其它打开文件以及设备输入  \n>> 是追加内容  \n> 是覆盖原有内容  \n示例：  \n1. bogon:Desktop wenxuechao$ echo 'abc' > test.txt    \n2. bogon:Desktop wenxuechao$ echo '123' >> test.txt  \n执行效果，第一句命令会在桌面创建个test.txt的文件，并且将abc写到文件中。  \n第二句命令，会在文件下方，再次写入内容。  \n<小于号    \nmysql -u root -p -h test < test.sql 导入数据    \n","source":"_posts/linux/Linux常用命令及操作.md","raw":"---\ntitle: Linux常用命令及操作\ndate: 2017-05-02 10:54:00\ntags: [linux]\ncategories: [linux,linux基本配置]\n---\n下面整理一些常用的命令，内容来自于网络整合，会把自己常用的收集在这里  \n\n#CURL命令\n下载单个文件，默认将输出打印到标准输出中(STDOUT)中  \ncurl http://www.centos.org  \n通过-o/-O选项保存下载的文件到指定的文件中：  \n-o：将文件保存为命令行中指定的文件名的文件中  \n-O：使用URL中默认的文件名保存文件到本地  \n# 将文件下载到本地并命名为mygettext.html  \ncurl -o mygettext.html http://www.gnu.org/software/gettext/manual/gettext.html  \n# 将文件保存到本地并命名为gettext.html  \ncurl -O http://www.gnu.org/software/gettext/manual/gettext.html  \n同样可以使用转向字符\">\"对输出进行转向输出  \n##同时获取多个文件  \ncurl -O URL1 -O URL2  \n\n\n#ECHO命令\n在终端下打印变量value的时候也是常常用到的, 因此有必要了解下echo的用法  \necho命令的功能是在显示器上显示一段文字，一般起到一个提示的作用。  \n该命令的一般格式为： echo [ -n ] 字符串  \n其中选项n表示输出文字后不换行；字符串能加引号，也能不加引号。用echo命令输出加引号的字符串时，将字符串原样输出；用echo命令输出不加引号的字符串时，将字符串中的各个单词作为字符串输出，各字符串之间用一个空格分割。  \n功能说明：显示文字。  \n语 　 法：echo [-ne][字符串]或 echo [--help][--version]\n补充说明：echo会将输入的字符串送往标准输出。输出的字符串间以空白字符隔开, 并在最后加上换行号。  \n参　　 数：-n 不要在最后自动换行  \n-e 若字符串中出现以下字符，则特别加以处理，而不会将它当成一般  \n文字输出：  \n   \\a 发出警告声；  \n   \\b 删除前一个字符；  \n   \\c 最后不加上换行符号；  \n   \\f 换行但光标仍旧停留在原来的位置；  \n   \\n 换行且光标移至行首；  \n   \\r 光标移至行首，但不换行；  \n   \\t 插入tab；  \n   \\v 与\\f相同；  \n   \\\\ 插入\\字符；  \n   \\nnn 插入nnn（八进制）所代表的ASCII字符；  \n–help 显示帮助  \n–version 显示版本信息  \n  \n##其它功能\nECHO命令是大家都熟悉的DOS批处理命令的一条子命令，但它的一些功能和用法也许你并不是全都知道，不信你瞧：\n1． 作为控制批处理命令在执行时是否显示命令行自身的开关 格式：ECHO [ON|OFF] 如果想关闭“ECHO OFF”命令行自身的显示，则需要在该命令行前加上“@”。  \n2． 显示当前ECHO设置状态 格式：ECHO  \n3． 输出提示信息 格式：ECHO信息内容 上述是ECHO命令常见的三种用法，也是大家熟悉和会用的，但作为DOS命令淘金者你还应该知道下面的技巧：  \n4． 关闭DOS命令提示符 在DOS提示符状态下键入ECHO OFF，能够关闭DOS提示符的显示使屏幕只留下光标，直至键入ECHO ON，提示符才会重新出现。  \n5． 输出空行，即相当于输入一个回车 格式：ECHO． 值得注意的是命令行中的“．”要紧跟在ECHO后面中间不能有空格，否则“．”将被当作提示信息输出到屏幕。另外“．”可以用，：；”／[/]＋等任一符号替代。 在下面的例子中ECHO．输出的回车，经DOS管道转向作为TIME命令的输入，即相当于在TIME命令执行后给出一个回车。所以执行时系统会在显示当前时间后，自动返回到DOS提示符状态： C:〉ECHO.|TIME ECHO命令输出空行的另一个应用实例是：将ECHO．加在自动批处理文件中，使原本在屏幕下方显示的提示画面，出现在屏幕上方。  \n6． 答复命令中的提问 格式：ECHO答复语|命令文件名 上述格式可以用于简化一些需要人机对话的命令（如：CHKDSK／F；FORMAT Drive:；del *.*）的操作，它是通过DOS管道命令把ECHO命令输出的预置答复语作为人机对话命令的输入。下面的例子就相当于在调用的命令出现人机对话时输入“Y”回车： C:〉ECHO Y|CHKDSK/F C:〉ECHO Y|DEL A :*.*  \n7． 建立新文件或增加文件内容 格式：ECHO 文件内容＞文件名 ECHO 文件内容＞＞文件名 例如：C:〉ECHO @ECHO OFF〉AUTOEXEC.BAT建立自动批处理文件 C:〉ECHO C:/CPAV/BOOTSAFE〉〉AUTOEXEC.BAT向自动批处理文件中追加内容 C:TYPE AUTOEXEC.BAT显示该自动批处理文件 @ECHO OFF C:/CPAV/BOOTSAFE\n    可用于设置环境变量，如：  \n    $sudo echo \"export HIVE_HOME=$PWD/hive-0.9.0\" > /etc/profile.d/hive.sh  \n    $sudo echo \"PATH=$PATH:$HIVE_HOME/bin\" >> /etc/profile.d/hive.sh  \n8． 向打印机输出打印内容或打印控制码 格式：ECHO 打印机控制码＞PRN ECHO 打印内容＞PRN 下面的例子是向M－1724打印机输入打印控制码。＜Alt＞156是按住Alt键在小键盘键入156，类似情况依此类推： C:〉ECHO 〈Alt〉+156〈Alt〉+42〈Alt〉+116〉PRN（输入下划线命令FS＊t） C:〉ECHO 〈Alt〉+155@〉PRN（输入初始化命令ESC@） C:〉ECHO.〉PRN（换行）  \n9． 使喇叭鸣响 C:〉ECHO ^G “^G”是用Ctrl＋G或Alt＋007输入，输入多个^G可以产生多声鸣响。使用方法是直接将其加入批处理文件中或做成批处理文件调用。  \n10．执行ESC控制序列修改屏幕和键盘设置 我们知道DOS的设备驱动程序ANSI.SYS提供了一套用来修改屏幕和键盘设置的ESC控制序列。如执行下述内容的批处理程序可以把功能键F12定义为DOS命令“DIR／W”，并把屏幕颜色修改为白色字符蓝色背景。 @ECHO”←[0;134;”DIR/W”;13p @ECHO”←[1;37;44m （注：批处理文件中“←”字符的输入方法是在编辑状态下按Alt中小键盘上的27） DOS命令是接触计算机的人首先要学到的，对许多人来说是太熟悉太简单了，其实不然，在这些命令中蕴藏着丰富的内容，仍有待于我们进一步去理解去开发，如果你是一个有心人就一定会从这些自以为熟知的命令中发现新的闪光点，淘得真金。  \n\n#>和>>的区别,<号使用\nLinux中经常会用到将内容输出到某文件当中，只需要在执行命令后面加上>或者>>号即可进入操作。  \n大于号：将一条命令执行结果（标准输出，或者错误输出，本来都要打印到屏幕上面的）重定向其它输出设备（文件，打开文件操作符，或打印机等等）  \n小于号：命令默认从键盘获得的输入，改成从文件，或者其它打开文件以及设备输入  \n>> 是追加内容  \n> 是覆盖原有内容  \n示例：  \n1. bogon:Desktop wenxuechao$ echo 'abc' > test.txt    \n2. bogon:Desktop wenxuechao$ echo '123' >> test.txt  \n执行效果，第一句命令会在桌面创建个test.txt的文件，并且将abc写到文件中。  \n第二句命令，会在文件下方，再次写入内容。  \n<小于号    \nmysql -u root -p -h test < test.sql 导入数据    \n","slug":"linux/Linux常用命令及操作","published":1,"updated":"2017-08-18T01:44:35.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjh7polsf000lbp7rw3z937z8","content":"<p>下面整理一些常用的命令，内容来自于网络整合，会把自己常用的收集在这里  </p>\n<p>#CURL命令<br>下载单个文件，默认将输出打印到标准输出中(STDOUT)中<br>curl <a href=\"http://www.centos.org\" target=\"_blank\" rel=\"noopener\">http://www.centos.org</a><br>通过-o/-O选项保存下载的文件到指定的文件中：<br>-o：将文件保存为命令行中指定的文件名的文件中<br>-O：使用URL中默认的文件名保存文件到本地  </p>\n<h1 id=\"将文件下载到本地并命名为mygettext-html\"><a href=\"#将文件下载到本地并命名为mygettext-html\" class=\"headerlink\" title=\"将文件下载到本地并命名为mygettext.html\"></a>将文件下载到本地并命名为mygettext.html</h1><p>curl -o mygettext.html <a href=\"http://www.gnu.org/software/gettext/manual/gettext.html\" target=\"_blank\" rel=\"noopener\">http://www.gnu.org/software/gettext/manual/gettext.html</a>  </p>\n<h1 id=\"将文件保存到本地并命名为gettext-html\"><a href=\"#将文件保存到本地并命名为gettext-html\" class=\"headerlink\" title=\"将文件保存到本地并命名为gettext.html\"></a>将文件保存到本地并命名为gettext.html</h1><p>curl -O <a href=\"http://www.gnu.org/software/gettext/manual/gettext.html\" target=\"_blank\" rel=\"noopener\">http://www.gnu.org/software/gettext/manual/gettext.html</a><br>同样可以使用转向字符”&gt;”对输出进行转向输出  </p>\n<p>##同时获取多个文件<br>curl -O URL1 -O URL2  </p>\n<p>#ECHO命令<br>在终端下打印变量value的时候也是常常用到的, 因此有必要了解下echo的用法<br>echo命令的功能是在显示器上显示一段文字，一般起到一个提示的作用。<br>该命令的一般格式为： echo [ -n ] 字符串<br>其中选项n表示输出文字后不换行；字符串能加引号，也能不加引号。用echo命令输出加引号的字符串时，将字符串原样输出；用echo命令输出不加引号的字符串时，将字符串中的各个单词作为字符串输出，各字符串之间用一个空格分割。<br>功能说明：显示文字。<br>语 　 法：echo [-ne][字符串]或 echo [–help][–version]<br>补充说明：echo会将输入的字符串送往标准输出。输出的字符串间以空白字符隔开, 并在最后加上换行号。<br>参　　 数：-n 不要在最后自动换行<br>-e 若字符串中出现以下字符，则特别加以处理，而不会将它当成一般<br>文字输出：<br>   \\a 发出警告声；<br>   \\b 删除前一个字符；<br>   \\c 最后不加上换行符号；<br>   \\f 换行但光标仍旧停留在原来的位置；<br>   \\n 换行且光标移至行首；<br>   \\r 光标移至行首，但不换行；<br>   \\t 插入tab；<br>   \\v 与\\f相同；<br>   \\ 插入\\字符；<br>   \\nnn 插入nnn（八进制）所代表的ASCII字符；<br>–help 显示帮助<br>–version 显示版本信息  </p>\n<p>##其它功能<br>ECHO命令是大家都熟悉的DOS批处理命令的一条子命令，但它的一些功能和用法也许你并不是全都知道，不信你瞧：<br>1． 作为控制批处理命令在执行时是否显示命令行自身的开关 格式：ECHO [ON|OFF] 如果想关闭“ECHO OFF”命令行自身的显示，则需要在该命令行前加上“@”。<br>2． 显示当前ECHO设置状态 格式：ECHO<br>3． 输出提示信息 格式：ECHO信息内容 上述是ECHO命令常见的三种用法，也是大家熟悉和会用的，但作为DOS命令淘金者你还应该知道下面的技巧：<br>4． 关闭DOS命令提示符 在DOS提示符状态下键入ECHO OFF，能够关闭DOS提示符的显示使屏幕只留下光标，直至键入ECHO ON，提示符才会重新出现。<br>5． 输出空行，即相当于输入一个回车 格式：ECHO． 值得注意的是命令行中的“．”要紧跟在ECHO后面中间不能有空格，否则“．”将被当作提示信息输出到屏幕。另外“．”可以用，：；”／[/]＋等任一符号替代。 在下面的例子中ECHO．输出的回车，经DOS管道转向作为TIME命令的输入，即相当于在TIME命令执行后给出一个回车。所以执行时系统会在显示当前时间后，自动返回到DOS提示符状态： C:〉ECHO.|TIME ECHO命令输出空行的另一个应用实例是：将ECHO．加在自动批处理文件中，使原本在屏幕下方显示的提示画面，出现在屏幕上方。<br>6． 答复命令中的提问 格式：ECHO答复语|命令文件名 上述格式可以用于简化一些需要人机对话的命令（如：CHKDSK／F；FORMAT Drive:；del <em>.</em>）的操作，它是通过DOS管道命令把ECHO命令输出的预置答复语作为人机对话命令的输入。下面的例子就相当于在调用的命令出现人机对话时输入“Y”回车： C:〉ECHO Y|CHKDSK/F C:〉ECHO Y|DEL A :<em>.</em><br>7． 建立新文件或增加文件内容 格式：ECHO 文件内容＞文件名 ECHO 文件内容＞＞文件名 例如：C:〉ECHO @ECHO OFF〉AUTOEXEC.BAT建立自动批处理文件 C:〉ECHO C:/CPAV/BOOTSAFE〉〉AUTOEXEC.BAT向自动批处理文件中追加内容 C:TYPE AUTOEXEC.BAT显示该自动批处理文件 @ECHO OFF C:/CPAV/BOOTSAFE<br>    可用于设置环境变量，如：<br>    $sudo echo “export HIVE_HOME=$PWD/hive-0.9.0” &gt; /etc/profile.d/hive.sh<br>    $sudo echo “PATH=$PATH:$HIVE_HOME/bin” &gt;&gt; /etc/profile.d/hive.sh<br>8． 向打印机输出打印内容或打印控制码 格式：ECHO 打印机控制码＞PRN ECHO 打印内容＞PRN 下面的例子是向M－1724打印机输入打印控制码。＜Alt＞156是按住Alt键在小键盘键入156，类似情况依此类推： C:〉ECHO 〈Alt〉+156〈Alt〉+42〈Alt〉+116〉PRN（输入下划线命令FS＊t） C:〉ECHO 〈Alt〉+155@〉PRN（输入初始化命令ESC@） C:〉ECHO.〉PRN（换行）<br>9． 使喇叭鸣响 C:〉ECHO ^G “^G”是用Ctrl＋G或Alt＋007输入，输入多个^G可以产生多声鸣响。使用方法是直接将其加入批处理文件中或做成批处理文件调用。<br>10．执行ESC控制序列修改屏幕和键盘设置 我们知道DOS的设备驱动程序ANSI.SYS提供了一套用来修改屏幕和键盘设置的ESC控制序列。如执行下述内容的批处理程序可以把功能键F12定义为DOS命令“DIR／W”，并把屏幕颜色修改为白色字符蓝色背景。 @ECHO”←[0;134;”DIR/W”;13p @ECHO”←[1;37;44m （注：批处理文件中“←”字符的输入方法是在编辑状态下按Alt中小键盘上的27） DOS命令是接触计算机的人首先要学到的，对许多人来说是太熟悉太简单了，其实不然，在这些命令中蕴藏着丰富的内容，仍有待于我们进一步去理解去开发，如果你是一个有心人就一定会从这些自以为熟知的命令中发现新的闪光点，淘得真金。  </p>\n<p>#&gt;和&gt;&gt;的区别,&lt;号使用<br>Linux中经常会用到将内容输出到某文件当中，只需要在执行命令后面加上&gt;或者&gt;&gt;号即可进入操作。<br>大于号：将一条命令执行结果（标准输出，或者错误输出，本来都要打印到屏幕上面的）重定向其它输出设备（文件，打开文件操作符，或打印机等等）<br>小于号：命令默认从键盘获得的输入，改成从文件，或者其它打开文件以及设备输入  </p>\n<blockquote>\n<blockquote>\n<p>是追加内容<br>是覆盖原有内容<br>示例：  </p>\n<ol>\n<li>bogon:Desktop wenxuechao$ echo ‘abc’ &gt; test.txt    </li>\n<li>bogon:Desktop wenxuechao$ echo ‘123’ &gt;&gt; test.txt<br>执行效果，第一句命令会在桌面创建个test.txt的文件，并且将abc写到文件中。<br>第二句命令，会在文件下方，再次写入内容。<br>&lt;小于号<br>mysql -u root -p -h test &lt; test.sql 导入数据    </li>\n</ol>\n</blockquote>\n</blockquote>\n","site":{"data":{}},"excerpt":"","more":"<p>下面整理一些常用的命令，内容来自于网络整合，会把自己常用的收集在这里  </p>\n<p>#CURL命令<br>下载单个文件，默认将输出打印到标准输出中(STDOUT)中<br>curl <a href=\"http://www.centos.org\" target=\"_blank\" rel=\"noopener\">http://www.centos.org</a><br>通过-o/-O选项保存下载的文件到指定的文件中：<br>-o：将文件保存为命令行中指定的文件名的文件中<br>-O：使用URL中默认的文件名保存文件到本地  </p>\n<h1 id=\"将文件下载到本地并命名为mygettext-html\"><a href=\"#将文件下载到本地并命名为mygettext-html\" class=\"headerlink\" title=\"将文件下载到本地并命名为mygettext.html\"></a>将文件下载到本地并命名为mygettext.html</h1><p>curl -o mygettext.html <a href=\"http://www.gnu.org/software/gettext/manual/gettext.html\" target=\"_blank\" rel=\"noopener\">http://www.gnu.org/software/gettext/manual/gettext.html</a>  </p>\n<h1 id=\"将文件保存到本地并命名为gettext-html\"><a href=\"#将文件保存到本地并命名为gettext-html\" class=\"headerlink\" title=\"将文件保存到本地并命名为gettext.html\"></a>将文件保存到本地并命名为gettext.html</h1><p>curl -O <a href=\"http://www.gnu.org/software/gettext/manual/gettext.html\" target=\"_blank\" rel=\"noopener\">http://www.gnu.org/software/gettext/manual/gettext.html</a><br>同样可以使用转向字符”&gt;”对输出进行转向输出  </p>\n<p>##同时获取多个文件<br>curl -O URL1 -O URL2  </p>\n<p>#ECHO命令<br>在终端下打印变量value的时候也是常常用到的, 因此有必要了解下echo的用法<br>echo命令的功能是在显示器上显示一段文字，一般起到一个提示的作用。<br>该命令的一般格式为： echo [ -n ] 字符串<br>其中选项n表示输出文字后不换行；字符串能加引号，也能不加引号。用echo命令输出加引号的字符串时，将字符串原样输出；用echo命令输出不加引号的字符串时，将字符串中的各个单词作为字符串输出，各字符串之间用一个空格分割。<br>功能说明：显示文字。<br>语 　 法：echo [-ne][字符串]或 echo [–help][–version]<br>补充说明：echo会将输入的字符串送往标准输出。输出的字符串间以空白字符隔开, 并在最后加上换行号。<br>参　　 数：-n 不要在最后自动换行<br>-e 若字符串中出现以下字符，则特别加以处理，而不会将它当成一般<br>文字输出：<br>   \\a 发出警告声；<br>   \\b 删除前一个字符；<br>   \\c 最后不加上换行符号；<br>   \\f 换行但光标仍旧停留在原来的位置；<br>   \\n 换行且光标移至行首；<br>   \\r 光标移至行首，但不换行；<br>   \\t 插入tab；<br>   \\v 与\\f相同；<br>   \\ 插入\\字符；<br>   \\nnn 插入nnn（八进制）所代表的ASCII字符；<br>–help 显示帮助<br>–version 显示版本信息  </p>\n<p>##其它功能<br>ECHO命令是大家都熟悉的DOS批处理命令的一条子命令，但它的一些功能和用法也许你并不是全都知道，不信你瞧：<br>1． 作为控制批处理命令在执行时是否显示命令行自身的开关 格式：ECHO [ON|OFF] 如果想关闭“ECHO OFF”命令行自身的显示，则需要在该命令行前加上“@”。<br>2． 显示当前ECHO设置状态 格式：ECHO<br>3． 输出提示信息 格式：ECHO信息内容 上述是ECHO命令常见的三种用法，也是大家熟悉和会用的，但作为DOS命令淘金者你还应该知道下面的技巧：<br>4． 关闭DOS命令提示符 在DOS提示符状态下键入ECHO OFF，能够关闭DOS提示符的显示使屏幕只留下光标，直至键入ECHO ON，提示符才会重新出现。<br>5． 输出空行，即相当于输入一个回车 格式：ECHO． 值得注意的是命令行中的“．”要紧跟在ECHO后面中间不能有空格，否则“．”将被当作提示信息输出到屏幕。另外“．”可以用，：；”／[/]＋等任一符号替代。 在下面的例子中ECHO．输出的回车，经DOS管道转向作为TIME命令的输入，即相当于在TIME命令执行后给出一个回车。所以执行时系统会在显示当前时间后，自动返回到DOS提示符状态： C:〉ECHO.|TIME ECHO命令输出空行的另一个应用实例是：将ECHO．加在自动批处理文件中，使原本在屏幕下方显示的提示画面，出现在屏幕上方。<br>6． 答复命令中的提问 格式：ECHO答复语|命令文件名 上述格式可以用于简化一些需要人机对话的命令（如：CHKDSK／F；FORMAT Drive:；del <em>.</em>）的操作，它是通过DOS管道命令把ECHO命令输出的预置答复语作为人机对话命令的输入。下面的例子就相当于在调用的命令出现人机对话时输入“Y”回车： C:〉ECHO Y|CHKDSK/F C:〉ECHO Y|DEL A :<em>.</em><br>7． 建立新文件或增加文件内容 格式：ECHO 文件内容＞文件名 ECHO 文件内容＞＞文件名 例如：C:〉ECHO @ECHO OFF〉AUTOEXEC.BAT建立自动批处理文件 C:〉ECHO C:/CPAV/BOOTSAFE〉〉AUTOEXEC.BAT向自动批处理文件中追加内容 C:TYPE AUTOEXEC.BAT显示该自动批处理文件 @ECHO OFF C:/CPAV/BOOTSAFE<br>    可用于设置环境变量，如：<br>    $sudo echo “export HIVE_HOME=$PWD/hive-0.9.0” &gt; /etc/profile.d/hive.sh<br>    $sudo echo “PATH=$PATH:$HIVE_HOME/bin” &gt;&gt; /etc/profile.d/hive.sh<br>8． 向打印机输出打印内容或打印控制码 格式：ECHO 打印机控制码＞PRN ECHO 打印内容＞PRN 下面的例子是向M－1724打印机输入打印控制码。＜Alt＞156是按住Alt键在小键盘键入156，类似情况依此类推： C:〉ECHO 〈Alt〉+156〈Alt〉+42〈Alt〉+116〉PRN（输入下划线命令FS＊t） C:〉ECHO 〈Alt〉+155@〉PRN（输入初始化命令ESC@） C:〉ECHO.〉PRN（换行）<br>9． 使喇叭鸣响 C:〉ECHO ^G “^G”是用Ctrl＋G或Alt＋007输入，输入多个^G可以产生多声鸣响。使用方法是直接将其加入批处理文件中或做成批处理文件调用。<br>10．执行ESC控制序列修改屏幕和键盘设置 我们知道DOS的设备驱动程序ANSI.SYS提供了一套用来修改屏幕和键盘设置的ESC控制序列。如执行下述内容的批处理程序可以把功能键F12定义为DOS命令“DIR／W”，并把屏幕颜色修改为白色字符蓝色背景。 @ECHO”←[0;134;”DIR/W”;13p @ECHO”←[1;37;44m （注：批处理文件中“←”字符的输入方法是在编辑状态下按Alt中小键盘上的27） DOS命令是接触计算机的人首先要学到的，对许多人来说是太熟悉太简单了，其实不然，在这些命令中蕴藏着丰富的内容，仍有待于我们进一步去理解去开发，如果你是一个有心人就一定会从这些自以为熟知的命令中发现新的闪光点，淘得真金。  </p>\n<p>#&gt;和&gt;&gt;的区别,&lt;号使用<br>Linux中经常会用到将内容输出到某文件当中，只需要在执行命令后面加上&gt;或者&gt;&gt;号即可进入操作。<br>大于号：将一条命令执行结果（标准输出，或者错误输出，本来都要打印到屏幕上面的）重定向其它输出设备（文件，打开文件操作符，或打印机等等）<br>小于号：命令默认从键盘获得的输入，改成从文件，或者其它打开文件以及设备输入  </p>\n<blockquote>\n<blockquote>\n<p>是追加内容<br>是覆盖原有内容<br>示例：  </p>\n<ol>\n<li>bogon:Desktop wenxuechao$ echo ‘abc’ &gt; test.txt    </li>\n<li>bogon:Desktop wenxuechao$ echo ‘123’ &gt;&gt; test.txt<br>执行效果，第一句命令会在桌面创建个test.txt的文件，并且将abc写到文件中。<br>第二句命令，会在文件下方，再次写入内容。<br>&lt;小于号<br>mysql -u root -p -h test &lt; test.sql 导入数据    </li>\n</ol>\n</blockquote>\n</blockquote>\n"},{"title":"SSH用户等效性配置","date":"2016-05-20T06:43:49.000Z","_content":"#SSH用户等效性配置 #\n以下均以oracle用户执行\nlinuxrac1\n[oracle @linuxrac1 ~]$mkdir ~/.ssh\n[oracle @linuxrac1 ~]$chmod 755 ~/.ssh\n[oracle @linuxrac1 ~]$ssh-keygen -t rsa\nGenerating public/private rsa key pair.\nEnter file in which to save the key (/home/oracle/.ssh/id_rsa):\nEnter passphrase (empty for no passphrase):\nEnter same passphrase again:\nYour identification has been saved in /home/oracle/.ssh/id_rsa.\nYour public key has been saved in /home/oracle/.ssh/id_rsa.pub.\nThe key fingerprint is:\ne9:2b:1a:2b:ac:5f:91:be:0f:84:17:d7:bd:b7:15:d2 oracle@linuxrac1\n[oracle @linuxrac1 ~]$ssh-keygen -t dsa\nGenerating public/private dsa key pair.\nEnter file in which to save the key (/home/oracle/.ssh/id_dsa):\nEnter passphrase (empty for no passphrase):\nEnter same passphrase again:\nYour identification has been saved in /home/oracle/.ssh/id_dsa.\nYour public key has been saved in /home/oracle/.ssh/id_dsa.pub.\nThe key fingerprint is:\nf5:0f:f5:0c:55:37:6a:08:ef:06:07:37:65:25:4a:15 oracle@linuxrac1\n \nlinuxrac2\n[oracle @linuxrac2 ~]$ mkdir ~/.ssh\n[oracle @linuxrac2 ~]$ chmod 755 ~/.ssh\n[oracle @linuxrac2 ~]$ ssh-keygen -t rsa\nGenerating public/private rsa key pair.\nEnter file in which to save the key (/home/oracle/.ssh/id_rsa):\nEnter passphrase (empty for no passphrase):\nEnter same passphrase again:\nYour identification has been saved in /home/oracle/.ssh/id_rsa.\nYour public key has been saved in /home/oracle/.ssh/id_rsa.pub.\nThe key fingerprint is:\n56:47:a0:94:67:44:d9:31:12:57:44:08:9d:84:25:a1 oracle@linuxrac2\n \n[oracle @linuxrac2 ~]$ ssh-keygen -t dsa\nGenerating public/private dsa key pair.\nEnter file in which to save the key (/home/oracle/.ssh/id_dsa):\nEnter passphrase (empty for no passphrase):\nEnter same passphrase again:\nYour identification has been saved in /home/oracle/.ssh/id_dsa.\nYour public key has been saved in /home/oracle/.ssh/id_dsa.pub.\nThe key fingerprint is:\nae:f0:06:77:62:33:86:dc:f4:0d:d9:c6:38:5e:cb:61 oracle@linuxrac2\n \n以上用默认配置,一路回车即可\nlinuxrac1\ncat ~/.ssh/*.pub >> ~/.ssh/authorized_keys\nssh oracle@linuxrac2 cat ~/.ssh/*.pub >> ~/.ssh/authorized_keys\n或\nssh oracle@linuxrac2 cat ~/.ssh/id_rsa.pub >> ~/.ssh/authorized_keys\nssh oracle@linuxrac2 cat ~/.ssh/id_dsa.pub >> ~/.ssh/authorized_keys\n[oracle@linuxrac1 ~]$ cd .ssh\n[oracle@linuxrac1 .ssh]$ ll\ntotal 48\n-rw-r--r-- 1 oracle oinstall 2008 Sep 25 02:20 authorized_keys\n-rw------- 1 oracle oinstall  668 Sep 25 02:09 id_dsa\n-rw-r--r-- 1 oracle oinstall  606 Sep 25 02:09 id_dsa.pub\n-rw------- 1 oracle oinstall 1675 Sep 25 02:09 id_rsa\n-rw-r--r-- 1 oracle oinstall  398 Sep 25 02:09 id_rsa.pub\n-rw-r--r-- 1 oracle oinstall  404 Sep 25 02:20 known_hosts\nlinuxrac2\ncat ~/.ssh/*.pub >> ~/.ssh/authorized_keys\nssh oracle@linuxrac1 cat ~/.ssh/*.pub >> ~/.ssh/authorized_keys\n或\nssh oracle@linuxrac1 cat ~/.ssh/id_rsa.pub >> ~/.ssh/authorized_keys\nssh oracle@linuxrac1 cat ~/.ssh/id_dsa.pub >> ~/.ssh/authorized_keys\n \n\n建立等效性 rac1,rac2双节点执行\n[oracle@linuxrac1 ~]$ exec ssh-agent $SHELL\n[oracle@linuxrac1 ~]$ ssh-add\nIdentity added: /home/oracle/.ssh/id_rsa (/home/oracle/.ssh/id_rsa)\nIdentity added: /home/oracle/.ssh/id_dsa (/home/oracle/.ssh/id_dsa)\n[oracle@linuxrac1 ~]$ ssh linuxrac1 date\n[oracle@linuxrac1 ~]$ ssh linuxrac1-priv date\n[oracle@linuxrac1 ~]$ ssh linuxrac2 date\n[oracle@linuxrac1 ~]$ ssh linuxrac2-priv date\n \n[oracle@linuxrac2 ~]$ exec ssh-agent $SHELL\n[oracle@linuxrac2 ~]$ ssh-add\nIdentity added: /home/oracle/.ssh/id_rsa (/home/oracle/.ssh/id_rsa)\nIdentity added: /home/oracle/.ssh/id_dsa (/home/oracle/.ssh/id_dsa)\n\nThe authenticity of host '<host>' can't be established.  \n解决办法：在连接目标机上执行ssh  -o StrictHostKeyChecking=no  xxxx(机器名)\n\n\n\n","source":"_posts/linux/SSH用户等效性配置.md","raw":"---\ntitle: SSH用户等效性配置\ndate: 2016-05-20 14:43:49\ntags: [linux]\ncategories: [linux,linux基本配置]\n---\n#SSH用户等效性配置 #\n以下均以oracle用户执行\nlinuxrac1\n[oracle @linuxrac1 ~]$mkdir ~/.ssh\n[oracle @linuxrac1 ~]$chmod 755 ~/.ssh\n[oracle @linuxrac1 ~]$ssh-keygen -t rsa\nGenerating public/private rsa key pair.\nEnter file in which to save the key (/home/oracle/.ssh/id_rsa):\nEnter passphrase (empty for no passphrase):\nEnter same passphrase again:\nYour identification has been saved in /home/oracle/.ssh/id_rsa.\nYour public key has been saved in /home/oracle/.ssh/id_rsa.pub.\nThe key fingerprint is:\ne9:2b:1a:2b:ac:5f:91:be:0f:84:17:d7:bd:b7:15:d2 oracle@linuxrac1\n[oracle @linuxrac1 ~]$ssh-keygen -t dsa\nGenerating public/private dsa key pair.\nEnter file in which to save the key (/home/oracle/.ssh/id_dsa):\nEnter passphrase (empty for no passphrase):\nEnter same passphrase again:\nYour identification has been saved in /home/oracle/.ssh/id_dsa.\nYour public key has been saved in /home/oracle/.ssh/id_dsa.pub.\nThe key fingerprint is:\nf5:0f:f5:0c:55:37:6a:08:ef:06:07:37:65:25:4a:15 oracle@linuxrac1\n \nlinuxrac2\n[oracle @linuxrac2 ~]$ mkdir ~/.ssh\n[oracle @linuxrac2 ~]$ chmod 755 ~/.ssh\n[oracle @linuxrac2 ~]$ ssh-keygen -t rsa\nGenerating public/private rsa key pair.\nEnter file in which to save the key (/home/oracle/.ssh/id_rsa):\nEnter passphrase (empty for no passphrase):\nEnter same passphrase again:\nYour identification has been saved in /home/oracle/.ssh/id_rsa.\nYour public key has been saved in /home/oracle/.ssh/id_rsa.pub.\nThe key fingerprint is:\n56:47:a0:94:67:44:d9:31:12:57:44:08:9d:84:25:a1 oracle@linuxrac2\n \n[oracle @linuxrac2 ~]$ ssh-keygen -t dsa\nGenerating public/private dsa key pair.\nEnter file in which to save the key (/home/oracle/.ssh/id_dsa):\nEnter passphrase (empty for no passphrase):\nEnter same passphrase again:\nYour identification has been saved in /home/oracle/.ssh/id_dsa.\nYour public key has been saved in /home/oracle/.ssh/id_dsa.pub.\nThe key fingerprint is:\nae:f0:06:77:62:33:86:dc:f4:0d:d9:c6:38:5e:cb:61 oracle@linuxrac2\n \n以上用默认配置,一路回车即可\nlinuxrac1\ncat ~/.ssh/*.pub >> ~/.ssh/authorized_keys\nssh oracle@linuxrac2 cat ~/.ssh/*.pub >> ~/.ssh/authorized_keys\n或\nssh oracle@linuxrac2 cat ~/.ssh/id_rsa.pub >> ~/.ssh/authorized_keys\nssh oracle@linuxrac2 cat ~/.ssh/id_dsa.pub >> ~/.ssh/authorized_keys\n[oracle@linuxrac1 ~]$ cd .ssh\n[oracle@linuxrac1 .ssh]$ ll\ntotal 48\n-rw-r--r-- 1 oracle oinstall 2008 Sep 25 02:20 authorized_keys\n-rw------- 1 oracle oinstall  668 Sep 25 02:09 id_dsa\n-rw-r--r-- 1 oracle oinstall  606 Sep 25 02:09 id_dsa.pub\n-rw------- 1 oracle oinstall 1675 Sep 25 02:09 id_rsa\n-rw-r--r-- 1 oracle oinstall  398 Sep 25 02:09 id_rsa.pub\n-rw-r--r-- 1 oracle oinstall  404 Sep 25 02:20 known_hosts\nlinuxrac2\ncat ~/.ssh/*.pub >> ~/.ssh/authorized_keys\nssh oracle@linuxrac1 cat ~/.ssh/*.pub >> ~/.ssh/authorized_keys\n或\nssh oracle@linuxrac1 cat ~/.ssh/id_rsa.pub >> ~/.ssh/authorized_keys\nssh oracle@linuxrac1 cat ~/.ssh/id_dsa.pub >> ~/.ssh/authorized_keys\n \n\n建立等效性 rac1,rac2双节点执行\n[oracle@linuxrac1 ~]$ exec ssh-agent $SHELL\n[oracle@linuxrac1 ~]$ ssh-add\nIdentity added: /home/oracle/.ssh/id_rsa (/home/oracle/.ssh/id_rsa)\nIdentity added: /home/oracle/.ssh/id_dsa (/home/oracle/.ssh/id_dsa)\n[oracle@linuxrac1 ~]$ ssh linuxrac1 date\n[oracle@linuxrac1 ~]$ ssh linuxrac1-priv date\n[oracle@linuxrac1 ~]$ ssh linuxrac2 date\n[oracle@linuxrac1 ~]$ ssh linuxrac2-priv date\n \n[oracle@linuxrac2 ~]$ exec ssh-agent $SHELL\n[oracle@linuxrac2 ~]$ ssh-add\nIdentity added: /home/oracle/.ssh/id_rsa (/home/oracle/.ssh/id_rsa)\nIdentity added: /home/oracle/.ssh/id_dsa (/home/oracle/.ssh/id_dsa)\n\nThe authenticity of host '<host>' can't be established.  \n解决办法：在连接目标机上执行ssh  -o StrictHostKeyChecking=no  xxxx(机器名)\n\n\n\n","slug":"linux/SSH用户等效性配置","published":1,"updated":"2017-08-18T01:44:35.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjh7polsh000pbp7rbz2tamhy","content":"<p>#SSH用户等效性配置 #<br>以下均以oracle用户执行<br>linuxrac1<br>[oracle @linuxrac1 ~]$mkdir ~/.ssh<br>[oracle @linuxrac1 ~]$chmod 755 ~/.ssh<br>[oracle @linuxrac1 ~]$ssh-keygen -t rsa<br>Generating public/private rsa key pair.<br>Enter file in which to save the key (/home/oracle/.ssh/id_rsa):<br>Enter passphrase (empty for no passphrase):<br>Enter same passphrase again:<br>Your identification has been saved in /home/oracle/.ssh/id_rsa.<br>Your public key has been saved in /home/oracle/.ssh/id_rsa.pub.<br>The key fingerprint is:<br>e9:2b:1a:2b:ac:5f:91:be:0f:84:17:d7:bd:b7:15:d2 oracle@linuxrac1<br>[oracle @linuxrac1 ~]$ssh-keygen -t dsa<br>Generating public/private dsa key pair.<br>Enter file in which to save the key (/home/oracle/.ssh/id_dsa):<br>Enter passphrase (empty for no passphrase):<br>Enter same passphrase again:<br>Your identification has been saved in /home/oracle/.ssh/id_dsa.<br>Your public key has been saved in /home/oracle/.ssh/id_dsa.pub.<br>The key fingerprint is:<br>f5:0f:f5:0c:55:37:6a:08:ef:06:07:37:65:25:4a:15 oracle@linuxrac1</p>\n<p>linuxrac2<br>[oracle @linuxrac2 ~]$ mkdir ~/.ssh<br>[oracle @linuxrac2 ~]$ chmod 755 ~/.ssh<br>[oracle @linuxrac2 ~]$ ssh-keygen -t rsa<br>Generating public/private rsa key pair.<br>Enter file in which to save the key (/home/oracle/.ssh/id_rsa):<br>Enter passphrase (empty for no passphrase):<br>Enter same passphrase again:<br>Your identification has been saved in /home/oracle/.ssh/id_rsa.<br>Your public key has been saved in /home/oracle/.ssh/id_rsa.pub.<br>The key fingerprint is:<br>56:47:a0:94:67:44:d9:31:12:57:44:08:9d:84:25:a1 oracle@linuxrac2</p>\n<p>[oracle @linuxrac2 ~]$ ssh-keygen -t dsa<br>Generating public/private dsa key pair.<br>Enter file in which to save the key (/home/oracle/.ssh/id_dsa):<br>Enter passphrase (empty for no passphrase):<br>Enter same passphrase again:<br>Your identification has been saved in /home/oracle/.ssh/id_dsa.<br>Your public key has been saved in /home/oracle/.ssh/id_dsa.pub.<br>The key fingerprint is:<br>ae:f0:06:77:62:33:86:dc:f4:0d:d9:c6:38:5e:cb:61 oracle@linuxrac2</p>\n<p>以上用默认配置,一路回车即可<br>linuxrac1<br>cat ~/.ssh/<em>.pub &gt;&gt; ~/.ssh/authorized_keys<br>ssh oracle@linuxrac2 cat ~/.ssh/</em>.pub &gt;&gt; ~/.ssh/authorized_keys<br>或<br>ssh oracle@linuxrac2 cat ~/.ssh/id_rsa.pub &gt;&gt; ~/.ssh/authorized_keys<br>ssh oracle@linuxrac2 cat ~/.ssh/id_dsa.pub &gt;&gt; ~/.ssh/authorized_keys<br>[oracle@linuxrac1 ~]$ cd .ssh<br>[oracle@linuxrac1 .ssh]$ ll<br>total 48<br>-rw-r–r– 1 oracle oinstall 2008 Sep 25 02:20 authorized_keys<br>-rw——- 1 oracle oinstall  668 Sep 25 02:09 id_dsa<br>-rw-r–r– 1 oracle oinstall  606 Sep 25 02:09 id_dsa.pub<br>-rw——- 1 oracle oinstall 1675 Sep 25 02:09 id_rsa<br>-rw-r–r– 1 oracle oinstall  398 Sep 25 02:09 id_rsa.pub<br>-rw-r–r– 1 oracle oinstall  404 Sep 25 02:20 known_hosts<br>linuxrac2<br>cat ~/.ssh/<em>.pub &gt;&gt; ~/.ssh/authorized_keys<br>ssh oracle@linuxrac1 cat ~/.ssh/</em>.pub &gt;&gt; ~/.ssh/authorized_keys<br>或<br>ssh oracle@linuxrac1 cat ~/.ssh/id_rsa.pub &gt;&gt; ~/.ssh/authorized_keys<br>ssh oracle@linuxrac1 cat ~/.ssh/id_dsa.pub &gt;&gt; ~/.ssh/authorized_keys</p>\n<p>建立等效性 rac1,rac2双节点执行<br>[oracle@linuxrac1 ~]$ exec ssh-agent $SHELL<br>[oracle@linuxrac1 ~]$ ssh-add<br>Identity added: /home/oracle/.ssh/id_rsa (/home/oracle/.ssh/id_rsa)<br>Identity added: /home/oracle/.ssh/id_dsa (/home/oracle/.ssh/id_dsa)<br>[oracle@linuxrac1 ~]$ ssh linuxrac1 date<br>[oracle@linuxrac1 ~]$ ssh linuxrac1-priv date<br>[oracle@linuxrac1 ~]$ ssh linuxrac2 date<br>[oracle@linuxrac1 ~]$ ssh linuxrac2-priv date</p>\n<p>[oracle@linuxrac2 ~]$ exec ssh-agent $SHELL<br>[oracle@linuxrac2 ~]$ ssh-add<br>Identity added: /home/oracle/.ssh/id_rsa (/home/oracle/.ssh/id_rsa)<br>Identity added: /home/oracle/.ssh/id_dsa (/home/oracle/.ssh/id_dsa)</p>\n<p>The authenticity of host ‘<host>‘ can’t be established.<br>解决办法：在连接目标机上执行ssh  -o StrictHostKeyChecking=no  xxxx(机器名)</host></p>\n","site":{"data":{}},"excerpt":"","more":"<p>#SSH用户等效性配置 #<br>以下均以oracle用户执行<br>linuxrac1<br>[oracle @linuxrac1 ~]$mkdir ~/.ssh<br>[oracle @linuxrac1 ~]$chmod 755 ~/.ssh<br>[oracle @linuxrac1 ~]$ssh-keygen -t rsa<br>Generating public/private rsa key pair.<br>Enter file in which to save the key (/home/oracle/.ssh/id_rsa):<br>Enter passphrase (empty for no passphrase):<br>Enter same passphrase again:<br>Your identification has been saved in /home/oracle/.ssh/id_rsa.<br>Your public key has been saved in /home/oracle/.ssh/id_rsa.pub.<br>The key fingerprint is:<br>e9:2b:1a:2b:ac:5f:91:be:0f:84:17:d7:bd:b7:15:d2 oracle@linuxrac1<br>[oracle @linuxrac1 ~]$ssh-keygen -t dsa<br>Generating public/private dsa key pair.<br>Enter file in which to save the key (/home/oracle/.ssh/id_dsa):<br>Enter passphrase (empty for no passphrase):<br>Enter same passphrase again:<br>Your identification has been saved in /home/oracle/.ssh/id_dsa.<br>Your public key has been saved in /home/oracle/.ssh/id_dsa.pub.<br>The key fingerprint is:<br>f5:0f:f5:0c:55:37:6a:08:ef:06:07:37:65:25:4a:15 oracle@linuxrac1</p>\n<p>linuxrac2<br>[oracle @linuxrac2 ~]$ mkdir ~/.ssh<br>[oracle @linuxrac2 ~]$ chmod 755 ~/.ssh<br>[oracle @linuxrac2 ~]$ ssh-keygen -t rsa<br>Generating public/private rsa key pair.<br>Enter file in which to save the key (/home/oracle/.ssh/id_rsa):<br>Enter passphrase (empty for no passphrase):<br>Enter same passphrase again:<br>Your identification has been saved in /home/oracle/.ssh/id_rsa.<br>Your public key has been saved in /home/oracle/.ssh/id_rsa.pub.<br>The key fingerprint is:<br>56:47:a0:94:67:44:d9:31:12:57:44:08:9d:84:25:a1 oracle@linuxrac2</p>\n<p>[oracle @linuxrac2 ~]$ ssh-keygen -t dsa<br>Generating public/private dsa key pair.<br>Enter file in which to save the key (/home/oracle/.ssh/id_dsa):<br>Enter passphrase (empty for no passphrase):<br>Enter same passphrase again:<br>Your identification has been saved in /home/oracle/.ssh/id_dsa.<br>Your public key has been saved in /home/oracle/.ssh/id_dsa.pub.<br>The key fingerprint is:<br>ae:f0:06:77:62:33:86:dc:f4:0d:d9:c6:38:5e:cb:61 oracle@linuxrac2</p>\n<p>以上用默认配置,一路回车即可<br>linuxrac1<br>cat ~/.ssh/<em>.pub &gt;&gt; ~/.ssh/authorized_keys<br>ssh oracle@linuxrac2 cat ~/.ssh/</em>.pub &gt;&gt; ~/.ssh/authorized_keys<br>或<br>ssh oracle@linuxrac2 cat ~/.ssh/id_rsa.pub &gt;&gt; ~/.ssh/authorized_keys<br>ssh oracle@linuxrac2 cat ~/.ssh/id_dsa.pub &gt;&gt; ~/.ssh/authorized_keys<br>[oracle@linuxrac1 ~]$ cd .ssh<br>[oracle@linuxrac1 .ssh]$ ll<br>total 48<br>-rw-r–r– 1 oracle oinstall 2008 Sep 25 02:20 authorized_keys<br>-rw——- 1 oracle oinstall  668 Sep 25 02:09 id_dsa<br>-rw-r–r– 1 oracle oinstall  606 Sep 25 02:09 id_dsa.pub<br>-rw——- 1 oracle oinstall 1675 Sep 25 02:09 id_rsa<br>-rw-r–r– 1 oracle oinstall  398 Sep 25 02:09 id_rsa.pub<br>-rw-r–r– 1 oracle oinstall  404 Sep 25 02:20 known_hosts<br>linuxrac2<br>cat ~/.ssh/<em>.pub &gt;&gt; ~/.ssh/authorized_keys<br>ssh oracle@linuxrac1 cat ~/.ssh/</em>.pub &gt;&gt; ~/.ssh/authorized_keys<br>或<br>ssh oracle@linuxrac1 cat ~/.ssh/id_rsa.pub &gt;&gt; ~/.ssh/authorized_keys<br>ssh oracle@linuxrac1 cat ~/.ssh/id_dsa.pub &gt;&gt; ~/.ssh/authorized_keys</p>\n<p>建立等效性 rac1,rac2双节点执行<br>[oracle@linuxrac1 ~]$ exec ssh-agent $SHELL<br>[oracle@linuxrac1 ~]$ ssh-add<br>Identity added: /home/oracle/.ssh/id_rsa (/home/oracle/.ssh/id_rsa)<br>Identity added: /home/oracle/.ssh/id_dsa (/home/oracle/.ssh/id_dsa)<br>[oracle@linuxrac1 ~]$ ssh linuxrac1 date<br>[oracle@linuxrac1 ~]$ ssh linuxrac1-priv date<br>[oracle@linuxrac1 ~]$ ssh linuxrac2 date<br>[oracle@linuxrac1 ~]$ ssh linuxrac2-priv date</p>\n<p>[oracle@linuxrac2 ~]$ exec ssh-agent $SHELL<br>[oracle@linuxrac2 ~]$ ssh-add<br>Identity added: /home/oracle/.ssh/id_rsa (/home/oracle/.ssh/id_rsa)<br>Identity added: /home/oracle/.ssh/id_dsa (/home/oracle/.ssh/id_dsa)</p>\n<p>The authenticity of host ‘<host>‘ can’t be established.<br>解决办法：在连接目标机上执行ssh  -o StrictHostKeyChecking=no  xxxx(机器名)</host></p>\n"},{"title":"Kettle源码构建过程","date":"2016-05-22T06:43:49.000Z","_content":"#Kettle 源码构建过程#\n\n&emsp;&emsp;Kettle 的源码托管在 Github 和 SVN上，但是托管在SVN上的源码自 5.0 之后就一直没有更新了，而托管在Github上的源码一直保持着更新状态，所以我猜 svn 上的源码并不会去进行维护了，我们以后只关注 git 上的源码就行。\n\n&emsp;&emsp;Kettle 的二进制文件下载地址：\n  http://sourceforge.net/projects/pentaho/files/Data%20Integration/\n&emsp;&emsp;Kettle 源码地址:\n  git:https://github.com/pentaho/pentaho-kettle\n  svn:svn://source.pentaho.org/svnkettleroot/archive/Kettle/branches\n&emsp;&emsp;其中二进制文件的下载版本分支与 git 上的源码分支是保持一致的，所以git上面的源码是跟着 kettle 的版本随时发布更新的，我们以后fork这个项目就可以一直获取最新发布的源码了。\n\n**下面是我的构建过程(版本5.4)：**  \n下载 ivy (http://ant.apache.org/ivy/download.html)。5.0版本之上的 kettle 项目结构与之前的版本项目结构完全不同，构建工具也由ant 变为了 ant + ivy。所以需要下载 ivy 来完成构建过程。将下载的 ivy-2.4.0.jar包放置到 ant_home/lib 下即可。  \n从 git 上面 clone  kettle 5.4 的源码于某个目录。命令行进入到源码主目录，如下所示：  \n&emsp;&emsp;执行命令：ant clean-all resolve create-dot-classpath。  命令会执行很久很久(资源在国外，如果自己有 vpn 加速会快点)，而且会时不时的报错，找不到 jar 包。当遇到找不到jar 包时，我是自行到kettle 私服(http://nexus.pentaho.org/content/groups/omni/)中去下对应版本jar包，然后放置到ivy 的本地仓库中，并删掉 ivy 本地缓存中的源文件和未下载成功的垃圾文件，然后重复此命令进行构建。本地缓存文件位于C:\\Users\\${username}\\.ivy2\\cache 。一直执行此命令，直到出现 successful 的提示。\n\n执行命令：ant dist ，该命令会根据源码生成一份对应的kettle 版本。同样的，在执行此命令的过程中，也会经常报各种错误，也需要自己去私服中找对应的jar包放置到对应的本地缓存目录。构建成功后，在根目录下会生成 一个 dist 文件夹，打开里面的spoon.bat 就可以使用 kettle 了，我下的5.4 版本的界面与之前的大有不同，感觉比以前的好看些，如下所示：\n\n\n到现在为止，就已经可以用源码构建了。下一步就是搭建好源码，kettle 的源码中分别提供了.project 文件和 .ipr 文件，所以可以用 eclipse和 idea 进行搭建。我是用的 idea 搭建的，eclipse 搭建应该会更简单点。\n\n在搭建之前，我删掉了kettle 中所有的.gitignore、.gitignoreattribute、.template、.project文件，然后在 .ipr 文件中注释掉版本管理的信息和未提供的插件项目信息，这样在导入源码的时候不会提示错误，如下所示：\n\n\n\n源码导入idea 中之后，会出现很多错误，需要手动导入一些jar包，由于里面有很多测试文件，所以我们需要手动依赖 /core/test_lib 文件夹里面的jar 包，范围指定为 test；\n\n\n\n从 libext 目录中复制 win64 位的swt.jar 包到 lib 目录，然后删除 swt_x86_64.jar 包；\n\n复制 ojdbc.jar 包到lib目录(为了解决登录资源库)；\n\n复制 dist/ui/*.xul 文件到 ui/目录；\n\n\n\n\n\n新建simple-jndi 空目录；\n\n\n\n到了这里，源码编译、调试就没问题了，程序启动的入口为org.pentaho.di.ui.spoon.Spoon.java，运行main入口函数即可启动 kettle，如下所示：\n\n\n\n再来欣赏下折腾摸索了这么久才弄出来的东西，确实比以前的好看些了，整体风格感觉统一了，而且向上兼容以前版本的，我用以前版本的转换在5.4 下测试了完全没问题，同时我也把以前写过的插件按照之前的机制进行处理，插件也是能加载出来并正常使用的，所以新的版本下的插件机制并没有发生变化:\n\n\n\n总结：上面这些步骤中好几个步骤是跑代码调试才知道的问题，所以调试还是一如既往的重要哈。\n\n","source":"_posts/kettle/Kettle源码构建过程.md","raw":"---\ntitle: Kettle源码构建过程\ndate: 2016-05-22 14:43:49\ntags: [开源项目,kettle]\ncategories: [开源项目,kettle]\n---\n#Kettle 源码构建过程#\n\n&emsp;&emsp;Kettle 的源码托管在 Github 和 SVN上，但是托管在SVN上的源码自 5.0 之后就一直没有更新了，而托管在Github上的源码一直保持着更新状态，所以我猜 svn 上的源码并不会去进行维护了，我们以后只关注 git 上的源码就行。\n\n&emsp;&emsp;Kettle 的二进制文件下载地址：\n  http://sourceforge.net/projects/pentaho/files/Data%20Integration/\n&emsp;&emsp;Kettle 源码地址:\n  git:https://github.com/pentaho/pentaho-kettle\n  svn:svn://source.pentaho.org/svnkettleroot/archive/Kettle/branches\n&emsp;&emsp;其中二进制文件的下载版本分支与 git 上的源码分支是保持一致的，所以git上面的源码是跟着 kettle 的版本随时发布更新的，我们以后fork这个项目就可以一直获取最新发布的源码了。\n\n**下面是我的构建过程(版本5.4)：**  \n下载 ivy (http://ant.apache.org/ivy/download.html)。5.0版本之上的 kettle 项目结构与之前的版本项目结构完全不同，构建工具也由ant 变为了 ant + ivy。所以需要下载 ivy 来完成构建过程。将下载的 ivy-2.4.0.jar包放置到 ant_home/lib 下即可。  \n从 git 上面 clone  kettle 5.4 的源码于某个目录。命令行进入到源码主目录，如下所示：  \n&emsp;&emsp;执行命令：ant clean-all resolve create-dot-classpath。  命令会执行很久很久(资源在国外，如果自己有 vpn 加速会快点)，而且会时不时的报错，找不到 jar 包。当遇到找不到jar 包时，我是自行到kettle 私服(http://nexus.pentaho.org/content/groups/omni/)中去下对应版本jar包，然后放置到ivy 的本地仓库中，并删掉 ivy 本地缓存中的源文件和未下载成功的垃圾文件，然后重复此命令进行构建。本地缓存文件位于C:\\Users\\${username}\\.ivy2\\cache 。一直执行此命令，直到出现 successful 的提示。\n\n执行命令：ant dist ，该命令会根据源码生成一份对应的kettle 版本。同样的，在执行此命令的过程中，也会经常报各种错误，也需要自己去私服中找对应的jar包放置到对应的本地缓存目录。构建成功后，在根目录下会生成 一个 dist 文件夹，打开里面的spoon.bat 就可以使用 kettle 了，我下的5.4 版本的界面与之前的大有不同，感觉比以前的好看些，如下所示：\n\n\n到现在为止，就已经可以用源码构建了。下一步就是搭建好源码，kettle 的源码中分别提供了.project 文件和 .ipr 文件，所以可以用 eclipse和 idea 进行搭建。我是用的 idea 搭建的，eclipse 搭建应该会更简单点。\n\n在搭建之前，我删掉了kettle 中所有的.gitignore、.gitignoreattribute、.template、.project文件，然后在 .ipr 文件中注释掉版本管理的信息和未提供的插件项目信息，这样在导入源码的时候不会提示错误，如下所示：\n\n\n\n源码导入idea 中之后，会出现很多错误，需要手动导入一些jar包，由于里面有很多测试文件，所以我们需要手动依赖 /core/test_lib 文件夹里面的jar 包，范围指定为 test；\n\n\n\n从 libext 目录中复制 win64 位的swt.jar 包到 lib 目录，然后删除 swt_x86_64.jar 包；\n\n复制 ojdbc.jar 包到lib目录(为了解决登录资源库)；\n\n复制 dist/ui/*.xul 文件到 ui/目录；\n\n\n\n\n\n新建simple-jndi 空目录；\n\n\n\n到了这里，源码编译、调试就没问题了，程序启动的入口为org.pentaho.di.ui.spoon.Spoon.java，运行main入口函数即可启动 kettle，如下所示：\n\n\n\n再来欣赏下折腾摸索了这么久才弄出来的东西，确实比以前的好看些了，整体风格感觉统一了，而且向上兼容以前版本的，我用以前版本的转换在5.4 下测试了完全没问题，同时我也把以前写过的插件按照之前的机制进行处理，插件也是能加载出来并正常使用的，所以新的版本下的插件机制并没有发生变化:\n\n\n\n总结：上面这些步骤中好几个步骤是跑代码调试才知道的问题，所以调试还是一如既往的重要哈。\n\n","slug":"kettle/Kettle源码构建过程","published":1,"updated":"2017-08-18T01:44:35.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjh7polsi000rbp7r7343x8p1","content":"<p>#Kettle 源码构建过程#</p>\n<p>&emsp;&emsp;Kettle 的源码托管在 Github 和 SVN上，但是托管在SVN上的源码自 5.0 之后就一直没有更新了，而托管在Github上的源码一直保持着更新状态，所以我猜 svn 上的源码并不会去进行维护了，我们以后只关注 git 上的源码就行。</p>\n<p>&emsp;&emsp;Kettle 的二进制文件下载地址：<br>  <a href=\"http://sourceforge.net/projects/pentaho/files/Data%20Integration/\" target=\"_blank\" rel=\"noopener\">http://sourceforge.net/projects/pentaho/files/Data%20Integration/</a><br>&emsp;&emsp;Kettle 源码地址:<br>  git:<a href=\"https://github.com/pentaho/pentaho-kettle\" target=\"_blank\" rel=\"noopener\">https://github.com/pentaho/pentaho-kettle</a><br>  svn:svn://source.pentaho.org/svnkettleroot/archive/Kettle/branches<br>&emsp;&emsp;其中二进制文件的下载版本分支与 git 上的源码分支是保持一致的，所以git上面的源码是跟着 kettle 的版本随时发布更新的，我们以后fork这个项目就可以一直获取最新发布的源码了。</p>\n<p><strong>下面是我的构建过程(版本5.4)：</strong><br>下载 ivy (<a href=\"http://ant.apache.org/ivy/download.html)。5.0版本之上的\" target=\"_blank\" rel=\"noopener\">http://ant.apache.org/ivy/download.html)。5.0版本之上的</a> kettle 项目结构与之前的版本项目结构完全不同，构建工具也由ant 变为了 ant + ivy。所以需要下载 ivy 来完成构建过程。将下载的 ivy-2.4.0.jar包放置到 ant_home/lib 下即可。<br>从 git 上面 clone  kettle 5.4 的源码于某个目录。命令行进入到源码主目录，如下所示：<br>&emsp;&emsp;执行命令：ant clean-all resolve create-dot-classpath。  命令会执行很久很久(资源在国外，如果自己有 vpn 加速会快点)，而且会时不时的报错，找不到 jar 包。当遇到找不到jar 包时，我是自行到kettle 私服(<a href=\"http://nexus.pentaho.org/content/groups/omni/)中去下对应版本jar包，然后放置到ivy\" target=\"_blank\" rel=\"noopener\">http://nexus.pentaho.org/content/groups/omni/)中去下对应版本jar包，然后放置到ivy</a> 的本地仓库中，并删掉 ivy 本地缓存中的源文件和未下载成功的垃圾文件，然后重复此命令进行构建。本地缓存文件位于C:\\Users\\${username}.ivy2\\cache 。一直执行此命令，直到出现 successful 的提示。</p>\n<p>执行命令：ant dist ，该命令会根据源码生成一份对应的kettle 版本。同样的，在执行此命令的过程中，也会经常报各种错误，也需要自己去私服中找对应的jar包放置到对应的本地缓存目录。构建成功后，在根目录下会生成 一个 dist 文件夹，打开里面的spoon.bat 就可以使用 kettle 了，我下的5.4 版本的界面与之前的大有不同，感觉比以前的好看些，如下所示：</p>\n<p>到现在为止，就已经可以用源码构建了。下一步就是搭建好源码，kettle 的源码中分别提供了.project 文件和 .ipr 文件，所以可以用 eclipse和 idea 进行搭建。我是用的 idea 搭建的，eclipse 搭建应该会更简单点。</p>\n<p>在搭建之前，我删掉了kettle 中所有的.gitignore、.gitignoreattribute、.template、.project文件，然后在 .ipr 文件中注释掉版本管理的信息和未提供的插件项目信息，这样在导入源码的时候不会提示错误，如下所示：</p>\n<p>源码导入idea 中之后，会出现很多错误，需要手动导入一些jar包，由于里面有很多测试文件，所以我们需要手动依赖 /core/test_lib 文件夹里面的jar 包，范围指定为 test；</p>\n<p>从 libext 目录中复制 win64 位的swt.jar 包到 lib 目录，然后删除 swt_x86_64.jar 包；</p>\n<p>复制 ojdbc.jar 包到lib目录(为了解决登录资源库)；</p>\n<p>复制 dist/ui/*.xul 文件到 ui/目录；</p>\n<p>新建simple-jndi 空目录；</p>\n<p>到了这里，源码编译、调试就没问题了，程序启动的入口为org.pentaho.di.ui.spoon.Spoon.java，运行main入口函数即可启动 kettle，如下所示：</p>\n<p>再来欣赏下折腾摸索了这么久才弄出来的东西，确实比以前的好看些了，整体风格感觉统一了，而且向上兼容以前版本的，我用以前版本的转换在5.4 下测试了完全没问题，同时我也把以前写过的插件按照之前的机制进行处理，插件也是能加载出来并正常使用的，所以新的版本下的插件机制并没有发生变化:</p>\n<p>总结：上面这些步骤中好几个步骤是跑代码调试才知道的问题，所以调试还是一如既往的重要哈。</p>\n","site":{"data":{}},"excerpt":"","more":"<p>#Kettle 源码构建过程#</p>\n<p>&emsp;&emsp;Kettle 的源码托管在 Github 和 SVN上，但是托管在SVN上的源码自 5.0 之后就一直没有更新了，而托管在Github上的源码一直保持着更新状态，所以我猜 svn 上的源码并不会去进行维护了，我们以后只关注 git 上的源码就行。</p>\n<p>&emsp;&emsp;Kettle 的二进制文件下载地址：<br>  <a href=\"http://sourceforge.net/projects/pentaho/files/Data%20Integration/\" target=\"_blank\" rel=\"noopener\">http://sourceforge.net/projects/pentaho/files/Data%20Integration/</a><br>&emsp;&emsp;Kettle 源码地址:<br>  git:<a href=\"https://github.com/pentaho/pentaho-kettle\" target=\"_blank\" rel=\"noopener\">https://github.com/pentaho/pentaho-kettle</a><br>  svn:svn://source.pentaho.org/svnkettleroot/archive/Kettle/branches<br>&emsp;&emsp;其中二进制文件的下载版本分支与 git 上的源码分支是保持一致的，所以git上面的源码是跟着 kettle 的版本随时发布更新的，我们以后fork这个项目就可以一直获取最新发布的源码了。</p>\n<p><strong>下面是我的构建过程(版本5.4)：</strong><br>下载 ivy (<a href=\"http://ant.apache.org/ivy/download.html)。5.0版本之上的\" target=\"_blank\" rel=\"noopener\">http://ant.apache.org/ivy/download.html)。5.0版本之上的</a> kettle 项目结构与之前的版本项目结构完全不同，构建工具也由ant 变为了 ant + ivy。所以需要下载 ivy 来完成构建过程。将下载的 ivy-2.4.0.jar包放置到 ant_home/lib 下即可。<br>从 git 上面 clone  kettle 5.4 的源码于某个目录。命令行进入到源码主目录，如下所示：<br>&emsp;&emsp;执行命令：ant clean-all resolve create-dot-classpath。  命令会执行很久很久(资源在国外，如果自己有 vpn 加速会快点)，而且会时不时的报错，找不到 jar 包。当遇到找不到jar 包时，我是自行到kettle 私服(<a href=\"http://nexus.pentaho.org/content/groups/omni/)中去下对应版本jar包，然后放置到ivy\" target=\"_blank\" rel=\"noopener\">http://nexus.pentaho.org/content/groups/omni/)中去下对应版本jar包，然后放置到ivy</a> 的本地仓库中，并删掉 ivy 本地缓存中的源文件和未下载成功的垃圾文件，然后重复此命令进行构建。本地缓存文件位于C:\\Users\\${username}.ivy2\\cache 。一直执行此命令，直到出现 successful 的提示。</p>\n<p>执行命令：ant dist ，该命令会根据源码生成一份对应的kettle 版本。同样的，在执行此命令的过程中，也会经常报各种错误，也需要自己去私服中找对应的jar包放置到对应的本地缓存目录。构建成功后，在根目录下会生成 一个 dist 文件夹，打开里面的spoon.bat 就可以使用 kettle 了，我下的5.4 版本的界面与之前的大有不同，感觉比以前的好看些，如下所示：</p>\n<p>到现在为止，就已经可以用源码构建了。下一步就是搭建好源码，kettle 的源码中分别提供了.project 文件和 .ipr 文件，所以可以用 eclipse和 idea 进行搭建。我是用的 idea 搭建的，eclipse 搭建应该会更简单点。</p>\n<p>在搭建之前，我删掉了kettle 中所有的.gitignore、.gitignoreattribute、.template、.project文件，然后在 .ipr 文件中注释掉版本管理的信息和未提供的插件项目信息，这样在导入源码的时候不会提示错误，如下所示：</p>\n<p>源码导入idea 中之后，会出现很多错误，需要手动导入一些jar包，由于里面有很多测试文件，所以我们需要手动依赖 /core/test_lib 文件夹里面的jar 包，范围指定为 test；</p>\n<p>从 libext 目录中复制 win64 位的swt.jar 包到 lib 目录，然后删除 swt_x86_64.jar 包；</p>\n<p>复制 ojdbc.jar 包到lib目录(为了解决登录资源库)；</p>\n<p>复制 dist/ui/*.xul 文件到 ui/目录；</p>\n<p>新建simple-jndi 空目录；</p>\n<p>到了这里，源码编译、调试就没问题了，程序启动的入口为org.pentaho.di.ui.spoon.Spoon.java，运行main入口函数即可启动 kettle，如下所示：</p>\n<p>再来欣赏下折腾摸索了这么久才弄出来的东西，确实比以前的好看些了，整体风格感觉统一了，而且向上兼容以前版本的，我用以前版本的转换在5.4 下测试了完全没问题，同时我也把以前写过的插件按照之前的机制进行处理，插件也是能加载出来并正常使用的，所以新的版本下的插件机制并没有发生变化:</p>\n<p>总结：上面这些步骤中好几个步骤是跑代码调试才知道的问题，所以调试还是一如既往的重要哈。</p>\n"},{"title":"Sorted Merge组件","date":"2016-05-22T06:43:49.000Z","_content":"## Sorted Merge组件 ##\n### 概念##\n<table>\n\t<tr>\n\t\t<th>图标</th>\n\t\t<th>组件名称</th>\n\t\t<th>功能说明</th>\n\t</tr>\n\t<tr>\n\t\t<td>图片</td>\n\t\t<td>Sorted Merge</td>\n\t\t<td>合并来自多个输入步骤的记录，提供按给定的关键字段进行排序后的行记录。</td>\n\t</tr>\n</table>\nSorted Merge组件处理来自多个输入步骤的记录，提供按给定的关键字段进行排序后的行记录的功能。\n    用例:当您并行方式使用“记录排序”步骤的多个副本(通过“改变起始的副本数量”或集群环境中时)的每个排序块需要合并在一起,以确保正确的排序序列。可以通过在在“记录排序”组件后添加”Sorted Merge”组件来处理。\n    注：应用于多个复本或集群环境下的并行运算时，前置组件必须为“记录排序”组件。表 0 15Sorted Merge组件属性说明\n<table>\n    <tr>\n        <th>属性项</th>\n        <th>属性说明</th>\n    </tr>\n    <tr>\n        <td>组件名称</td>\n        <td>步骤的名字，这个名字在一个转换中必须是唯一的。</td>\n    </tr>\n    <tr>\n        <td>字段</td>\n        <td>指定字段名和排序方向(升序/降序);点击获取字段检索列表字段从输入流。</td>\n    </tr>\n    <tr>\n        <td>字段</td>\n        <td>指定排序的字段名。</td>\n    </tr>\n    <tr>\n        <td>升序</td>\n        <td>排序原则：升序或降序。如果选择升序，    \n        排序顺序将是：数字->英文->汉字，汉字是按照拼音排序的，    \n        也同样会按照声调排序。如果是多音字，只会取一个读音，    \n        无法根据语境判断其的读音。</td>\n    </tr>\n</table>\n\n###问题释疑###\n1.\tSorted Merge组件是如何工作的？\n它用来合并2个输入数据流（按同一字段排序后的、相同格式的数据流）。\n \n2.\tSorted Merge组件的原理？\n用于比较两组数据以确保合并后的输出也是排序的。它是通过串行方式来处理的，并不需要考虑整个数据集合。它实际上用于不同机器的集群环境下，而不是在单机上。运行效率取决于你有多少个可用的CPU。\n \n3.\tSorted Merge组件的使用前提？\n对于“Sorted Merge”组件的所有输入行结构需要是相同的（相同的字段布局、字段类型等）且需要按指定的键进行排序。只有这样才能保证输出是该键进行排序的。\n \n4.\t“Sorted Merge”组件的排序字段指定为与“字段排序”组件排序字段相同时，不能按指定排序?\n Sorted Merge组件的合并是基于前置组件（”记录排序“组件）排好顺序的集合里的指定字段进行合并，排序是基于前置组件中相同排序字段的数据集内的排序，所以说在相同排序字段集中指定排序方式时，排序是不变的。既然没有意义为什么要保留呢？是因为与前置组件（Sorted Merge组件）保持一致，如果设计成按照排序字段的选择来去掉”Sorted Merge“组件中的对应字段，会增加复杂性，同时又需要解释为什么少一个字段，索性保留。\n \n5.\t对于的分区功能，Sorted Merge组件必要性？\n不是必须的。因为Sorted Merge多用于并行处理环境，常与分区功能组合使用，Sorted Merge组件是对分区的一种收集方式（Sorted Merge）的支持，所以说不是必须的，如果收集的记录不要求排序，我们完成可以不使用此组件；\n注：分区的收集是指当分区的组件处理完成发送到没有分区的组件时，记录将被合并。目前支持的合并方法有两种方式，一种是轮流从各个分区接收记录，另一种是对于Sorted Merge的收集方式，Kettle中有”Sorted Merge“组件来完成。\n \n6.\t记录排序组件和Sorted Merge组件都可以对多输入流的数据进行排序，有什么区别。\n使用场景不同，Sorted Merge组件应用于并行数据处理环境，是分区后的一种数据收集方式（Sorted Merge收集方式）。在大数据量时，原则上希望把尽可能多的数据处理放在集群上处理，然后在主服务器上进行合并，因为Sorted Merge组件的算法是基于已排好顺序的集合(归并排序算法)，在对已排过序的集合的排序效率高于记录排序。我们完全没必须把从属服务器上收集到的已排序的数据，使用“记录排序”组件再打乱重新排序。基于效率上考虑建议在对已排好序的数据集进行排序时，使用Sorted Merge组件。\n \n7.\t建议：分布式排序\n当需要使用排序组件时，最好将排序组件水平扩展到集群上运行，以获得较好的运行效率。每个从属服务器对传入的记录进行排序，然后在主服务器上使用Sorted Merge组件合并来自从属服务器的排序结果。Sorted Merge组件被设计用来接收从前面的组件接收排序后的记录，并且在合并时能够保证记录的顺序。对于已排序过的多个数据集进行排序时，Sorted Merge的效率高于记录排序，这也是使用Sorted Merge组件的原因。\n\n![\"分布式排序\"](images/sorted merger1.jpg)\n\n","source":"_posts/kettle/Sorted Merge组件.md","raw":"---\ntitle: Sorted Merge组件\ndate: 2016-05-22 14:43:49\ntags: [开源项目,kettle]\ncategories: [开源项目,kettle]\n---\n## Sorted Merge组件 ##\n### 概念##\n<table>\n\t<tr>\n\t\t<th>图标</th>\n\t\t<th>组件名称</th>\n\t\t<th>功能说明</th>\n\t</tr>\n\t<tr>\n\t\t<td>图片</td>\n\t\t<td>Sorted Merge</td>\n\t\t<td>合并来自多个输入步骤的记录，提供按给定的关键字段进行排序后的行记录。</td>\n\t</tr>\n</table>\nSorted Merge组件处理来自多个输入步骤的记录，提供按给定的关键字段进行排序后的行记录的功能。\n    用例:当您并行方式使用“记录排序”步骤的多个副本(通过“改变起始的副本数量”或集群环境中时)的每个排序块需要合并在一起,以确保正确的排序序列。可以通过在在“记录排序”组件后添加”Sorted Merge”组件来处理。\n    注：应用于多个复本或集群环境下的并行运算时，前置组件必须为“记录排序”组件。表 0 15Sorted Merge组件属性说明\n<table>\n    <tr>\n        <th>属性项</th>\n        <th>属性说明</th>\n    </tr>\n    <tr>\n        <td>组件名称</td>\n        <td>步骤的名字，这个名字在一个转换中必须是唯一的。</td>\n    </tr>\n    <tr>\n        <td>字段</td>\n        <td>指定字段名和排序方向(升序/降序);点击获取字段检索列表字段从输入流。</td>\n    </tr>\n    <tr>\n        <td>字段</td>\n        <td>指定排序的字段名。</td>\n    </tr>\n    <tr>\n        <td>升序</td>\n        <td>排序原则：升序或降序。如果选择升序，    \n        排序顺序将是：数字->英文->汉字，汉字是按照拼音排序的，    \n        也同样会按照声调排序。如果是多音字，只会取一个读音，    \n        无法根据语境判断其的读音。</td>\n    </tr>\n</table>\n\n###问题释疑###\n1.\tSorted Merge组件是如何工作的？\n它用来合并2个输入数据流（按同一字段排序后的、相同格式的数据流）。\n \n2.\tSorted Merge组件的原理？\n用于比较两组数据以确保合并后的输出也是排序的。它是通过串行方式来处理的，并不需要考虑整个数据集合。它实际上用于不同机器的集群环境下，而不是在单机上。运行效率取决于你有多少个可用的CPU。\n \n3.\tSorted Merge组件的使用前提？\n对于“Sorted Merge”组件的所有输入行结构需要是相同的（相同的字段布局、字段类型等）且需要按指定的键进行排序。只有这样才能保证输出是该键进行排序的。\n \n4.\t“Sorted Merge”组件的排序字段指定为与“字段排序”组件排序字段相同时，不能按指定排序?\n Sorted Merge组件的合并是基于前置组件（”记录排序“组件）排好顺序的集合里的指定字段进行合并，排序是基于前置组件中相同排序字段的数据集内的排序，所以说在相同排序字段集中指定排序方式时，排序是不变的。既然没有意义为什么要保留呢？是因为与前置组件（Sorted Merge组件）保持一致，如果设计成按照排序字段的选择来去掉”Sorted Merge“组件中的对应字段，会增加复杂性，同时又需要解释为什么少一个字段，索性保留。\n \n5.\t对于的分区功能，Sorted Merge组件必要性？\n不是必须的。因为Sorted Merge多用于并行处理环境，常与分区功能组合使用，Sorted Merge组件是对分区的一种收集方式（Sorted Merge）的支持，所以说不是必须的，如果收集的记录不要求排序，我们完成可以不使用此组件；\n注：分区的收集是指当分区的组件处理完成发送到没有分区的组件时，记录将被合并。目前支持的合并方法有两种方式，一种是轮流从各个分区接收记录，另一种是对于Sorted Merge的收集方式，Kettle中有”Sorted Merge“组件来完成。\n \n6.\t记录排序组件和Sorted Merge组件都可以对多输入流的数据进行排序，有什么区别。\n使用场景不同，Sorted Merge组件应用于并行数据处理环境，是分区后的一种数据收集方式（Sorted Merge收集方式）。在大数据量时，原则上希望把尽可能多的数据处理放在集群上处理，然后在主服务器上进行合并，因为Sorted Merge组件的算法是基于已排好顺序的集合(归并排序算法)，在对已排过序的集合的排序效率高于记录排序。我们完全没必须把从属服务器上收集到的已排序的数据，使用“记录排序”组件再打乱重新排序。基于效率上考虑建议在对已排好序的数据集进行排序时，使用Sorted Merge组件。\n \n7.\t建议：分布式排序\n当需要使用排序组件时，最好将排序组件水平扩展到集群上运行，以获得较好的运行效率。每个从属服务器对传入的记录进行排序，然后在主服务器上使用Sorted Merge组件合并来自从属服务器的排序结果。Sorted Merge组件被设计用来接收从前面的组件接收排序后的记录，并且在合并时能够保证记录的顺序。对于已排序过的多个数据集进行排序时，Sorted Merge的效率高于记录排序，这也是使用Sorted Merge组件的原因。\n\n![\"分布式排序\"](images/sorted merger1.jpg)\n\n","slug":"kettle/Sorted Merge组件","published":1,"updated":"2017-08-18T01:44:35.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjh7polsk000vbp7ri727gj7n","content":"<h2 id=\"Sorted-Merge组件\"><a href=\"#Sorted-Merge组件\" class=\"headerlink\" title=\"Sorted Merge组件\"></a>Sorted Merge组件</h2><h3 id=\"概念\"><a href=\"#概念\" class=\"headerlink\" title=\"概念\"></a>概念</h3><table><br>    <tr><br>        <th>图标</th><br>        <th>组件名称</th><br>        <th>功能说明</th><br>    </tr><br>    <tr><br>        <td>图片</td><br>        <td>Sorted Merge</td><br>        <td>合并来自多个输入步骤的记录，提供按给定的关键字段进行排序后的行记录。</td><br>    </tr><br></table><br>Sorted Merge组件处理来自多个输入步骤的记录，提供按给定的关键字段进行排序后的行记录的功能。<br>    用例:当您并行方式使用“记录排序”步骤的多个副本(通过“改变起始的副本数量”或集群环境中时)的每个排序块需要合并在一起,以确保正确的排序序列。可以通过在在“记录排序”组件后添加”Sorted Merge”组件来处理。<br>    注：应用于多个复本或集群环境下的并行运算时，前置组件必须为“记录排序”组件。表 0 15Sorted Merge组件属性说明<br><table><br>    <tr><br>        <th>属性项</th><br>        <th>属性说明</th><br>    </tr><br>    <tr><br>        <td>组件名称</td><br>        <td>步骤的名字，这个名字在一个转换中必须是唯一的。</td><br>    </tr><br>    <tr><br>        <td>字段</td><br>        <td>指定字段名和排序方向(升序/降序);点击获取字段检索列表字段从输入流。</td><br>    </tr><br>    <tr><br>        <td>字段</td><br>        <td>指定排序的字段名。</td><br>    </tr><br>    <tr><br>        <td>升序</td><br>        <td>排序原则：升序或降序。如果选择升序，<br>        排序顺序将是：数字-&gt;英文-&gt;汉字，汉字是按照拼音排序的，<br>        也同样会按照声调排序。如果是多音字，只会取一个读音，<br>        无法根据语境判断其的读音。</td><br>    </tr><br></table>\n\n<p>###问题释疑###</p>\n<ol>\n<li><p>Sorted Merge组件是如何工作的？<br>它用来合并2个输入数据流（按同一字段排序后的、相同格式的数据流）。</p>\n</li>\n<li><p>Sorted Merge组件的原理？<br>用于比较两组数据以确保合并后的输出也是排序的。它是通过串行方式来处理的，并不需要考虑整个数据集合。它实际上用于不同机器的集群环境下，而不是在单机上。运行效率取决于你有多少个可用的CPU。</p>\n</li>\n<li><p>Sorted Merge组件的使用前提？<br>对于“Sorted Merge”组件的所有输入行结构需要是相同的（相同的字段布局、字段类型等）且需要按指定的键进行排序。只有这样才能保证输出是该键进行排序的。</p>\n</li>\n<li><p>“Sorted Merge”组件的排序字段指定为与“字段排序”组件排序字段相同时，不能按指定排序?<br>Sorted Merge组件的合并是基于前置组件（”记录排序“组件）排好顺序的集合里的指定字段进行合并，排序是基于前置组件中相同排序字段的数据集内的排序，所以说在相同排序字段集中指定排序方式时，排序是不变的。既然没有意义为什么要保留呢？是因为与前置组件（Sorted Merge组件）保持一致，如果设计成按照排序字段的选择来去掉”Sorted Merge“组件中的对应字段，会增加复杂性，同时又需要解释为什么少一个字段，索性保留。</p>\n</li>\n<li><p>对于的分区功能，Sorted Merge组件必要性？<br>不是必须的。因为Sorted Merge多用于并行处理环境，常与分区功能组合使用，Sorted Merge组件是对分区的一种收集方式（Sorted Merge）的支持，所以说不是必须的，如果收集的记录不要求排序，我们完成可以不使用此组件；<br>注：分区的收集是指当分区的组件处理完成发送到没有分区的组件时，记录将被合并。目前支持的合并方法有两种方式，一种是轮流从各个分区接收记录，另一种是对于Sorted Merge的收集方式，Kettle中有”Sorted Merge“组件来完成。</p>\n</li>\n<li><p>记录排序组件和Sorted Merge组件都可以对多输入流的数据进行排序，有什么区别。<br>使用场景不同，Sorted Merge组件应用于并行数据处理环境，是分区后的一种数据收集方式（Sorted Merge收集方式）。在大数据量时，原则上希望把尽可能多的数据处理放在集群上处理，然后在主服务器上进行合并，因为Sorted Merge组件的算法是基于已排好顺序的集合(归并排序算法)，在对已排过序的集合的排序效率高于记录排序。我们完全没必须把从属服务器上收集到的已排序的数据，使用“记录排序”组件再打乱重新排序。基于效率上考虑建议在对已排好序的数据集进行排序时，使用Sorted Merge组件。</p>\n</li>\n<li><p>建议：分布式排序<br>当需要使用排序组件时，最好将排序组件水平扩展到集群上运行，以获得较好的运行效率。每个从属服务器对传入的记录进行排序，然后在主服务器上使用Sorted Merge组件合并来自从属服务器的排序结果。Sorted Merge组件被设计用来接收从前面的组件接收排序后的记录，并且在合并时能够保证记录的顺序。对于已排序过的多个数据集进行排序时，Sorted Merge的效率高于记录排序，这也是使用Sorted Merge组件的原因。</p>\n</li>\n</ol>\n<p><img src=\"images/sorted merger1.jpg\" alt=\"&quot;分布式排序&quot;\"></p>\n","site":{"data":{}},"excerpt":"","more":"<h2 id=\"Sorted-Merge组件\"><a href=\"#Sorted-Merge组件\" class=\"headerlink\" title=\"Sorted Merge组件\"></a>Sorted Merge组件</h2><h3 id=\"概念\"><a href=\"#概念\" class=\"headerlink\" title=\"概念\"></a>概念</h3><table><br>    <tr><br>        <th>图标</th><br>        <th>组件名称</th><br>        <th>功能说明</th><br>    </tr><br>    <tr><br>        <td>图片</td><br>        <td>Sorted Merge</td><br>        <td>合并来自多个输入步骤的记录，提供按给定的关键字段进行排序后的行记录。</td><br>    </tr><br></table><br>Sorted Merge组件处理来自多个输入步骤的记录，提供按给定的关键字段进行排序后的行记录的功能。<br>    用例:当您并行方式使用“记录排序”步骤的多个副本(通过“改变起始的副本数量”或集群环境中时)的每个排序块需要合并在一起,以确保正确的排序序列。可以通过在在“记录排序”组件后添加”Sorted Merge”组件来处理。<br>    注：应用于多个复本或集群环境下的并行运算时，前置组件必须为“记录排序”组件。表 0 15Sorted Merge组件属性说明<br><table><br>    <tr><br>        <th>属性项</th><br>        <th>属性说明</th><br>    </tr><br>    <tr><br>        <td>组件名称</td><br>        <td>步骤的名字，这个名字在一个转换中必须是唯一的。</td><br>    </tr><br>    <tr><br>        <td>字段</td><br>        <td>指定字段名和排序方向(升序/降序);点击获取字段检索列表字段从输入流。</td><br>    </tr><br>    <tr><br>        <td>字段</td><br>        <td>指定排序的字段名。</td><br>    </tr><br>    <tr><br>        <td>升序</td><br>        <td>排序原则：升序或降序。如果选择升序，<br>        排序顺序将是：数字-&gt;英文-&gt;汉字，汉字是按照拼音排序的，<br>        也同样会按照声调排序。如果是多音字，只会取一个读音，<br>        无法根据语境判断其的读音。</td><br>    </tr><br></table>\n\n<p>###问题释疑###</p>\n<ol>\n<li><p>Sorted Merge组件是如何工作的？<br>它用来合并2个输入数据流（按同一字段排序后的、相同格式的数据流）。</p>\n</li>\n<li><p>Sorted Merge组件的原理？<br>用于比较两组数据以确保合并后的输出也是排序的。它是通过串行方式来处理的，并不需要考虑整个数据集合。它实际上用于不同机器的集群环境下，而不是在单机上。运行效率取决于你有多少个可用的CPU。</p>\n</li>\n<li><p>Sorted Merge组件的使用前提？<br>对于“Sorted Merge”组件的所有输入行结构需要是相同的（相同的字段布局、字段类型等）且需要按指定的键进行排序。只有这样才能保证输出是该键进行排序的。</p>\n</li>\n<li><p>“Sorted Merge”组件的排序字段指定为与“字段排序”组件排序字段相同时，不能按指定排序?<br>Sorted Merge组件的合并是基于前置组件（”记录排序“组件）排好顺序的集合里的指定字段进行合并，排序是基于前置组件中相同排序字段的数据集内的排序，所以说在相同排序字段集中指定排序方式时，排序是不变的。既然没有意义为什么要保留呢？是因为与前置组件（Sorted Merge组件）保持一致，如果设计成按照排序字段的选择来去掉”Sorted Merge“组件中的对应字段，会增加复杂性，同时又需要解释为什么少一个字段，索性保留。</p>\n</li>\n<li><p>对于的分区功能，Sorted Merge组件必要性？<br>不是必须的。因为Sorted Merge多用于并行处理环境，常与分区功能组合使用，Sorted Merge组件是对分区的一种收集方式（Sorted Merge）的支持，所以说不是必须的，如果收集的记录不要求排序，我们完成可以不使用此组件；<br>注：分区的收集是指当分区的组件处理完成发送到没有分区的组件时，记录将被合并。目前支持的合并方法有两种方式，一种是轮流从各个分区接收记录，另一种是对于Sorted Merge的收集方式，Kettle中有”Sorted Merge“组件来完成。</p>\n</li>\n<li><p>记录排序组件和Sorted Merge组件都可以对多输入流的数据进行排序，有什么区别。<br>使用场景不同，Sorted Merge组件应用于并行数据处理环境，是分区后的一种数据收集方式（Sorted Merge收集方式）。在大数据量时，原则上希望把尽可能多的数据处理放在集群上处理，然后在主服务器上进行合并，因为Sorted Merge组件的算法是基于已排好顺序的集合(归并排序算法)，在对已排过序的集合的排序效率高于记录排序。我们完全没必须把从属服务器上收集到的已排序的数据，使用“记录排序”组件再打乱重新排序。基于效率上考虑建议在对已排好序的数据集进行排序时，使用Sorted Merge组件。</p>\n</li>\n<li><p>建议：分布式排序<br>当需要使用排序组件时，最好将排序组件水平扩展到集群上运行，以获得较好的运行效率。每个从属服务器对传入的记录进行排序，然后在主服务器上使用Sorted Merge组件合并来自从属服务器的排序结果。Sorted Merge组件被设计用来接收从前面的组件接收排序后的记录，并且在合并时能够保证记录的顺序。对于已排序过的多个数据集进行排序时，Sorted Merge的效率高于记录排序，这也是使用Sorted Merge组件的原因。</p>\n</li>\n</ol>\n<p><img src=\"images/sorted merger1.jpg\" alt=\"&quot;分布式排序&quot;\"></p>\n"},{"title":"Kettle插件架构","date":"2017-04-15T11:42:49.000Z","_content":"#  #Kettle插件体系\n最近公司内有业务系统到数据中心同步的升级改造需求，从各个业务系统收集增量数据到数据中心的数据仓库平台。因为开发周期短暂，需要快速的响应，开发出可用的产品，所以决定借鉴开源程序Kettle，开发一个文件解析组件，然后利用Kettle平台的大数据组件进行与数据中心大数据平台对接\n\n数据同步部分是：业务系统（RDBMS）->Kettle(azkaban进行调度)->数据中心，因为Kettle的增量抽取组件经常出现数据不一致等问题，所以目前已更改为：业务系统（RDBMS）->OGG（CDC增量抽取）->数据中心的方式。\n\n本文主要介绍如何扩展Kettle的功能，部分内容来自《Pentaho Kettle解决方案：使用PDI构建开源ETL解决方案》一书，推荐购买阅读。\n##  ##架构\n我们先看Kettle插件架构。\n ![](http://i.imgur.com/mLvXMuV.jpg)\n从功能上看，Kettle内部的对象和外部插件没有任何区别。因为它们使用的API都是一样的，它们只是在运行时的加载方式不同。\n从Kettle4以后，Kettle内部有一个插件注册系统，它负责加载各种内部和外部插件。插件有以下两个标识属性。\n**插件类型**：由PluginTypeInterface接口定义。例如StepPluginType、JobEntryPluginType、PartitionerPluginType和RepositoryPluginType。\n**插件ID**：这是一个字符串数组，用来唯一标识一个插件。因为旧的插件可以被新的插件代替，一个插件可以有多个ID。在大多数情况下，插件只使用一个单一的字符串，如TableInput是“表输入”步骤的ID，MYSQL是MySQL数据库类型的ID。\n当Kettle环境初始化以后，插件注册系统首先加载所有的内部对象，Kettle读取下面的配置文件来加载内部对象，这些配置文件位于Kettle的.jar文件中。\n\t Kettle-steps.xml：内部转换步骤。\n\t Kettle-job-entries.xml：内部作业项。\n\t Kettle-partition-plugins.xml：内部分区类型。\n\t Kettle-database-types.xml：内部数据库类型。\n\t Kettle-repositories.xml：内部资源库类型。\n\n插件注册系统加载了所有的内部对象后，就要搜索可用的外部插件。通过浏览plugins/目录的各个子目录下的.jar文件来完成。它搜索特定的Kettle annotations来判断一个类是否是插件。加载过程将在本章的后面介绍。\n因为在内部对象加载后才加载插件，所以插件会替代相同ID的已加载的内部对象。例如，你创建了插件，插件的ID是TableInput，就可以替换Kettle标准的“表输入”步骤。这个功能可以让你用插件替换Kettle内置的步骤。可以通过子类继承方式，直接扩展已有步骤的某些功能。\n\n##  ##插件类型\nKettle有下面几种插件类型（下面的插件是Kettle4.0的插件类型，新版kettle包含了很多新的插件，比如视图插件、大数据插件等等）。\n\n- 转换步骤插件：在Kettle转换中使用的步骤，用来处理数据行。\n\n\n- 作业项插件：在Kettle作业中使用的作业项，用来实现某个任务。\n\t\n\n- 分区方法插件：利用输入字段的值指定自己的分区规则。\n\t\n\n- 数据库类型插件：用来扩展不同的数据库类型。\n\t\n\n- 资源库类型插件：可以把Kettle元数据保存为自定义类型或格式。\n\n说明：除了这些类型，还有Spoon类型的插件，可以把功能扩展到Spoon，本书不介绍这个功能。\n##  ##转换步骤插件\n转换步骤插件包括了四个Java类，这四个类分别实现四个接口。\n\n\n- StepMetaInterface：这个接口对外 提供步骤的元数据并处理串行化。\n\n\n- StepInterface:这个接口根据上面接口提供的元数据，来实现步骤的具体功能。\n\n\n- StepDataInterface:这个接口用来存储步骤的临时数据、文件句柄等。\n\n\n- StepDialogInterface:这个接口是Spoon里的图形界面，用来编辑步骤的元数据。\n\n接下来，我们介绍这些接口的基本内容。对于每个接口，在一个简单的“Hello World”例子里提供这些类的相应实现。“Hello World”例子将在数据流里增加一个字段，字段名用户可以自定义，字段值是”Hello world!“。最后介绍一下如何部署这个例子。\n###  ###StepMetaInterface\n接口org.pentaho.di.trans.step.StepMetaInterface负责步骤里所有和元数据相关的任务。和元数据相关的工作包括：  \n元数据和XML(或资源库)之间的序列化和反序列化  \ngetXML（）和loadXML()  \nsaveRep()和readRep()  \n\n描述输出字段  \ngetFields()  \n\n检验元数据是否正确  \nCheck()  \n\n获取步骤相应的要SQL语句，使步骤可以正确运行  \ngetSQLStatements()  \n\n给元数据设置默认值  \nsetDefault()  \n\n完成对数据库的影响分析  \nanalyseImpact()  \n\n描述各类输入和输出流  \ngetStepIOMeta()  \nsearchInfoAndTargetSteps()  \nhandleStreamSelection()  \ngetOptionalStreams()  \nresetStepIoMeta()  \n\n导出元数据资源  \nexportResources()  \ngetResourceDependencies()  \n\n描述使用的库  \ngetUsedLibraries()  \n\n描述使用的数据库连接  \ngetUsedDatabaseConnections()  \n\n描述这个步骤需要的字段（通常是一个数据库表）  \ngetRequiredFields()  \n\n描述步骤是否具有某些功能  \nsupportsErrorHandling()  \nexcludeFromRowLayoutVerification()  \nexcludeFromCopyDistributeVerification()  \n\n这个接口里还定义了几个方法来说明这四个接口如何结合到一起。  \nString getDialogClassName():用来描述实现了StepDialogInterface接口的对话框类的名字。如果这个方法返回了null，调用类会根据实现了StepMetaInterface接口的类的类名和包名来自动生成对话框类的名字。  \nSetpInterface getStep():创建一个实现了StepInterface接口的类。  \nStepDataInterface getStepData():创建一个实现了StepDataInterface接口的类。  \n现在我们看看”Hello World”例子里对SetpMetaInterface接口的实现  \nHelloworldStepMeta.java\n    package org.kettlesolutions.plugin.step.helloworld;\n    \n    import java.util.List;\n    import java.util.Map;\n    \n    import org.pentaho.di.core.CheckResult;\n    import org.pentaho.di.core.CheckResultInterface;\n    import org.pentaho.di.core.Const;\n    import org.pentaho.di.core.Counter;\n    import org.pentaho.di.core.annotations.Step;\n    import org.pentaho.di.core.database.DatabaseMeta;\n    import org.pentaho.di.core.exception.KettleException;\n    import org.pentaho.di.core.exception.KettleStepException;\n    import org.pentaho.di.core.exception.KettleXMLException;\n    import org.pentaho.di.core.row.RowMetaInterface;\n    import org.pentaho.di.core.row.ValueMeta;\n    import org.pentaho.di.core.row.ValueMetaInterface;\n    import org.pentaho.di.core.variables.VariableSpace;\n    import org.pentaho.di.core.xml.XMLHandler;\n    import org.pentaho.di.i18n.BaseMessages;\n    import org.pentaho.di.repository.ObjectId;\n    import org.pentaho.di.repository.Repository;\n    import org.pentaho.di.trans.Trans;\n    import org.pentaho.di.trans.TransMeta;\n    import org.pentaho.di.trans.step.BaseStepMeta;\n    import org.pentaho.di.trans.step.StepDataInterface;\n    import org.pentaho.di.trans.step.StepInterface;\n    import org.pentaho.di.trans.step.StepMeta;\n    import org.pentaho.di.trans.step.StepMetaInterface;\n    import org.w3c.dom.Node;\n    \n    @Step(\n    \t\tid=\"Helloworld\",\n    \t\tname=\"name\",\n    \t\tdescription=\"description\",\n    \t\tcategoryDescription=\"categoryDescription\", \n    \t\timage=\"org/kettlesolutions/plugin/step/helloworld/HelloWorld.png\",\n    \t\ti18nPackageName=\"org.kettlesolutions.plugin.step.helloworld\"\n    ) \n    public class HelloworldStepMeta extends BaseStepMeta implements StepMetaInterface {\n    \t/**\n    \t * PKG变量说明了messages包的位置，在messages包里有各种国际化的资源文件。\n    \t * 在本章后面经常要看到的BaseMessages.getString()方法，就是根据软件的国际化\n    \t * 设置，从不同的文件中获取文字。PKG变量通常位于类的最上方，被国际化图形工具使用，\n    \t * 通过国际化图形工具，国际化人员可以编辑不同的国际化资源文件。所以我们会在很多Kettle\n    \t * 代码里看见这样的结构。\n    \t */\n    \tprivate static Class<?> PKG = HelloworldStep.class; //for i18n\n    \tpublic enum Tag {//field_name用于保存用户输入的字段名：保存“Hello，world！\"字符串的字段名。\n    \t\tfield_name,\n    \t};\n    \t\n    \tprivate String fieldName;\n    \t\n    \t/**\n    \t * @return the fieldName\n    \t */\n    \tpublic String getFieldName() {\n    \t\treturn fieldName;\n    \t}\n    \n    \t/**\n    \t * @param fieldName the fieldName to set\n    \t */\n    \tpublic void setFieldName(String fieldName) {\n    \t\tthis.fieldName = fieldName;\n    \t}\n    \n    \t/**\n    \t * checks parameters, adds result to List<CheckResultInterface>\n    \t * used in Action > Verify transformation\n    \t * 验证用户是否在对话框里输入了字段名，并把验证结果添加到检验转换时出现的问题列表里。（最好\n    \t * 要检验用户输入的所有选项，而不只是容易出错的选项）\n    \t */\n    \tpublic void check(List<CheckResultInterface> remarks, TransMeta transMeta, StepMeta stepMeta, \n    \t\t\tRowMetaInterface prev, String input[], String output[], RowMetaInterface info) {\n    \t\t\n    \t\tif (Const.isEmpty(fieldName)) {\n    \t\t\tCheckResultInterface error = new CheckResult(\n    \t\t\t\tCheckResult.TYPE_RESULT_ERROR, \n    \t\t\t\tBaseMessages.getString(PKG, \"HelloworldMeta.CHECK_ERR_NO_FIELD\"), \n    \t\t\t\tstepMeta\n    \t\t\t);\n    \t\t\tremarks.add(error);\n    \t\t} else {\n    \t\t\tCheckResultInterface ok = new CheckResult(\n    \t\t\t\tCheckResult.TYPE_RESULT_OK, \n    \t\t\t\tBaseMessages.getString(PKG, \"HelloworldMeta.CHECK_OK_FIELD\"), \n    \t\t\t\tstepMeta\n    \t\t\t);\n    \t\t\tremarks.add(ok);//把验证结果添加到检验转换时出现的问题列表里。\n    \t\t}\n    \t}\n    \n    \t/**\n    \t *\tcreates a new instance of the step (factory)\n    \t * getStep、getStepData和getDialogClassName()方法提供了与这个步骤里其它三个接口之间的桥梁\n    \t 这个接口里还定义了几个方法来说明这四个接口如何结合到一起。\n    \tString getDialogClassName():用来描述实现了StepDialogInterace接口的对话框类的名字。如果这个方法返回\n    \t\t\t\t了null，调用类会根据实现了StepMetaInterface接口的类的类名和包名来自动生成对话框类的名字。\n    \tStepInterface getStep():创建一个实现了StepInterface接口的类。\n    \tStepInterface getStepData():创建一个实现了StepDataInterface接口的类。\n    \n    \t */\n    \tpublic StepInterface getStep(StepMeta stepMeta, StepDataInterface stepDataInterface,\n    \t\t\tint copyNr, TransMeta transMeta, Trans trans) {\n    \t\treturn new HelloworldStep(stepMeta, stepDataInterface, copyNr, transMeta, trans);\n    \t}\n    \n    \t/**\n    \t * creates new instance of the step data (factory)\n    \t * getStep、getStepData和getDialogClassName()方法提供了与这个步骤里其它三个接口之间的桥梁\n    \t */\n    \tpublic StepDataInterface getStepData() {\n    \t\treturn new HelloworldStepData();\n    \t}\n    \t/**\n    \t * getStep、getStepData和getDialogClassName()方法提供了与这个步骤里其它三个接口之间的桥梁\n    \t */\n    \t@Override\n    \tpublic String getDialogClassName() {\n    \t\treturn HelloworldStepDialog.class.getName();\n    \t}\n    \n    \t/**\n    \t * deserialize from xml \n    \t * databases = list of available connections\n    \t * counters = list of sequence steps\n    \t * \n    \t * 下面的四个方法loadXML()、getXML()、readRep()和saveRep()把元数据保存到XML文件或资源库里，\n    \t * 或者从XML文件或资源库读取元数据。保存到文件的方法利用了像XStream（http://xstream.codehaus.org）这\n    \t * 样的XML串行化技术。\n    \t */\n    \tpublic void loadXML(Node stepDomNode, List<DatabaseMeta> databases,\n    \t\t\tMap<String, Counter> sequenceCounters) throws KettleXMLException {\n    \t\tfieldName = XMLHandler.getTagValue(stepDomNode, Tag.field_name.name());\n    \t}\n    \t\n    \t/**\n    \t * @Override\n    \t */\n    \tpublic String getXML() throws KettleException {\n    \t\tStringBuilder xml = new StringBuilder();\n    \t\txml.append(XMLHandler.addTagValue(Tag.field_name.name(), fieldName));\n    \t\treturn xml.toString();\n    \t}\n    \t\n    \t/**\n    \t * De-serialize from repository (see loadXML)\n    \t */\n    \tpublic void readRep(Repository repository, ObjectId stepIdInRepository,\n    \t\t\tList<DatabaseMeta> databases, Map<String, Counter> sequenceCounters)\n    \t\t\tthrows KettleException {\n    \t\tfieldName = repository.getStepAttributeString(stepIdInRepository, Tag.field_name.name());\n    \t}\n    \n    \t/**\n    \t * serialize to repository\n    \t */\n    \tpublic void saveRep(Repository repository, ObjectId idOfTransformation, ObjectId idOfStep)\n    \t\t\tthrows KettleException {\n    \t\trepository.saveStepAttribute(idOfTransformation, idOfStep, Tag.field_name.name(), fieldName);\n    \t}\n    \t\n    \t\n    \t/**\n    \t * initiailize parameters to default\n    \t */\n    \tpublic void setDefault() {\n    \t\tfieldName = \"helloField\";\n    \t}\n    \n    \t/**\n    \t * getFields()方法非常重要，因为它描述了输出数据行的结构。这个方法需要修改inputRowMeta对象，使这个对象和\n    \t * 输出格式匹配。Spoon和后面的步骤都需要知道这个步骤要输出哪些字段。最常见的一种方法，可以给输出的RowMetaInterface对象\n    \t * 添加一个ValueMetaInterface对象。在ValueMetaInterface对象里设置的信息越详细越好，可以设置的信息包括数据类型、长度、\n    \t * 精度、格式掩码，等等。添加的字段描述元信息越多，后面生成的SQL就越准确。\n    \t */\n    \t@Override\n    \tpublic void getFields(RowMetaInterface inputRowMeta, String name,\n    \t\t\tRowMetaInterface[] info, StepMeta nextStep, VariableSpace space)\n    \t\t\tthrows KettleStepException {\n    \t\tString realFieldName = space.environmentSubstitute(fieldName);\n    \t\t//值的元数据使用ValueMetaInterface接口描述数据流里的一个字段。这个接口里定义了字段的名字、数据类型、长度、精度，等等。下面的例子用于创建一个ValueMetaInterface对象。\n    \t\tValueMetaInterface field = new ValueMeta(realFieldName, ValueMetaInterface.TYPE_STRING);\n    \t\tfield.setOrigin(name);\t\t\n    \t\tinputRowMeta.addValueMeta(field);\n    \t}\n    }\n\n\n\n代码解析\n    @Step(\n    \t\tid=\"Helloworld\",\n    \t\tname=\"name\",\n    \t\tdescription=\"description\",\n    \t\tcategoryDescription=\"categoryDescription\", \n    \t\timage=\"org/kettlesolutions/plugin/step/helloworld/HelloWorld.png\",\n    \t\ti18nPackageName=\"org.kettlesolutions.plugin.step.helloworld\"\n    )\n这段代码里的@Step annotation用来通知Kettle的插件注册系统：这个类是一个步骤类型的插件。在annotation里可以指定插件的ID、图标、国际代的包、本地化的名称、类别、描述。其中后三项是资源文件里的Key，需要在资源文件里设置真正的值。i18nPackageName指定了资源文件的包名，例如我们这个例子的资源文件位于org/kettlesolutions/plugin/step/helloworld/messages目录下，en_US（英语，美国）的本地代资源文件是messages_en_US.properties。我们例子里的这个资源文件的内容是：\nname=Hello world\ndescription=A very simple step that adds a new \"Helllo world\" field to the incoming stream\n注意，如果你指定了不存在的分类，Spoon会创建这个分类，并在Spoon的分类树的最上方显示这个分类。\n最后，annotation里的image标签指定了插件的图标。需要32*32像素的PNG文件，可以使用透明样式。\n后面的代码行说明这个类实现了StepMetaInterface接口。在BaseStepMeta抽象类里定义了这个接口的很多默认实现，可以直接继承这个抽象类，然后把工作集中在插件特有的功能上。\n\n    public class HelloworldStepMeta extends BaseStepMeta implements StepMetaInterface\n\t\n\n\n下面的四个方法loadXML()、getXML()、readRep()和saveRep()把元数据保存到XML文件或资源库里，或者从XML文件或资源库读取元数据。保存到文件的方法利用了像XStream（http://xstream.codehaus.org）这样的XML串行化技术。\n\n\ngetFields()方法非常重要，因为它描述了输出数据行的结构。这个方法需要修改inputRowMeta对象，使这个对象和输出格式匹配。Spoon和后面的步骤都需要知道这个步骤要输出哪些字段。最常见的一种方法，可以给输出的RowMetaInterface对象添加一个ValueMetaInterface对象。在ValueMetaInterface对象里设置的信息越详细越好，可以设置的信息包括数据类型、长度、精度、格式掩码，等等。添加的字段描述元信息越多，后面生成的SQL就越准确。\n####  ####值的元数据（Value Metadata）\n值的元数据使用ValueMetaInterface接口描述数据流里的一个字段。这个接口里定义了字段的名字、数据类型、长度、精度，等等。下面的例子用于创建一个ValueMetaInterface对象。  \n    ValueMetaInterface dateMeta = new ValueMeta(“birthdate”,ValueMetaInterface.TYPE_DATE);\n这个接口也负责转换数据格式。我们建议使用ValueMetaInterface接口来完成所有数据转换的工作。例如，日期类型的数据，如果想把它转换为dateMeta对象里定义的字符串格式，可以用下面的代码：  \n    //java.util.Date birthdate\n    String birthDateString = dateMeta.getString(birthdate);\nValueMeta类负责转换。因为有ValueMetaInterface进行数据类型的转换，所以你不用再去做额外的数据类型转换的工作。  \n使用ValueMetaInterface接口时还要注意数据对象是否为Null。从上一个步骤可以接收到一个数据对象和一个描述数据对象的ValueMetaInterface对象。我们要检查这个数据对象是否为null，在某些情况下如果数据对象为空是不正确的。例如：  \n数据对象是String类型，有10个空格，Value Metadata需要trim这个字符串。  \n在Value Metadata里已经定义了从文本文件里加载的数据，要延迟转换为字符串。所以数据要由二进制的格式（原始数据格式），转换为字符串格式，然后再转换为其它格式的数据。  \n一般使用下面的方法检查数据对象是否为空：  \n    Boolean n = valueMeta.isNull(valueDate);\n重要：要保证传给ValueMetaInterface对象的数据是在元数据里定义的数据类型。表23-1说明了  ValueMetaInterface里定义的数据类型和Java数据类型的对应关系。  \nKettle元数据类型和Java里数据类型的对应关系  \n<table>\n    <tr>\n        <th>Value Meta Type</th>\n        <th>Java Class</th>\n    </tr>\n    <tr>\n        <td>ValueMetaInterface.TYPE_STRING</td>\n        <td>Java.lang.String</td>\n    </tr>\n    <tr>\n        <td>ValueMetaInterface.TYPE_DATE</td>\n        <td>Java.util.Date</td>\n    </tr>\n    <tr>\n        <td>ValueMetaInterface.TYPE_BOOLEAN</td>\n        <td>Java.lang.Boolean</td>\n    </tr>\n\t<tr>\n        <td>ValueMetaInterface.TYPE_NUMBER</td>\n        <td>Java.lang.Double</td>\n    </tr>\n\t<tr>\n        <td>ValueMetaInterface.TYPE_INTEGER</td>\n        <td>Java.lang.Long</td>\n    </tr>\n\t<tr>\n        <td>ValueMetaInterface.TYPE_BIGNUMBER</td>\n        <td>Java.math.BigDecimal</td>\n    </tr>\n\t<tr>\n        <td>ValueMetaInterface.TYPE_BINARY</td>\n        <td>Byte[]</td>\n    </tr>\n</table>\n\n\n####  ####行的元数据（Row Meatadata）\n行的元数据使用RowMetaInterface接口来描述数据行的元数据，而不是一个列的元数据。实际上，RowMetaInterface的类里包含了一组ValueMetaInterface。另外还包括了一些方法来操作行元数据，倒如查询值、检查值是否存、替换值的元数据等。  \n行的元数据里唯一的规则就是一行里的列的名字必须唯一。当你添加了一个新列时，如果新列的名字和已有列的名字相同，列名后面会自动加上“_2”后缀。如果再加一个同名的列会自动加上”_3“后缀，等等。  \n因为在步骤里通常是和数据行打交道，所以从数据行里直接取数据会更方便。可以使用很多类似于getNumber()、getString()这样的方法直接从数据行取数据。例如，销售数据存储在第四列里，可以用下面的代码获取这个数据：  \n\n    Double sales = getInputRowMeta().getNumber(rowData,3);\n通过索引获取数据是最快的方式。通过indexOfValue()方法可以获取列在一行里的索引。这个方法扫描列数组，速度并不快。所以，如果要处理所有数据行，我们建议只查询一次列索引。一般是在步骤接收到第一行数据时，就查询列索引，将查询到的列索引保存起来，供后面的数据行使用。  \n###  ###StepDatainterface\n实现了org.pentaho.di.trans.step.StepDataInterface接口的类用来维护步骤的执行状态，以及存储临时对象。例如，可以把输出行的元数据、数据库连接、输入输出流等存储到这个对象里。  \nHelloworldStepData.java  \n    package org.kettlesolutions.plugin.step.helloworld;\n\nimport org.pentaho.di.core.row.RowMetaInterface;\nimport org.pentaho.di.trans.step.BaseStepData;\nimport org.pentaho.di.trans.step.StepDataInterface;\n\npublic class HelloworldStepData extends BaseStepData implements StepDataInterface {\n\n\tpublic RowMetaInterface outputRowMeta;\n\n}\n    \n###  ###StepDialogInterface\n实现org.pentaho.di.trans.step.StepDialogInterfac接口的类用来提供一个用户界面，用户通过这个界面输入元数据（转换参数）。用户界面就是一个对话框。这个接口里包含了类似open()和setRepository()等的几个简单的方法。    \n####  ####Eclipse SWT\nKettle里使用Eclipse SWT作为界面开发包，所以你也要使用SWT来开发对话框窗口。SWT为不同的操作系统Windows、OS X、Linux和Unix提供了一个抽象层。所以SWT的图形界面和操作系统期货的程序的界面风格非常相近。  \n在开始进行SWT开发之前，建议先访问SWT主面以了解更多的内容http://www.eclipse/org/swt。在SWT的网站上，你可以了解到SWT能做出什么样的界面效果：  \nSWT控件页，http://www.eclipse.org/swt/widgets/，给出了你能使用的所有控件。  \nSWT样例页，http://www.eclipse.org/swt/snippets/，给出了许多代码例子。  \n最好的资源就是Kettle里150个内置步骤的对话框源代码。  \n\nHelloworldStepDialog.java  \n    package org.kettlesolutions.plugin.step.helloworld;\n    \n    import org.eclipse.swt.SWT;\n    import org.eclipse.swt.events.ModifyEvent;\n    import org.eclipse.swt.events.ModifyListener;\n    import org.eclipse.swt.events.SelectionAdapter;\n    import org.eclipse.swt.events.SelectionEvent;\n    import org.eclipse.swt.events.ShellAdapter;\n    import org.eclipse.swt.events.ShellEvent;\n    import org.eclipse.swt.layout.FormAttachment;\n    import org.eclipse.swt.layout.FormData;\n    import org.eclipse.swt.layout.FormLayout;\n    import org.eclipse.swt.widgets.Button;\n    import org.eclipse.swt.widgets.Control;\n    import org.eclipse.swt.widgets.Display;\n    import org.eclipse.swt.widgets.Event;\n    import org.eclipse.swt.widgets.Label;\n    import org.eclipse.swt.widgets.Listener;\n    import org.eclipse.swt.widgets.Shell;\n    import org.eclipse.swt.widgets.Text;\n    import org.pentaho.di.core.Const;\n    import org.pentaho.di.i18n.BaseMessages;\n    import org.pentaho.di.trans.TransMeta;\n    import org.pentaho.di.trans.step.BaseStepMeta;\n    import org.pentaho.di.trans.step.StepDialogInterface;\n    import org.pentaho.di.ui.core.widget.TextVar;\n    import org.pentaho.di.ui.trans.step.BaseStepDialog;\n    \n    public class HelloworldStepDialog extends BaseStepDialog implements\n    \t\tStepDialogInterface {\n    \n    \tprivate static Class<?> PKG = HelloworldStepMeta.class; // for i18n\n    \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t// purposes, needed\n    \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t// by Translator2!!\n    \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t// $NON-NLS-1$\n    \n    \tprivate HelloworldStepMeta input;\n    \n    \tprivate TextVar wFieldname;\n    \n    \tpublic HelloworldStepDialog(Shell parent, Object baseStepMeta,\n    \t\t\tTransMeta transMeta, String stepname) {\n    \t\t//初始化元数据对象以及步骤对话框的父类\n    \t\tsuper(parent, (BaseStepMeta) baseStepMeta, transMeta, stepname);\n    \t\tinput = (HelloworldStepMeta) baseStepMeta;\n    \t}\n    \n    \tpublic String open() {\n    \t\tShell parent = getParent();\n    \t\tDisplay display = parent.getDisplay();\n    \n    \t\tshell = new Shell(parent, SWT.DIALOG_TRIM | SWT.RESIZE | SWT.MIN\n    \t\t\t\t| SWT.MAX);\n    \t\tprops.setLook(shell);\n    \t\tsetShellImage(shell, input);\n    \n    \t\tModifyListener lsMod = new ModifyListener() {\n    \t\t\tpublic void modifyText(ModifyEvent e) {\n    \t\t\t\tinput.setChanged();\n    \t\t\t}\n    \t\t};\n    \t\tchanged = input.hasChanged();\n    \n    \t\tFormLayout formLayout = new FormLayout();\n    \t\tformLayout.marginWidth = Const.FORM_MARGIN;\n    \t\tformLayout.marginHeight = Const.FORM_MARGIN;\n    \n    \t\tshell.setLayout(formLayout);\n    \t\tshell.setText(BaseMessages.getString(PKG,\n    \t\t\t\t\"HelloworldDialog.Shell.Title\")); //$NON-NLS-1$\n    \t\t\n    \t\t//所有控件的右侧使用一个自定义的百分对对齐。控件之间的间距使用一个常量，常量值是4像素。\n    \t\tint middle = props.getMiddlePct();\n    \t\tint margin = Const.MARGIN;\n    \n    \t\t// Stepname line\n    \t\twlStepname = new Label(shell, SWT.RIGHT);\n    \t\twlStepname.setText(BaseMessages.getString(PKG,\n    \t\t\t\t\"HelloworldDialog.Stepname.Label\")); //$NON-NLS-1$\n    \t\tprops.setLook(wlStepname);\n    \t\tfdlStepname = new FormData();\n    \t\tfdlStepname.left = new FormAttachment(0, 0);\n    \t\tfdlStepname.right = new FormAttachment(middle, -margin);\n    \t\tfdlStepname.top = new FormAttachment(0, margin);\n    \t\twlStepname.setLayoutData(fdlStepname);\n    \t\twStepname = new Text(shell, SWT.SINGLE | SWT.LEFT | SWT.BORDER);\n    \t\twStepname.setText(stepname);\n    \t\tprops.setLook(wStepname);\n    \t\twStepname.addModifyListener(lsMod);\n    \t\tfdStepname = new FormData();\n    \t\tfdStepname.left = new FormAttachment(middle, 0);\n    \t\tfdStepname.top = new FormAttachment(0, margin);\n    \t\tfdStepname.right = new FormAttachment(100, 0);\n    \t\twStepname.setLayoutData(fdStepname);\n    \t\tControl lastControl = wStepname;\n    \n    \t\t// Fieldname line\n    \t\t//创建一个新的标签控件，控件里文本靠右对齐\n    \t\tLabel wlFieldname = new Label(shell, SWT.RIGHT);\n    \t\twlFieldname.setText(BaseMessages.getString(PKG,\n    \t\t\t\t\"HelloworldDialog.Fieldname.Label\")); //$NON-NLS-1$\n    \t\t//下面一行为控件设置用户定义的背景色和字体\n    \t\tprops.setLook(wlFieldname);\n    \t\tFormData fdlFieldname = new FormData();\n    \t\tfdlFieldname.left = new FormAttachment(0, 0);\n    \t\tfdlFieldname.right = new FormAttachment(middle, -margin);\n    \t\tfdlFieldname.top = new FormAttachment(lastControl, margin);\n    \t\twlFieldname.setLayoutData(fdlFieldname);\n    \t\twFieldname = new TextVar(transMeta, shell, SWT.SINGLE | SWT.LEFT\n    \t\t\t\t| SWT.BORDER);\n    \t\tprops.setLook(wFieldname);\n    \t\twFieldname.addModifyListener(lsMod);\n    \t\tFormData fdFieldname = new FormData();\n    \t\tfdFieldname.left = new FormAttachment(middle, 0);\n    \t\tfdFieldname.top = new FormAttachment(lastControl, margin);\n    \t\tfdFieldname.right = new FormAttachment(100, 0);\n    \t\twFieldname.setLayoutData(fdFieldname);\n    \t\tlastControl = wFieldname;\n    \n    \t\t// Some buttons\n    \t\twOK = new Button(shell, SWT.PUSH);\n    \t\twOK.setText(BaseMessages.getString(PKG, \"System.Button.OK\")); //$NON-NLS-1$\n    \t\twCancel = new Button(shell, SWT.PUSH);\n    \t\twCancel.setText(BaseMessages.getString(PKG, \"System.Button.Cancel\")); //$NON-NLS-1$\n    \n    \t\tsetButtonPositions(new Button[] { wOK, wCancel }, margin, lastControl);\n    \n    \t\t// Add listeners\n    \t\tlsCancel = new Listener() {\n    \t\t\tpublic void handleEvent(Event e) {\n    \t\t\t\tcancel();\n    \t\t\t}\n    \t\t};\n    \t\tlsOK = new Listener() {\n    \t\t\tpublic void handleEvent(Event e) {\n    \t\t\t\tok();\n    \t\t\t}\n    \t\t};\n    \n    \t\twCancel.addListener(SWT.Selection, lsCancel);\n    \t\twOK.addListener(SWT.Selection, lsOK);\n    \n    \t\tlsDef = new SelectionAdapter() {\n    \t\t\tpublic void widgetDefaultSelected(SelectionEvent e) {\n    \t\t\t\tok();\n    \t\t\t}\n    \t\t};\n    \n    \t\twStepname.addSelectionListener(lsDef);\n    \t\twFieldname.addSelectionListener(lsDef);\n    \n    \t\t// Detect X or ALT-F4 or something that kills this window...\n    \t\tshell.addShellListener(new ShellAdapter() {//保证了窗口在非正常关闭时，取消用户的编辑\n    \t\t\tpublic void shellClosed(ShellEvent e) {\n    \t\t\t\tcancel();\n    \t\t\t}\n    \t\t});\n    \n    \t\t// Populate the data of the controls\n    \t\t//下面的代码把数据从步骤的元数据对象里复制到窗口的控件里\n    \t\tgetData();\n    \n    \t\t// Set the shell size, based upon previous time...\n    \t\t//窗口的大小和位置将根据窗口的自然属性、上次窗口大小和位置，以及显示屏的大小自动设置\n    \t\tsetSize();\n    \n    \t\tinput.setChanged(changed);\n    \n    \t\tshell.open();\n    \t\twhile (!shell.isDisposed()) {\n    \t\t\tif (!display.readAndDispatch())\n    \t\t\t\tdisplay.sleep();\n    \t\t}\n    \t\treturn stepname;\n    \t}\n    \n    \t/**\n    \t * Copy information from the meta-data input to the dialog fields.\n    \t */\n    \tpublic void getData() {\n    \t\twStepname.selectAll();\n    \t\t//为了防止用户向控件里输入空值，Kettle提供了一个静态方法来检查宿舍，Const.NVL()\n    \t\twFieldname.setText(Const.NVL(input.getFieldName(), \"\"));\n    \t}\n    \n    \tprivate void cancel() {\n    \t\tstepname = null;\n    \t\tinput.setChanged(changed);\n    \t\tdispose();\n    \t}\n    \t//单击OK把控件里用户输入的数据都写入到步骤的元数据对象中。\n    \tprivate void ok() {\n    \t\tif (Const.isEmpty(wStepname.getText()))\n    \t\t\treturn;\n    \n    \t\tstepname = wStepname.getText(); // return value\n    \n    \t\tinput.setFieldName(wFieldname.getText());\n    \n    \t\tdispose();\n    \t}\n    }\n    \n\n####  ####窗体布局\n如果你看过步骤对话框的源代码，你就会发现窗体类里有很多烦琐的代码。这些代码确保Kettle可以在各种操作系统下以合适的方式展现窗体。可以发现窗体里的大部分代码都和布局以及控件位置有关。  \nFormLayout是SWT里经常看到的布局方式。程序员可以通过FormLayout指定控件的百分比、偏移。下面是我们例子里的窗口布局的代码（HelloworldStepDialog.java）  \n    //创建一个新的标签控件，控件里文本靠右对齐\n    Label label = new Label(shell, SWT.RIGHT);\n    label.setText(BaseMessages.getString(PKG,\"HelloworldDialog.Fieldname.Label\")); //$NON-NLS-1$\n    //下面一行为控件设置用户定义的背景色和字体\n    props.setLook(label);\n    /**\n    * 下面几行将标签的左侧和对话框的最左侧对齐，把标签的右侧放在对话框中间（50%）的左侧10个像素\n    * 的位置。标签的顶部放在距离对话框顶部25个像素的位置。\n    */\n    FormData fdLabel = new FormData();\n    fdlFieldname.left = new FormAttachment(0, 0);\n    fdlFieldname.right = new FormAttachment(50, -10);\n    fdlFieldname.top = new FormAttachment(0, 25);\n    wlFieldname.setLayoutData(fdLabel);  \n简而言之，不要感到痛苦；图形用户界面的代码都比较烦琐，但代码并不复杂。  \n####  ####Kettle UI元素  \n除了标准的SWT组件，还可以使用Kettle自带的一些控件，Kettle开发人员的工作可以更简单一些。Kettle自带的组件包括以下一些。  \nTableView：这是一个数据表格组件，支持排序、选择、键盘快捷键和撤销/重做，以及右键菜单。  \nTextVar：这是一个支持变量的文本输入框，这个输入框的右上角有一个$符号。用户可以通过”Ctrl  +Alt+空格”的方式，在弹出的下拉列表中选择变量。其他功能和普通的文本框相同。  \nComboVar：标准的组合下拉列表，支持变量。  \nConditionEditor：过滤行步骤里使用的输入条件控件。  \n另外还有很多常用的对话框帮你完成相应的工作，如下所示:  \nEnterListDialog:从字符串列表里选择一个或多个字符串。左侧显示字符串列表，右侧是选中的字符串，并提供把字符串从左侧移动到右侧的按钮。  \nEnterNumberDialog:用户可以输入数字  \nEnterPasswordDialog:让用户输入密码  \nEnterSelectionDialog:通过高亮显示，从列表里选择多项  \nEnterMappingDialog:输入两组字符串的映射  \nPreviewRowsDialog:在对话框里预览一组数据行。  \nSQLEditor:一个简单的SQL编辑器，可以输入查询和DDL.  \nErrorDialog:显示异常信息，列出详细的错误栈对话框  \n####  ####Hello World例子对话框\n现在我们已经基本了解了SWT以及对话框的布局方式，再看看我们的例子，下面的代码是HelloWorldStepDialog.java里的例子。\n代码的第一部分是初始化元数据对象以及步骤对话框的父类：\n    public class HelloworldStepDialog extends BaseStepDialog implements\n    \t\tStepDialogInterface {\n    \tprivate static Class<?> PKG = HelloworldStepMeta.class; \n    \tprivate HelloworldStepMeta input;\n    \tprivate TextVar wFieldname;\n    \tpublic HelloworldStepDialog(Shell parent, Object baseStepMeta,\n    \t\t\tTransMeta transMeta, String stepname) {\n    \t\t//初始化元数据对象以及步骤对话框的父类\n    \t\tsuper(parent, (BaseStepMeta) baseStepMeta, transMeta, stepname);\n    \t\tinput = (HelloworldStepMeta) baseStepMeta;\n    \t}\n在下面的open()方法里创建对话框里的所有控件。SWT使用事件监听模式，可以为控件创建各种监听方法，以响应控件内容的变化和用户的动作。\n    public String open() {\n    \t\tShell parent = getParent();\n    \t\tDisplay display = parent.getDisplay();\n    \t\tshell = new Shell(parent, SWT.DIALOG_TRIM | SWT.RESIZE | SWT.MIN\n    \t\t\t\t| SWT.MAX);\n    \t\tprops.setLook(shell);\n    \t\tsetShellImage(shell, input);\n    \t\tModifyListener lsMod = new ModifyListener() {\n    \t\t\tpublic void modifyText(ModifyEvent e) {\n    \t\t\t\tinput.setChanged();\n    \t\t\t}\n    \t\t};\n    \t\tchanged = input.hasChanged();\n    \n下面代码说明窗体里的控件将使用formLayout的布局方式：\n    FormLayout formLayout = new FormLayout();\n    \t\tformLayout.marginWidth = Const.FORM_MARGIN;\n    \t\tformLayout.marginHeight = Const.FORM_MARGIN;\n    \t\tshell.setLayout(formLayout);\n所有控件的右侧使用一个自定义的百分比对齐：props.getMiddlePct()；控件之间的间距使用一个常量，常量值是4像素。\n    shell.setLayout(formLayout);\n    \t\tshell.setText(BaseMessages.getString(PKG,\n    \t\t\t\t\"HelloworldDialog.Shell.Title\")); //$NON-NLS-1$\n    \t\tint middle = props.getMiddlePct();\n    \t\tint margin = Const.MARGIN;\n下面的代码在对话框的最上面添加了一行步骤名称标签和输入文本框：\n    // Stepname line\n    \t\twlStepname = new Label(shell, SWT.RIGHT);\n    \t\twlStepname.setText(BaseMessages.getString(PKG,\n    \t\t\t\t\"HelloworldDialog.Stepname.Label\")); //$NON-NLS-1$\n    \t\tprops.setLook(wlStepname);\n    \t\tfdlStepname = new FormData();\n    \t\tfdlStepname.left = new FormAttachment(0, 0);\n    \t\tfdlStepname.right = new FormAttachment(middle, -margin);\n    \t\tfdlStepname.top = new FormAttachment(0, margin);\n    \t\twlStepname.setLayoutData(fdlStepname);\n    \t\twStepname = new Text(shell, SWT.SINGLE | SWT.LEFT | SWT.BORDER);\n    \t\twStepname.setText(stepname);\n    \t\tprops.setLook(wStepname);\n    \t\twStepname.addModifyListener(lsMod);\n    \t\tfdStepname = new FormData();\n    \t\tfdStepname.left = new FormAttachment(middle, 0);\n    \t\tfdStepname.top = new FormAttachment(0, margin);\n    \t\tfdStepname.right = new FormAttachment(100, 0);\n    \t\twStepname.setLayoutData(fdStepname);\n    \t\tControl lastControl = wStepname;\n\n下面是新增输出列的列名设置的输入框：\n    // Fieldname line\n\t\t//创建一个新的标签控件，控件里文本靠右对齐\n\t\tLabel wlFieldname = new Label(shell, SWT.RIGHT);\n\t\twlFieldname.setText(BaseMessages.getString(PKG,\n\t\t\t\t\"HelloworldDialog.Fieldname.Label\")); //$NON-NLS-1$\n\t\t//下面一行为控件设置用户定义的背景色和字体\n\t\tprops.setLook(wlFieldname);\n\t\tFormData fdlFieldname = new FormData();\n\t\tfdlFieldname.left = new FormAttachment(0, 0);\n\t\tfdlFieldname.right = new FormAttachment(middle, -margin);\n\t\tfdlFieldname.top = new FormAttachment(lastControl, margin);\n\t\twlFieldname.setLayoutData(fdlFieldname);\n\t\twFieldname = new TextVar(transMeta, shell, SWT.SINGLE | SWT.LEFT\n\t\t\t\t| SWT.BORDER);\n\t\tprops.setLook(wFieldname);\n\t\twFieldname.addModifyListener(lsMod);\n\t\tFormData fdFieldname = new FormData();\n\t\tfdFieldname.left = new FormAttachment(middle, 0);\n\t\tfdFieldname.top = new FormAttachment(lastControl, margin);\n\t\tfdFieldname.right = new FormAttachment(100, 0);\n\t\twFieldname.setLayoutData(fdFieldname);\n\t\tlastControl = wFieldname;\n    \n然后创建两个按钮，“确认”和“取消”按钮，以及按钮单击事件的监听方法，把按钮放在对话框的最下面：\n    // Some buttons\n\t\twOK = new Button(shell, SWT.PUSH);\n\t\twOK.setText(BaseMessages.getString(PKG, \"System.Button.OK\")); //$NON-NLS-1$\n\t\twCancel = new Button(shell, SWT.PUSH);\n\t\twCancel.setText(BaseMessages.getString(PKG, \"System.Button.Cancel\")); //$NON-NLS-1$\n\n\t\tsetButtonPositions(new Button[] { wOK, wCancel }, margin, lastControl);\n\n\t\t// Add listeners\n\t\tlsCancel = new Listener() {\n\t\t\tpublic void handleEvent(Event e) {\n\t\t\t\tcancel();\n\t\t\t}\n\t\t};\n\t\tlsOK = new Listener() {\n\t\t\tpublic void handleEvent(Event e) {\n\t\t\t\tok();\n\t\t\t}\n\t\t};\n\t\twCancel.addListener(SWT.Selection, lsCancel);\n\t\twOK.addListener(SWT.Selection, lsOK);\n    \n下面的代码做了两件事情，上部代码可以保证当步骤名称或输出字段名称的输入框在编辑状态时，单击“确定”按钮，正在编辑的内容不会丢失；下部的代码保证了窗口在非正常关闭时（没有使用“确定”或“取消”按钮关闭），取消用户的编辑。\n    lsDef = new SelectionAdapter() {\n\t\t\tpublic void widgetDefaultSelected(SelectionEvent e) {\n\t\t\t\tok();\n\t\t\t}\n\t\t};\n\n\t\twStepname.addSelectionListener(lsDef);\n\t\twFieldname.addSelectionListener(lsDef);\n\n\t\t// Detect X or ALT-F4 or something that kills this window...\n\t\tshell.addShellListener(new ShellAdapter() {//保证了窗口在非正常关闭时，取消用户的编辑\n\t\t\tpublic void shellClosed(ShellEvent e) {\n\t\t\t\tcancel();\n\t\t\t}\n\t\t});\n\n\n下面的代码把数据从步骤的元数据对象里复制到窗口的控件里：\n    // Populate the data of the controls\n    \t\tgetData();\n窗口的大小和位置将根据窗口的自然属性、上次窗口大小和位置，以及显示屏的大小自动设置。\n    // Set the shell size, based upon previous time...\n    \t\t//窗口的大小和位置将根据窗口的自然属性、上次窗口大小和位置，以及显示屏的大小自动设置\n    \t\tsetSize();\n    \t\tinput.setChanged(changed);\n    \n    \t\tshell.open();\n    \t\twhile (!shell.isDisposed()) {\n    \t\t\tif (!display.readAndDispatch())\n    \t\t\t\tdisplay.sleep();\n    \t\t}\n    \t\treturn stepname;\n    \t}\n为了防止用户身控件里输入空值，Kettle提供了一个静态方法来检查空值，ConstNVL();\n    /**\n    \t * Copy information from the meta-data input to the dialog fields.\n    \t */\n    \tpublic void getData() {\n    \t\twStepname.selectAll();\n    \t\twFieldname.setText(Const.NVL(input.getFieldName(), \"\"));\n    \t}\n最后，单击OK按钮后，把控件里用户输入的数据都写入到步骤的元数据对象中：\n    private void cancel() {\n    \t\tstepname = null;\n    \t\tinput.setChanged(changed);\n    \t\tdispose();\n    \t}\n    \t//单击OK把控件里用户输入的数据都写入到步骤的元数据对象中。\n    \tprivate void ok() {\n    \t\tif (Const.isEmpty(wStepname.getText()))\n    \t\t\treturn;\n    \n    \t\tstepname = wStepname.getText(); // return value\n    \n    \t\tinput.setFieldName(wFieldname.getText());\n    \n    \t\tdispose();\n    \t}\n\n###  ###StepInteface\n\t这个类实现了org.pentaho.di.trans.step.StepInterface接口，这个类读取上个步骤传来的数据行，利用StepMetaInterface对象里定义的元数据，逐行转换和处理上个步骤传来的数据行，Kettle引擎直接使用这个接口里的很多方法来执行转换过程，但大部分方法都已经由BaseStep类实现了，通常开发人员只需要重载其中的几个方法。\n\tInit():步骤初始化方法，用来初始化一个步骤。初始化结果是一个true或者false的Boolean值。如果你的步骤没有任何初始化的工作，可以不用重载这个方法。\n\tDispose():如果有需要释放的资源，可以在dispose()方法里释放，例如可以关闭数据库连接、释放文件、清除缓存等。在转换的最后Kettle引擎会调用这个方法。如果没有需要释放或清除的资源，可以不用重载这个方法。\n\tprocessRow():这个方法，是步骤实现工作的地方。只要这个方法返回true，转换引擎就会重复调用这个方法。\n下面是HellWorld例子实现的StepInterface接口（HelloworldStep.java）\n\nHelloworldStep.java\n    package org.kettlesolutions.plugin.step.helloworld;\n    \n    import org.pentaho.di.core.exception.KettleException;\n    import org.pentaho.di.core.row.RowDataUtil;\n    import org.pentaho.di.trans.Trans;\n    import org.pentaho.di.trans.TransMeta;\n    import org.pentaho.di.trans.step.BaseStep;\n    import org.pentaho.di.trans.step.StepDataInterface;\n    import org.pentaho.di.trans.step.StepInterface;\n    import org.pentaho.di.trans.step.StepMeta;\n    import org.pentaho.di.trans.step.StepMetaInterface;\n    /**\n     * BaseStep抽象类已经实现了接口里的很多方法，我们只要覆盖需要修改的方法即可。\n     * @author Administrator\n     *\n     */\n    public class HelloworldStep extends BaseStep implements StepInterface {\n    \t/**\n    \t * 类的构造函数通常直接把参数传递给BaseStep父类。由父类里的方法来构造对象，然后可以直接\n    \t * 使用类似transMeta这样的对象。\n    \t * @param stepMeta\n    \t * @param stepDataInterface\n    \t * @param copyNr\n    \t * @param transMeta\n    \t * @param trans\n    \t */\n    \tpublic HelloworldStep(StepMeta stepMeta, StepDataInterface stepDataInterface,\n    \t\t\tint copyNr, TransMeta transMeta, Trans trans) {\n    \t\tsuper(stepMeta, stepDataInterface, copyNr, transMeta, trans);\n    \t\t// TODO Auto-generated constructor stub\n    \t}\n    \n    \t\n    \tpublic boolean processRow(StepMetaInterface smi, StepDataInterface sdi) throws KettleException {\n    \n    \t\tHelloworldStepMeta meta  = (HelloworldStepMeta) smi;\n    \t\tHelloworldStepData data = (HelloworldStepData) sdi;\n    \t\t/**\n    \t\t * getRow()方法从上一个步骤获取一行数据。如果没有更多要获取的数据行，这个方法就会返回null。\n    \t\t * 如果前面的步骤不能及时提供数据，这个方法就会阻塞，直到有可用的数据行。这样这个步骤的速度就会降低，也会影响\n    \t\t * 其它步骤的速度。\n    \t\t */\n    \t\tObject[] row = getRow();\n    \t\tif (row==null) {\n    \t\t\t/**\n    \t\t\t * setOutputDone()方法用来通知其它的步骤，本步骤已经没有输出数据行。下一个步骤如果\n    \t\t\t * 再调用getRow()方法就会返回null,转换也不再调用processRow()方法。\n    \t\t\t */\n    \t\t\tsetOutputDone();\n    \t\t\treturn false;\n    \t\t}\n    \t\t\n    \t\tif (first) {\n    \t\t\tfirst=false;\n    \t\t\t/**\n    \t\t\t * 从性能上考虑，getRow()方法不提供数据行的元数据，只提供上个步骤输出的数据。可以使用getInputRowMeta()方法\n    \t\t\t获取元数据，元数据只获取一次即可，所以在first代码块里获取元数据。\n    \t\t\t   如果要把数据传到下一个步骤，要使用putRow()方法。除了输出数据，还要输出RowMetaInterface元数据。\n    \t\t\t   第一行使用clone()方法把输入行的元数据结构复制给输出行。输出行的元数据结构是在输入行的基础上增加一个字段，但\n    \t\t\t   构造输出行的元数据结构只能构造一次，因为所有输出数据行的结构都是一样的，产生了输出行以后，元数据结构就不能再变化。\n    \t\t\t   所以输出行的元数据结构在first代码块里构造。first是一个内部成员，first代码块里的代码只在处理第一行数据时执行。\n    \t\t\t   下面代码的最后一行，给输出数据增加了一个字段。\n    \t\t\t */\n    \t\t\tdata.outputRowMeta = getInputRowMeta().clone();\n    \t\t\tmeta.getFields(data.outputRowMeta, getStepname(), null, null, this);\n    \t\t}\n    \t\t/**\n    \t\t * 下面的代码，把数据写入输出流。从性能角度考虑，数据行实现就是Java数组。为了开发方便，可以使用RowDataUtil类提供\n    \t\t * 的一些静态方法来操作数据。使用RowDatautil静态方法复制数据，还可以提高性能。\n    \t\t */\n    \t\tString value = \"Hello, world!\";\n    \t\t\n    \t\tObject[] outputRow = RowDataUtil.addValueData(row, getInputRowMeta().size(), value);\n    \t\t\n    \t\tputRow(data.outputRowMeta, outputRow);\n    \t\t\n    \t\treturn true;\n    \t}\n    }\n解析：\npublic class HelloworldStep extends BaseStep implements StepInterface {\nBaseStep抽象类已经实现了接口里的很多方法，我们只要覆盖需要修改的方法即可。\n类的构造函数通常直接把参数传递给BaseStep父类。由父类里的方法来构造对象，然后可以直接使用类似transMeta这样的对象。\npublic HelloworldStep(StepMeta stepMeta, StepDataInterface stepDataInterface,\n\t\t\tint copyNr, TransMeta transMeta, Trans trans) {\n\t\tsuper(stepMeta, stepDataInterface, copyNr, transMeta, trans);\n\t}\ngetRow()方法从上一个步骤获取一行数据。如果没有更多要获取的数据行，这个方法就会返回null。如果前面的步骤不能及时提供数据，这个方法就会阻塞，直到有可用的数据行。这样这个步骤的速度就会降低，也会影响其它步骤的速度。\npublic boolean processRow(StepMetaInterface smi, StepDataInterface sdi) throws KettleException {\n\t\tHelloworldStepMeta meta  = (HelloworldStepMeta) smi;\n\t\tHelloworldStepData data = (HelloworldStepData) sdi;\n\t\tObject[] row = getRow();\n\t\tif (row==null) {\n\t\t\tsetOutputDone();\n\t\t\treturn false;\n\t\t}\n\t\t\n\t\tif (first) {\n\t\t\tfirst=false;\n\t\t\tdata.outputRowMeta = getInputRowMeta().clone();\n\t\t\tmeta.getFields(data.outputRowMeta, getStepname(), null, null, this);\n\t\t}\n\t\tString value = \"Hello, world!\";\n\t\tObject[] outputRow = RowDataUtil.addValueData(row, getInputRowMeta().size(), value);\n\t\t\n\t\tputRow(data.outputRowMeta, outputRow);\n\t\t\n\t\treturn true;\n\t}\n从性能上考虑，getRow()方法不提供数据行的元数据，只提供上个步骤输出的数据。可以使用getInputRowMeta()方法获取元数据，元数据只获取一次即可，所以在first代码块里获取元数据。\nsetOutputDone()方法用来通知其它的步骤，本步骤已经没有输出数据行。下一个步骤如果再调用getRow()方法就会返回null,转换也不再调用processRow()方法。\n\n    Object[] row = getRow();\n    \t\tif (row==null) {\n    \t\t\tsetOutputDone();\n    \t\t\treturn false;\n    \t\t}\n   如果要把数据传到下一个步骤，要使用putRow()方法。除了输出数据，还要输出RowMetaInterface元数据。\n    data.outputRowMeta = getInputRowMeta().clone();\n    meta.getFields(data.outputRowMeta, getStepname(), null, null, this);\n第一行使用clone()方法把输入行的元数据结构复制给输出行。输出行的元数据结构是在输入行的基础上增加一个字段，但构造输出行的元数据结构只能构造一次，因为所有输出数据行的结构都是一样的，产生了输出行以后，元数据结构就不能再变化。所以输出行的元数据结构在first代码块里构造。first是一个内部成员，first代码块里的代码只在处理第一行数据时执行。下面代码的最后一行，给输出数据增加了一个字段。\n\n下面的代码，把数据写入输出流。\n\t\tString value = \"Hello, world!\";\n\t\tObject[] outputRow = RowDataUtil.addValueData(row, getInputRowMeta().size(), value);\n\t\tputRow(data.outputRowMeta, outputRow);\n从性能角度考虑，数据行实现就是Java数组。为了开发方便，可以使用RowDataUtil类提供的一些静态方法来操作数据。使用RowDatautil静态方法复制数据，还可以提高性能。\n从指定的步骤读取数据行\n如果你想从前面的某个指定的步骤读取数据行，例如”流查询“步骤，可以使用getRowFrom()方法。\n\t   RowSet rowSet = findInputRowSet(Source Step Name);\n\t   Object[] rowData = getRowFrom(rowSet);\n\t       还可以通过rowSet对象获得数据行的元数据：\n\t   RowMetaInterface rowMeta = rowSet.getRowMeta();\n把数据行写入指定的步骤\n如果想把数据写入到某个特定的步骤，例如”过滤“步骤，可以使用putRowTo()方法\n\t  RowSet rowSet = findOutputRowSet(Target Step Name);\n\t  ....\n\t  putRowTo(outputRowMeta,rowData,rowSet);\n很明显，输入和输出的RowSet对象只需获得一次即可，这样才更有效率。\n把数据行写入到错误处理步骤\n如果想让你的步骤支持错误处理，而且元数据类返回的supportErrorHandling()方法返回了true，就可以把数据输出\n\t  到错误处理步骤里。下面是使用putError()方法的例子：\n\t  Object[] rowData = getRow();\n\t  ...\n\t  try{\n\t  \t...\n\t  \tputRow(...);\n\t  }catch(Exception e){\n\t  \tif(getStepMeta().isDoingErrorHandling()){\n\t  \t\tputError(getInputRowMeta(),rowData,errorCode);\n\t  \t}else{\n\t  \t\tthrow(e);\n\t  \t}\n\t  }\n\t  从例子里可以看到，这段代码把错误的行数、错误字段名、消息、错误编码都传递给错误处理步骤。\n\t  错误处理的其他工作都自动完成了。\n#### ####识别一个步骤拷贝\n因为一个步骤可以有多份拷贝同时执行，有时需要识别出正在使用的是哪个步骤拷贝，可以用下面几个方法。\n\t getCopy():获得拷贝号。拷贝号可以唯一标识出步骤的一个拷贝，拷贝号的聚会范围是0-N，N=getStepMeta().getCopies()-1\n\t getUniqueStepNrAcrossSlaves():获得在集群模式下运行的步骤拷贝号。\n\t getUniqueStepCountAcrossSlaves():获得在集群模式下运行的步骤拷贝总数。\n\t 通过这些方法可以把一个步骤的工作分配给多份拷贝去完成。例如”CSV文件输入“和”固定文件输入“步骤里都有并行读取文件的选项，这样可以把读取文件的工作放在多个拷贝里或集群里来完成。\n#### ####结果反馈\n在调用getRow()和putRow()方法时，引擎会自动计算两类度量值，读行数和写行数。这两类度量值可以在界面或日志中记录下来，以监控程序运行的状态。下面几个方法用来操作这两类度量值。\n\tincrementLinesRead():增加从前面步骤读取到的行数。\n\tincrementLinesWritten():增加定稿到后面步骤中的行数。\n\tincrementLinesInput():增加从文件、数据库、网络等资源读取到的行数\n\tincrementLinesOutput:增加写入到文件、数据库、网络等资源的行数。\n\tincrementLinesUpdate():增加更新的行数。\n\tincrementLinesSkipped()：增加跳过的数据行的行数。\n\tincrementLinesRejected():增加拒绝的数据行的行数。\n\t这些度量值用来说明步骤执行的情况。可以在Spoon的转换度量面板里看到，也可以存到日志数据库表里。\n\t使用addResultFile()方法，可以把步骤用到的文件保留下来，保存到结果文件列表里。结果文件列表可以被其它转换或作业项使用。例如，下面的”CSV文件输入“的代码：\nResultFile resultFile = new ResultFile(\n\tResultFile.FILE_TYPE_GENERAL,\n\tfileObject,\ngetTransMeta().getName(),\ngetStepName()\n);\nresultFile.setComment(“File was read by a Csv Input step”);\naddREsultFile(resultFile);\n#### ####变量替换\n\t如果输入框需要支持变量，可以使用environmentSubstritute()方法获取变量。例如，若想在“Hello World”例子的字段名输入框里使用变量，就要把StepMetaInterface里的getFields()方法修改成下面的语句：\nString realFiledName = apace. environmentSubstritute(fieldName)；\n因为步骤本身是一个VariableSpace对象，所以也可以使用下面的语句做变量替换：String value = environmentSubstritute(meta.getSringWithVariables());\n#### ####Apache VFS\nKettle里所有操作文件的步骤，都使用Apache VFS系统的方式操作。Apache VFS不但可以从文件系统读取文件（如java.io.File），还可以从很多其他来源读取文件，如FTP服务器、Z学压缩文件，等 等 。  \nApache VFS里的FileObject对象提供了文件的抽象层，然后在Kettle的KettleVFS类里还提供了一系列的静态方法，来更方便使用FileObject对象，例如下面的代码 ：  \n\n    FileObject fileObject = KettleVFS.getFileObject(“zip:http://www.example.com/archive.zip!file.txt”);\n`String value = environmentSubstritute(meta.getSringWithVariables());`\n\n\n应该尽可能多地使用KettleVFS,因为它解决了或饶过了很多Apache VFS目前已知的问题。它也增强了SFTP协议。\n#### ####步骤插件部署\n部署之前，要把四个Java源代码文件编译为class文件。把编译好的class文件放到一个Jar包里。可以使用IDE来做这些事情，也可以手工使用ant脚本来做这些事情。  \n.jar文件应该放在Kettle的plugins/steps目录下。也可以使用一个子目录，把所有的依赖的jar包放在插件jar包所在目录的/lib目录下，不必再放Kettle的类路径中（Kettle的libext/目录）已经有了的jar包。另外可以把多个插件放在一个jar包里。\n如果想在IDE里调试插件，可以把插件元数据类的名字放在Kettle_PLUGIN_CLASSES变量里（一个逗号分隔的列表）。关于这个主题的更多信息，请参考pentaho Wiki:http://wiki.pentaho.com/display/EAI/How+to+debug+a+Kettle+4+plugin 。\n\n","source":"_posts/kettle/Kettle插件架构.md","raw":"---\ntitle: Kettle插件架构\ndate: 2017-04-15 19:42:49\ntags: [开源项目,kettle]\ncategories: [开源项目,kettle]\n---\n#  #Kettle插件体系\n最近公司内有业务系统到数据中心同步的升级改造需求，从各个业务系统收集增量数据到数据中心的数据仓库平台。因为开发周期短暂，需要快速的响应，开发出可用的产品，所以决定借鉴开源程序Kettle，开发一个文件解析组件，然后利用Kettle平台的大数据组件进行与数据中心大数据平台对接\n\n数据同步部分是：业务系统（RDBMS）->Kettle(azkaban进行调度)->数据中心，因为Kettle的增量抽取组件经常出现数据不一致等问题，所以目前已更改为：业务系统（RDBMS）->OGG（CDC增量抽取）->数据中心的方式。\n\n本文主要介绍如何扩展Kettle的功能，部分内容来自《Pentaho Kettle解决方案：使用PDI构建开源ETL解决方案》一书，推荐购买阅读。\n##  ##架构\n我们先看Kettle插件架构。\n ![](http://i.imgur.com/mLvXMuV.jpg)\n从功能上看，Kettle内部的对象和外部插件没有任何区别。因为它们使用的API都是一样的，它们只是在运行时的加载方式不同。\n从Kettle4以后，Kettle内部有一个插件注册系统，它负责加载各种内部和外部插件。插件有以下两个标识属性。\n**插件类型**：由PluginTypeInterface接口定义。例如StepPluginType、JobEntryPluginType、PartitionerPluginType和RepositoryPluginType。\n**插件ID**：这是一个字符串数组，用来唯一标识一个插件。因为旧的插件可以被新的插件代替，一个插件可以有多个ID。在大多数情况下，插件只使用一个单一的字符串，如TableInput是“表输入”步骤的ID，MYSQL是MySQL数据库类型的ID。\n当Kettle环境初始化以后，插件注册系统首先加载所有的内部对象，Kettle读取下面的配置文件来加载内部对象，这些配置文件位于Kettle的.jar文件中。\n\t Kettle-steps.xml：内部转换步骤。\n\t Kettle-job-entries.xml：内部作业项。\n\t Kettle-partition-plugins.xml：内部分区类型。\n\t Kettle-database-types.xml：内部数据库类型。\n\t Kettle-repositories.xml：内部资源库类型。\n\n插件注册系统加载了所有的内部对象后，就要搜索可用的外部插件。通过浏览plugins/目录的各个子目录下的.jar文件来完成。它搜索特定的Kettle annotations来判断一个类是否是插件。加载过程将在本章的后面介绍。\n因为在内部对象加载后才加载插件，所以插件会替代相同ID的已加载的内部对象。例如，你创建了插件，插件的ID是TableInput，就可以替换Kettle标准的“表输入”步骤。这个功能可以让你用插件替换Kettle内置的步骤。可以通过子类继承方式，直接扩展已有步骤的某些功能。\n\n##  ##插件类型\nKettle有下面几种插件类型（下面的插件是Kettle4.0的插件类型，新版kettle包含了很多新的插件，比如视图插件、大数据插件等等）。\n\n- 转换步骤插件：在Kettle转换中使用的步骤，用来处理数据行。\n\n\n- 作业项插件：在Kettle作业中使用的作业项，用来实现某个任务。\n\t\n\n- 分区方法插件：利用输入字段的值指定自己的分区规则。\n\t\n\n- 数据库类型插件：用来扩展不同的数据库类型。\n\t\n\n- 资源库类型插件：可以把Kettle元数据保存为自定义类型或格式。\n\n说明：除了这些类型，还有Spoon类型的插件，可以把功能扩展到Spoon，本书不介绍这个功能。\n##  ##转换步骤插件\n转换步骤插件包括了四个Java类，这四个类分别实现四个接口。\n\n\n- StepMetaInterface：这个接口对外 提供步骤的元数据并处理串行化。\n\n\n- StepInterface:这个接口根据上面接口提供的元数据，来实现步骤的具体功能。\n\n\n- StepDataInterface:这个接口用来存储步骤的临时数据、文件句柄等。\n\n\n- StepDialogInterface:这个接口是Spoon里的图形界面，用来编辑步骤的元数据。\n\n接下来，我们介绍这些接口的基本内容。对于每个接口，在一个简单的“Hello World”例子里提供这些类的相应实现。“Hello World”例子将在数据流里增加一个字段，字段名用户可以自定义，字段值是”Hello world!“。最后介绍一下如何部署这个例子。\n###  ###StepMetaInterface\n接口org.pentaho.di.trans.step.StepMetaInterface负责步骤里所有和元数据相关的任务。和元数据相关的工作包括：  \n元数据和XML(或资源库)之间的序列化和反序列化  \ngetXML（）和loadXML()  \nsaveRep()和readRep()  \n\n描述输出字段  \ngetFields()  \n\n检验元数据是否正确  \nCheck()  \n\n获取步骤相应的要SQL语句，使步骤可以正确运行  \ngetSQLStatements()  \n\n给元数据设置默认值  \nsetDefault()  \n\n完成对数据库的影响分析  \nanalyseImpact()  \n\n描述各类输入和输出流  \ngetStepIOMeta()  \nsearchInfoAndTargetSteps()  \nhandleStreamSelection()  \ngetOptionalStreams()  \nresetStepIoMeta()  \n\n导出元数据资源  \nexportResources()  \ngetResourceDependencies()  \n\n描述使用的库  \ngetUsedLibraries()  \n\n描述使用的数据库连接  \ngetUsedDatabaseConnections()  \n\n描述这个步骤需要的字段（通常是一个数据库表）  \ngetRequiredFields()  \n\n描述步骤是否具有某些功能  \nsupportsErrorHandling()  \nexcludeFromRowLayoutVerification()  \nexcludeFromCopyDistributeVerification()  \n\n这个接口里还定义了几个方法来说明这四个接口如何结合到一起。  \nString getDialogClassName():用来描述实现了StepDialogInterface接口的对话框类的名字。如果这个方法返回了null，调用类会根据实现了StepMetaInterface接口的类的类名和包名来自动生成对话框类的名字。  \nSetpInterface getStep():创建一个实现了StepInterface接口的类。  \nStepDataInterface getStepData():创建一个实现了StepDataInterface接口的类。  \n现在我们看看”Hello World”例子里对SetpMetaInterface接口的实现  \nHelloworldStepMeta.java\n    package org.kettlesolutions.plugin.step.helloworld;\n    \n    import java.util.List;\n    import java.util.Map;\n    \n    import org.pentaho.di.core.CheckResult;\n    import org.pentaho.di.core.CheckResultInterface;\n    import org.pentaho.di.core.Const;\n    import org.pentaho.di.core.Counter;\n    import org.pentaho.di.core.annotations.Step;\n    import org.pentaho.di.core.database.DatabaseMeta;\n    import org.pentaho.di.core.exception.KettleException;\n    import org.pentaho.di.core.exception.KettleStepException;\n    import org.pentaho.di.core.exception.KettleXMLException;\n    import org.pentaho.di.core.row.RowMetaInterface;\n    import org.pentaho.di.core.row.ValueMeta;\n    import org.pentaho.di.core.row.ValueMetaInterface;\n    import org.pentaho.di.core.variables.VariableSpace;\n    import org.pentaho.di.core.xml.XMLHandler;\n    import org.pentaho.di.i18n.BaseMessages;\n    import org.pentaho.di.repository.ObjectId;\n    import org.pentaho.di.repository.Repository;\n    import org.pentaho.di.trans.Trans;\n    import org.pentaho.di.trans.TransMeta;\n    import org.pentaho.di.trans.step.BaseStepMeta;\n    import org.pentaho.di.trans.step.StepDataInterface;\n    import org.pentaho.di.trans.step.StepInterface;\n    import org.pentaho.di.trans.step.StepMeta;\n    import org.pentaho.di.trans.step.StepMetaInterface;\n    import org.w3c.dom.Node;\n    \n    @Step(\n    \t\tid=\"Helloworld\",\n    \t\tname=\"name\",\n    \t\tdescription=\"description\",\n    \t\tcategoryDescription=\"categoryDescription\", \n    \t\timage=\"org/kettlesolutions/plugin/step/helloworld/HelloWorld.png\",\n    \t\ti18nPackageName=\"org.kettlesolutions.plugin.step.helloworld\"\n    ) \n    public class HelloworldStepMeta extends BaseStepMeta implements StepMetaInterface {\n    \t/**\n    \t * PKG变量说明了messages包的位置，在messages包里有各种国际化的资源文件。\n    \t * 在本章后面经常要看到的BaseMessages.getString()方法，就是根据软件的国际化\n    \t * 设置，从不同的文件中获取文字。PKG变量通常位于类的最上方，被国际化图形工具使用，\n    \t * 通过国际化图形工具，国际化人员可以编辑不同的国际化资源文件。所以我们会在很多Kettle\n    \t * 代码里看见这样的结构。\n    \t */\n    \tprivate static Class<?> PKG = HelloworldStep.class; //for i18n\n    \tpublic enum Tag {//field_name用于保存用户输入的字段名：保存“Hello，world！\"字符串的字段名。\n    \t\tfield_name,\n    \t};\n    \t\n    \tprivate String fieldName;\n    \t\n    \t/**\n    \t * @return the fieldName\n    \t */\n    \tpublic String getFieldName() {\n    \t\treturn fieldName;\n    \t}\n    \n    \t/**\n    \t * @param fieldName the fieldName to set\n    \t */\n    \tpublic void setFieldName(String fieldName) {\n    \t\tthis.fieldName = fieldName;\n    \t}\n    \n    \t/**\n    \t * checks parameters, adds result to List<CheckResultInterface>\n    \t * used in Action > Verify transformation\n    \t * 验证用户是否在对话框里输入了字段名，并把验证结果添加到检验转换时出现的问题列表里。（最好\n    \t * 要检验用户输入的所有选项，而不只是容易出错的选项）\n    \t */\n    \tpublic void check(List<CheckResultInterface> remarks, TransMeta transMeta, StepMeta stepMeta, \n    \t\t\tRowMetaInterface prev, String input[], String output[], RowMetaInterface info) {\n    \t\t\n    \t\tif (Const.isEmpty(fieldName)) {\n    \t\t\tCheckResultInterface error = new CheckResult(\n    \t\t\t\tCheckResult.TYPE_RESULT_ERROR, \n    \t\t\t\tBaseMessages.getString(PKG, \"HelloworldMeta.CHECK_ERR_NO_FIELD\"), \n    \t\t\t\tstepMeta\n    \t\t\t);\n    \t\t\tremarks.add(error);\n    \t\t} else {\n    \t\t\tCheckResultInterface ok = new CheckResult(\n    \t\t\t\tCheckResult.TYPE_RESULT_OK, \n    \t\t\t\tBaseMessages.getString(PKG, \"HelloworldMeta.CHECK_OK_FIELD\"), \n    \t\t\t\tstepMeta\n    \t\t\t);\n    \t\t\tremarks.add(ok);//把验证结果添加到检验转换时出现的问题列表里。\n    \t\t}\n    \t}\n    \n    \t/**\n    \t *\tcreates a new instance of the step (factory)\n    \t * getStep、getStepData和getDialogClassName()方法提供了与这个步骤里其它三个接口之间的桥梁\n    \t 这个接口里还定义了几个方法来说明这四个接口如何结合到一起。\n    \tString getDialogClassName():用来描述实现了StepDialogInterace接口的对话框类的名字。如果这个方法返回\n    \t\t\t\t了null，调用类会根据实现了StepMetaInterface接口的类的类名和包名来自动生成对话框类的名字。\n    \tStepInterface getStep():创建一个实现了StepInterface接口的类。\n    \tStepInterface getStepData():创建一个实现了StepDataInterface接口的类。\n    \n    \t */\n    \tpublic StepInterface getStep(StepMeta stepMeta, StepDataInterface stepDataInterface,\n    \t\t\tint copyNr, TransMeta transMeta, Trans trans) {\n    \t\treturn new HelloworldStep(stepMeta, stepDataInterface, copyNr, transMeta, trans);\n    \t}\n    \n    \t/**\n    \t * creates new instance of the step data (factory)\n    \t * getStep、getStepData和getDialogClassName()方法提供了与这个步骤里其它三个接口之间的桥梁\n    \t */\n    \tpublic StepDataInterface getStepData() {\n    \t\treturn new HelloworldStepData();\n    \t}\n    \t/**\n    \t * getStep、getStepData和getDialogClassName()方法提供了与这个步骤里其它三个接口之间的桥梁\n    \t */\n    \t@Override\n    \tpublic String getDialogClassName() {\n    \t\treturn HelloworldStepDialog.class.getName();\n    \t}\n    \n    \t/**\n    \t * deserialize from xml \n    \t * databases = list of available connections\n    \t * counters = list of sequence steps\n    \t * \n    \t * 下面的四个方法loadXML()、getXML()、readRep()和saveRep()把元数据保存到XML文件或资源库里，\n    \t * 或者从XML文件或资源库读取元数据。保存到文件的方法利用了像XStream（http://xstream.codehaus.org）这\n    \t * 样的XML串行化技术。\n    \t */\n    \tpublic void loadXML(Node stepDomNode, List<DatabaseMeta> databases,\n    \t\t\tMap<String, Counter> sequenceCounters) throws KettleXMLException {\n    \t\tfieldName = XMLHandler.getTagValue(stepDomNode, Tag.field_name.name());\n    \t}\n    \t\n    \t/**\n    \t * @Override\n    \t */\n    \tpublic String getXML() throws KettleException {\n    \t\tStringBuilder xml = new StringBuilder();\n    \t\txml.append(XMLHandler.addTagValue(Tag.field_name.name(), fieldName));\n    \t\treturn xml.toString();\n    \t}\n    \t\n    \t/**\n    \t * De-serialize from repository (see loadXML)\n    \t */\n    \tpublic void readRep(Repository repository, ObjectId stepIdInRepository,\n    \t\t\tList<DatabaseMeta> databases, Map<String, Counter> sequenceCounters)\n    \t\t\tthrows KettleException {\n    \t\tfieldName = repository.getStepAttributeString(stepIdInRepository, Tag.field_name.name());\n    \t}\n    \n    \t/**\n    \t * serialize to repository\n    \t */\n    \tpublic void saveRep(Repository repository, ObjectId idOfTransformation, ObjectId idOfStep)\n    \t\t\tthrows KettleException {\n    \t\trepository.saveStepAttribute(idOfTransformation, idOfStep, Tag.field_name.name(), fieldName);\n    \t}\n    \t\n    \t\n    \t/**\n    \t * initiailize parameters to default\n    \t */\n    \tpublic void setDefault() {\n    \t\tfieldName = \"helloField\";\n    \t}\n    \n    \t/**\n    \t * getFields()方法非常重要，因为它描述了输出数据行的结构。这个方法需要修改inputRowMeta对象，使这个对象和\n    \t * 输出格式匹配。Spoon和后面的步骤都需要知道这个步骤要输出哪些字段。最常见的一种方法，可以给输出的RowMetaInterface对象\n    \t * 添加一个ValueMetaInterface对象。在ValueMetaInterface对象里设置的信息越详细越好，可以设置的信息包括数据类型、长度、\n    \t * 精度、格式掩码，等等。添加的字段描述元信息越多，后面生成的SQL就越准确。\n    \t */\n    \t@Override\n    \tpublic void getFields(RowMetaInterface inputRowMeta, String name,\n    \t\t\tRowMetaInterface[] info, StepMeta nextStep, VariableSpace space)\n    \t\t\tthrows KettleStepException {\n    \t\tString realFieldName = space.environmentSubstitute(fieldName);\n    \t\t//值的元数据使用ValueMetaInterface接口描述数据流里的一个字段。这个接口里定义了字段的名字、数据类型、长度、精度，等等。下面的例子用于创建一个ValueMetaInterface对象。\n    \t\tValueMetaInterface field = new ValueMeta(realFieldName, ValueMetaInterface.TYPE_STRING);\n    \t\tfield.setOrigin(name);\t\t\n    \t\tinputRowMeta.addValueMeta(field);\n    \t}\n    }\n\n\n\n代码解析\n    @Step(\n    \t\tid=\"Helloworld\",\n    \t\tname=\"name\",\n    \t\tdescription=\"description\",\n    \t\tcategoryDescription=\"categoryDescription\", \n    \t\timage=\"org/kettlesolutions/plugin/step/helloworld/HelloWorld.png\",\n    \t\ti18nPackageName=\"org.kettlesolutions.plugin.step.helloworld\"\n    )\n这段代码里的@Step annotation用来通知Kettle的插件注册系统：这个类是一个步骤类型的插件。在annotation里可以指定插件的ID、图标、国际代的包、本地化的名称、类别、描述。其中后三项是资源文件里的Key，需要在资源文件里设置真正的值。i18nPackageName指定了资源文件的包名，例如我们这个例子的资源文件位于org/kettlesolutions/plugin/step/helloworld/messages目录下，en_US（英语，美国）的本地代资源文件是messages_en_US.properties。我们例子里的这个资源文件的内容是：\nname=Hello world\ndescription=A very simple step that adds a new \"Helllo world\" field to the incoming stream\n注意，如果你指定了不存在的分类，Spoon会创建这个分类，并在Spoon的分类树的最上方显示这个分类。\n最后，annotation里的image标签指定了插件的图标。需要32*32像素的PNG文件，可以使用透明样式。\n后面的代码行说明这个类实现了StepMetaInterface接口。在BaseStepMeta抽象类里定义了这个接口的很多默认实现，可以直接继承这个抽象类，然后把工作集中在插件特有的功能上。\n\n    public class HelloworldStepMeta extends BaseStepMeta implements StepMetaInterface\n\t\n\n\n下面的四个方法loadXML()、getXML()、readRep()和saveRep()把元数据保存到XML文件或资源库里，或者从XML文件或资源库读取元数据。保存到文件的方法利用了像XStream（http://xstream.codehaus.org）这样的XML串行化技术。\n\n\ngetFields()方法非常重要，因为它描述了输出数据行的结构。这个方法需要修改inputRowMeta对象，使这个对象和输出格式匹配。Spoon和后面的步骤都需要知道这个步骤要输出哪些字段。最常见的一种方法，可以给输出的RowMetaInterface对象添加一个ValueMetaInterface对象。在ValueMetaInterface对象里设置的信息越详细越好，可以设置的信息包括数据类型、长度、精度、格式掩码，等等。添加的字段描述元信息越多，后面生成的SQL就越准确。\n####  ####值的元数据（Value Metadata）\n值的元数据使用ValueMetaInterface接口描述数据流里的一个字段。这个接口里定义了字段的名字、数据类型、长度、精度，等等。下面的例子用于创建一个ValueMetaInterface对象。  \n    ValueMetaInterface dateMeta = new ValueMeta(“birthdate”,ValueMetaInterface.TYPE_DATE);\n这个接口也负责转换数据格式。我们建议使用ValueMetaInterface接口来完成所有数据转换的工作。例如，日期类型的数据，如果想把它转换为dateMeta对象里定义的字符串格式，可以用下面的代码：  \n    //java.util.Date birthdate\n    String birthDateString = dateMeta.getString(birthdate);\nValueMeta类负责转换。因为有ValueMetaInterface进行数据类型的转换，所以你不用再去做额外的数据类型转换的工作。  \n使用ValueMetaInterface接口时还要注意数据对象是否为Null。从上一个步骤可以接收到一个数据对象和一个描述数据对象的ValueMetaInterface对象。我们要检查这个数据对象是否为null，在某些情况下如果数据对象为空是不正确的。例如：  \n数据对象是String类型，有10个空格，Value Metadata需要trim这个字符串。  \n在Value Metadata里已经定义了从文本文件里加载的数据，要延迟转换为字符串。所以数据要由二进制的格式（原始数据格式），转换为字符串格式，然后再转换为其它格式的数据。  \n一般使用下面的方法检查数据对象是否为空：  \n    Boolean n = valueMeta.isNull(valueDate);\n重要：要保证传给ValueMetaInterface对象的数据是在元数据里定义的数据类型。表23-1说明了  ValueMetaInterface里定义的数据类型和Java数据类型的对应关系。  \nKettle元数据类型和Java里数据类型的对应关系  \n<table>\n    <tr>\n        <th>Value Meta Type</th>\n        <th>Java Class</th>\n    </tr>\n    <tr>\n        <td>ValueMetaInterface.TYPE_STRING</td>\n        <td>Java.lang.String</td>\n    </tr>\n    <tr>\n        <td>ValueMetaInterface.TYPE_DATE</td>\n        <td>Java.util.Date</td>\n    </tr>\n    <tr>\n        <td>ValueMetaInterface.TYPE_BOOLEAN</td>\n        <td>Java.lang.Boolean</td>\n    </tr>\n\t<tr>\n        <td>ValueMetaInterface.TYPE_NUMBER</td>\n        <td>Java.lang.Double</td>\n    </tr>\n\t<tr>\n        <td>ValueMetaInterface.TYPE_INTEGER</td>\n        <td>Java.lang.Long</td>\n    </tr>\n\t<tr>\n        <td>ValueMetaInterface.TYPE_BIGNUMBER</td>\n        <td>Java.math.BigDecimal</td>\n    </tr>\n\t<tr>\n        <td>ValueMetaInterface.TYPE_BINARY</td>\n        <td>Byte[]</td>\n    </tr>\n</table>\n\n\n####  ####行的元数据（Row Meatadata）\n行的元数据使用RowMetaInterface接口来描述数据行的元数据，而不是一个列的元数据。实际上，RowMetaInterface的类里包含了一组ValueMetaInterface。另外还包括了一些方法来操作行元数据，倒如查询值、检查值是否存、替换值的元数据等。  \n行的元数据里唯一的规则就是一行里的列的名字必须唯一。当你添加了一个新列时，如果新列的名字和已有列的名字相同，列名后面会自动加上“_2”后缀。如果再加一个同名的列会自动加上”_3“后缀，等等。  \n因为在步骤里通常是和数据行打交道，所以从数据行里直接取数据会更方便。可以使用很多类似于getNumber()、getString()这样的方法直接从数据行取数据。例如，销售数据存储在第四列里，可以用下面的代码获取这个数据：  \n\n    Double sales = getInputRowMeta().getNumber(rowData,3);\n通过索引获取数据是最快的方式。通过indexOfValue()方法可以获取列在一行里的索引。这个方法扫描列数组，速度并不快。所以，如果要处理所有数据行，我们建议只查询一次列索引。一般是在步骤接收到第一行数据时，就查询列索引，将查询到的列索引保存起来，供后面的数据行使用。  \n###  ###StepDatainterface\n实现了org.pentaho.di.trans.step.StepDataInterface接口的类用来维护步骤的执行状态，以及存储临时对象。例如，可以把输出行的元数据、数据库连接、输入输出流等存储到这个对象里。  \nHelloworldStepData.java  \n    package org.kettlesolutions.plugin.step.helloworld;\n\nimport org.pentaho.di.core.row.RowMetaInterface;\nimport org.pentaho.di.trans.step.BaseStepData;\nimport org.pentaho.di.trans.step.StepDataInterface;\n\npublic class HelloworldStepData extends BaseStepData implements StepDataInterface {\n\n\tpublic RowMetaInterface outputRowMeta;\n\n}\n    \n###  ###StepDialogInterface\n实现org.pentaho.di.trans.step.StepDialogInterfac接口的类用来提供一个用户界面，用户通过这个界面输入元数据（转换参数）。用户界面就是一个对话框。这个接口里包含了类似open()和setRepository()等的几个简单的方法。    \n####  ####Eclipse SWT\nKettle里使用Eclipse SWT作为界面开发包，所以你也要使用SWT来开发对话框窗口。SWT为不同的操作系统Windows、OS X、Linux和Unix提供了一个抽象层。所以SWT的图形界面和操作系统期货的程序的界面风格非常相近。  \n在开始进行SWT开发之前，建议先访问SWT主面以了解更多的内容http://www.eclipse/org/swt。在SWT的网站上，你可以了解到SWT能做出什么样的界面效果：  \nSWT控件页，http://www.eclipse.org/swt/widgets/，给出了你能使用的所有控件。  \nSWT样例页，http://www.eclipse.org/swt/snippets/，给出了许多代码例子。  \n最好的资源就是Kettle里150个内置步骤的对话框源代码。  \n\nHelloworldStepDialog.java  \n    package org.kettlesolutions.plugin.step.helloworld;\n    \n    import org.eclipse.swt.SWT;\n    import org.eclipse.swt.events.ModifyEvent;\n    import org.eclipse.swt.events.ModifyListener;\n    import org.eclipse.swt.events.SelectionAdapter;\n    import org.eclipse.swt.events.SelectionEvent;\n    import org.eclipse.swt.events.ShellAdapter;\n    import org.eclipse.swt.events.ShellEvent;\n    import org.eclipse.swt.layout.FormAttachment;\n    import org.eclipse.swt.layout.FormData;\n    import org.eclipse.swt.layout.FormLayout;\n    import org.eclipse.swt.widgets.Button;\n    import org.eclipse.swt.widgets.Control;\n    import org.eclipse.swt.widgets.Display;\n    import org.eclipse.swt.widgets.Event;\n    import org.eclipse.swt.widgets.Label;\n    import org.eclipse.swt.widgets.Listener;\n    import org.eclipse.swt.widgets.Shell;\n    import org.eclipse.swt.widgets.Text;\n    import org.pentaho.di.core.Const;\n    import org.pentaho.di.i18n.BaseMessages;\n    import org.pentaho.di.trans.TransMeta;\n    import org.pentaho.di.trans.step.BaseStepMeta;\n    import org.pentaho.di.trans.step.StepDialogInterface;\n    import org.pentaho.di.ui.core.widget.TextVar;\n    import org.pentaho.di.ui.trans.step.BaseStepDialog;\n    \n    public class HelloworldStepDialog extends BaseStepDialog implements\n    \t\tStepDialogInterface {\n    \n    \tprivate static Class<?> PKG = HelloworldStepMeta.class; // for i18n\n    \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t// purposes, needed\n    \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t// by Translator2!!\n    \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t// $NON-NLS-1$\n    \n    \tprivate HelloworldStepMeta input;\n    \n    \tprivate TextVar wFieldname;\n    \n    \tpublic HelloworldStepDialog(Shell parent, Object baseStepMeta,\n    \t\t\tTransMeta transMeta, String stepname) {\n    \t\t//初始化元数据对象以及步骤对话框的父类\n    \t\tsuper(parent, (BaseStepMeta) baseStepMeta, transMeta, stepname);\n    \t\tinput = (HelloworldStepMeta) baseStepMeta;\n    \t}\n    \n    \tpublic String open() {\n    \t\tShell parent = getParent();\n    \t\tDisplay display = parent.getDisplay();\n    \n    \t\tshell = new Shell(parent, SWT.DIALOG_TRIM | SWT.RESIZE | SWT.MIN\n    \t\t\t\t| SWT.MAX);\n    \t\tprops.setLook(shell);\n    \t\tsetShellImage(shell, input);\n    \n    \t\tModifyListener lsMod = new ModifyListener() {\n    \t\t\tpublic void modifyText(ModifyEvent e) {\n    \t\t\t\tinput.setChanged();\n    \t\t\t}\n    \t\t};\n    \t\tchanged = input.hasChanged();\n    \n    \t\tFormLayout formLayout = new FormLayout();\n    \t\tformLayout.marginWidth = Const.FORM_MARGIN;\n    \t\tformLayout.marginHeight = Const.FORM_MARGIN;\n    \n    \t\tshell.setLayout(formLayout);\n    \t\tshell.setText(BaseMessages.getString(PKG,\n    \t\t\t\t\"HelloworldDialog.Shell.Title\")); //$NON-NLS-1$\n    \t\t\n    \t\t//所有控件的右侧使用一个自定义的百分对对齐。控件之间的间距使用一个常量，常量值是4像素。\n    \t\tint middle = props.getMiddlePct();\n    \t\tint margin = Const.MARGIN;\n    \n    \t\t// Stepname line\n    \t\twlStepname = new Label(shell, SWT.RIGHT);\n    \t\twlStepname.setText(BaseMessages.getString(PKG,\n    \t\t\t\t\"HelloworldDialog.Stepname.Label\")); //$NON-NLS-1$\n    \t\tprops.setLook(wlStepname);\n    \t\tfdlStepname = new FormData();\n    \t\tfdlStepname.left = new FormAttachment(0, 0);\n    \t\tfdlStepname.right = new FormAttachment(middle, -margin);\n    \t\tfdlStepname.top = new FormAttachment(0, margin);\n    \t\twlStepname.setLayoutData(fdlStepname);\n    \t\twStepname = new Text(shell, SWT.SINGLE | SWT.LEFT | SWT.BORDER);\n    \t\twStepname.setText(stepname);\n    \t\tprops.setLook(wStepname);\n    \t\twStepname.addModifyListener(lsMod);\n    \t\tfdStepname = new FormData();\n    \t\tfdStepname.left = new FormAttachment(middle, 0);\n    \t\tfdStepname.top = new FormAttachment(0, margin);\n    \t\tfdStepname.right = new FormAttachment(100, 0);\n    \t\twStepname.setLayoutData(fdStepname);\n    \t\tControl lastControl = wStepname;\n    \n    \t\t// Fieldname line\n    \t\t//创建一个新的标签控件，控件里文本靠右对齐\n    \t\tLabel wlFieldname = new Label(shell, SWT.RIGHT);\n    \t\twlFieldname.setText(BaseMessages.getString(PKG,\n    \t\t\t\t\"HelloworldDialog.Fieldname.Label\")); //$NON-NLS-1$\n    \t\t//下面一行为控件设置用户定义的背景色和字体\n    \t\tprops.setLook(wlFieldname);\n    \t\tFormData fdlFieldname = new FormData();\n    \t\tfdlFieldname.left = new FormAttachment(0, 0);\n    \t\tfdlFieldname.right = new FormAttachment(middle, -margin);\n    \t\tfdlFieldname.top = new FormAttachment(lastControl, margin);\n    \t\twlFieldname.setLayoutData(fdlFieldname);\n    \t\twFieldname = new TextVar(transMeta, shell, SWT.SINGLE | SWT.LEFT\n    \t\t\t\t| SWT.BORDER);\n    \t\tprops.setLook(wFieldname);\n    \t\twFieldname.addModifyListener(lsMod);\n    \t\tFormData fdFieldname = new FormData();\n    \t\tfdFieldname.left = new FormAttachment(middle, 0);\n    \t\tfdFieldname.top = new FormAttachment(lastControl, margin);\n    \t\tfdFieldname.right = new FormAttachment(100, 0);\n    \t\twFieldname.setLayoutData(fdFieldname);\n    \t\tlastControl = wFieldname;\n    \n    \t\t// Some buttons\n    \t\twOK = new Button(shell, SWT.PUSH);\n    \t\twOK.setText(BaseMessages.getString(PKG, \"System.Button.OK\")); //$NON-NLS-1$\n    \t\twCancel = new Button(shell, SWT.PUSH);\n    \t\twCancel.setText(BaseMessages.getString(PKG, \"System.Button.Cancel\")); //$NON-NLS-1$\n    \n    \t\tsetButtonPositions(new Button[] { wOK, wCancel }, margin, lastControl);\n    \n    \t\t// Add listeners\n    \t\tlsCancel = new Listener() {\n    \t\t\tpublic void handleEvent(Event e) {\n    \t\t\t\tcancel();\n    \t\t\t}\n    \t\t};\n    \t\tlsOK = new Listener() {\n    \t\t\tpublic void handleEvent(Event e) {\n    \t\t\t\tok();\n    \t\t\t}\n    \t\t};\n    \n    \t\twCancel.addListener(SWT.Selection, lsCancel);\n    \t\twOK.addListener(SWT.Selection, lsOK);\n    \n    \t\tlsDef = new SelectionAdapter() {\n    \t\t\tpublic void widgetDefaultSelected(SelectionEvent e) {\n    \t\t\t\tok();\n    \t\t\t}\n    \t\t};\n    \n    \t\twStepname.addSelectionListener(lsDef);\n    \t\twFieldname.addSelectionListener(lsDef);\n    \n    \t\t// Detect X or ALT-F4 or something that kills this window...\n    \t\tshell.addShellListener(new ShellAdapter() {//保证了窗口在非正常关闭时，取消用户的编辑\n    \t\t\tpublic void shellClosed(ShellEvent e) {\n    \t\t\t\tcancel();\n    \t\t\t}\n    \t\t});\n    \n    \t\t// Populate the data of the controls\n    \t\t//下面的代码把数据从步骤的元数据对象里复制到窗口的控件里\n    \t\tgetData();\n    \n    \t\t// Set the shell size, based upon previous time...\n    \t\t//窗口的大小和位置将根据窗口的自然属性、上次窗口大小和位置，以及显示屏的大小自动设置\n    \t\tsetSize();\n    \n    \t\tinput.setChanged(changed);\n    \n    \t\tshell.open();\n    \t\twhile (!shell.isDisposed()) {\n    \t\t\tif (!display.readAndDispatch())\n    \t\t\t\tdisplay.sleep();\n    \t\t}\n    \t\treturn stepname;\n    \t}\n    \n    \t/**\n    \t * Copy information from the meta-data input to the dialog fields.\n    \t */\n    \tpublic void getData() {\n    \t\twStepname.selectAll();\n    \t\t//为了防止用户向控件里输入空值，Kettle提供了一个静态方法来检查宿舍，Const.NVL()\n    \t\twFieldname.setText(Const.NVL(input.getFieldName(), \"\"));\n    \t}\n    \n    \tprivate void cancel() {\n    \t\tstepname = null;\n    \t\tinput.setChanged(changed);\n    \t\tdispose();\n    \t}\n    \t//单击OK把控件里用户输入的数据都写入到步骤的元数据对象中。\n    \tprivate void ok() {\n    \t\tif (Const.isEmpty(wStepname.getText()))\n    \t\t\treturn;\n    \n    \t\tstepname = wStepname.getText(); // return value\n    \n    \t\tinput.setFieldName(wFieldname.getText());\n    \n    \t\tdispose();\n    \t}\n    }\n    \n\n####  ####窗体布局\n如果你看过步骤对话框的源代码，你就会发现窗体类里有很多烦琐的代码。这些代码确保Kettle可以在各种操作系统下以合适的方式展现窗体。可以发现窗体里的大部分代码都和布局以及控件位置有关。  \nFormLayout是SWT里经常看到的布局方式。程序员可以通过FormLayout指定控件的百分比、偏移。下面是我们例子里的窗口布局的代码（HelloworldStepDialog.java）  \n    //创建一个新的标签控件，控件里文本靠右对齐\n    Label label = new Label(shell, SWT.RIGHT);\n    label.setText(BaseMessages.getString(PKG,\"HelloworldDialog.Fieldname.Label\")); //$NON-NLS-1$\n    //下面一行为控件设置用户定义的背景色和字体\n    props.setLook(label);\n    /**\n    * 下面几行将标签的左侧和对话框的最左侧对齐，把标签的右侧放在对话框中间（50%）的左侧10个像素\n    * 的位置。标签的顶部放在距离对话框顶部25个像素的位置。\n    */\n    FormData fdLabel = new FormData();\n    fdlFieldname.left = new FormAttachment(0, 0);\n    fdlFieldname.right = new FormAttachment(50, -10);\n    fdlFieldname.top = new FormAttachment(0, 25);\n    wlFieldname.setLayoutData(fdLabel);  \n简而言之，不要感到痛苦；图形用户界面的代码都比较烦琐，但代码并不复杂。  \n####  ####Kettle UI元素  \n除了标准的SWT组件，还可以使用Kettle自带的一些控件，Kettle开发人员的工作可以更简单一些。Kettle自带的组件包括以下一些。  \nTableView：这是一个数据表格组件，支持排序、选择、键盘快捷键和撤销/重做，以及右键菜单。  \nTextVar：这是一个支持变量的文本输入框，这个输入框的右上角有一个$符号。用户可以通过”Ctrl  +Alt+空格”的方式，在弹出的下拉列表中选择变量。其他功能和普通的文本框相同。  \nComboVar：标准的组合下拉列表，支持变量。  \nConditionEditor：过滤行步骤里使用的输入条件控件。  \n另外还有很多常用的对话框帮你完成相应的工作，如下所示:  \nEnterListDialog:从字符串列表里选择一个或多个字符串。左侧显示字符串列表，右侧是选中的字符串，并提供把字符串从左侧移动到右侧的按钮。  \nEnterNumberDialog:用户可以输入数字  \nEnterPasswordDialog:让用户输入密码  \nEnterSelectionDialog:通过高亮显示，从列表里选择多项  \nEnterMappingDialog:输入两组字符串的映射  \nPreviewRowsDialog:在对话框里预览一组数据行。  \nSQLEditor:一个简单的SQL编辑器，可以输入查询和DDL.  \nErrorDialog:显示异常信息，列出详细的错误栈对话框  \n####  ####Hello World例子对话框\n现在我们已经基本了解了SWT以及对话框的布局方式，再看看我们的例子，下面的代码是HelloWorldStepDialog.java里的例子。\n代码的第一部分是初始化元数据对象以及步骤对话框的父类：\n    public class HelloworldStepDialog extends BaseStepDialog implements\n    \t\tStepDialogInterface {\n    \tprivate static Class<?> PKG = HelloworldStepMeta.class; \n    \tprivate HelloworldStepMeta input;\n    \tprivate TextVar wFieldname;\n    \tpublic HelloworldStepDialog(Shell parent, Object baseStepMeta,\n    \t\t\tTransMeta transMeta, String stepname) {\n    \t\t//初始化元数据对象以及步骤对话框的父类\n    \t\tsuper(parent, (BaseStepMeta) baseStepMeta, transMeta, stepname);\n    \t\tinput = (HelloworldStepMeta) baseStepMeta;\n    \t}\n在下面的open()方法里创建对话框里的所有控件。SWT使用事件监听模式，可以为控件创建各种监听方法，以响应控件内容的变化和用户的动作。\n    public String open() {\n    \t\tShell parent = getParent();\n    \t\tDisplay display = parent.getDisplay();\n    \t\tshell = new Shell(parent, SWT.DIALOG_TRIM | SWT.RESIZE | SWT.MIN\n    \t\t\t\t| SWT.MAX);\n    \t\tprops.setLook(shell);\n    \t\tsetShellImage(shell, input);\n    \t\tModifyListener lsMod = new ModifyListener() {\n    \t\t\tpublic void modifyText(ModifyEvent e) {\n    \t\t\t\tinput.setChanged();\n    \t\t\t}\n    \t\t};\n    \t\tchanged = input.hasChanged();\n    \n下面代码说明窗体里的控件将使用formLayout的布局方式：\n    FormLayout formLayout = new FormLayout();\n    \t\tformLayout.marginWidth = Const.FORM_MARGIN;\n    \t\tformLayout.marginHeight = Const.FORM_MARGIN;\n    \t\tshell.setLayout(formLayout);\n所有控件的右侧使用一个自定义的百分比对齐：props.getMiddlePct()；控件之间的间距使用一个常量，常量值是4像素。\n    shell.setLayout(formLayout);\n    \t\tshell.setText(BaseMessages.getString(PKG,\n    \t\t\t\t\"HelloworldDialog.Shell.Title\")); //$NON-NLS-1$\n    \t\tint middle = props.getMiddlePct();\n    \t\tint margin = Const.MARGIN;\n下面的代码在对话框的最上面添加了一行步骤名称标签和输入文本框：\n    // Stepname line\n    \t\twlStepname = new Label(shell, SWT.RIGHT);\n    \t\twlStepname.setText(BaseMessages.getString(PKG,\n    \t\t\t\t\"HelloworldDialog.Stepname.Label\")); //$NON-NLS-1$\n    \t\tprops.setLook(wlStepname);\n    \t\tfdlStepname = new FormData();\n    \t\tfdlStepname.left = new FormAttachment(0, 0);\n    \t\tfdlStepname.right = new FormAttachment(middle, -margin);\n    \t\tfdlStepname.top = new FormAttachment(0, margin);\n    \t\twlStepname.setLayoutData(fdlStepname);\n    \t\twStepname = new Text(shell, SWT.SINGLE | SWT.LEFT | SWT.BORDER);\n    \t\twStepname.setText(stepname);\n    \t\tprops.setLook(wStepname);\n    \t\twStepname.addModifyListener(lsMod);\n    \t\tfdStepname = new FormData();\n    \t\tfdStepname.left = new FormAttachment(middle, 0);\n    \t\tfdStepname.top = new FormAttachment(0, margin);\n    \t\tfdStepname.right = new FormAttachment(100, 0);\n    \t\twStepname.setLayoutData(fdStepname);\n    \t\tControl lastControl = wStepname;\n\n下面是新增输出列的列名设置的输入框：\n    // Fieldname line\n\t\t//创建一个新的标签控件，控件里文本靠右对齐\n\t\tLabel wlFieldname = new Label(shell, SWT.RIGHT);\n\t\twlFieldname.setText(BaseMessages.getString(PKG,\n\t\t\t\t\"HelloworldDialog.Fieldname.Label\")); //$NON-NLS-1$\n\t\t//下面一行为控件设置用户定义的背景色和字体\n\t\tprops.setLook(wlFieldname);\n\t\tFormData fdlFieldname = new FormData();\n\t\tfdlFieldname.left = new FormAttachment(0, 0);\n\t\tfdlFieldname.right = new FormAttachment(middle, -margin);\n\t\tfdlFieldname.top = new FormAttachment(lastControl, margin);\n\t\twlFieldname.setLayoutData(fdlFieldname);\n\t\twFieldname = new TextVar(transMeta, shell, SWT.SINGLE | SWT.LEFT\n\t\t\t\t| SWT.BORDER);\n\t\tprops.setLook(wFieldname);\n\t\twFieldname.addModifyListener(lsMod);\n\t\tFormData fdFieldname = new FormData();\n\t\tfdFieldname.left = new FormAttachment(middle, 0);\n\t\tfdFieldname.top = new FormAttachment(lastControl, margin);\n\t\tfdFieldname.right = new FormAttachment(100, 0);\n\t\twFieldname.setLayoutData(fdFieldname);\n\t\tlastControl = wFieldname;\n    \n然后创建两个按钮，“确认”和“取消”按钮，以及按钮单击事件的监听方法，把按钮放在对话框的最下面：\n    // Some buttons\n\t\twOK = new Button(shell, SWT.PUSH);\n\t\twOK.setText(BaseMessages.getString(PKG, \"System.Button.OK\")); //$NON-NLS-1$\n\t\twCancel = new Button(shell, SWT.PUSH);\n\t\twCancel.setText(BaseMessages.getString(PKG, \"System.Button.Cancel\")); //$NON-NLS-1$\n\n\t\tsetButtonPositions(new Button[] { wOK, wCancel }, margin, lastControl);\n\n\t\t// Add listeners\n\t\tlsCancel = new Listener() {\n\t\t\tpublic void handleEvent(Event e) {\n\t\t\t\tcancel();\n\t\t\t}\n\t\t};\n\t\tlsOK = new Listener() {\n\t\t\tpublic void handleEvent(Event e) {\n\t\t\t\tok();\n\t\t\t}\n\t\t};\n\t\twCancel.addListener(SWT.Selection, lsCancel);\n\t\twOK.addListener(SWT.Selection, lsOK);\n    \n下面的代码做了两件事情，上部代码可以保证当步骤名称或输出字段名称的输入框在编辑状态时，单击“确定”按钮，正在编辑的内容不会丢失；下部的代码保证了窗口在非正常关闭时（没有使用“确定”或“取消”按钮关闭），取消用户的编辑。\n    lsDef = new SelectionAdapter() {\n\t\t\tpublic void widgetDefaultSelected(SelectionEvent e) {\n\t\t\t\tok();\n\t\t\t}\n\t\t};\n\n\t\twStepname.addSelectionListener(lsDef);\n\t\twFieldname.addSelectionListener(lsDef);\n\n\t\t// Detect X or ALT-F4 or something that kills this window...\n\t\tshell.addShellListener(new ShellAdapter() {//保证了窗口在非正常关闭时，取消用户的编辑\n\t\t\tpublic void shellClosed(ShellEvent e) {\n\t\t\t\tcancel();\n\t\t\t}\n\t\t});\n\n\n下面的代码把数据从步骤的元数据对象里复制到窗口的控件里：\n    // Populate the data of the controls\n    \t\tgetData();\n窗口的大小和位置将根据窗口的自然属性、上次窗口大小和位置，以及显示屏的大小自动设置。\n    // Set the shell size, based upon previous time...\n    \t\t//窗口的大小和位置将根据窗口的自然属性、上次窗口大小和位置，以及显示屏的大小自动设置\n    \t\tsetSize();\n    \t\tinput.setChanged(changed);\n    \n    \t\tshell.open();\n    \t\twhile (!shell.isDisposed()) {\n    \t\t\tif (!display.readAndDispatch())\n    \t\t\t\tdisplay.sleep();\n    \t\t}\n    \t\treturn stepname;\n    \t}\n为了防止用户身控件里输入空值，Kettle提供了一个静态方法来检查空值，ConstNVL();\n    /**\n    \t * Copy information from the meta-data input to the dialog fields.\n    \t */\n    \tpublic void getData() {\n    \t\twStepname.selectAll();\n    \t\twFieldname.setText(Const.NVL(input.getFieldName(), \"\"));\n    \t}\n最后，单击OK按钮后，把控件里用户输入的数据都写入到步骤的元数据对象中：\n    private void cancel() {\n    \t\tstepname = null;\n    \t\tinput.setChanged(changed);\n    \t\tdispose();\n    \t}\n    \t//单击OK把控件里用户输入的数据都写入到步骤的元数据对象中。\n    \tprivate void ok() {\n    \t\tif (Const.isEmpty(wStepname.getText()))\n    \t\t\treturn;\n    \n    \t\tstepname = wStepname.getText(); // return value\n    \n    \t\tinput.setFieldName(wFieldname.getText());\n    \n    \t\tdispose();\n    \t}\n\n###  ###StepInteface\n\t这个类实现了org.pentaho.di.trans.step.StepInterface接口，这个类读取上个步骤传来的数据行，利用StepMetaInterface对象里定义的元数据，逐行转换和处理上个步骤传来的数据行，Kettle引擎直接使用这个接口里的很多方法来执行转换过程，但大部分方法都已经由BaseStep类实现了，通常开发人员只需要重载其中的几个方法。\n\tInit():步骤初始化方法，用来初始化一个步骤。初始化结果是一个true或者false的Boolean值。如果你的步骤没有任何初始化的工作，可以不用重载这个方法。\n\tDispose():如果有需要释放的资源，可以在dispose()方法里释放，例如可以关闭数据库连接、释放文件、清除缓存等。在转换的最后Kettle引擎会调用这个方法。如果没有需要释放或清除的资源，可以不用重载这个方法。\n\tprocessRow():这个方法，是步骤实现工作的地方。只要这个方法返回true，转换引擎就会重复调用这个方法。\n下面是HellWorld例子实现的StepInterface接口（HelloworldStep.java）\n\nHelloworldStep.java\n    package org.kettlesolutions.plugin.step.helloworld;\n    \n    import org.pentaho.di.core.exception.KettleException;\n    import org.pentaho.di.core.row.RowDataUtil;\n    import org.pentaho.di.trans.Trans;\n    import org.pentaho.di.trans.TransMeta;\n    import org.pentaho.di.trans.step.BaseStep;\n    import org.pentaho.di.trans.step.StepDataInterface;\n    import org.pentaho.di.trans.step.StepInterface;\n    import org.pentaho.di.trans.step.StepMeta;\n    import org.pentaho.di.trans.step.StepMetaInterface;\n    /**\n     * BaseStep抽象类已经实现了接口里的很多方法，我们只要覆盖需要修改的方法即可。\n     * @author Administrator\n     *\n     */\n    public class HelloworldStep extends BaseStep implements StepInterface {\n    \t/**\n    \t * 类的构造函数通常直接把参数传递给BaseStep父类。由父类里的方法来构造对象，然后可以直接\n    \t * 使用类似transMeta这样的对象。\n    \t * @param stepMeta\n    \t * @param stepDataInterface\n    \t * @param copyNr\n    \t * @param transMeta\n    \t * @param trans\n    \t */\n    \tpublic HelloworldStep(StepMeta stepMeta, StepDataInterface stepDataInterface,\n    \t\t\tint copyNr, TransMeta transMeta, Trans trans) {\n    \t\tsuper(stepMeta, stepDataInterface, copyNr, transMeta, trans);\n    \t\t// TODO Auto-generated constructor stub\n    \t}\n    \n    \t\n    \tpublic boolean processRow(StepMetaInterface smi, StepDataInterface sdi) throws KettleException {\n    \n    \t\tHelloworldStepMeta meta  = (HelloworldStepMeta) smi;\n    \t\tHelloworldStepData data = (HelloworldStepData) sdi;\n    \t\t/**\n    \t\t * getRow()方法从上一个步骤获取一行数据。如果没有更多要获取的数据行，这个方法就会返回null。\n    \t\t * 如果前面的步骤不能及时提供数据，这个方法就会阻塞，直到有可用的数据行。这样这个步骤的速度就会降低，也会影响\n    \t\t * 其它步骤的速度。\n    \t\t */\n    \t\tObject[] row = getRow();\n    \t\tif (row==null) {\n    \t\t\t/**\n    \t\t\t * setOutputDone()方法用来通知其它的步骤，本步骤已经没有输出数据行。下一个步骤如果\n    \t\t\t * 再调用getRow()方法就会返回null,转换也不再调用processRow()方法。\n    \t\t\t */\n    \t\t\tsetOutputDone();\n    \t\t\treturn false;\n    \t\t}\n    \t\t\n    \t\tif (first) {\n    \t\t\tfirst=false;\n    \t\t\t/**\n    \t\t\t * 从性能上考虑，getRow()方法不提供数据行的元数据，只提供上个步骤输出的数据。可以使用getInputRowMeta()方法\n    \t\t\t获取元数据，元数据只获取一次即可，所以在first代码块里获取元数据。\n    \t\t\t   如果要把数据传到下一个步骤，要使用putRow()方法。除了输出数据，还要输出RowMetaInterface元数据。\n    \t\t\t   第一行使用clone()方法把输入行的元数据结构复制给输出行。输出行的元数据结构是在输入行的基础上增加一个字段，但\n    \t\t\t   构造输出行的元数据结构只能构造一次，因为所有输出数据行的结构都是一样的，产生了输出行以后，元数据结构就不能再变化。\n    \t\t\t   所以输出行的元数据结构在first代码块里构造。first是一个内部成员，first代码块里的代码只在处理第一行数据时执行。\n    \t\t\t   下面代码的最后一行，给输出数据增加了一个字段。\n    \t\t\t */\n    \t\t\tdata.outputRowMeta = getInputRowMeta().clone();\n    \t\t\tmeta.getFields(data.outputRowMeta, getStepname(), null, null, this);\n    \t\t}\n    \t\t/**\n    \t\t * 下面的代码，把数据写入输出流。从性能角度考虑，数据行实现就是Java数组。为了开发方便，可以使用RowDataUtil类提供\n    \t\t * 的一些静态方法来操作数据。使用RowDatautil静态方法复制数据，还可以提高性能。\n    \t\t */\n    \t\tString value = \"Hello, world!\";\n    \t\t\n    \t\tObject[] outputRow = RowDataUtil.addValueData(row, getInputRowMeta().size(), value);\n    \t\t\n    \t\tputRow(data.outputRowMeta, outputRow);\n    \t\t\n    \t\treturn true;\n    \t}\n    }\n解析：\npublic class HelloworldStep extends BaseStep implements StepInterface {\nBaseStep抽象类已经实现了接口里的很多方法，我们只要覆盖需要修改的方法即可。\n类的构造函数通常直接把参数传递给BaseStep父类。由父类里的方法来构造对象，然后可以直接使用类似transMeta这样的对象。\npublic HelloworldStep(StepMeta stepMeta, StepDataInterface stepDataInterface,\n\t\t\tint copyNr, TransMeta transMeta, Trans trans) {\n\t\tsuper(stepMeta, stepDataInterface, copyNr, transMeta, trans);\n\t}\ngetRow()方法从上一个步骤获取一行数据。如果没有更多要获取的数据行，这个方法就会返回null。如果前面的步骤不能及时提供数据，这个方法就会阻塞，直到有可用的数据行。这样这个步骤的速度就会降低，也会影响其它步骤的速度。\npublic boolean processRow(StepMetaInterface smi, StepDataInterface sdi) throws KettleException {\n\t\tHelloworldStepMeta meta  = (HelloworldStepMeta) smi;\n\t\tHelloworldStepData data = (HelloworldStepData) sdi;\n\t\tObject[] row = getRow();\n\t\tif (row==null) {\n\t\t\tsetOutputDone();\n\t\t\treturn false;\n\t\t}\n\t\t\n\t\tif (first) {\n\t\t\tfirst=false;\n\t\t\tdata.outputRowMeta = getInputRowMeta().clone();\n\t\t\tmeta.getFields(data.outputRowMeta, getStepname(), null, null, this);\n\t\t}\n\t\tString value = \"Hello, world!\";\n\t\tObject[] outputRow = RowDataUtil.addValueData(row, getInputRowMeta().size(), value);\n\t\t\n\t\tputRow(data.outputRowMeta, outputRow);\n\t\t\n\t\treturn true;\n\t}\n从性能上考虑，getRow()方法不提供数据行的元数据，只提供上个步骤输出的数据。可以使用getInputRowMeta()方法获取元数据，元数据只获取一次即可，所以在first代码块里获取元数据。\nsetOutputDone()方法用来通知其它的步骤，本步骤已经没有输出数据行。下一个步骤如果再调用getRow()方法就会返回null,转换也不再调用processRow()方法。\n\n    Object[] row = getRow();\n    \t\tif (row==null) {\n    \t\t\tsetOutputDone();\n    \t\t\treturn false;\n    \t\t}\n   如果要把数据传到下一个步骤，要使用putRow()方法。除了输出数据，还要输出RowMetaInterface元数据。\n    data.outputRowMeta = getInputRowMeta().clone();\n    meta.getFields(data.outputRowMeta, getStepname(), null, null, this);\n第一行使用clone()方法把输入行的元数据结构复制给输出行。输出行的元数据结构是在输入行的基础上增加一个字段，但构造输出行的元数据结构只能构造一次，因为所有输出数据行的结构都是一样的，产生了输出行以后，元数据结构就不能再变化。所以输出行的元数据结构在first代码块里构造。first是一个内部成员，first代码块里的代码只在处理第一行数据时执行。下面代码的最后一行，给输出数据增加了一个字段。\n\n下面的代码，把数据写入输出流。\n\t\tString value = \"Hello, world!\";\n\t\tObject[] outputRow = RowDataUtil.addValueData(row, getInputRowMeta().size(), value);\n\t\tputRow(data.outputRowMeta, outputRow);\n从性能角度考虑，数据行实现就是Java数组。为了开发方便，可以使用RowDataUtil类提供的一些静态方法来操作数据。使用RowDatautil静态方法复制数据，还可以提高性能。\n从指定的步骤读取数据行\n如果你想从前面的某个指定的步骤读取数据行，例如”流查询“步骤，可以使用getRowFrom()方法。\n\t   RowSet rowSet = findInputRowSet(Source Step Name);\n\t   Object[] rowData = getRowFrom(rowSet);\n\t       还可以通过rowSet对象获得数据行的元数据：\n\t   RowMetaInterface rowMeta = rowSet.getRowMeta();\n把数据行写入指定的步骤\n如果想把数据写入到某个特定的步骤，例如”过滤“步骤，可以使用putRowTo()方法\n\t  RowSet rowSet = findOutputRowSet(Target Step Name);\n\t  ....\n\t  putRowTo(outputRowMeta,rowData,rowSet);\n很明显，输入和输出的RowSet对象只需获得一次即可，这样才更有效率。\n把数据行写入到错误处理步骤\n如果想让你的步骤支持错误处理，而且元数据类返回的supportErrorHandling()方法返回了true，就可以把数据输出\n\t  到错误处理步骤里。下面是使用putError()方法的例子：\n\t  Object[] rowData = getRow();\n\t  ...\n\t  try{\n\t  \t...\n\t  \tputRow(...);\n\t  }catch(Exception e){\n\t  \tif(getStepMeta().isDoingErrorHandling()){\n\t  \t\tputError(getInputRowMeta(),rowData,errorCode);\n\t  \t}else{\n\t  \t\tthrow(e);\n\t  \t}\n\t  }\n\t  从例子里可以看到，这段代码把错误的行数、错误字段名、消息、错误编码都传递给错误处理步骤。\n\t  错误处理的其他工作都自动完成了。\n#### ####识别一个步骤拷贝\n因为一个步骤可以有多份拷贝同时执行，有时需要识别出正在使用的是哪个步骤拷贝，可以用下面几个方法。\n\t getCopy():获得拷贝号。拷贝号可以唯一标识出步骤的一个拷贝，拷贝号的聚会范围是0-N，N=getStepMeta().getCopies()-1\n\t getUniqueStepNrAcrossSlaves():获得在集群模式下运行的步骤拷贝号。\n\t getUniqueStepCountAcrossSlaves():获得在集群模式下运行的步骤拷贝总数。\n\t 通过这些方法可以把一个步骤的工作分配给多份拷贝去完成。例如”CSV文件输入“和”固定文件输入“步骤里都有并行读取文件的选项，这样可以把读取文件的工作放在多个拷贝里或集群里来完成。\n#### ####结果反馈\n在调用getRow()和putRow()方法时，引擎会自动计算两类度量值，读行数和写行数。这两类度量值可以在界面或日志中记录下来，以监控程序运行的状态。下面几个方法用来操作这两类度量值。\n\tincrementLinesRead():增加从前面步骤读取到的行数。\n\tincrementLinesWritten():增加定稿到后面步骤中的行数。\n\tincrementLinesInput():增加从文件、数据库、网络等资源读取到的行数\n\tincrementLinesOutput:增加写入到文件、数据库、网络等资源的行数。\n\tincrementLinesUpdate():增加更新的行数。\n\tincrementLinesSkipped()：增加跳过的数据行的行数。\n\tincrementLinesRejected():增加拒绝的数据行的行数。\n\t这些度量值用来说明步骤执行的情况。可以在Spoon的转换度量面板里看到，也可以存到日志数据库表里。\n\t使用addResultFile()方法，可以把步骤用到的文件保留下来，保存到结果文件列表里。结果文件列表可以被其它转换或作业项使用。例如，下面的”CSV文件输入“的代码：\nResultFile resultFile = new ResultFile(\n\tResultFile.FILE_TYPE_GENERAL,\n\tfileObject,\ngetTransMeta().getName(),\ngetStepName()\n);\nresultFile.setComment(“File was read by a Csv Input step”);\naddREsultFile(resultFile);\n#### ####变量替换\n\t如果输入框需要支持变量，可以使用environmentSubstritute()方法获取变量。例如，若想在“Hello World”例子的字段名输入框里使用变量，就要把StepMetaInterface里的getFields()方法修改成下面的语句：\nString realFiledName = apace. environmentSubstritute(fieldName)；\n因为步骤本身是一个VariableSpace对象，所以也可以使用下面的语句做变量替换：String value = environmentSubstritute(meta.getSringWithVariables());\n#### ####Apache VFS\nKettle里所有操作文件的步骤，都使用Apache VFS系统的方式操作。Apache VFS不但可以从文件系统读取文件（如java.io.File），还可以从很多其他来源读取文件，如FTP服务器、Z学压缩文件，等 等 。  \nApache VFS里的FileObject对象提供了文件的抽象层，然后在Kettle的KettleVFS类里还提供了一系列的静态方法，来更方便使用FileObject对象，例如下面的代码 ：  \n\n    FileObject fileObject = KettleVFS.getFileObject(“zip:http://www.example.com/archive.zip!file.txt”);\n`String value = environmentSubstritute(meta.getSringWithVariables());`\n\n\n应该尽可能多地使用KettleVFS,因为它解决了或饶过了很多Apache VFS目前已知的问题。它也增强了SFTP协议。\n#### ####步骤插件部署\n部署之前，要把四个Java源代码文件编译为class文件。把编译好的class文件放到一个Jar包里。可以使用IDE来做这些事情，也可以手工使用ant脚本来做这些事情。  \n.jar文件应该放在Kettle的plugins/steps目录下。也可以使用一个子目录，把所有的依赖的jar包放在插件jar包所在目录的/lib目录下，不必再放Kettle的类路径中（Kettle的libext/目录）已经有了的jar包。另外可以把多个插件放在一个jar包里。\n如果想在IDE里调试插件，可以把插件元数据类的名字放在Kettle_PLUGIN_CLASSES变量里（一个逗号分隔的列表）。关于这个主题的更多信息，请参考pentaho Wiki:http://wiki.pentaho.com/display/EAI/How+to+debug+a+Kettle+4+plugin 。\n\n","slug":"kettle/Kettle插件架构","published":1,"updated":"2017-08-18T01:44:35.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjh7polsl000wbp7rhwe4qgjz","content":"<h1 id=\"Kettle插件体系\"><a href=\"#Kettle插件体系\" class=\"headerlink\" title=\"#Kettle插件体系\"></a>#Kettle插件体系</h1><p>最近公司内有业务系统到数据中心同步的升级改造需求，从各个业务系统收集增量数据到数据中心的数据仓库平台。因为开发周期短暂，需要快速的响应，开发出可用的产品，所以决定借鉴开源程序Kettle，开发一个文件解析组件，然后利用Kettle平台的大数据组件进行与数据中心大数据平台对接</p>\n<p>数据同步部分是：业务系统（RDBMS）-&gt;Kettle(azkaban进行调度)-&gt;数据中心，因为Kettle的增量抽取组件经常出现数据不一致等问题，所以目前已更改为：业务系统（RDBMS）-&gt;OGG（CDC增量抽取）-&gt;数据中心的方式。</p>\n<p>本文主要介绍如何扩展Kettle的功能，部分内容来自《Pentaho Kettle解决方案：使用PDI构建开源ETL解决方案》一书，推荐购买阅读。</p>\n<h2 id=\"架构\"><a href=\"#架构\" class=\"headerlink\" title=\"##架构\"></a>##架构</h2><p>我们先看Kettle插件架构。<br> <img src=\"http://i.imgur.com/mLvXMuV.jpg\" alt=\"\"><br>从功能上看，Kettle内部的对象和外部插件没有任何区别。因为它们使用的API都是一样的，它们只是在运行时的加载方式不同。<br>从Kettle4以后，Kettle内部有一个插件注册系统，它负责加载各种内部和外部插件。插件有以下两个标识属性。<br><strong>插件类型</strong>：由PluginTypeInterface接口定义。例如StepPluginType、JobEntryPluginType、PartitionerPluginType和RepositoryPluginType。<br><strong>插件ID</strong>：这是一个字符串数组，用来唯一标识一个插件。因为旧的插件可以被新的插件代替，一个插件可以有多个ID。在大多数情况下，插件只使用一个单一的字符串，如TableInput是“表输入”步骤的ID，MYSQL是MySQL数据库类型的ID。<br>当Kettle环境初始化以后，插件注册系统首先加载所有的内部对象，Kettle读取下面的配置文件来加载内部对象，这些配置文件位于Kettle的.jar文件中。<br>     Kettle-steps.xml：内部转换步骤。<br>     Kettle-job-entries.xml：内部作业项。<br>     Kettle-partition-plugins.xml：内部分区类型。<br>     Kettle-database-types.xml：内部数据库类型。<br>     Kettle-repositories.xml：内部资源库类型。</p>\n<p>插件注册系统加载了所有的内部对象后，就要搜索可用的外部插件。通过浏览plugins/目录的各个子目录下的.jar文件来完成。它搜索特定的Kettle annotations来判断一个类是否是插件。加载过程将在本章的后面介绍。<br>因为在内部对象加载后才加载插件，所以插件会替代相同ID的已加载的内部对象。例如，你创建了插件，插件的ID是TableInput，就可以替换Kettle标准的“表输入”步骤。这个功能可以让你用插件替换Kettle内置的步骤。可以通过子类继承方式，直接扩展已有步骤的某些功能。</p>\n<h2 id=\"插件类型\"><a href=\"#插件类型\" class=\"headerlink\" title=\"##插件类型\"></a>##插件类型</h2><p>Kettle有下面几种插件类型（下面的插件是Kettle4.0的插件类型，新版kettle包含了很多新的插件，比如视图插件、大数据插件等等）。</p>\n<ul>\n<li>转换步骤插件：在Kettle转换中使用的步骤，用来处理数据行。</li>\n</ul>\n<ul>\n<li>作业项插件：在Kettle作业中使用的作业项，用来实现某个任务。</li>\n</ul>\n<ul>\n<li>分区方法插件：利用输入字段的值指定自己的分区规则。</li>\n</ul>\n<ul>\n<li>数据库类型插件：用来扩展不同的数据库类型。</li>\n</ul>\n<ul>\n<li>资源库类型插件：可以把Kettle元数据保存为自定义类型或格式。</li>\n</ul>\n<p>说明：除了这些类型，还有Spoon类型的插件，可以把功能扩展到Spoon，本书不介绍这个功能。</p>\n<h2 id=\"转换步骤插件\"><a href=\"#转换步骤插件\" class=\"headerlink\" title=\"##转换步骤插件\"></a>##转换步骤插件</h2><p>转换步骤插件包括了四个Java类，这四个类分别实现四个接口。</p>\n<ul>\n<li>StepMetaInterface：这个接口对外 提供步骤的元数据并处理串行化。</li>\n</ul>\n<ul>\n<li>StepInterface:这个接口根据上面接口提供的元数据，来实现步骤的具体功能。</li>\n</ul>\n<ul>\n<li>StepDataInterface:这个接口用来存储步骤的临时数据、文件句柄等。</li>\n</ul>\n<ul>\n<li>StepDialogInterface:这个接口是Spoon里的图形界面，用来编辑步骤的元数据。</li>\n</ul>\n<p>接下来，我们介绍这些接口的基本内容。对于每个接口，在一个简单的“Hello World”例子里提供这些类的相应实现。“Hello World”例子将在数据流里增加一个字段，字段名用户可以自定义，字段值是”Hello world!“。最后介绍一下如何部署这个例子。</p>\n<h3 id=\"StepMetaInterface\"><a href=\"#StepMetaInterface\" class=\"headerlink\" title=\"###StepMetaInterface\"></a>###StepMetaInterface</h3><p>接口org.pentaho.di.trans.step.StepMetaInterface负责步骤里所有和元数据相关的任务。和元数据相关的工作包括：<br>元数据和XML(或资源库)之间的序列化和反序列化<br>getXML（）和loadXML()<br>saveRep()和readRep()  </p>\n<p>描述输出字段<br>getFields()  </p>\n<p>检验元数据是否正确<br>Check()  </p>\n<p>获取步骤相应的要SQL语句，使步骤可以正确运行<br>getSQLStatements()  </p>\n<p>给元数据设置默认值<br>setDefault()  </p>\n<p>完成对数据库的影响分析<br>analyseImpact()  </p>\n<p>描述各类输入和输出流<br>getStepIOMeta()<br>searchInfoAndTargetSteps()<br>handleStreamSelection()<br>getOptionalStreams()<br>resetStepIoMeta()  </p>\n<p>导出元数据资源<br>exportResources()<br>getResourceDependencies()  </p>\n<p>描述使用的库<br>getUsedLibraries()  </p>\n<p>描述使用的数据库连接<br>getUsedDatabaseConnections()  </p>\n<p>描述这个步骤需要的字段（通常是一个数据库表）<br>getRequiredFields()  </p>\n<p>描述步骤是否具有某些功能<br>supportsErrorHandling()<br>excludeFromRowLayoutVerification()<br>excludeFromCopyDistributeVerification()  </p>\n<p>这个接口里还定义了几个方法来说明这四个接口如何结合到一起。<br>String getDialogClassName():用来描述实现了StepDialogInterface接口的对话框类的名字。如果这个方法返回了null，调用类会根据实现了StepMetaInterface接口的类的类名和包名来自动生成对话框类的名字。<br>SetpInterface getStep():创建一个实现了StepInterface接口的类。<br>StepDataInterface getStepData():创建一个实现了StepDataInterface接口的类。<br>现在我们看看”Hello World”例子里对SetpMetaInterface接口的实现<br>HelloworldStepMeta.java<br>    package org.kettlesolutions.plugin.step.helloworld;</p>\n<pre><code>import java.util.List;\nimport java.util.Map;\n\nimport org.pentaho.di.core.CheckResult;\nimport org.pentaho.di.core.CheckResultInterface;\nimport org.pentaho.di.core.Const;\nimport org.pentaho.di.core.Counter;\nimport org.pentaho.di.core.annotations.Step;\nimport org.pentaho.di.core.database.DatabaseMeta;\nimport org.pentaho.di.core.exception.KettleException;\nimport org.pentaho.di.core.exception.KettleStepException;\nimport org.pentaho.di.core.exception.KettleXMLException;\nimport org.pentaho.di.core.row.RowMetaInterface;\nimport org.pentaho.di.core.row.ValueMeta;\nimport org.pentaho.di.core.row.ValueMetaInterface;\nimport org.pentaho.di.core.variables.VariableSpace;\nimport org.pentaho.di.core.xml.XMLHandler;\nimport org.pentaho.di.i18n.BaseMessages;\nimport org.pentaho.di.repository.ObjectId;\nimport org.pentaho.di.repository.Repository;\nimport org.pentaho.di.trans.Trans;\nimport org.pentaho.di.trans.TransMeta;\nimport org.pentaho.di.trans.step.BaseStepMeta;\nimport org.pentaho.di.trans.step.StepDataInterface;\nimport org.pentaho.di.trans.step.StepInterface;\nimport org.pentaho.di.trans.step.StepMeta;\nimport org.pentaho.di.trans.step.StepMetaInterface;\nimport org.w3c.dom.Node;\n\n@Step(\n        id=&quot;Helloworld&quot;,\n        name=&quot;name&quot;,\n        description=&quot;description&quot;,\n        categoryDescription=&quot;categoryDescription&quot;, \n        image=&quot;org/kettlesolutions/plugin/step/helloworld/HelloWorld.png&quot;,\n        i18nPackageName=&quot;org.kettlesolutions.plugin.step.helloworld&quot;\n) \npublic class HelloworldStepMeta extends BaseStepMeta implements StepMetaInterface {\n    /**\n     * PKG变量说明了messages包的位置，在messages包里有各种国际化的资源文件。\n     * 在本章后面经常要看到的BaseMessages.getString()方法，就是根据软件的国际化\n     * 设置，从不同的文件中获取文字。PKG变量通常位于类的最上方，被国际化图形工具使用，\n     * 通过国际化图形工具，国际化人员可以编辑不同的国际化资源文件。所以我们会在很多Kettle\n     * 代码里看见这样的结构。\n     */\n    private static Class&lt;?&gt; PKG = HelloworldStep.class; //for i18n\n    public enum Tag {//field_name用于保存用户输入的字段名：保存“Hello，world！&quot;字符串的字段名。\n        field_name,\n    };\n\n    private String fieldName;\n\n    /**\n     * @return the fieldName\n     */\n    public String getFieldName() {\n        return fieldName;\n    }\n\n    /**\n     * @param fieldName the fieldName to set\n     */\n    public void setFieldName(String fieldName) {\n        this.fieldName = fieldName;\n    }\n\n    /**\n     * checks parameters, adds result to List&lt;CheckResultInterface&gt;\n     * used in Action &gt; Verify transformation\n     * 验证用户是否在对话框里输入了字段名，并把验证结果添加到检验转换时出现的问题列表里。（最好\n     * 要检验用户输入的所有选项，而不只是容易出错的选项）\n     */\n    public void check(List&lt;CheckResultInterface&gt; remarks, TransMeta transMeta, StepMeta stepMeta, \n            RowMetaInterface prev, String input[], String output[], RowMetaInterface info) {\n\n        if (Const.isEmpty(fieldName)) {\n            CheckResultInterface error = new CheckResult(\n                CheckResult.TYPE_RESULT_ERROR, \n                BaseMessages.getString(PKG, &quot;HelloworldMeta.CHECK_ERR_NO_FIELD&quot;), \n                stepMeta\n            );\n            remarks.add(error);\n        } else {\n            CheckResultInterface ok = new CheckResult(\n                CheckResult.TYPE_RESULT_OK, \n                BaseMessages.getString(PKG, &quot;HelloworldMeta.CHECK_OK_FIELD&quot;), \n                stepMeta\n            );\n            remarks.add(ok);//把验证结果添加到检验转换时出现的问题列表里。\n        }\n    }\n\n    /**\n     *    creates a new instance of the step (factory)\n     * getStep、getStepData和getDialogClassName()方法提供了与这个步骤里其它三个接口之间的桥梁\n     这个接口里还定义了几个方法来说明这四个接口如何结合到一起。\n    String getDialogClassName():用来描述实现了StepDialogInterace接口的对话框类的名字。如果这个方法返回\n                了null，调用类会根据实现了StepMetaInterface接口的类的类名和包名来自动生成对话框类的名字。\n    StepInterface getStep():创建一个实现了StepInterface接口的类。\n    StepInterface getStepData():创建一个实现了StepDataInterface接口的类。\n\n     */\n    public StepInterface getStep(StepMeta stepMeta, StepDataInterface stepDataInterface,\n            int copyNr, TransMeta transMeta, Trans trans) {\n        return new HelloworldStep(stepMeta, stepDataInterface, copyNr, transMeta, trans);\n    }\n\n    /**\n     * creates new instance of the step data (factory)\n     * getStep、getStepData和getDialogClassName()方法提供了与这个步骤里其它三个接口之间的桥梁\n     */\n    public StepDataInterface getStepData() {\n        return new HelloworldStepData();\n    }\n    /**\n     * getStep、getStepData和getDialogClassName()方法提供了与这个步骤里其它三个接口之间的桥梁\n     */\n    @Override\n    public String getDialogClassName() {\n        return HelloworldStepDialog.class.getName();\n    }\n\n    /**\n     * deserialize from xml \n     * databases = list of available connections\n     * counters = list of sequence steps\n     * \n     * 下面的四个方法loadXML()、getXML()、readRep()和saveRep()把元数据保存到XML文件或资源库里，\n     * 或者从XML文件或资源库读取元数据。保存到文件的方法利用了像XStream（http://xstream.codehaus.org）这\n     * 样的XML串行化技术。\n     */\n    public void loadXML(Node stepDomNode, List&lt;DatabaseMeta&gt; databases,\n            Map&lt;String, Counter&gt; sequenceCounters) throws KettleXMLException {\n        fieldName = XMLHandler.getTagValue(stepDomNode, Tag.field_name.name());\n    }\n\n    /**\n     * @Override\n     */\n    public String getXML() throws KettleException {\n        StringBuilder xml = new StringBuilder();\n        xml.append(XMLHandler.addTagValue(Tag.field_name.name(), fieldName));\n        return xml.toString();\n    }\n\n    /**\n     * De-serialize from repository (see loadXML)\n     */\n    public void readRep(Repository repository, ObjectId stepIdInRepository,\n            List&lt;DatabaseMeta&gt; databases, Map&lt;String, Counter&gt; sequenceCounters)\n            throws KettleException {\n        fieldName = repository.getStepAttributeString(stepIdInRepository, Tag.field_name.name());\n    }\n\n    /**\n     * serialize to repository\n     */\n    public void saveRep(Repository repository, ObjectId idOfTransformation, ObjectId idOfStep)\n            throws KettleException {\n        repository.saveStepAttribute(idOfTransformation, idOfStep, Tag.field_name.name(), fieldName);\n    }\n\n\n    /**\n     * initiailize parameters to default\n     */\n    public void setDefault() {\n        fieldName = &quot;helloField&quot;;\n    }\n\n    /**\n     * getFields()方法非常重要，因为它描述了输出数据行的结构。这个方法需要修改inputRowMeta对象，使这个对象和\n     * 输出格式匹配。Spoon和后面的步骤都需要知道这个步骤要输出哪些字段。最常见的一种方法，可以给输出的RowMetaInterface对象\n     * 添加一个ValueMetaInterface对象。在ValueMetaInterface对象里设置的信息越详细越好，可以设置的信息包括数据类型、长度、\n     * 精度、格式掩码，等等。添加的字段描述元信息越多，后面生成的SQL就越准确。\n     */\n    @Override\n    public void getFields(RowMetaInterface inputRowMeta, String name,\n            RowMetaInterface[] info, StepMeta nextStep, VariableSpace space)\n            throws KettleStepException {\n        String realFieldName = space.environmentSubstitute(fieldName);\n        //值的元数据使用ValueMetaInterface接口描述数据流里的一个字段。这个接口里定义了字段的名字、数据类型、长度、精度，等等。下面的例子用于创建一个ValueMetaInterface对象。\n        ValueMetaInterface field = new ValueMeta(realFieldName, ValueMetaInterface.TYPE_STRING);\n        field.setOrigin(name);        \n        inputRowMeta.addValueMeta(field);\n    }\n}\n</code></pre><p>代码解析<br>    @Step(<br>            id=”Helloworld”,<br>            name=”name”,<br>            description=”description”,<br>            categoryDescription=”categoryDescription”,<br>            image=”org/kettlesolutions/plugin/step/helloworld/HelloWorld.png”,<br>            i18nPackageName=”org.kettlesolutions.plugin.step.helloworld”<br>    )<br>这段代码里的@Step annotation用来通知Kettle的插件注册系统：这个类是一个步骤类型的插件。在annotation里可以指定插件的ID、图标、国际代的包、本地化的名称、类别、描述。其中后三项是资源文件里的Key，需要在资源文件里设置真正的值。i18nPackageName指定了资源文件的包名，例如我们这个例子的资源文件位于org/kettlesolutions/plugin/step/helloworld/messages目录下，en_US（英语，美国）的本地代资源文件是messages_en_US.properties。我们例子里的这个资源文件的内容是：<br>name=Hello world<br>description=A very simple step that adds a new “Helllo world” field to the incoming stream<br>注意，如果你指定了不存在的分类，Spoon会创建这个分类，并在Spoon的分类树的最上方显示这个分类。<br>最后，annotation里的image标签指定了插件的图标。需要32*32像素的PNG文件，可以使用透明样式。<br>后面的代码行说明这个类实现了StepMetaInterface接口。在BaseStepMeta抽象类里定义了这个接口的很多默认实现，可以直接继承这个抽象类，然后把工作集中在插件特有的功能上。</p>\n<pre><code>public class HelloworldStepMeta extends BaseStepMeta implements StepMetaInterface\n</code></pre><p>下面的四个方法loadXML()、getXML()、readRep()和saveRep()把元数据保存到XML文件或资源库里，或者从XML文件或资源库读取元数据。保存到文件的方法利用了像XStream（<a href=\"http://xstream.codehaus.org）这样的XML串行化技术。\" target=\"_blank\" rel=\"noopener\">http://xstream.codehaus.org）这样的XML串行化技术。</a></p>\n<p>getFields()方法非常重要，因为它描述了输出数据行的结构。这个方法需要修改inputRowMeta对象，使这个对象和输出格式匹配。Spoon和后面的步骤都需要知道这个步骤要输出哪些字段。最常见的一种方法，可以给输出的RowMetaInterface对象添加一个ValueMetaInterface对象。在ValueMetaInterface对象里设置的信息越详细越好，可以设置的信息包括数据类型、长度、精度、格式掩码，等等。添加的字段描述元信息越多，后面生成的SQL就越准确。</p>\n<h4 id=\"值的元数据（Value-Metadata）\"><a href=\"#值的元数据（Value-Metadata）\" class=\"headerlink\" title=\"####值的元数据（Value Metadata）\"></a>####值的元数据（Value Metadata）</h4><p>值的元数据使用ValueMetaInterface接口描述数据流里的一个字段。这个接口里定义了字段的名字、数据类型、长度、精度，等等。下面的例子用于创建一个ValueMetaInterface对象。<br>    ValueMetaInterface dateMeta = new ValueMeta(“birthdate”,ValueMetaInterface.TYPE_DATE);<br>这个接口也负责转换数据格式。我们建议使用ValueMetaInterface接口来完成所有数据转换的工作。例如，日期类型的数据，如果想把它转换为dateMeta对象里定义的字符串格式，可以用下面的代码：<br>    //java.util.Date birthdate<br>    String birthDateString = dateMeta.getString(birthdate);<br>ValueMeta类负责转换。因为有ValueMetaInterface进行数据类型的转换，所以你不用再去做额外的数据类型转换的工作。<br>使用ValueMetaInterface接口时还要注意数据对象是否为Null。从上一个步骤可以接收到一个数据对象和一个描述数据对象的ValueMetaInterface对象。我们要检查这个数据对象是否为null，在某些情况下如果数据对象为空是不正确的。例如：<br>数据对象是String类型，有10个空格，Value Metadata需要trim这个字符串。<br>在Value Metadata里已经定义了从文本文件里加载的数据，要延迟转换为字符串。所以数据要由二进制的格式（原始数据格式），转换为字符串格式，然后再转换为其它格式的数据。<br>一般使用下面的方法检查数据对象是否为空：<br>    Boolean n = valueMeta.isNull(valueDate);<br>重要：要保证传给ValueMetaInterface对象的数据是在元数据里定义的数据类型。表23-1说明了  ValueMetaInterface里定义的数据类型和Java数据类型的对应关系。<br>Kettle元数据类型和Java里数据类型的对应关系  </p>\n<table><br>    <tr><br>        <th>Value Meta Type</th><br>        <th>Java Class</th><br>    </tr><br>    <tr><br>        <td>ValueMetaInterface.TYPE_STRING</td><br>        <td>Java.lang.String</td><br>    </tr><br>    <tr><br>        <td>ValueMetaInterface.TYPE_DATE</td><br>        <td>Java.util.Date</td><br>    </tr><br>    <tr><br>        <td>ValueMetaInterface.TYPE_BOOLEAN</td><br>        <td>Java.lang.Boolean</td><br>    </tr><br>    <tr><br>        <td>ValueMetaInterface.TYPE_NUMBER</td><br>        <td>Java.lang.Double</td><br>    </tr><br>    <tr><br>        <td>ValueMetaInterface.TYPE_INTEGER</td><br>        <td>Java.lang.Long</td><br>    </tr><br>    <tr><br>        <td>ValueMetaInterface.TYPE_BIGNUMBER</td><br>        <td>Java.math.BigDecimal</td><br>    </tr><br>    <tr><br>        <td>ValueMetaInterface.TYPE_BINARY</td><br>        <td>Byte[]</td><br>    </tr><br></table>\n\n\n<h4 id=\"行的元数据（Row-Meatadata）\"><a href=\"#行的元数据（Row-Meatadata）\" class=\"headerlink\" title=\"####行的元数据（Row Meatadata）\"></a>####行的元数据（Row Meatadata）</h4><p>行的元数据使用RowMetaInterface接口来描述数据行的元数据，而不是一个列的元数据。实际上，RowMetaInterface的类里包含了一组ValueMetaInterface。另外还包括了一些方法来操作行元数据，倒如查询值、检查值是否存、替换值的元数据等。<br>行的元数据里唯一的规则就是一行里的列的名字必须唯一。当你添加了一个新列时，如果新列的名字和已有列的名字相同，列名后面会自动加上“_2”后缀。如果再加一个同名的列会自动加上”_3“后缀，等等。<br>因为在步骤里通常是和数据行打交道，所以从数据行里直接取数据会更方便。可以使用很多类似于getNumber()、getString()这样的方法直接从数据行取数据。例如，销售数据存储在第四列里，可以用下面的代码获取这个数据：  </p>\n<pre><code>Double sales = getInputRowMeta().getNumber(rowData,3);\n</code></pre><p>通过索引获取数据是最快的方式。通过indexOfValue()方法可以获取列在一行里的索引。这个方法扫描列数组，速度并不快。所以，如果要处理所有数据行，我们建议只查询一次列索引。一般是在步骤接收到第一行数据时，就查询列索引，将查询到的列索引保存起来，供后面的数据行使用。  </p>\n<h3 id=\"StepDatainterface\"><a href=\"#StepDatainterface\" class=\"headerlink\" title=\"###StepDatainterface\"></a>###StepDatainterface</h3><p>实现了org.pentaho.di.trans.step.StepDataInterface接口的类用来维护步骤的执行状态，以及存储临时对象。例如，可以把输出行的元数据、数据库连接、输入输出流等存储到这个对象里。<br>HelloworldStepData.java<br>    package org.kettlesolutions.plugin.step.helloworld;</p>\n<p>import org.pentaho.di.core.row.RowMetaInterface;<br>import org.pentaho.di.trans.step.BaseStepData;<br>import org.pentaho.di.trans.step.StepDataInterface;</p>\n<p>public class HelloworldStepData extends BaseStepData implements StepDataInterface {</p>\n<pre><code>public RowMetaInterface outputRowMeta;\n</code></pre><p>}</p>\n<h3 id=\"StepDialogInterface\"><a href=\"#StepDialogInterface\" class=\"headerlink\" title=\"###StepDialogInterface\"></a>###StepDialogInterface</h3><p>实现org.pentaho.di.trans.step.StepDialogInterfac接口的类用来提供一个用户界面，用户通过这个界面输入元数据（转换参数）。用户界面就是一个对话框。这个接口里包含了类似open()和setRepository()等的几个简单的方法。    </p>\n<h4 id=\"Eclipse-SWT\"><a href=\"#Eclipse-SWT\" class=\"headerlink\" title=\"####Eclipse SWT\"></a>####Eclipse SWT</h4><p>Kettle里使用Eclipse SWT作为界面开发包，所以你也要使用SWT来开发对话框窗口。SWT为不同的操作系统Windows、OS X、Linux和Unix提供了一个抽象层。所以SWT的图形界面和操作系统期货的程序的界面风格非常相近。<br>在开始进行SWT开发之前，建议先访问SWT主面以了解更多的内容<a href=\"http://www.eclipse/org/swt。在SWT的网站上，你可以了解到SWT能做出什么样的界面效果：\" target=\"_blank\" rel=\"noopener\">http://www.eclipse/org/swt。在SWT的网站上，你可以了解到SWT能做出什么样的界面效果：</a><br>SWT控件页，<a href=\"http://www.eclipse.org/swt/widgets/，给出了你能使用的所有控件。\" target=\"_blank\" rel=\"noopener\">http://www.eclipse.org/swt/widgets/，给出了你能使用的所有控件。</a><br>SWT样例页，<a href=\"http://www.eclipse.org/swt/snippets/，给出了许多代码例子。\" target=\"_blank\" rel=\"noopener\">http://www.eclipse.org/swt/snippets/，给出了许多代码例子。</a><br>最好的资源就是Kettle里150个内置步骤的对话框源代码。  </p>\n<p>HelloworldStepDialog.java<br>    package org.kettlesolutions.plugin.step.helloworld;</p>\n<pre><code>import org.eclipse.swt.SWT;\nimport org.eclipse.swt.events.ModifyEvent;\nimport org.eclipse.swt.events.ModifyListener;\nimport org.eclipse.swt.events.SelectionAdapter;\nimport org.eclipse.swt.events.SelectionEvent;\nimport org.eclipse.swt.events.ShellAdapter;\nimport org.eclipse.swt.events.ShellEvent;\nimport org.eclipse.swt.layout.FormAttachment;\nimport org.eclipse.swt.layout.FormData;\nimport org.eclipse.swt.layout.FormLayout;\nimport org.eclipse.swt.widgets.Button;\nimport org.eclipse.swt.widgets.Control;\nimport org.eclipse.swt.widgets.Display;\nimport org.eclipse.swt.widgets.Event;\nimport org.eclipse.swt.widgets.Label;\nimport org.eclipse.swt.widgets.Listener;\nimport org.eclipse.swt.widgets.Shell;\nimport org.eclipse.swt.widgets.Text;\nimport org.pentaho.di.core.Const;\nimport org.pentaho.di.i18n.BaseMessages;\nimport org.pentaho.di.trans.TransMeta;\nimport org.pentaho.di.trans.step.BaseStepMeta;\nimport org.pentaho.di.trans.step.StepDialogInterface;\nimport org.pentaho.di.ui.core.widget.TextVar;\nimport org.pentaho.di.ui.trans.step.BaseStepDialog;\n\npublic class HelloworldStepDialog extends BaseStepDialog implements\n        StepDialogInterface {\n\n    private static Class&lt;?&gt; PKG = HelloworldStepMeta.class; // for i18n\n                                                            // purposes, needed\n                                                            // by Translator2!!\n                                                            // $NON-NLS-1$\n\n    private HelloworldStepMeta input;\n\n    private TextVar wFieldname;\n\n    public HelloworldStepDialog(Shell parent, Object baseStepMeta,\n            TransMeta transMeta, String stepname) {\n        //初始化元数据对象以及步骤对话框的父类\n        super(parent, (BaseStepMeta) baseStepMeta, transMeta, stepname);\n        input = (HelloworldStepMeta) baseStepMeta;\n    }\n\n    public String open() {\n        Shell parent = getParent();\n        Display display = parent.getDisplay();\n\n        shell = new Shell(parent, SWT.DIALOG_TRIM | SWT.RESIZE | SWT.MIN\n                | SWT.MAX);\n        props.setLook(shell);\n        setShellImage(shell, input);\n\n        ModifyListener lsMod = new ModifyListener() {\n            public void modifyText(ModifyEvent e) {\n                input.setChanged();\n            }\n        };\n        changed = input.hasChanged();\n\n        FormLayout formLayout = new FormLayout();\n        formLayout.marginWidth = Const.FORM_MARGIN;\n        formLayout.marginHeight = Const.FORM_MARGIN;\n\n        shell.setLayout(formLayout);\n        shell.setText(BaseMessages.getString(PKG,\n                &quot;HelloworldDialog.Shell.Title&quot;)); //$NON-NLS-1$\n\n        //所有控件的右侧使用一个自定义的百分对对齐。控件之间的间距使用一个常量，常量值是4像素。\n        int middle = props.getMiddlePct();\n        int margin = Const.MARGIN;\n\n        // Stepname line\n        wlStepname = new Label(shell, SWT.RIGHT);\n        wlStepname.setText(BaseMessages.getString(PKG,\n                &quot;HelloworldDialog.Stepname.Label&quot;)); //$NON-NLS-1$\n        props.setLook(wlStepname);\n        fdlStepname = new FormData();\n        fdlStepname.left = new FormAttachment(0, 0);\n        fdlStepname.right = new FormAttachment(middle, -margin);\n        fdlStepname.top = new FormAttachment(0, margin);\n        wlStepname.setLayoutData(fdlStepname);\n        wStepname = new Text(shell, SWT.SINGLE | SWT.LEFT | SWT.BORDER);\n        wStepname.setText(stepname);\n        props.setLook(wStepname);\n        wStepname.addModifyListener(lsMod);\n        fdStepname = new FormData();\n        fdStepname.left = new FormAttachment(middle, 0);\n        fdStepname.top = new FormAttachment(0, margin);\n        fdStepname.right = new FormAttachment(100, 0);\n        wStepname.setLayoutData(fdStepname);\n        Control lastControl = wStepname;\n\n        // Fieldname line\n        //创建一个新的标签控件，控件里文本靠右对齐\n        Label wlFieldname = new Label(shell, SWT.RIGHT);\n        wlFieldname.setText(BaseMessages.getString(PKG,\n                &quot;HelloworldDialog.Fieldname.Label&quot;)); //$NON-NLS-1$\n        //下面一行为控件设置用户定义的背景色和字体\n        props.setLook(wlFieldname);\n        FormData fdlFieldname = new FormData();\n        fdlFieldname.left = new FormAttachment(0, 0);\n        fdlFieldname.right = new FormAttachment(middle, -margin);\n        fdlFieldname.top = new FormAttachment(lastControl, margin);\n        wlFieldname.setLayoutData(fdlFieldname);\n        wFieldname = new TextVar(transMeta, shell, SWT.SINGLE | SWT.LEFT\n                | SWT.BORDER);\n        props.setLook(wFieldname);\n        wFieldname.addModifyListener(lsMod);\n        FormData fdFieldname = new FormData();\n        fdFieldname.left = new FormAttachment(middle, 0);\n        fdFieldname.top = new FormAttachment(lastControl, margin);\n        fdFieldname.right = new FormAttachment(100, 0);\n        wFieldname.setLayoutData(fdFieldname);\n        lastControl = wFieldname;\n\n        // Some buttons\n        wOK = new Button(shell, SWT.PUSH);\n        wOK.setText(BaseMessages.getString(PKG, &quot;System.Button.OK&quot;)); //$NON-NLS-1$\n        wCancel = new Button(shell, SWT.PUSH);\n        wCancel.setText(BaseMessages.getString(PKG, &quot;System.Button.Cancel&quot;)); //$NON-NLS-1$\n\n        setButtonPositions(new Button[] { wOK, wCancel }, margin, lastControl);\n\n        // Add listeners\n        lsCancel = new Listener() {\n            public void handleEvent(Event e) {\n                cancel();\n            }\n        };\n        lsOK = new Listener() {\n            public void handleEvent(Event e) {\n                ok();\n            }\n        };\n\n        wCancel.addListener(SWT.Selection, lsCancel);\n        wOK.addListener(SWT.Selection, lsOK);\n\n        lsDef = new SelectionAdapter() {\n            public void widgetDefaultSelected(SelectionEvent e) {\n                ok();\n            }\n        };\n\n        wStepname.addSelectionListener(lsDef);\n        wFieldname.addSelectionListener(lsDef);\n\n        // Detect X or ALT-F4 or something that kills this window...\n        shell.addShellListener(new ShellAdapter() {//保证了窗口在非正常关闭时，取消用户的编辑\n            public void shellClosed(ShellEvent e) {\n                cancel();\n            }\n        });\n\n        // Populate the data of the controls\n        //下面的代码把数据从步骤的元数据对象里复制到窗口的控件里\n        getData();\n\n        // Set the shell size, based upon previous time...\n        //窗口的大小和位置将根据窗口的自然属性、上次窗口大小和位置，以及显示屏的大小自动设置\n        setSize();\n\n        input.setChanged(changed);\n\n        shell.open();\n        while (!shell.isDisposed()) {\n            if (!display.readAndDispatch())\n                display.sleep();\n        }\n        return stepname;\n    }\n\n    /**\n     * Copy information from the meta-data input to the dialog fields.\n     */\n    public void getData() {\n        wStepname.selectAll();\n        //为了防止用户向控件里输入空值，Kettle提供了一个静态方法来检查宿舍，Const.NVL()\n        wFieldname.setText(Const.NVL(input.getFieldName(), &quot;&quot;));\n    }\n\n    private void cancel() {\n        stepname = null;\n        input.setChanged(changed);\n        dispose();\n    }\n    //单击OK把控件里用户输入的数据都写入到步骤的元数据对象中。\n    private void ok() {\n        if (Const.isEmpty(wStepname.getText()))\n            return;\n\n        stepname = wStepname.getText(); // return value\n\n        input.setFieldName(wFieldname.getText());\n\n        dispose();\n    }\n}\n</code></pre><h4 id=\"窗体布局\"><a href=\"#窗体布局\" class=\"headerlink\" title=\"####窗体布局\"></a>####窗体布局</h4><p>如果你看过步骤对话框的源代码，你就会发现窗体类里有很多烦琐的代码。这些代码确保Kettle可以在各种操作系统下以合适的方式展现窗体。可以发现窗体里的大部分代码都和布局以及控件位置有关。<br>FormLayout是SWT里经常看到的布局方式。程序员可以通过FormLayout指定控件的百分比、偏移。下面是我们例子里的窗口布局的代码（HelloworldStepDialog.java）<br>    //创建一个新的标签控件，控件里文本靠右对齐<br>    Label label = new Label(shell, SWT.RIGHT);<br>    label.setText(BaseMessages.getString(PKG,”HelloworldDialog.Fieldname.Label”)); //$NON-NLS-1$<br>    //下面一行为控件设置用户定义的背景色和字体<br>    props.setLook(label);<br>    /**</p>\n<pre><code>* 下面几行将标签的左侧和对话框的最左侧对齐，把标签的右侧放在对话框中间（50%）的左侧10个像素\n* 的位置。标签的顶部放在距离对话框顶部25个像素的位置。\n*/\nFormData fdLabel = new FormData();\nfdlFieldname.left = new FormAttachment(0, 0);\nfdlFieldname.right = new FormAttachment(50, -10);\nfdlFieldname.top = new FormAttachment(0, 25);\nwlFieldname.setLayoutData(fdLabel);  \n</code></pre><p>简而言之，不要感到痛苦；图形用户界面的代码都比较烦琐，但代码并不复杂。  </p>\n<h4 id=\"Kettle-UI元素\"><a href=\"#Kettle-UI元素\" class=\"headerlink\" title=\"####Kettle UI元素\"></a>####Kettle UI元素</h4><p>除了标准的SWT组件，还可以使用Kettle自带的一些控件，Kettle开发人员的工作可以更简单一些。Kettle自带的组件包括以下一些。<br>TableView：这是一个数据表格组件，支持排序、选择、键盘快捷键和撤销/重做，以及右键菜单。<br>TextVar：这是一个支持变量的文本输入框，这个输入框的右上角有一个$符号。用户可以通过”Ctrl  +Alt+空格”的方式，在弹出的下拉列表中选择变量。其他功能和普通的文本框相同。<br>ComboVar：标准的组合下拉列表，支持变量。<br>ConditionEditor：过滤行步骤里使用的输入条件控件。<br>另外还有很多常用的对话框帮你完成相应的工作，如下所示:<br>EnterListDialog:从字符串列表里选择一个或多个字符串。左侧显示字符串列表，右侧是选中的字符串，并提供把字符串从左侧移动到右侧的按钮。<br>EnterNumberDialog:用户可以输入数字<br>EnterPasswordDialog:让用户输入密码<br>EnterSelectionDialog:通过高亮显示，从列表里选择多项<br>EnterMappingDialog:输入两组字符串的映射<br>PreviewRowsDialog:在对话框里预览一组数据行。<br>SQLEditor:一个简单的SQL编辑器，可以输入查询和DDL.<br>ErrorDialog:显示异常信息，列出详细的错误栈对话框  </p>\n<h4 id=\"Hello-World例子对话框\"><a href=\"#Hello-World例子对话框\" class=\"headerlink\" title=\"####Hello World例子对话框\"></a>####Hello World例子对话框</h4><p>现在我们已经基本了解了SWT以及对话框的布局方式，再看看我们的例子，下面的代码是HelloWorldStepDialog.java里的例子。<br>代码的第一部分是初始化元数据对象以及步骤对话框的父类：<br>    public class HelloworldStepDialog extends BaseStepDialog implements<br>            StepDialogInterface {<br>        private static Class&lt;?&gt; PKG = HelloworldStepMeta.class;<br>        private HelloworldStepMeta input;<br>        private TextVar wFieldname;<br>        public HelloworldStepDialog(Shell parent, Object baseStepMeta,<br>                TransMeta transMeta, String stepname) {<br>            //初始化元数据对象以及步骤对话框的父类<br>            super(parent, (BaseStepMeta) baseStepMeta, transMeta, stepname);<br>            input = (HelloworldStepMeta) baseStepMeta;<br>        }<br>在下面的open()方法里创建对话框里的所有控件。SWT使用事件监听模式，可以为控件创建各种监听方法，以响应控件内容的变化和用户的动作。<br>    public String open() {<br>            Shell parent = getParent();<br>            Display display = parent.getDisplay();<br>            shell = new Shell(parent, SWT.DIALOG_TRIM | SWT.RESIZE | SWT.MIN<br>                    | SWT.MAX);<br>            props.setLook(shell);<br>            setShellImage(shell, input);<br>            ModifyListener lsMod = new ModifyListener() {<br>                public void modifyText(ModifyEvent e) {<br>                    input.setChanged();<br>                }<br>            };<br>            changed = input.hasChanged();</p>\n<p>下面代码说明窗体里的控件将使用formLayout的布局方式：<br>    FormLayout formLayout = new FormLayout();<br>            formLayout.marginWidth = Const.FORM_MARGIN;<br>            formLayout.marginHeight = Const.FORM_MARGIN;<br>            shell.setLayout(formLayout);<br>所有控件的右侧使用一个自定义的百分比对齐：props.getMiddlePct()；控件之间的间距使用一个常量，常量值是4像素。<br>    shell.setLayout(formLayout);<br>            shell.setText(BaseMessages.getString(PKG,<br>                    “HelloworldDialog.Shell.Title”)); //$NON-NLS-1$<br>            int middle = props.getMiddlePct();<br>            int margin = Const.MARGIN;<br>下面的代码在对话框的最上面添加了一行步骤名称标签和输入文本框：<br>    // Stepname line<br>            wlStepname = new Label(shell, SWT.RIGHT);<br>            wlStepname.setText(BaseMessages.getString(PKG,<br>                    “HelloworldDialog.Stepname.Label”)); //$NON-NLS-1$<br>            props.setLook(wlStepname);<br>            fdlStepname = new FormData();<br>            fdlStepname.left = new FormAttachment(0, 0);<br>            fdlStepname.right = new FormAttachment(middle, -margin);<br>            fdlStepname.top = new FormAttachment(0, margin);<br>            wlStepname.setLayoutData(fdlStepname);<br>            wStepname = new Text(shell, SWT.SINGLE | SWT.LEFT | SWT.BORDER);<br>            wStepname.setText(stepname);<br>            props.setLook(wStepname);<br>            wStepname.addModifyListener(lsMod);<br>            fdStepname = new FormData();<br>            fdStepname.left = new FormAttachment(middle, 0);<br>            fdStepname.top = new FormAttachment(0, margin);<br>            fdStepname.right = new FormAttachment(100, 0);<br>            wStepname.setLayoutData(fdStepname);<br>            Control lastControl = wStepname;</p>\n<p>下面是新增输出列的列名设置的输入框：<br>    // Fieldname line<br>        //创建一个新的标签控件，控件里文本靠右对齐<br>        Label wlFieldname = new Label(shell, SWT.RIGHT);<br>        wlFieldname.setText(BaseMessages.getString(PKG,<br>                “HelloworldDialog.Fieldname.Label”)); //$NON-NLS-1$<br>        //下面一行为控件设置用户定义的背景色和字体<br>        props.setLook(wlFieldname);<br>        FormData fdlFieldname = new FormData();<br>        fdlFieldname.left = new FormAttachment(0, 0);<br>        fdlFieldname.right = new FormAttachment(middle, -margin);<br>        fdlFieldname.top = new FormAttachment(lastControl, margin);<br>        wlFieldname.setLayoutData(fdlFieldname);<br>        wFieldname = new TextVar(transMeta, shell, SWT.SINGLE | SWT.LEFT<br>                | SWT.BORDER);<br>        props.setLook(wFieldname);<br>        wFieldname.addModifyListener(lsMod);<br>        FormData fdFieldname = new FormData();<br>        fdFieldname.left = new FormAttachment(middle, 0);<br>        fdFieldname.top = new FormAttachment(lastControl, margin);<br>        fdFieldname.right = new FormAttachment(100, 0);<br>        wFieldname.setLayoutData(fdFieldname);<br>        lastControl = wFieldname;</p>\n<p>然后创建两个按钮，“确认”和“取消”按钮，以及按钮单击事件的监听方法，把按钮放在对话框的最下面：<br>    // Some buttons<br>        wOK = new Button(shell, SWT.PUSH);<br>        wOK.setText(BaseMessages.getString(PKG, “System.Button.OK”)); //$NON-NLS-1$<br>        wCancel = new Button(shell, SWT.PUSH);<br>        wCancel.setText(BaseMessages.getString(PKG, “System.Button.Cancel”)); //$NON-NLS-1$</p>\n<pre><code>setButtonPositions(new Button[] { wOK, wCancel }, margin, lastControl);\n\n// Add listeners\nlsCancel = new Listener() {\n    public void handleEvent(Event e) {\n        cancel();\n    }\n};\nlsOK = new Listener() {\n    public void handleEvent(Event e) {\n        ok();\n    }\n};\nwCancel.addListener(SWT.Selection, lsCancel);\nwOK.addListener(SWT.Selection, lsOK);\n</code></pre><p>下面的代码做了两件事情，上部代码可以保证当步骤名称或输出字段名称的输入框在编辑状态时，单击“确定”按钮，正在编辑的内容不会丢失；下部的代码保证了窗口在非正常关闭时（没有使用“确定”或“取消”按钮关闭），取消用户的编辑。<br>    lsDef = new SelectionAdapter() {<br>            public void widgetDefaultSelected(SelectionEvent e) {<br>                ok();<br>            }<br>        };</p>\n<pre><code>wStepname.addSelectionListener(lsDef);\nwFieldname.addSelectionListener(lsDef);\n\n// Detect X or ALT-F4 or something that kills this window...\nshell.addShellListener(new ShellAdapter() {//保证了窗口在非正常关闭时，取消用户的编辑\n    public void shellClosed(ShellEvent e) {\n        cancel();\n    }\n});\n</code></pre><p>下面的代码把数据从步骤的元数据对象里复制到窗口的控件里：<br>    // Populate the data of the controls<br>            getData();<br>窗口的大小和位置将根据窗口的自然属性、上次窗口大小和位置，以及显示屏的大小自动设置。<br>    // Set the shell size, based upon previous time…<br>            //窗口的大小和位置将根据窗口的自然属性、上次窗口大小和位置，以及显示屏的大小自动设置<br>            setSize();<br>            input.setChanged(changed);</p>\n<pre><code>    shell.open();\n    while (!shell.isDisposed()) {\n        if (!display.readAndDispatch())\n            display.sleep();\n    }\n    return stepname;\n}\n</code></pre><p>为了防止用户身控件里输入空值，Kettle提供了一个静态方法来检查空值，ConstNVL();<br>    /**</p>\n<pre><code> * Copy information from the meta-data input to the dialog fields.\n */\npublic void getData() {\n    wStepname.selectAll();\n    wFieldname.setText(Const.NVL(input.getFieldName(), &quot;&quot;));\n}\n</code></pre><p>最后，单击OK按钮后，把控件里用户输入的数据都写入到步骤的元数据对象中：<br>    private void cancel() {<br>            stepname = null;<br>            input.setChanged(changed);<br>            dispose();<br>        }<br>        //单击OK把控件里用户输入的数据都写入到步骤的元数据对象中。<br>        private void ok() {<br>            if (Const.isEmpty(wStepname.getText()))<br>                return;</p>\n<pre><code>    stepname = wStepname.getText(); // return value\n\n    input.setFieldName(wFieldname.getText());\n\n    dispose();\n}\n</code></pre><h3 id=\"StepInteface\"><a href=\"#StepInteface\" class=\"headerlink\" title=\"###StepInteface\"></a>###StepInteface</h3><pre><code>这个类实现了org.pentaho.di.trans.step.StepInterface接口，这个类读取上个步骤传来的数据行，利用StepMetaInterface对象里定义的元数据，逐行转换和处理上个步骤传来的数据行，Kettle引擎直接使用这个接口里的很多方法来执行转换过程，但大部分方法都已经由BaseStep类实现了，通常开发人员只需要重载其中的几个方法。\nInit():步骤初始化方法，用来初始化一个步骤。初始化结果是一个true或者false的Boolean值。如果你的步骤没有任何初始化的工作，可以不用重载这个方法。\nDispose():如果有需要释放的资源，可以在dispose()方法里释放，例如可以关闭数据库连接、释放文件、清除缓存等。在转换的最后Kettle引擎会调用这个方法。如果没有需要释放或清除的资源，可以不用重载这个方法。\nprocessRow():这个方法，是步骤实现工作的地方。只要这个方法返回true，转换引擎就会重复调用这个方法。\n</code></pre><p>下面是HellWorld例子实现的StepInterface接口（HelloworldStep.java）</p>\n<p>HelloworldStep.java<br>    package org.kettlesolutions.plugin.step.helloworld;</p>\n<pre><code>import org.pentaho.di.core.exception.KettleException;\nimport org.pentaho.di.core.row.RowDataUtil;\nimport org.pentaho.di.trans.Trans;\nimport org.pentaho.di.trans.TransMeta;\nimport org.pentaho.di.trans.step.BaseStep;\nimport org.pentaho.di.trans.step.StepDataInterface;\nimport org.pentaho.di.trans.step.StepInterface;\nimport org.pentaho.di.trans.step.StepMeta;\nimport org.pentaho.di.trans.step.StepMetaInterface;\n/**\n * BaseStep抽象类已经实现了接口里的很多方法，我们只要覆盖需要修改的方法即可。\n * @author Administrator\n *\n */\npublic class HelloworldStep extends BaseStep implements StepInterface {\n    /**\n     * 类的构造函数通常直接把参数传递给BaseStep父类。由父类里的方法来构造对象，然后可以直接\n     * 使用类似transMeta这样的对象。\n     * @param stepMeta\n     * @param stepDataInterface\n     * @param copyNr\n     * @param transMeta\n     * @param trans\n     */\n    public HelloworldStep(StepMeta stepMeta, StepDataInterface stepDataInterface,\n            int copyNr, TransMeta transMeta, Trans trans) {\n        super(stepMeta, stepDataInterface, copyNr, transMeta, trans);\n        // TODO Auto-generated constructor stub\n    }\n\n\n    public boolean processRow(StepMetaInterface smi, StepDataInterface sdi) throws KettleException {\n\n        HelloworldStepMeta meta  = (HelloworldStepMeta) smi;\n        HelloworldStepData data = (HelloworldStepData) sdi;\n        /**\n         * getRow()方法从上一个步骤获取一行数据。如果没有更多要获取的数据行，这个方法就会返回null。\n         * 如果前面的步骤不能及时提供数据，这个方法就会阻塞，直到有可用的数据行。这样这个步骤的速度就会降低，也会影响\n         * 其它步骤的速度。\n         */\n        Object[] row = getRow();\n        if (row==null) {\n            /**\n             * setOutputDone()方法用来通知其它的步骤，本步骤已经没有输出数据行。下一个步骤如果\n             * 再调用getRow()方法就会返回null,转换也不再调用processRow()方法。\n             */\n            setOutputDone();\n            return false;\n        }\n\n        if (first) {\n            first=false;\n            /**\n             * 从性能上考虑，getRow()方法不提供数据行的元数据，只提供上个步骤输出的数据。可以使用getInputRowMeta()方法\n            获取元数据，元数据只获取一次即可，所以在first代码块里获取元数据。\n               如果要把数据传到下一个步骤，要使用putRow()方法。除了输出数据，还要输出RowMetaInterface元数据。\n               第一行使用clone()方法把输入行的元数据结构复制给输出行。输出行的元数据结构是在输入行的基础上增加一个字段，但\n               构造输出行的元数据结构只能构造一次，因为所有输出数据行的结构都是一样的，产生了输出行以后，元数据结构就不能再变化。\n               所以输出行的元数据结构在first代码块里构造。first是一个内部成员，first代码块里的代码只在处理第一行数据时执行。\n               下面代码的最后一行，给输出数据增加了一个字段。\n             */\n            data.outputRowMeta = getInputRowMeta().clone();\n            meta.getFields(data.outputRowMeta, getStepname(), null, null, this);\n        }\n        /**\n         * 下面的代码，把数据写入输出流。从性能角度考虑，数据行实现就是Java数组。为了开发方便，可以使用RowDataUtil类提供\n         * 的一些静态方法来操作数据。使用RowDatautil静态方法复制数据，还可以提高性能。\n         */\n        String value = &quot;Hello, world!&quot;;\n\n        Object[] outputRow = RowDataUtil.addValueData(row, getInputRowMeta().size(), value);\n\n        putRow(data.outputRowMeta, outputRow);\n\n        return true;\n    }\n}\n</code></pre><p>解析：<br>public class HelloworldStep extends BaseStep implements StepInterface {<br>BaseStep抽象类已经实现了接口里的很多方法，我们只要覆盖需要修改的方法即可。<br>类的构造函数通常直接把参数传递给BaseStep父类。由父类里的方法来构造对象，然后可以直接使用类似transMeta这样的对象。<br>public HelloworldStep(StepMeta stepMeta, StepDataInterface stepDataInterface,<br>            int copyNr, TransMeta transMeta, Trans trans) {<br>        super(stepMeta, stepDataInterface, copyNr, transMeta, trans);<br>    }<br>getRow()方法从上一个步骤获取一行数据。如果没有更多要获取的数据行，这个方法就会返回null。如果前面的步骤不能及时提供数据，这个方法就会阻塞，直到有可用的数据行。这样这个步骤的速度就会降低，也会影响其它步骤的速度。<br>public boolean processRow(StepMetaInterface smi, StepDataInterface sdi) throws KettleException {<br>        HelloworldStepMeta meta  = (HelloworldStepMeta) smi;<br>        HelloworldStepData data = (HelloworldStepData) sdi;<br>        Object[] row = getRow();<br>        if (row==null) {<br>            setOutputDone();<br>            return false;<br>        }</p>\n<pre><code>    if (first) {\n        first=false;\n        data.outputRowMeta = getInputRowMeta().clone();\n        meta.getFields(data.outputRowMeta, getStepname(), null, null, this);\n    }\n    String value = &quot;Hello, world!&quot;;\n    Object[] outputRow = RowDataUtil.addValueData(row, getInputRowMeta().size(), value);\n\n    putRow(data.outputRowMeta, outputRow);\n\n    return true;\n}\n</code></pre><p>从性能上考虑，getRow()方法不提供数据行的元数据，只提供上个步骤输出的数据。可以使用getInputRowMeta()方法获取元数据，元数据只获取一次即可，所以在first代码块里获取元数据。<br>setOutputDone()方法用来通知其它的步骤，本步骤已经没有输出数据行。下一个步骤如果再调用getRow()方法就会返回null,转换也不再调用processRow()方法。</p>\n<pre><code>Object[] row = getRow();\n        if (row==null) {\n            setOutputDone();\n            return false;\n        }\n</code></pre><p>   如果要把数据传到下一个步骤，要使用putRow()方法。除了输出数据，还要输出RowMetaInterface元数据。<br>    data.outputRowMeta = getInputRowMeta().clone();<br>    meta.getFields(data.outputRowMeta, getStepname(), null, null, this);<br>第一行使用clone()方法把输入行的元数据结构复制给输出行。输出行的元数据结构是在输入行的基础上增加一个字段，但构造输出行的元数据结构只能构造一次，因为所有输出数据行的结构都是一样的，产生了输出行以后，元数据结构就不能再变化。所以输出行的元数据结构在first代码块里构造。first是一个内部成员，first代码块里的代码只在处理第一行数据时执行。下面代码的最后一行，给输出数据增加了一个字段。</p>\n<p>下面的代码，把数据写入输出流。<br>        String value = “Hello, world!”;<br>        Object[] outputRow = RowDataUtil.addValueData(row, getInputRowMeta().size(), value);<br>        putRow(data.outputRowMeta, outputRow);<br>从性能角度考虑，数据行实现就是Java数组。为了开发方便，可以使用RowDataUtil类提供的一些静态方法来操作数据。使用RowDatautil静态方法复制数据，还可以提高性能。<br>从指定的步骤读取数据行<br>如果你想从前面的某个指定的步骤读取数据行，例如”流查询“步骤，可以使用getRowFrom()方法。<br>       RowSet rowSet = findInputRowSet(Source Step Name);<br>       Object[] rowData = getRowFrom(rowSet);<br>           还可以通过rowSet对象获得数据行的元数据：<br>       RowMetaInterface rowMeta = rowSet.getRowMeta();<br>把数据行写入指定的步骤<br>如果想把数据写入到某个特定的步骤，例如”过滤“步骤，可以使用putRowTo()方法<br>      RowSet rowSet = findOutputRowSet(Target Step Name);<br>      ….<br>      putRowTo(outputRowMeta,rowData,rowSet);<br>很明显，输入和输出的RowSet对象只需获得一次即可，这样才更有效率。<br>把数据行写入到错误处理步骤<br>如果想让你的步骤支持错误处理，而且元数据类返回的supportErrorHandling()方法返回了true，就可以把数据输出<br>      到错误处理步骤里。下面是使用putError()方法的例子：<br>      Object[] rowData = getRow();<br>      …<br>      try{<br>          …<br>          putRow(…);<br>      }catch(Exception e){<br>          if(getStepMeta().isDoingErrorHandling()){<br>              putError(getInputRowMeta(),rowData,errorCode);<br>          }else{<br>              throw(e);<br>          }<br>      }<br>      从例子里可以看到，这段代码把错误的行数、错误字段名、消息、错误编码都传递给错误处理步骤。<br>      错误处理的其他工作都自动完成了。</p>\n<h4 id=\"识别一个步骤拷贝\"><a href=\"#识别一个步骤拷贝\" class=\"headerlink\" title=\"####识别一个步骤拷贝\"></a>####识别一个步骤拷贝</h4><p>因为一个步骤可以有多份拷贝同时执行，有时需要识别出正在使用的是哪个步骤拷贝，可以用下面几个方法。<br>     getCopy():获得拷贝号。拷贝号可以唯一标识出步骤的一个拷贝，拷贝号的聚会范围是0-N，N=getStepMeta().getCopies()-1<br>     getUniqueStepNrAcrossSlaves():获得在集群模式下运行的步骤拷贝号。<br>     getUniqueStepCountAcrossSlaves():获得在集群模式下运行的步骤拷贝总数。<br>     通过这些方法可以把一个步骤的工作分配给多份拷贝去完成。例如”CSV文件输入“和”固定文件输入“步骤里都有并行读取文件的选项，这样可以把读取文件的工作放在多个拷贝里或集群里来完成。</p>\n<h4 id=\"结果反馈\"><a href=\"#结果反馈\" class=\"headerlink\" title=\"####结果反馈\"></a>####结果反馈</h4><p>在调用getRow()和putRow()方法时，引擎会自动计算两类度量值，读行数和写行数。这两类度量值可以在界面或日志中记录下来，以监控程序运行的状态。下面几个方法用来操作这两类度量值。<br>    incrementLinesRead():增加从前面步骤读取到的行数。<br>    incrementLinesWritten():增加定稿到后面步骤中的行数。<br>    incrementLinesInput():增加从文件、数据库、网络等资源读取到的行数<br>    incrementLinesOutput:增加写入到文件、数据库、网络等资源的行数。<br>    incrementLinesUpdate():增加更新的行数。<br>    incrementLinesSkipped()：增加跳过的数据行的行数。<br>    incrementLinesRejected():增加拒绝的数据行的行数。<br>    这些度量值用来说明步骤执行的情况。可以在Spoon的转换度量面板里看到，也可以存到日志数据库表里。<br>    使用addResultFile()方法，可以把步骤用到的文件保留下来，保存到结果文件列表里。结果文件列表可以被其它转换或作业项使用。例如，下面的”CSV文件输入“的代码：<br>ResultFile resultFile = new ResultFile(<br>    ResultFile.FILE_TYPE_GENERAL,<br>    fileObject,<br>getTransMeta().getName(),<br>getStepName()<br>);<br>resultFile.setComment(“File was read by a Csv Input step”);<br>addREsultFile(resultFile);</p>\n<h4 id=\"变量替换\"><a href=\"#变量替换\" class=\"headerlink\" title=\"####变量替换\"></a>####变量替换</h4><pre><code>如果输入框需要支持变量，可以使用environmentSubstritute()方法获取变量。例如，若想在“Hello World”例子的字段名输入框里使用变量，就要把StepMetaInterface里的getFields()方法修改成下面的语句：\n</code></pre><p>String realFiledName = apace. environmentSubstritute(fieldName)；<br>因为步骤本身是一个VariableSpace对象，所以也可以使用下面的语句做变量替换：String value = environmentSubstritute(meta.getSringWithVariables());</p>\n<h4 id=\"Apache-VFS\"><a href=\"#Apache-VFS\" class=\"headerlink\" title=\"####Apache VFS\"></a>####Apache VFS</h4><p>Kettle里所有操作文件的步骤，都使用Apache VFS系统的方式操作。Apache VFS不但可以从文件系统读取文件（如java.io.File），还可以从很多其他来源读取文件，如FTP服务器、Z学压缩文件，等 等 。<br>Apache VFS里的FileObject对象提供了文件的抽象层，然后在Kettle的KettleVFS类里还提供了一系列的静态方法，来更方便使用FileObject对象，例如下面的代码 ：  </p>\n<pre><code>FileObject fileObject = KettleVFS.getFileObject(“zip:http://www.example.com/archive.zip!file.txt”);\n</code></pre><p><code>String value = environmentSubstritute(meta.getSringWithVariables());</code></p>\n<p>应该尽可能多地使用KettleVFS,因为它解决了或饶过了很多Apache VFS目前已知的问题。它也增强了SFTP协议。</p>\n<h4 id=\"步骤插件部署\"><a href=\"#步骤插件部署\" class=\"headerlink\" title=\"####步骤插件部署\"></a>####步骤插件部署</h4><p>部署之前，要把四个Java源代码文件编译为class文件。把编译好的class文件放到一个Jar包里。可以使用IDE来做这些事情，也可以手工使用ant脚本来做这些事情。<br>.jar文件应该放在Kettle的plugins/steps目录下。也可以使用一个子目录，把所有的依赖的jar包放在插件jar包所在目录的/lib目录下，不必再放Kettle的类路径中（Kettle的libext/目录）已经有了的jar包。另外可以把多个插件放在一个jar包里。<br>如果想在IDE里调试插件，可以把插件元数据类的名字放在Kettle_PLUGIN_CLASSES变量里（一个逗号分隔的列表）。关于这个主题的更多信息，请参考pentaho Wiki:<a href=\"http://wiki.pentaho.com/display/EAI/How+to+debug+a+Kettle+4+plugin\" target=\"_blank\" rel=\"noopener\">http://wiki.pentaho.com/display/EAI/How+to+debug+a+Kettle+4+plugin</a> 。</p>\n","site":{"data":{}},"excerpt":"","more":"<h1 id=\"Kettle插件体系\"><a href=\"#Kettle插件体系\" class=\"headerlink\" title=\"#Kettle插件体系\"></a>#Kettle插件体系</h1><p>最近公司内有业务系统到数据中心同步的升级改造需求，从各个业务系统收集增量数据到数据中心的数据仓库平台。因为开发周期短暂，需要快速的响应，开发出可用的产品，所以决定借鉴开源程序Kettle，开发一个文件解析组件，然后利用Kettle平台的大数据组件进行与数据中心大数据平台对接</p>\n<p>数据同步部分是：业务系统（RDBMS）-&gt;Kettle(azkaban进行调度)-&gt;数据中心，因为Kettle的增量抽取组件经常出现数据不一致等问题，所以目前已更改为：业务系统（RDBMS）-&gt;OGG（CDC增量抽取）-&gt;数据中心的方式。</p>\n<p>本文主要介绍如何扩展Kettle的功能，部分内容来自《Pentaho Kettle解决方案：使用PDI构建开源ETL解决方案》一书，推荐购买阅读。</p>\n<h2 id=\"架构\"><a href=\"#架构\" class=\"headerlink\" title=\"##架构\"></a>##架构</h2><p>我们先看Kettle插件架构。<br> <img src=\"http://i.imgur.com/mLvXMuV.jpg\" alt=\"\"><br>从功能上看，Kettle内部的对象和外部插件没有任何区别。因为它们使用的API都是一样的，它们只是在运行时的加载方式不同。<br>从Kettle4以后，Kettle内部有一个插件注册系统，它负责加载各种内部和外部插件。插件有以下两个标识属性。<br><strong>插件类型</strong>：由PluginTypeInterface接口定义。例如StepPluginType、JobEntryPluginType、PartitionerPluginType和RepositoryPluginType。<br><strong>插件ID</strong>：这是一个字符串数组，用来唯一标识一个插件。因为旧的插件可以被新的插件代替，一个插件可以有多个ID。在大多数情况下，插件只使用一个单一的字符串，如TableInput是“表输入”步骤的ID，MYSQL是MySQL数据库类型的ID。<br>当Kettle环境初始化以后，插件注册系统首先加载所有的内部对象，Kettle读取下面的配置文件来加载内部对象，这些配置文件位于Kettle的.jar文件中。<br>     Kettle-steps.xml：内部转换步骤。<br>     Kettle-job-entries.xml：内部作业项。<br>     Kettle-partition-plugins.xml：内部分区类型。<br>     Kettle-database-types.xml：内部数据库类型。<br>     Kettle-repositories.xml：内部资源库类型。</p>\n<p>插件注册系统加载了所有的内部对象后，就要搜索可用的外部插件。通过浏览plugins/目录的各个子目录下的.jar文件来完成。它搜索特定的Kettle annotations来判断一个类是否是插件。加载过程将在本章的后面介绍。<br>因为在内部对象加载后才加载插件，所以插件会替代相同ID的已加载的内部对象。例如，你创建了插件，插件的ID是TableInput，就可以替换Kettle标准的“表输入”步骤。这个功能可以让你用插件替换Kettle内置的步骤。可以通过子类继承方式，直接扩展已有步骤的某些功能。</p>\n<h2 id=\"插件类型\"><a href=\"#插件类型\" class=\"headerlink\" title=\"##插件类型\"></a>##插件类型</h2><p>Kettle有下面几种插件类型（下面的插件是Kettle4.0的插件类型，新版kettle包含了很多新的插件，比如视图插件、大数据插件等等）。</p>\n<ul>\n<li>转换步骤插件：在Kettle转换中使用的步骤，用来处理数据行。</li>\n</ul>\n<ul>\n<li>作业项插件：在Kettle作业中使用的作业项，用来实现某个任务。</li>\n</ul>\n<ul>\n<li>分区方法插件：利用输入字段的值指定自己的分区规则。</li>\n</ul>\n<ul>\n<li>数据库类型插件：用来扩展不同的数据库类型。</li>\n</ul>\n<ul>\n<li>资源库类型插件：可以把Kettle元数据保存为自定义类型或格式。</li>\n</ul>\n<p>说明：除了这些类型，还有Spoon类型的插件，可以把功能扩展到Spoon，本书不介绍这个功能。</p>\n<h2 id=\"转换步骤插件\"><a href=\"#转换步骤插件\" class=\"headerlink\" title=\"##转换步骤插件\"></a>##转换步骤插件</h2><p>转换步骤插件包括了四个Java类，这四个类分别实现四个接口。</p>\n<ul>\n<li>StepMetaInterface：这个接口对外 提供步骤的元数据并处理串行化。</li>\n</ul>\n<ul>\n<li>StepInterface:这个接口根据上面接口提供的元数据，来实现步骤的具体功能。</li>\n</ul>\n<ul>\n<li>StepDataInterface:这个接口用来存储步骤的临时数据、文件句柄等。</li>\n</ul>\n<ul>\n<li>StepDialogInterface:这个接口是Spoon里的图形界面，用来编辑步骤的元数据。</li>\n</ul>\n<p>接下来，我们介绍这些接口的基本内容。对于每个接口，在一个简单的“Hello World”例子里提供这些类的相应实现。“Hello World”例子将在数据流里增加一个字段，字段名用户可以自定义，字段值是”Hello world!“。最后介绍一下如何部署这个例子。</p>\n<h3 id=\"StepMetaInterface\"><a href=\"#StepMetaInterface\" class=\"headerlink\" title=\"###StepMetaInterface\"></a>###StepMetaInterface</h3><p>接口org.pentaho.di.trans.step.StepMetaInterface负责步骤里所有和元数据相关的任务。和元数据相关的工作包括：<br>元数据和XML(或资源库)之间的序列化和反序列化<br>getXML（）和loadXML()<br>saveRep()和readRep()  </p>\n<p>描述输出字段<br>getFields()  </p>\n<p>检验元数据是否正确<br>Check()  </p>\n<p>获取步骤相应的要SQL语句，使步骤可以正确运行<br>getSQLStatements()  </p>\n<p>给元数据设置默认值<br>setDefault()  </p>\n<p>完成对数据库的影响分析<br>analyseImpact()  </p>\n<p>描述各类输入和输出流<br>getStepIOMeta()<br>searchInfoAndTargetSteps()<br>handleStreamSelection()<br>getOptionalStreams()<br>resetStepIoMeta()  </p>\n<p>导出元数据资源<br>exportResources()<br>getResourceDependencies()  </p>\n<p>描述使用的库<br>getUsedLibraries()  </p>\n<p>描述使用的数据库连接<br>getUsedDatabaseConnections()  </p>\n<p>描述这个步骤需要的字段（通常是一个数据库表）<br>getRequiredFields()  </p>\n<p>描述步骤是否具有某些功能<br>supportsErrorHandling()<br>excludeFromRowLayoutVerification()<br>excludeFromCopyDistributeVerification()  </p>\n<p>这个接口里还定义了几个方法来说明这四个接口如何结合到一起。<br>String getDialogClassName():用来描述实现了StepDialogInterface接口的对话框类的名字。如果这个方法返回了null，调用类会根据实现了StepMetaInterface接口的类的类名和包名来自动生成对话框类的名字。<br>SetpInterface getStep():创建一个实现了StepInterface接口的类。<br>StepDataInterface getStepData():创建一个实现了StepDataInterface接口的类。<br>现在我们看看”Hello World”例子里对SetpMetaInterface接口的实现<br>HelloworldStepMeta.java<br>    package org.kettlesolutions.plugin.step.helloworld;</p>\n<pre><code>import java.util.List;\nimport java.util.Map;\n\nimport org.pentaho.di.core.CheckResult;\nimport org.pentaho.di.core.CheckResultInterface;\nimport org.pentaho.di.core.Const;\nimport org.pentaho.di.core.Counter;\nimport org.pentaho.di.core.annotations.Step;\nimport org.pentaho.di.core.database.DatabaseMeta;\nimport org.pentaho.di.core.exception.KettleException;\nimport org.pentaho.di.core.exception.KettleStepException;\nimport org.pentaho.di.core.exception.KettleXMLException;\nimport org.pentaho.di.core.row.RowMetaInterface;\nimport org.pentaho.di.core.row.ValueMeta;\nimport org.pentaho.di.core.row.ValueMetaInterface;\nimport org.pentaho.di.core.variables.VariableSpace;\nimport org.pentaho.di.core.xml.XMLHandler;\nimport org.pentaho.di.i18n.BaseMessages;\nimport org.pentaho.di.repository.ObjectId;\nimport org.pentaho.di.repository.Repository;\nimport org.pentaho.di.trans.Trans;\nimport org.pentaho.di.trans.TransMeta;\nimport org.pentaho.di.trans.step.BaseStepMeta;\nimport org.pentaho.di.trans.step.StepDataInterface;\nimport org.pentaho.di.trans.step.StepInterface;\nimport org.pentaho.di.trans.step.StepMeta;\nimport org.pentaho.di.trans.step.StepMetaInterface;\nimport org.w3c.dom.Node;\n\n@Step(\n        id=&quot;Helloworld&quot;,\n        name=&quot;name&quot;,\n        description=&quot;description&quot;,\n        categoryDescription=&quot;categoryDescription&quot;, \n        image=&quot;org/kettlesolutions/plugin/step/helloworld/HelloWorld.png&quot;,\n        i18nPackageName=&quot;org.kettlesolutions.plugin.step.helloworld&quot;\n) \npublic class HelloworldStepMeta extends BaseStepMeta implements StepMetaInterface {\n    /**\n     * PKG变量说明了messages包的位置，在messages包里有各种国际化的资源文件。\n     * 在本章后面经常要看到的BaseMessages.getString()方法，就是根据软件的国际化\n     * 设置，从不同的文件中获取文字。PKG变量通常位于类的最上方，被国际化图形工具使用，\n     * 通过国际化图形工具，国际化人员可以编辑不同的国际化资源文件。所以我们会在很多Kettle\n     * 代码里看见这样的结构。\n     */\n    private static Class&lt;?&gt; PKG = HelloworldStep.class; //for i18n\n    public enum Tag {//field_name用于保存用户输入的字段名：保存“Hello，world！&quot;字符串的字段名。\n        field_name,\n    };\n\n    private String fieldName;\n\n    /**\n     * @return the fieldName\n     */\n    public String getFieldName() {\n        return fieldName;\n    }\n\n    /**\n     * @param fieldName the fieldName to set\n     */\n    public void setFieldName(String fieldName) {\n        this.fieldName = fieldName;\n    }\n\n    /**\n     * checks parameters, adds result to List&lt;CheckResultInterface&gt;\n     * used in Action &gt; Verify transformation\n     * 验证用户是否在对话框里输入了字段名，并把验证结果添加到检验转换时出现的问题列表里。（最好\n     * 要检验用户输入的所有选项，而不只是容易出错的选项）\n     */\n    public void check(List&lt;CheckResultInterface&gt; remarks, TransMeta transMeta, StepMeta stepMeta, \n            RowMetaInterface prev, String input[], String output[], RowMetaInterface info) {\n\n        if (Const.isEmpty(fieldName)) {\n            CheckResultInterface error = new CheckResult(\n                CheckResult.TYPE_RESULT_ERROR, \n                BaseMessages.getString(PKG, &quot;HelloworldMeta.CHECK_ERR_NO_FIELD&quot;), \n                stepMeta\n            );\n            remarks.add(error);\n        } else {\n            CheckResultInterface ok = new CheckResult(\n                CheckResult.TYPE_RESULT_OK, \n                BaseMessages.getString(PKG, &quot;HelloworldMeta.CHECK_OK_FIELD&quot;), \n                stepMeta\n            );\n            remarks.add(ok);//把验证结果添加到检验转换时出现的问题列表里。\n        }\n    }\n\n    /**\n     *    creates a new instance of the step (factory)\n     * getStep、getStepData和getDialogClassName()方法提供了与这个步骤里其它三个接口之间的桥梁\n     这个接口里还定义了几个方法来说明这四个接口如何结合到一起。\n    String getDialogClassName():用来描述实现了StepDialogInterace接口的对话框类的名字。如果这个方法返回\n                了null，调用类会根据实现了StepMetaInterface接口的类的类名和包名来自动生成对话框类的名字。\n    StepInterface getStep():创建一个实现了StepInterface接口的类。\n    StepInterface getStepData():创建一个实现了StepDataInterface接口的类。\n\n     */\n    public StepInterface getStep(StepMeta stepMeta, StepDataInterface stepDataInterface,\n            int copyNr, TransMeta transMeta, Trans trans) {\n        return new HelloworldStep(stepMeta, stepDataInterface, copyNr, transMeta, trans);\n    }\n\n    /**\n     * creates new instance of the step data (factory)\n     * getStep、getStepData和getDialogClassName()方法提供了与这个步骤里其它三个接口之间的桥梁\n     */\n    public StepDataInterface getStepData() {\n        return new HelloworldStepData();\n    }\n    /**\n     * getStep、getStepData和getDialogClassName()方法提供了与这个步骤里其它三个接口之间的桥梁\n     */\n    @Override\n    public String getDialogClassName() {\n        return HelloworldStepDialog.class.getName();\n    }\n\n    /**\n     * deserialize from xml \n     * databases = list of available connections\n     * counters = list of sequence steps\n     * \n     * 下面的四个方法loadXML()、getXML()、readRep()和saveRep()把元数据保存到XML文件或资源库里，\n     * 或者从XML文件或资源库读取元数据。保存到文件的方法利用了像XStream（http://xstream.codehaus.org）这\n     * 样的XML串行化技术。\n     */\n    public void loadXML(Node stepDomNode, List&lt;DatabaseMeta&gt; databases,\n            Map&lt;String, Counter&gt; sequenceCounters) throws KettleXMLException {\n        fieldName = XMLHandler.getTagValue(stepDomNode, Tag.field_name.name());\n    }\n\n    /**\n     * @Override\n     */\n    public String getXML() throws KettleException {\n        StringBuilder xml = new StringBuilder();\n        xml.append(XMLHandler.addTagValue(Tag.field_name.name(), fieldName));\n        return xml.toString();\n    }\n\n    /**\n     * De-serialize from repository (see loadXML)\n     */\n    public void readRep(Repository repository, ObjectId stepIdInRepository,\n            List&lt;DatabaseMeta&gt; databases, Map&lt;String, Counter&gt; sequenceCounters)\n            throws KettleException {\n        fieldName = repository.getStepAttributeString(stepIdInRepository, Tag.field_name.name());\n    }\n\n    /**\n     * serialize to repository\n     */\n    public void saveRep(Repository repository, ObjectId idOfTransformation, ObjectId idOfStep)\n            throws KettleException {\n        repository.saveStepAttribute(idOfTransformation, idOfStep, Tag.field_name.name(), fieldName);\n    }\n\n\n    /**\n     * initiailize parameters to default\n     */\n    public void setDefault() {\n        fieldName = &quot;helloField&quot;;\n    }\n\n    /**\n     * getFields()方法非常重要，因为它描述了输出数据行的结构。这个方法需要修改inputRowMeta对象，使这个对象和\n     * 输出格式匹配。Spoon和后面的步骤都需要知道这个步骤要输出哪些字段。最常见的一种方法，可以给输出的RowMetaInterface对象\n     * 添加一个ValueMetaInterface对象。在ValueMetaInterface对象里设置的信息越详细越好，可以设置的信息包括数据类型、长度、\n     * 精度、格式掩码，等等。添加的字段描述元信息越多，后面生成的SQL就越准确。\n     */\n    @Override\n    public void getFields(RowMetaInterface inputRowMeta, String name,\n            RowMetaInterface[] info, StepMeta nextStep, VariableSpace space)\n            throws KettleStepException {\n        String realFieldName = space.environmentSubstitute(fieldName);\n        //值的元数据使用ValueMetaInterface接口描述数据流里的一个字段。这个接口里定义了字段的名字、数据类型、长度、精度，等等。下面的例子用于创建一个ValueMetaInterface对象。\n        ValueMetaInterface field = new ValueMeta(realFieldName, ValueMetaInterface.TYPE_STRING);\n        field.setOrigin(name);        \n        inputRowMeta.addValueMeta(field);\n    }\n}\n</code></pre><p>代码解析<br>    @Step(<br>            id=”Helloworld”,<br>            name=”name”,<br>            description=”description”,<br>            categoryDescription=”categoryDescription”,<br>            image=”org/kettlesolutions/plugin/step/helloworld/HelloWorld.png”,<br>            i18nPackageName=”org.kettlesolutions.plugin.step.helloworld”<br>    )<br>这段代码里的@Step annotation用来通知Kettle的插件注册系统：这个类是一个步骤类型的插件。在annotation里可以指定插件的ID、图标、国际代的包、本地化的名称、类别、描述。其中后三项是资源文件里的Key，需要在资源文件里设置真正的值。i18nPackageName指定了资源文件的包名，例如我们这个例子的资源文件位于org/kettlesolutions/plugin/step/helloworld/messages目录下，en_US（英语，美国）的本地代资源文件是messages_en_US.properties。我们例子里的这个资源文件的内容是：<br>name=Hello world<br>description=A very simple step that adds a new “Helllo world” field to the incoming stream<br>注意，如果你指定了不存在的分类，Spoon会创建这个分类，并在Spoon的分类树的最上方显示这个分类。<br>最后，annotation里的image标签指定了插件的图标。需要32*32像素的PNG文件，可以使用透明样式。<br>后面的代码行说明这个类实现了StepMetaInterface接口。在BaseStepMeta抽象类里定义了这个接口的很多默认实现，可以直接继承这个抽象类，然后把工作集中在插件特有的功能上。</p>\n<pre><code>public class HelloworldStepMeta extends BaseStepMeta implements StepMetaInterface\n</code></pre><p>下面的四个方法loadXML()、getXML()、readRep()和saveRep()把元数据保存到XML文件或资源库里，或者从XML文件或资源库读取元数据。保存到文件的方法利用了像XStream（<a href=\"http://xstream.codehaus.org）这样的XML串行化技术。\" target=\"_blank\" rel=\"noopener\">http://xstream.codehaus.org）这样的XML串行化技术。</a></p>\n<p>getFields()方法非常重要，因为它描述了输出数据行的结构。这个方法需要修改inputRowMeta对象，使这个对象和输出格式匹配。Spoon和后面的步骤都需要知道这个步骤要输出哪些字段。最常见的一种方法，可以给输出的RowMetaInterface对象添加一个ValueMetaInterface对象。在ValueMetaInterface对象里设置的信息越详细越好，可以设置的信息包括数据类型、长度、精度、格式掩码，等等。添加的字段描述元信息越多，后面生成的SQL就越准确。</p>\n<h4 id=\"值的元数据（Value-Metadata）\"><a href=\"#值的元数据（Value-Metadata）\" class=\"headerlink\" title=\"####值的元数据（Value Metadata）\"></a>####值的元数据（Value Metadata）</h4><p>值的元数据使用ValueMetaInterface接口描述数据流里的一个字段。这个接口里定义了字段的名字、数据类型、长度、精度，等等。下面的例子用于创建一个ValueMetaInterface对象。<br>    ValueMetaInterface dateMeta = new ValueMeta(“birthdate”,ValueMetaInterface.TYPE_DATE);<br>这个接口也负责转换数据格式。我们建议使用ValueMetaInterface接口来完成所有数据转换的工作。例如，日期类型的数据，如果想把它转换为dateMeta对象里定义的字符串格式，可以用下面的代码：<br>    //java.util.Date birthdate<br>    String birthDateString = dateMeta.getString(birthdate);<br>ValueMeta类负责转换。因为有ValueMetaInterface进行数据类型的转换，所以你不用再去做额外的数据类型转换的工作。<br>使用ValueMetaInterface接口时还要注意数据对象是否为Null。从上一个步骤可以接收到一个数据对象和一个描述数据对象的ValueMetaInterface对象。我们要检查这个数据对象是否为null，在某些情况下如果数据对象为空是不正确的。例如：<br>数据对象是String类型，有10个空格，Value Metadata需要trim这个字符串。<br>在Value Metadata里已经定义了从文本文件里加载的数据，要延迟转换为字符串。所以数据要由二进制的格式（原始数据格式），转换为字符串格式，然后再转换为其它格式的数据。<br>一般使用下面的方法检查数据对象是否为空：<br>    Boolean n = valueMeta.isNull(valueDate);<br>重要：要保证传给ValueMetaInterface对象的数据是在元数据里定义的数据类型。表23-1说明了  ValueMetaInterface里定义的数据类型和Java数据类型的对应关系。<br>Kettle元数据类型和Java里数据类型的对应关系  </p>\n<table><br>    <tr><br>        <th>Value Meta Type</th><br>        <th>Java Class</th><br>    </tr><br>    <tr><br>        <td>ValueMetaInterface.TYPE_STRING</td><br>        <td>Java.lang.String</td><br>    </tr><br>    <tr><br>        <td>ValueMetaInterface.TYPE_DATE</td><br>        <td>Java.util.Date</td><br>    </tr><br>    <tr><br>        <td>ValueMetaInterface.TYPE_BOOLEAN</td><br>        <td>Java.lang.Boolean</td><br>    </tr><br>    <tr><br>        <td>ValueMetaInterface.TYPE_NUMBER</td><br>        <td>Java.lang.Double</td><br>    </tr><br>    <tr><br>        <td>ValueMetaInterface.TYPE_INTEGER</td><br>        <td>Java.lang.Long</td><br>    </tr><br>    <tr><br>        <td>ValueMetaInterface.TYPE_BIGNUMBER</td><br>        <td>Java.math.BigDecimal</td><br>    </tr><br>    <tr><br>        <td>ValueMetaInterface.TYPE_BINARY</td><br>        <td>Byte[]</td><br>    </tr><br></table>\n\n\n<h4 id=\"行的元数据（Row-Meatadata）\"><a href=\"#行的元数据（Row-Meatadata）\" class=\"headerlink\" title=\"####行的元数据（Row Meatadata）\"></a>####行的元数据（Row Meatadata）</h4><p>行的元数据使用RowMetaInterface接口来描述数据行的元数据，而不是一个列的元数据。实际上，RowMetaInterface的类里包含了一组ValueMetaInterface。另外还包括了一些方法来操作行元数据，倒如查询值、检查值是否存、替换值的元数据等。<br>行的元数据里唯一的规则就是一行里的列的名字必须唯一。当你添加了一个新列时，如果新列的名字和已有列的名字相同，列名后面会自动加上“_2”后缀。如果再加一个同名的列会自动加上”_3“后缀，等等。<br>因为在步骤里通常是和数据行打交道，所以从数据行里直接取数据会更方便。可以使用很多类似于getNumber()、getString()这样的方法直接从数据行取数据。例如，销售数据存储在第四列里，可以用下面的代码获取这个数据：  </p>\n<pre><code>Double sales = getInputRowMeta().getNumber(rowData,3);\n</code></pre><p>通过索引获取数据是最快的方式。通过indexOfValue()方法可以获取列在一行里的索引。这个方法扫描列数组，速度并不快。所以，如果要处理所有数据行，我们建议只查询一次列索引。一般是在步骤接收到第一行数据时，就查询列索引，将查询到的列索引保存起来，供后面的数据行使用。  </p>\n<h3 id=\"StepDatainterface\"><a href=\"#StepDatainterface\" class=\"headerlink\" title=\"###StepDatainterface\"></a>###StepDatainterface</h3><p>实现了org.pentaho.di.trans.step.StepDataInterface接口的类用来维护步骤的执行状态，以及存储临时对象。例如，可以把输出行的元数据、数据库连接、输入输出流等存储到这个对象里。<br>HelloworldStepData.java<br>    package org.kettlesolutions.plugin.step.helloworld;</p>\n<p>import org.pentaho.di.core.row.RowMetaInterface;<br>import org.pentaho.di.trans.step.BaseStepData;<br>import org.pentaho.di.trans.step.StepDataInterface;</p>\n<p>public class HelloworldStepData extends BaseStepData implements StepDataInterface {</p>\n<pre><code>public RowMetaInterface outputRowMeta;\n</code></pre><p>}</p>\n<h3 id=\"StepDialogInterface\"><a href=\"#StepDialogInterface\" class=\"headerlink\" title=\"###StepDialogInterface\"></a>###StepDialogInterface</h3><p>实现org.pentaho.di.trans.step.StepDialogInterfac接口的类用来提供一个用户界面，用户通过这个界面输入元数据（转换参数）。用户界面就是一个对话框。这个接口里包含了类似open()和setRepository()等的几个简单的方法。    </p>\n<h4 id=\"Eclipse-SWT\"><a href=\"#Eclipse-SWT\" class=\"headerlink\" title=\"####Eclipse SWT\"></a>####Eclipse SWT</h4><p>Kettle里使用Eclipse SWT作为界面开发包，所以你也要使用SWT来开发对话框窗口。SWT为不同的操作系统Windows、OS X、Linux和Unix提供了一个抽象层。所以SWT的图形界面和操作系统期货的程序的界面风格非常相近。<br>在开始进行SWT开发之前，建议先访问SWT主面以了解更多的内容<a href=\"http://www.eclipse/org/swt。在SWT的网站上，你可以了解到SWT能做出什么样的界面效果：\" target=\"_blank\" rel=\"noopener\">http://www.eclipse/org/swt。在SWT的网站上，你可以了解到SWT能做出什么样的界面效果：</a><br>SWT控件页，<a href=\"http://www.eclipse.org/swt/widgets/，给出了你能使用的所有控件。\" target=\"_blank\" rel=\"noopener\">http://www.eclipse.org/swt/widgets/，给出了你能使用的所有控件。</a><br>SWT样例页，<a href=\"http://www.eclipse.org/swt/snippets/，给出了许多代码例子。\" target=\"_blank\" rel=\"noopener\">http://www.eclipse.org/swt/snippets/，给出了许多代码例子。</a><br>最好的资源就是Kettle里150个内置步骤的对话框源代码。  </p>\n<p>HelloworldStepDialog.java<br>    package org.kettlesolutions.plugin.step.helloworld;</p>\n<pre><code>import org.eclipse.swt.SWT;\nimport org.eclipse.swt.events.ModifyEvent;\nimport org.eclipse.swt.events.ModifyListener;\nimport org.eclipse.swt.events.SelectionAdapter;\nimport org.eclipse.swt.events.SelectionEvent;\nimport org.eclipse.swt.events.ShellAdapter;\nimport org.eclipse.swt.events.ShellEvent;\nimport org.eclipse.swt.layout.FormAttachment;\nimport org.eclipse.swt.layout.FormData;\nimport org.eclipse.swt.layout.FormLayout;\nimport org.eclipse.swt.widgets.Button;\nimport org.eclipse.swt.widgets.Control;\nimport org.eclipse.swt.widgets.Display;\nimport org.eclipse.swt.widgets.Event;\nimport org.eclipse.swt.widgets.Label;\nimport org.eclipse.swt.widgets.Listener;\nimport org.eclipse.swt.widgets.Shell;\nimport org.eclipse.swt.widgets.Text;\nimport org.pentaho.di.core.Const;\nimport org.pentaho.di.i18n.BaseMessages;\nimport org.pentaho.di.trans.TransMeta;\nimport org.pentaho.di.trans.step.BaseStepMeta;\nimport org.pentaho.di.trans.step.StepDialogInterface;\nimport org.pentaho.di.ui.core.widget.TextVar;\nimport org.pentaho.di.ui.trans.step.BaseStepDialog;\n\npublic class HelloworldStepDialog extends BaseStepDialog implements\n        StepDialogInterface {\n\n    private static Class&lt;?&gt; PKG = HelloworldStepMeta.class; // for i18n\n                                                            // purposes, needed\n                                                            // by Translator2!!\n                                                            // $NON-NLS-1$\n\n    private HelloworldStepMeta input;\n\n    private TextVar wFieldname;\n\n    public HelloworldStepDialog(Shell parent, Object baseStepMeta,\n            TransMeta transMeta, String stepname) {\n        //初始化元数据对象以及步骤对话框的父类\n        super(parent, (BaseStepMeta) baseStepMeta, transMeta, stepname);\n        input = (HelloworldStepMeta) baseStepMeta;\n    }\n\n    public String open() {\n        Shell parent = getParent();\n        Display display = parent.getDisplay();\n\n        shell = new Shell(parent, SWT.DIALOG_TRIM | SWT.RESIZE | SWT.MIN\n                | SWT.MAX);\n        props.setLook(shell);\n        setShellImage(shell, input);\n\n        ModifyListener lsMod = new ModifyListener() {\n            public void modifyText(ModifyEvent e) {\n                input.setChanged();\n            }\n        };\n        changed = input.hasChanged();\n\n        FormLayout formLayout = new FormLayout();\n        formLayout.marginWidth = Const.FORM_MARGIN;\n        formLayout.marginHeight = Const.FORM_MARGIN;\n\n        shell.setLayout(formLayout);\n        shell.setText(BaseMessages.getString(PKG,\n                &quot;HelloworldDialog.Shell.Title&quot;)); //$NON-NLS-1$\n\n        //所有控件的右侧使用一个自定义的百分对对齐。控件之间的间距使用一个常量，常量值是4像素。\n        int middle = props.getMiddlePct();\n        int margin = Const.MARGIN;\n\n        // Stepname line\n        wlStepname = new Label(shell, SWT.RIGHT);\n        wlStepname.setText(BaseMessages.getString(PKG,\n                &quot;HelloworldDialog.Stepname.Label&quot;)); //$NON-NLS-1$\n        props.setLook(wlStepname);\n        fdlStepname = new FormData();\n        fdlStepname.left = new FormAttachment(0, 0);\n        fdlStepname.right = new FormAttachment(middle, -margin);\n        fdlStepname.top = new FormAttachment(0, margin);\n        wlStepname.setLayoutData(fdlStepname);\n        wStepname = new Text(shell, SWT.SINGLE | SWT.LEFT | SWT.BORDER);\n        wStepname.setText(stepname);\n        props.setLook(wStepname);\n        wStepname.addModifyListener(lsMod);\n        fdStepname = new FormData();\n        fdStepname.left = new FormAttachment(middle, 0);\n        fdStepname.top = new FormAttachment(0, margin);\n        fdStepname.right = new FormAttachment(100, 0);\n        wStepname.setLayoutData(fdStepname);\n        Control lastControl = wStepname;\n\n        // Fieldname line\n        //创建一个新的标签控件，控件里文本靠右对齐\n        Label wlFieldname = new Label(shell, SWT.RIGHT);\n        wlFieldname.setText(BaseMessages.getString(PKG,\n                &quot;HelloworldDialog.Fieldname.Label&quot;)); //$NON-NLS-1$\n        //下面一行为控件设置用户定义的背景色和字体\n        props.setLook(wlFieldname);\n        FormData fdlFieldname = new FormData();\n        fdlFieldname.left = new FormAttachment(0, 0);\n        fdlFieldname.right = new FormAttachment(middle, -margin);\n        fdlFieldname.top = new FormAttachment(lastControl, margin);\n        wlFieldname.setLayoutData(fdlFieldname);\n        wFieldname = new TextVar(transMeta, shell, SWT.SINGLE | SWT.LEFT\n                | SWT.BORDER);\n        props.setLook(wFieldname);\n        wFieldname.addModifyListener(lsMod);\n        FormData fdFieldname = new FormData();\n        fdFieldname.left = new FormAttachment(middle, 0);\n        fdFieldname.top = new FormAttachment(lastControl, margin);\n        fdFieldname.right = new FormAttachment(100, 0);\n        wFieldname.setLayoutData(fdFieldname);\n        lastControl = wFieldname;\n\n        // Some buttons\n        wOK = new Button(shell, SWT.PUSH);\n        wOK.setText(BaseMessages.getString(PKG, &quot;System.Button.OK&quot;)); //$NON-NLS-1$\n        wCancel = new Button(shell, SWT.PUSH);\n        wCancel.setText(BaseMessages.getString(PKG, &quot;System.Button.Cancel&quot;)); //$NON-NLS-1$\n\n        setButtonPositions(new Button[] { wOK, wCancel }, margin, lastControl);\n\n        // Add listeners\n        lsCancel = new Listener() {\n            public void handleEvent(Event e) {\n                cancel();\n            }\n        };\n        lsOK = new Listener() {\n            public void handleEvent(Event e) {\n                ok();\n            }\n        };\n\n        wCancel.addListener(SWT.Selection, lsCancel);\n        wOK.addListener(SWT.Selection, lsOK);\n\n        lsDef = new SelectionAdapter() {\n            public void widgetDefaultSelected(SelectionEvent e) {\n                ok();\n            }\n        };\n\n        wStepname.addSelectionListener(lsDef);\n        wFieldname.addSelectionListener(lsDef);\n\n        // Detect X or ALT-F4 or something that kills this window...\n        shell.addShellListener(new ShellAdapter() {//保证了窗口在非正常关闭时，取消用户的编辑\n            public void shellClosed(ShellEvent e) {\n                cancel();\n            }\n        });\n\n        // Populate the data of the controls\n        //下面的代码把数据从步骤的元数据对象里复制到窗口的控件里\n        getData();\n\n        // Set the shell size, based upon previous time...\n        //窗口的大小和位置将根据窗口的自然属性、上次窗口大小和位置，以及显示屏的大小自动设置\n        setSize();\n\n        input.setChanged(changed);\n\n        shell.open();\n        while (!shell.isDisposed()) {\n            if (!display.readAndDispatch())\n                display.sleep();\n        }\n        return stepname;\n    }\n\n    /**\n     * Copy information from the meta-data input to the dialog fields.\n     */\n    public void getData() {\n        wStepname.selectAll();\n        //为了防止用户向控件里输入空值，Kettle提供了一个静态方法来检查宿舍，Const.NVL()\n        wFieldname.setText(Const.NVL(input.getFieldName(), &quot;&quot;));\n    }\n\n    private void cancel() {\n        stepname = null;\n        input.setChanged(changed);\n        dispose();\n    }\n    //单击OK把控件里用户输入的数据都写入到步骤的元数据对象中。\n    private void ok() {\n        if (Const.isEmpty(wStepname.getText()))\n            return;\n\n        stepname = wStepname.getText(); // return value\n\n        input.setFieldName(wFieldname.getText());\n\n        dispose();\n    }\n}\n</code></pre><h4 id=\"窗体布局\"><a href=\"#窗体布局\" class=\"headerlink\" title=\"####窗体布局\"></a>####窗体布局</h4><p>如果你看过步骤对话框的源代码，你就会发现窗体类里有很多烦琐的代码。这些代码确保Kettle可以在各种操作系统下以合适的方式展现窗体。可以发现窗体里的大部分代码都和布局以及控件位置有关。<br>FormLayout是SWT里经常看到的布局方式。程序员可以通过FormLayout指定控件的百分比、偏移。下面是我们例子里的窗口布局的代码（HelloworldStepDialog.java）<br>    //创建一个新的标签控件，控件里文本靠右对齐<br>    Label label = new Label(shell, SWT.RIGHT);<br>    label.setText(BaseMessages.getString(PKG,”HelloworldDialog.Fieldname.Label”)); //$NON-NLS-1$<br>    //下面一行为控件设置用户定义的背景色和字体<br>    props.setLook(label);<br>    /**</p>\n<pre><code>* 下面几行将标签的左侧和对话框的最左侧对齐，把标签的右侧放在对话框中间（50%）的左侧10个像素\n* 的位置。标签的顶部放在距离对话框顶部25个像素的位置。\n*/\nFormData fdLabel = new FormData();\nfdlFieldname.left = new FormAttachment(0, 0);\nfdlFieldname.right = new FormAttachment(50, -10);\nfdlFieldname.top = new FormAttachment(0, 25);\nwlFieldname.setLayoutData(fdLabel);  \n</code></pre><p>简而言之，不要感到痛苦；图形用户界面的代码都比较烦琐，但代码并不复杂。  </p>\n<h4 id=\"Kettle-UI元素\"><a href=\"#Kettle-UI元素\" class=\"headerlink\" title=\"####Kettle UI元素\"></a>####Kettle UI元素</h4><p>除了标准的SWT组件，还可以使用Kettle自带的一些控件，Kettle开发人员的工作可以更简单一些。Kettle自带的组件包括以下一些。<br>TableView：这是一个数据表格组件，支持排序、选择、键盘快捷键和撤销/重做，以及右键菜单。<br>TextVar：这是一个支持变量的文本输入框，这个输入框的右上角有一个$符号。用户可以通过”Ctrl  +Alt+空格”的方式，在弹出的下拉列表中选择变量。其他功能和普通的文本框相同。<br>ComboVar：标准的组合下拉列表，支持变量。<br>ConditionEditor：过滤行步骤里使用的输入条件控件。<br>另外还有很多常用的对话框帮你完成相应的工作，如下所示:<br>EnterListDialog:从字符串列表里选择一个或多个字符串。左侧显示字符串列表，右侧是选中的字符串，并提供把字符串从左侧移动到右侧的按钮。<br>EnterNumberDialog:用户可以输入数字<br>EnterPasswordDialog:让用户输入密码<br>EnterSelectionDialog:通过高亮显示，从列表里选择多项<br>EnterMappingDialog:输入两组字符串的映射<br>PreviewRowsDialog:在对话框里预览一组数据行。<br>SQLEditor:一个简单的SQL编辑器，可以输入查询和DDL.<br>ErrorDialog:显示异常信息，列出详细的错误栈对话框  </p>\n<h4 id=\"Hello-World例子对话框\"><a href=\"#Hello-World例子对话框\" class=\"headerlink\" title=\"####Hello World例子对话框\"></a>####Hello World例子对话框</h4><p>现在我们已经基本了解了SWT以及对话框的布局方式，再看看我们的例子，下面的代码是HelloWorldStepDialog.java里的例子。<br>代码的第一部分是初始化元数据对象以及步骤对话框的父类：<br>    public class HelloworldStepDialog extends BaseStepDialog implements<br>            StepDialogInterface {<br>        private static Class&lt;?&gt; PKG = HelloworldStepMeta.class;<br>        private HelloworldStepMeta input;<br>        private TextVar wFieldname;<br>        public HelloworldStepDialog(Shell parent, Object baseStepMeta,<br>                TransMeta transMeta, String stepname) {<br>            //初始化元数据对象以及步骤对话框的父类<br>            super(parent, (BaseStepMeta) baseStepMeta, transMeta, stepname);<br>            input = (HelloworldStepMeta) baseStepMeta;<br>        }<br>在下面的open()方法里创建对话框里的所有控件。SWT使用事件监听模式，可以为控件创建各种监听方法，以响应控件内容的变化和用户的动作。<br>    public String open() {<br>            Shell parent = getParent();<br>            Display display = parent.getDisplay();<br>            shell = new Shell(parent, SWT.DIALOG_TRIM | SWT.RESIZE | SWT.MIN<br>                    | SWT.MAX);<br>            props.setLook(shell);<br>            setShellImage(shell, input);<br>            ModifyListener lsMod = new ModifyListener() {<br>                public void modifyText(ModifyEvent e) {<br>                    input.setChanged();<br>                }<br>            };<br>            changed = input.hasChanged();</p>\n<p>下面代码说明窗体里的控件将使用formLayout的布局方式：<br>    FormLayout formLayout = new FormLayout();<br>            formLayout.marginWidth = Const.FORM_MARGIN;<br>            formLayout.marginHeight = Const.FORM_MARGIN;<br>            shell.setLayout(formLayout);<br>所有控件的右侧使用一个自定义的百分比对齐：props.getMiddlePct()；控件之间的间距使用一个常量，常量值是4像素。<br>    shell.setLayout(formLayout);<br>            shell.setText(BaseMessages.getString(PKG,<br>                    “HelloworldDialog.Shell.Title”)); //$NON-NLS-1$<br>            int middle = props.getMiddlePct();<br>            int margin = Const.MARGIN;<br>下面的代码在对话框的最上面添加了一行步骤名称标签和输入文本框：<br>    // Stepname line<br>            wlStepname = new Label(shell, SWT.RIGHT);<br>            wlStepname.setText(BaseMessages.getString(PKG,<br>                    “HelloworldDialog.Stepname.Label”)); //$NON-NLS-1$<br>            props.setLook(wlStepname);<br>            fdlStepname = new FormData();<br>            fdlStepname.left = new FormAttachment(0, 0);<br>            fdlStepname.right = new FormAttachment(middle, -margin);<br>            fdlStepname.top = new FormAttachment(0, margin);<br>            wlStepname.setLayoutData(fdlStepname);<br>            wStepname = new Text(shell, SWT.SINGLE | SWT.LEFT | SWT.BORDER);<br>            wStepname.setText(stepname);<br>            props.setLook(wStepname);<br>            wStepname.addModifyListener(lsMod);<br>            fdStepname = new FormData();<br>            fdStepname.left = new FormAttachment(middle, 0);<br>            fdStepname.top = new FormAttachment(0, margin);<br>            fdStepname.right = new FormAttachment(100, 0);<br>            wStepname.setLayoutData(fdStepname);<br>            Control lastControl = wStepname;</p>\n<p>下面是新增输出列的列名设置的输入框：<br>    // Fieldname line<br>        //创建一个新的标签控件，控件里文本靠右对齐<br>        Label wlFieldname = new Label(shell, SWT.RIGHT);<br>        wlFieldname.setText(BaseMessages.getString(PKG,<br>                “HelloworldDialog.Fieldname.Label”)); //$NON-NLS-1$<br>        //下面一行为控件设置用户定义的背景色和字体<br>        props.setLook(wlFieldname);<br>        FormData fdlFieldname = new FormData();<br>        fdlFieldname.left = new FormAttachment(0, 0);<br>        fdlFieldname.right = new FormAttachment(middle, -margin);<br>        fdlFieldname.top = new FormAttachment(lastControl, margin);<br>        wlFieldname.setLayoutData(fdlFieldname);<br>        wFieldname = new TextVar(transMeta, shell, SWT.SINGLE | SWT.LEFT<br>                | SWT.BORDER);<br>        props.setLook(wFieldname);<br>        wFieldname.addModifyListener(lsMod);<br>        FormData fdFieldname = new FormData();<br>        fdFieldname.left = new FormAttachment(middle, 0);<br>        fdFieldname.top = new FormAttachment(lastControl, margin);<br>        fdFieldname.right = new FormAttachment(100, 0);<br>        wFieldname.setLayoutData(fdFieldname);<br>        lastControl = wFieldname;</p>\n<p>然后创建两个按钮，“确认”和“取消”按钮，以及按钮单击事件的监听方法，把按钮放在对话框的最下面：<br>    // Some buttons<br>        wOK = new Button(shell, SWT.PUSH);<br>        wOK.setText(BaseMessages.getString(PKG, “System.Button.OK”)); //$NON-NLS-1$<br>        wCancel = new Button(shell, SWT.PUSH);<br>        wCancel.setText(BaseMessages.getString(PKG, “System.Button.Cancel”)); //$NON-NLS-1$</p>\n<pre><code>setButtonPositions(new Button[] { wOK, wCancel }, margin, lastControl);\n\n// Add listeners\nlsCancel = new Listener() {\n    public void handleEvent(Event e) {\n        cancel();\n    }\n};\nlsOK = new Listener() {\n    public void handleEvent(Event e) {\n        ok();\n    }\n};\nwCancel.addListener(SWT.Selection, lsCancel);\nwOK.addListener(SWT.Selection, lsOK);\n</code></pre><p>下面的代码做了两件事情，上部代码可以保证当步骤名称或输出字段名称的输入框在编辑状态时，单击“确定”按钮，正在编辑的内容不会丢失；下部的代码保证了窗口在非正常关闭时（没有使用“确定”或“取消”按钮关闭），取消用户的编辑。<br>    lsDef = new SelectionAdapter() {<br>            public void widgetDefaultSelected(SelectionEvent e) {<br>                ok();<br>            }<br>        };</p>\n<pre><code>wStepname.addSelectionListener(lsDef);\nwFieldname.addSelectionListener(lsDef);\n\n// Detect X or ALT-F4 or something that kills this window...\nshell.addShellListener(new ShellAdapter() {//保证了窗口在非正常关闭时，取消用户的编辑\n    public void shellClosed(ShellEvent e) {\n        cancel();\n    }\n});\n</code></pre><p>下面的代码把数据从步骤的元数据对象里复制到窗口的控件里：<br>    // Populate the data of the controls<br>            getData();<br>窗口的大小和位置将根据窗口的自然属性、上次窗口大小和位置，以及显示屏的大小自动设置。<br>    // Set the shell size, based upon previous time…<br>            //窗口的大小和位置将根据窗口的自然属性、上次窗口大小和位置，以及显示屏的大小自动设置<br>            setSize();<br>            input.setChanged(changed);</p>\n<pre><code>    shell.open();\n    while (!shell.isDisposed()) {\n        if (!display.readAndDispatch())\n            display.sleep();\n    }\n    return stepname;\n}\n</code></pre><p>为了防止用户身控件里输入空值，Kettle提供了一个静态方法来检查空值，ConstNVL();<br>    /**</p>\n<pre><code> * Copy information from the meta-data input to the dialog fields.\n */\npublic void getData() {\n    wStepname.selectAll();\n    wFieldname.setText(Const.NVL(input.getFieldName(), &quot;&quot;));\n}\n</code></pre><p>最后，单击OK按钮后，把控件里用户输入的数据都写入到步骤的元数据对象中：<br>    private void cancel() {<br>            stepname = null;<br>            input.setChanged(changed);<br>            dispose();<br>        }<br>        //单击OK把控件里用户输入的数据都写入到步骤的元数据对象中。<br>        private void ok() {<br>            if (Const.isEmpty(wStepname.getText()))<br>                return;</p>\n<pre><code>    stepname = wStepname.getText(); // return value\n\n    input.setFieldName(wFieldname.getText());\n\n    dispose();\n}\n</code></pre><h3 id=\"StepInteface\"><a href=\"#StepInteface\" class=\"headerlink\" title=\"###StepInteface\"></a>###StepInteface</h3><pre><code>这个类实现了org.pentaho.di.trans.step.StepInterface接口，这个类读取上个步骤传来的数据行，利用StepMetaInterface对象里定义的元数据，逐行转换和处理上个步骤传来的数据行，Kettle引擎直接使用这个接口里的很多方法来执行转换过程，但大部分方法都已经由BaseStep类实现了，通常开发人员只需要重载其中的几个方法。\nInit():步骤初始化方法，用来初始化一个步骤。初始化结果是一个true或者false的Boolean值。如果你的步骤没有任何初始化的工作，可以不用重载这个方法。\nDispose():如果有需要释放的资源，可以在dispose()方法里释放，例如可以关闭数据库连接、释放文件、清除缓存等。在转换的最后Kettle引擎会调用这个方法。如果没有需要释放或清除的资源，可以不用重载这个方法。\nprocessRow():这个方法，是步骤实现工作的地方。只要这个方法返回true，转换引擎就会重复调用这个方法。\n</code></pre><p>下面是HellWorld例子实现的StepInterface接口（HelloworldStep.java）</p>\n<p>HelloworldStep.java<br>    package org.kettlesolutions.plugin.step.helloworld;</p>\n<pre><code>import org.pentaho.di.core.exception.KettleException;\nimport org.pentaho.di.core.row.RowDataUtil;\nimport org.pentaho.di.trans.Trans;\nimport org.pentaho.di.trans.TransMeta;\nimport org.pentaho.di.trans.step.BaseStep;\nimport org.pentaho.di.trans.step.StepDataInterface;\nimport org.pentaho.di.trans.step.StepInterface;\nimport org.pentaho.di.trans.step.StepMeta;\nimport org.pentaho.di.trans.step.StepMetaInterface;\n/**\n * BaseStep抽象类已经实现了接口里的很多方法，我们只要覆盖需要修改的方法即可。\n * @author Administrator\n *\n */\npublic class HelloworldStep extends BaseStep implements StepInterface {\n    /**\n     * 类的构造函数通常直接把参数传递给BaseStep父类。由父类里的方法来构造对象，然后可以直接\n     * 使用类似transMeta这样的对象。\n     * @param stepMeta\n     * @param stepDataInterface\n     * @param copyNr\n     * @param transMeta\n     * @param trans\n     */\n    public HelloworldStep(StepMeta stepMeta, StepDataInterface stepDataInterface,\n            int copyNr, TransMeta transMeta, Trans trans) {\n        super(stepMeta, stepDataInterface, copyNr, transMeta, trans);\n        // TODO Auto-generated constructor stub\n    }\n\n\n    public boolean processRow(StepMetaInterface smi, StepDataInterface sdi) throws KettleException {\n\n        HelloworldStepMeta meta  = (HelloworldStepMeta) smi;\n        HelloworldStepData data = (HelloworldStepData) sdi;\n        /**\n         * getRow()方法从上一个步骤获取一行数据。如果没有更多要获取的数据行，这个方法就会返回null。\n         * 如果前面的步骤不能及时提供数据，这个方法就会阻塞，直到有可用的数据行。这样这个步骤的速度就会降低，也会影响\n         * 其它步骤的速度。\n         */\n        Object[] row = getRow();\n        if (row==null) {\n            /**\n             * setOutputDone()方法用来通知其它的步骤，本步骤已经没有输出数据行。下一个步骤如果\n             * 再调用getRow()方法就会返回null,转换也不再调用processRow()方法。\n             */\n            setOutputDone();\n            return false;\n        }\n\n        if (first) {\n            first=false;\n            /**\n             * 从性能上考虑，getRow()方法不提供数据行的元数据，只提供上个步骤输出的数据。可以使用getInputRowMeta()方法\n            获取元数据，元数据只获取一次即可，所以在first代码块里获取元数据。\n               如果要把数据传到下一个步骤，要使用putRow()方法。除了输出数据，还要输出RowMetaInterface元数据。\n               第一行使用clone()方法把输入行的元数据结构复制给输出行。输出行的元数据结构是在输入行的基础上增加一个字段，但\n               构造输出行的元数据结构只能构造一次，因为所有输出数据行的结构都是一样的，产生了输出行以后，元数据结构就不能再变化。\n               所以输出行的元数据结构在first代码块里构造。first是一个内部成员，first代码块里的代码只在处理第一行数据时执行。\n               下面代码的最后一行，给输出数据增加了一个字段。\n             */\n            data.outputRowMeta = getInputRowMeta().clone();\n            meta.getFields(data.outputRowMeta, getStepname(), null, null, this);\n        }\n        /**\n         * 下面的代码，把数据写入输出流。从性能角度考虑，数据行实现就是Java数组。为了开发方便，可以使用RowDataUtil类提供\n         * 的一些静态方法来操作数据。使用RowDatautil静态方法复制数据，还可以提高性能。\n         */\n        String value = &quot;Hello, world!&quot;;\n\n        Object[] outputRow = RowDataUtil.addValueData(row, getInputRowMeta().size(), value);\n\n        putRow(data.outputRowMeta, outputRow);\n\n        return true;\n    }\n}\n</code></pre><p>解析：<br>public class HelloworldStep extends BaseStep implements StepInterface {<br>BaseStep抽象类已经实现了接口里的很多方法，我们只要覆盖需要修改的方法即可。<br>类的构造函数通常直接把参数传递给BaseStep父类。由父类里的方法来构造对象，然后可以直接使用类似transMeta这样的对象。<br>public HelloworldStep(StepMeta stepMeta, StepDataInterface stepDataInterface,<br>            int copyNr, TransMeta transMeta, Trans trans) {<br>        super(stepMeta, stepDataInterface, copyNr, transMeta, trans);<br>    }<br>getRow()方法从上一个步骤获取一行数据。如果没有更多要获取的数据行，这个方法就会返回null。如果前面的步骤不能及时提供数据，这个方法就会阻塞，直到有可用的数据行。这样这个步骤的速度就会降低，也会影响其它步骤的速度。<br>public boolean processRow(StepMetaInterface smi, StepDataInterface sdi) throws KettleException {<br>        HelloworldStepMeta meta  = (HelloworldStepMeta) smi;<br>        HelloworldStepData data = (HelloworldStepData) sdi;<br>        Object[] row = getRow();<br>        if (row==null) {<br>            setOutputDone();<br>            return false;<br>        }</p>\n<pre><code>    if (first) {\n        first=false;\n        data.outputRowMeta = getInputRowMeta().clone();\n        meta.getFields(data.outputRowMeta, getStepname(), null, null, this);\n    }\n    String value = &quot;Hello, world!&quot;;\n    Object[] outputRow = RowDataUtil.addValueData(row, getInputRowMeta().size(), value);\n\n    putRow(data.outputRowMeta, outputRow);\n\n    return true;\n}\n</code></pre><p>从性能上考虑，getRow()方法不提供数据行的元数据，只提供上个步骤输出的数据。可以使用getInputRowMeta()方法获取元数据，元数据只获取一次即可，所以在first代码块里获取元数据。<br>setOutputDone()方法用来通知其它的步骤，本步骤已经没有输出数据行。下一个步骤如果再调用getRow()方法就会返回null,转换也不再调用processRow()方法。</p>\n<pre><code>Object[] row = getRow();\n        if (row==null) {\n            setOutputDone();\n            return false;\n        }\n</code></pre><p>   如果要把数据传到下一个步骤，要使用putRow()方法。除了输出数据，还要输出RowMetaInterface元数据。<br>    data.outputRowMeta = getInputRowMeta().clone();<br>    meta.getFields(data.outputRowMeta, getStepname(), null, null, this);<br>第一行使用clone()方法把输入行的元数据结构复制给输出行。输出行的元数据结构是在输入行的基础上增加一个字段，但构造输出行的元数据结构只能构造一次，因为所有输出数据行的结构都是一样的，产生了输出行以后，元数据结构就不能再变化。所以输出行的元数据结构在first代码块里构造。first是一个内部成员，first代码块里的代码只在处理第一行数据时执行。下面代码的最后一行，给输出数据增加了一个字段。</p>\n<p>下面的代码，把数据写入输出流。<br>        String value = “Hello, world!”;<br>        Object[] outputRow = RowDataUtil.addValueData(row, getInputRowMeta().size(), value);<br>        putRow(data.outputRowMeta, outputRow);<br>从性能角度考虑，数据行实现就是Java数组。为了开发方便，可以使用RowDataUtil类提供的一些静态方法来操作数据。使用RowDatautil静态方法复制数据，还可以提高性能。<br>从指定的步骤读取数据行<br>如果你想从前面的某个指定的步骤读取数据行，例如”流查询“步骤，可以使用getRowFrom()方法。<br>       RowSet rowSet = findInputRowSet(Source Step Name);<br>       Object[] rowData = getRowFrom(rowSet);<br>           还可以通过rowSet对象获得数据行的元数据：<br>       RowMetaInterface rowMeta = rowSet.getRowMeta();<br>把数据行写入指定的步骤<br>如果想把数据写入到某个特定的步骤，例如”过滤“步骤，可以使用putRowTo()方法<br>      RowSet rowSet = findOutputRowSet(Target Step Name);<br>      ….<br>      putRowTo(outputRowMeta,rowData,rowSet);<br>很明显，输入和输出的RowSet对象只需获得一次即可，这样才更有效率。<br>把数据行写入到错误处理步骤<br>如果想让你的步骤支持错误处理，而且元数据类返回的supportErrorHandling()方法返回了true，就可以把数据输出<br>      到错误处理步骤里。下面是使用putError()方法的例子：<br>      Object[] rowData = getRow();<br>      …<br>      try{<br>          …<br>          putRow(…);<br>      }catch(Exception e){<br>          if(getStepMeta().isDoingErrorHandling()){<br>              putError(getInputRowMeta(),rowData,errorCode);<br>          }else{<br>              throw(e);<br>          }<br>      }<br>      从例子里可以看到，这段代码把错误的行数、错误字段名、消息、错误编码都传递给错误处理步骤。<br>      错误处理的其他工作都自动完成了。</p>\n<h4 id=\"识别一个步骤拷贝\"><a href=\"#识别一个步骤拷贝\" class=\"headerlink\" title=\"####识别一个步骤拷贝\"></a>####识别一个步骤拷贝</h4><p>因为一个步骤可以有多份拷贝同时执行，有时需要识别出正在使用的是哪个步骤拷贝，可以用下面几个方法。<br>     getCopy():获得拷贝号。拷贝号可以唯一标识出步骤的一个拷贝，拷贝号的聚会范围是0-N，N=getStepMeta().getCopies()-1<br>     getUniqueStepNrAcrossSlaves():获得在集群模式下运行的步骤拷贝号。<br>     getUniqueStepCountAcrossSlaves():获得在集群模式下运行的步骤拷贝总数。<br>     通过这些方法可以把一个步骤的工作分配给多份拷贝去完成。例如”CSV文件输入“和”固定文件输入“步骤里都有并行读取文件的选项，这样可以把读取文件的工作放在多个拷贝里或集群里来完成。</p>\n<h4 id=\"结果反馈\"><a href=\"#结果反馈\" class=\"headerlink\" title=\"####结果反馈\"></a>####结果反馈</h4><p>在调用getRow()和putRow()方法时，引擎会自动计算两类度量值，读行数和写行数。这两类度量值可以在界面或日志中记录下来，以监控程序运行的状态。下面几个方法用来操作这两类度量值。<br>    incrementLinesRead():增加从前面步骤读取到的行数。<br>    incrementLinesWritten():增加定稿到后面步骤中的行数。<br>    incrementLinesInput():增加从文件、数据库、网络等资源读取到的行数<br>    incrementLinesOutput:增加写入到文件、数据库、网络等资源的行数。<br>    incrementLinesUpdate():增加更新的行数。<br>    incrementLinesSkipped()：增加跳过的数据行的行数。<br>    incrementLinesRejected():增加拒绝的数据行的行数。<br>    这些度量值用来说明步骤执行的情况。可以在Spoon的转换度量面板里看到，也可以存到日志数据库表里。<br>    使用addResultFile()方法，可以把步骤用到的文件保留下来，保存到结果文件列表里。结果文件列表可以被其它转换或作业项使用。例如，下面的”CSV文件输入“的代码：<br>ResultFile resultFile = new ResultFile(<br>    ResultFile.FILE_TYPE_GENERAL,<br>    fileObject,<br>getTransMeta().getName(),<br>getStepName()<br>);<br>resultFile.setComment(“File was read by a Csv Input step”);<br>addREsultFile(resultFile);</p>\n<h4 id=\"变量替换\"><a href=\"#变量替换\" class=\"headerlink\" title=\"####变量替换\"></a>####变量替换</h4><pre><code>如果输入框需要支持变量，可以使用environmentSubstritute()方法获取变量。例如，若想在“Hello World”例子的字段名输入框里使用变量，就要把StepMetaInterface里的getFields()方法修改成下面的语句：\n</code></pre><p>String realFiledName = apace. environmentSubstritute(fieldName)；<br>因为步骤本身是一个VariableSpace对象，所以也可以使用下面的语句做变量替换：String value = environmentSubstritute(meta.getSringWithVariables());</p>\n<h4 id=\"Apache-VFS\"><a href=\"#Apache-VFS\" class=\"headerlink\" title=\"####Apache VFS\"></a>####Apache VFS</h4><p>Kettle里所有操作文件的步骤，都使用Apache VFS系统的方式操作。Apache VFS不但可以从文件系统读取文件（如java.io.File），还可以从很多其他来源读取文件，如FTP服务器、Z学压缩文件，等 等 。<br>Apache VFS里的FileObject对象提供了文件的抽象层，然后在Kettle的KettleVFS类里还提供了一系列的静态方法，来更方便使用FileObject对象，例如下面的代码 ：  </p>\n<pre><code>FileObject fileObject = KettleVFS.getFileObject(“zip:http://www.example.com/archive.zip!file.txt”);\n</code></pre><p><code>String value = environmentSubstritute(meta.getSringWithVariables());</code></p>\n<p>应该尽可能多地使用KettleVFS,因为它解决了或饶过了很多Apache VFS目前已知的问题。它也增强了SFTP协议。</p>\n<h4 id=\"步骤插件部署\"><a href=\"#步骤插件部署\" class=\"headerlink\" title=\"####步骤插件部署\"></a>####步骤插件部署</h4><p>部署之前，要把四个Java源代码文件编译为class文件。把编译好的class文件放到一个Jar包里。可以使用IDE来做这些事情，也可以手工使用ant脚本来做这些事情。<br>.jar文件应该放在Kettle的plugins/steps目录下。也可以使用一个子目录，把所有的依赖的jar包放在插件jar包所在目录的/lib目录下，不必再放Kettle的类路径中（Kettle的libext/目录）已经有了的jar包。另外可以把多个插件放在一个jar包里。<br>如果想在IDE里调试插件，可以把插件元数据类的名字放在Kettle_PLUGIN_CLASSES变量里（一个逗号分隔的列表）。关于这个主题的更多信息，请参考pentaho Wiki:<a href=\"http://wiki.pentaho.com/display/EAI/How+to+debug+a+Kettle+4+plugin\" target=\"_blank\" rel=\"noopener\">http://wiki.pentaho.com/display/EAI/How+to+debug+a+Kettle+4+plugin</a> 。</p>\n"},{"title":"开源ETL工具-kettle","date":"2017-04-15T06:43:49.000Z","_content":"#开源ETL工具-kettle\n说明：本文部分内容参考网络的资料，如果侵权之处请告知一下，不胜感激！\n\nKettle是Pentaho公司开发的一款ETL产品，以工作流为核心，强调面向解决方案而非工具的，基于java平台的商业智能(Business Intelligence,BI)套件。Kettle的开源协议是LGPL，该协议来自GNU，因功能强大，被FSF(Free Software Foundation)列为首选协议。LGPL协议允许Kettle作为商业（非开源）代码的链接库，使用Kettle的商业代码无须开源。LGPL带来的不仅是Kettle API，你还可以对它进行拓展对外提供商业软件或服务。\n##  ##ETL是什么\n\nETL早期作为数据仓库的关键环节，负责将分布的、异构数据源中的数据如关系数据、平面数据文件等抽取到临时中间层后进行清洗、转换、集成，最后加载到数据仓库（Data Warehouse）或数据集市（Data Mart）中，成为联机分析处理（On-Line Analytical Processing，OLAP）、数据挖掘（Data Mining）的基础。\n\n\nExract：从多种异构数据源中抽取数据\n\nTransform：经过清洗，统一化和转换\n\nLoad：将数据加载到目的数据源中\n\n\n\n##  ##Kettle产品特点\n\n适用于将多个应用系统的大批量的、异构的数据进行整合，有强大的数据转换功能。\n高效适配多种类型的异构数据库、文件和应用系统。\n快速构建复杂数据大集中应用、无需编码。\n\nKettle构成\n\nTODO以Github上的名词进行定义Spoon，Cart等\n\n\n左边是集成开发工具（Spoon），可以进行流程的开发、配置、调试、部署、执行(转换、任务)，也可以对运行情况进行监控、对处理过程的日志进行查看、也可以通过接口调用方式进行远程管理。\n\n中间是服务器(Carte)，包括实际执行转换和任务的ETL引擎、监控管理的接口、认证授权接口，还有一个可以拓展的接口。\n\n下面是在开发过程中，用于保存集成开发工具中创建的转换、任务、数据库等项的，资源库包含两类，一个是数据库资源库，一个是文件资源库。\n\n右边个是是第三方平台，可以基于kettle提供的接口实现相应的功能包括状态监控、启停控制、日志查看等功能。\n\n\n组成部分\n名称\n描述\nSpoon\n一个基于swt开发的流式处理客户端，用户开发转换、任务、创建数据库、集群、分区等\nPan\n一个独立的命令行程序，支持通过命令行实现界面的功能，如果转换启停、任务启停。状态查看等\nKitchen\n一个独立的命令行程序，用于执行由Spoon编辑的作业。\nCarte\nCarte是一个轻量级的Web容器，用于建立专用、远程的ETL Server。\n\n\nPDI相关术语和概念\nJob(任务)、Transformation(转换)是kettle的两个最重要的概念。任务做的一件完整的事，包含开始、结束等整个生命周期；而转换是要做这件事的某一个小的功能。比如你要从A数据源中解析数据后放入B数据源，那么你可以创建两个转换，一个是从A数据源加载数据->处理数据->放入存储中；另一个是把数据放入B数据源，然后在一个任务中处理他们。\n\n下面我们通过集成开发工具去了解一个转换和任务\n\nTransformation（转换）\nTransformation（转换)是由step(步骤)和hops(节点连接线)组成，一个转换，可以看成一段数据流，每一个步骤完成一项数据处理的工作，节点连接线用于数据的流动。\n\n转换可以单独运行完成某一项工作，文件的扩展名为.ktr\n\nSteps（步骤）\nSteps（步骤）是转换的重要组件部分，在Spoon中步骤根据功能分为输入类、输出类、脚本类等，每一个步骤完成一种特定的功能，比如excel输出组件，用于把数据流输出为excel文件格式。参考如下：\n\n\nHops（节点连接）\nHops（节点连接）是数据传输的通道，用于连接两个步骤，使数据从一个步骤传递到另一个步骤，支持分发、复制等方式。注意数据处理的顺序并不是按照节点连接箭头的顺序，因为第个步骤都是单独的线程。\n\nJobs（工作）\nJobs（工作）是基于工作流模型的，顺序处理。把步骤、转换组织在一起完成一件完整的事情。\n文件扩展名为.kjb\n\n\n下载使用\nkettle下载 目前最新版7.0\nhttps://sourceforge.net/projects/pentaho/files/Data%20Integration/\n \n下载解压后是一个如：pdi-ce-7.0.0.0-25的文件，目录内容如下\n\nwindown下直接双拼Spoon.bat、linux下直接运行./spoon.sh即可。\n注： could not find the main class:org.pentaho.commons.launcher.Launcher. Program will exit. 表示jdk版本错误 。7.0版本只支持jdk1.8，可以单独配置kettle的jdk，添加配置到系统中即可：\n名称：PENTAHO_JAVA_HOME\n值：C:\\Program Files\\Java\\jdk1.8.0_45 \n    mac系统下/Library/Java/JavaVirtualMachines/jdk1.8.0_111.jdk/Contents/Home/\n启动后界面如下\n\n新建转换：快捷键Ctrl+N\n\n\n首先从核心对象区选择“生成记录组件，编辑：\n\n\n然后选择Excel输出组件到工作区，创建生成记录步骤到Excel输出步骤的连接线，编辑excel输出目录和字段\n\n\n\n\n最后生成如下，点击运行：\n\n\n\n\n运行后的结果是输出excel文件，并可以查看每个步骤的处理情况，读、写、输入、输出等\n\n\n其它参考链接\nkettle源码下载，可以选择各个版本下载，自己编译。\nhttps://github.com/pentaho/pentaho-kettle\n大数据插件源码\nhttps://github.com/pentaho/big-data-plugin\nkettle支持的大数据环境源码，主要是hdp,cdh。\nhttps://github.com/pentaho/pentaho-hadoop-shims\nkettle nexus\nhttp://repo.pentaho.org/content/groups/omni/pentaho/  \nhttp://repository.pentaho.org/artifactory/repo/\n所有组件实现说明\nhttp://wiki.pentaho.com/display/EAI/Pentaho+Data+Integration+Steps  \n所有组件测试说明\nhttp://wiki.pentaho.com/display/EAI/test \n帮助\nhttp://help.pentaho.com/Documentation \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n","source":"_posts/kettle/开源ETL工具-kettle.md","raw":"---\ntitle: 开源ETL工具-kettle\ndate: 2017-04-15 14:43:49\ntags: [开源项目,kettle]\ncategories: [开源项目,kettle]\n---\n#开源ETL工具-kettle\n说明：本文部分内容参考网络的资料，如果侵权之处请告知一下，不胜感激！\n\nKettle是Pentaho公司开发的一款ETL产品，以工作流为核心，强调面向解决方案而非工具的，基于java平台的商业智能(Business Intelligence,BI)套件。Kettle的开源协议是LGPL，该协议来自GNU，因功能强大，被FSF(Free Software Foundation)列为首选协议。LGPL协议允许Kettle作为商业（非开源）代码的链接库，使用Kettle的商业代码无须开源。LGPL带来的不仅是Kettle API，你还可以对它进行拓展对外提供商业软件或服务。\n##  ##ETL是什么\n\nETL早期作为数据仓库的关键环节，负责将分布的、异构数据源中的数据如关系数据、平面数据文件等抽取到临时中间层后进行清洗、转换、集成，最后加载到数据仓库（Data Warehouse）或数据集市（Data Mart）中，成为联机分析处理（On-Line Analytical Processing，OLAP）、数据挖掘（Data Mining）的基础。\n\n\nExract：从多种异构数据源中抽取数据\n\nTransform：经过清洗，统一化和转换\n\nLoad：将数据加载到目的数据源中\n\n\n\n##  ##Kettle产品特点\n\n适用于将多个应用系统的大批量的、异构的数据进行整合，有强大的数据转换功能。\n高效适配多种类型的异构数据库、文件和应用系统。\n快速构建复杂数据大集中应用、无需编码。\n\nKettle构成\n\nTODO以Github上的名词进行定义Spoon，Cart等\n\n\n左边是集成开发工具（Spoon），可以进行流程的开发、配置、调试、部署、执行(转换、任务)，也可以对运行情况进行监控、对处理过程的日志进行查看、也可以通过接口调用方式进行远程管理。\n\n中间是服务器(Carte)，包括实际执行转换和任务的ETL引擎、监控管理的接口、认证授权接口，还有一个可以拓展的接口。\n\n下面是在开发过程中，用于保存集成开发工具中创建的转换、任务、数据库等项的，资源库包含两类，一个是数据库资源库，一个是文件资源库。\n\n右边个是是第三方平台，可以基于kettle提供的接口实现相应的功能包括状态监控、启停控制、日志查看等功能。\n\n\n组成部分\n名称\n描述\nSpoon\n一个基于swt开发的流式处理客户端，用户开发转换、任务、创建数据库、集群、分区等\nPan\n一个独立的命令行程序，支持通过命令行实现界面的功能，如果转换启停、任务启停。状态查看等\nKitchen\n一个独立的命令行程序，用于执行由Spoon编辑的作业。\nCarte\nCarte是一个轻量级的Web容器，用于建立专用、远程的ETL Server。\n\n\nPDI相关术语和概念\nJob(任务)、Transformation(转换)是kettle的两个最重要的概念。任务做的一件完整的事，包含开始、结束等整个生命周期；而转换是要做这件事的某一个小的功能。比如你要从A数据源中解析数据后放入B数据源，那么你可以创建两个转换，一个是从A数据源加载数据->处理数据->放入存储中；另一个是把数据放入B数据源，然后在一个任务中处理他们。\n\n下面我们通过集成开发工具去了解一个转换和任务\n\nTransformation（转换）\nTransformation（转换)是由step(步骤)和hops(节点连接线)组成，一个转换，可以看成一段数据流，每一个步骤完成一项数据处理的工作，节点连接线用于数据的流动。\n\n转换可以单独运行完成某一项工作，文件的扩展名为.ktr\n\nSteps（步骤）\nSteps（步骤）是转换的重要组件部分，在Spoon中步骤根据功能分为输入类、输出类、脚本类等，每一个步骤完成一种特定的功能，比如excel输出组件，用于把数据流输出为excel文件格式。参考如下：\n\n\nHops（节点连接）\nHops（节点连接）是数据传输的通道，用于连接两个步骤，使数据从一个步骤传递到另一个步骤，支持分发、复制等方式。注意数据处理的顺序并不是按照节点连接箭头的顺序，因为第个步骤都是单独的线程。\n\nJobs（工作）\nJobs（工作）是基于工作流模型的，顺序处理。把步骤、转换组织在一起完成一件完整的事情。\n文件扩展名为.kjb\n\n\n下载使用\nkettle下载 目前最新版7.0\nhttps://sourceforge.net/projects/pentaho/files/Data%20Integration/\n \n下载解压后是一个如：pdi-ce-7.0.0.0-25的文件，目录内容如下\n\nwindown下直接双拼Spoon.bat、linux下直接运行./spoon.sh即可。\n注： could not find the main class:org.pentaho.commons.launcher.Launcher. Program will exit. 表示jdk版本错误 。7.0版本只支持jdk1.8，可以单独配置kettle的jdk，添加配置到系统中即可：\n名称：PENTAHO_JAVA_HOME\n值：C:\\Program Files\\Java\\jdk1.8.0_45 \n    mac系统下/Library/Java/JavaVirtualMachines/jdk1.8.0_111.jdk/Contents/Home/\n启动后界面如下\n\n新建转换：快捷键Ctrl+N\n\n\n首先从核心对象区选择“生成记录组件，编辑：\n\n\n然后选择Excel输出组件到工作区，创建生成记录步骤到Excel输出步骤的连接线，编辑excel输出目录和字段\n\n\n\n\n最后生成如下，点击运行：\n\n\n\n\n运行后的结果是输出excel文件，并可以查看每个步骤的处理情况，读、写、输入、输出等\n\n\n其它参考链接\nkettle源码下载，可以选择各个版本下载，自己编译。\nhttps://github.com/pentaho/pentaho-kettle\n大数据插件源码\nhttps://github.com/pentaho/big-data-plugin\nkettle支持的大数据环境源码，主要是hdp,cdh。\nhttps://github.com/pentaho/pentaho-hadoop-shims\nkettle nexus\nhttp://repo.pentaho.org/content/groups/omni/pentaho/  \nhttp://repository.pentaho.org/artifactory/repo/\n所有组件实现说明\nhttp://wiki.pentaho.com/display/EAI/Pentaho+Data+Integration+Steps  \n所有组件测试说明\nhttp://wiki.pentaho.com/display/EAI/test \n帮助\nhttp://help.pentaho.com/Documentation \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n","slug":"kettle/开源ETL工具-kettle","published":1,"updated":"2017-08-18T01:44:35.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjh7polsn000zbp7ro6e59uhf","content":"<p>#开源ETL工具-kettle<br>说明：本文部分内容参考网络的资料，如果侵权之处请告知一下，不胜感激！</p>\n<p>Kettle是Pentaho公司开发的一款ETL产品，以工作流为核心，强调面向解决方案而非工具的，基于java平台的商业智能(Business Intelligence,BI)套件。Kettle的开源协议是LGPL，该协议来自GNU，因功能强大，被FSF(Free Software Foundation)列为首选协议。LGPL协议允许Kettle作为商业（非开源）代码的链接库，使用Kettle的商业代码无须开源。LGPL带来的不仅是Kettle API，你还可以对它进行拓展对外提供商业软件或服务。</p>\n<h2 id=\"ETL是什么\"><a href=\"#ETL是什么\" class=\"headerlink\" title=\"##ETL是什么\"></a>##ETL是什么</h2><p>ETL早期作为数据仓库的关键环节，负责将分布的、异构数据源中的数据如关系数据、平面数据文件等抽取到临时中间层后进行清洗、转换、集成，最后加载到数据仓库（Data Warehouse）或数据集市（Data Mart）中，成为联机分析处理（On-Line Analytical Processing，OLAP）、数据挖掘（Data Mining）的基础。</p>\n<p>Exract：从多种异构数据源中抽取数据</p>\n<p>Transform：经过清洗，统一化和转换</p>\n<p>Load：将数据加载到目的数据源中</p>\n<h2 id=\"Kettle产品特点\"><a href=\"#Kettle产品特点\" class=\"headerlink\" title=\"##Kettle产品特点\"></a>##Kettle产品特点</h2><p>适用于将多个应用系统的大批量的、异构的数据进行整合，有强大的数据转换功能。<br>高效适配多种类型的异构数据库、文件和应用系统。<br>快速构建复杂数据大集中应用、无需编码。</p>\n<p>Kettle构成</p>\n<p>TODO以Github上的名词进行定义Spoon，Cart等</p>\n<p>左边是集成开发工具（Spoon），可以进行流程的开发、配置、调试、部署、执行(转换、任务)，也可以对运行情况进行监控、对处理过程的日志进行查看、也可以通过接口调用方式进行远程管理。</p>\n<p>中间是服务器(Carte)，包括实际执行转换和任务的ETL引擎、监控管理的接口、认证授权接口，还有一个可以拓展的接口。</p>\n<p>下面是在开发过程中，用于保存集成开发工具中创建的转换、任务、数据库等项的，资源库包含两类，一个是数据库资源库，一个是文件资源库。</p>\n<p>右边个是是第三方平台，可以基于kettle提供的接口实现相应的功能包括状态监控、启停控制、日志查看等功能。</p>\n<p>组成部分<br>名称<br>描述<br>Spoon<br>一个基于swt开发的流式处理客户端，用户开发转换、任务、创建数据库、集群、分区等<br>Pan<br>一个独立的命令行程序，支持通过命令行实现界面的功能，如果转换启停、任务启停。状态查看等<br>Kitchen<br>一个独立的命令行程序，用于执行由Spoon编辑的作业。<br>Carte<br>Carte是一个轻量级的Web容器，用于建立专用、远程的ETL Server。</p>\n<p>PDI相关术语和概念<br>Job(任务)、Transformation(转换)是kettle的两个最重要的概念。任务做的一件完整的事，包含开始、结束等整个生命周期；而转换是要做这件事的某一个小的功能。比如你要从A数据源中解析数据后放入B数据源，那么你可以创建两个转换，一个是从A数据源加载数据-&gt;处理数据-&gt;放入存储中；另一个是把数据放入B数据源，然后在一个任务中处理他们。</p>\n<p>下面我们通过集成开发工具去了解一个转换和任务</p>\n<p>Transformation（转换）<br>Transformation（转换)是由step(步骤)和hops(节点连接线)组成，一个转换，可以看成一段数据流，每一个步骤完成一项数据处理的工作，节点连接线用于数据的流动。</p>\n<p>转换可以单独运行完成某一项工作，文件的扩展名为.ktr</p>\n<p>Steps（步骤）<br>Steps（步骤）是转换的重要组件部分，在Spoon中步骤根据功能分为输入类、输出类、脚本类等，每一个步骤完成一种特定的功能，比如excel输出组件，用于把数据流输出为excel文件格式。参考如下：</p>\n<p>Hops（节点连接）<br>Hops（节点连接）是数据传输的通道，用于连接两个步骤，使数据从一个步骤传递到另一个步骤，支持分发、复制等方式。注意数据处理的顺序并不是按照节点连接箭头的顺序，因为第个步骤都是单独的线程。</p>\n<p>Jobs（工作）<br>Jobs（工作）是基于工作流模型的，顺序处理。把步骤、转换组织在一起完成一件完整的事情。<br>文件扩展名为.kjb</p>\n<p>下载使用<br>kettle下载 目前最新版7.0<br><a href=\"https://sourceforge.net/projects/pentaho/files/Data%20Integration/\" target=\"_blank\" rel=\"noopener\">https://sourceforge.net/projects/pentaho/files/Data%20Integration/</a></p>\n<p>下载解压后是一个如：pdi-ce-7.0.0.0-25的文件，目录内容如下</p>\n<p>windown下直接双拼Spoon.bat、linux下直接运行./spoon.sh即可。<br>注： could not find the main class:org.pentaho.commons.launcher.Launcher. Program will exit. 表示jdk版本错误 。7.0版本只支持jdk1.8，可以单独配置kettle的jdk，添加配置到系统中即可：<br>名称：PENTAHO_JAVA_HOME<br>值：C:\\Program Files\\Java\\jdk1.8.0_45<br>    mac系统下/Library/Java/JavaVirtualMachines/jdk1.8.0_111.jdk/Contents/Home/<br>启动后界面如下</p>\n<p>新建转换：快捷键Ctrl+N</p>\n<p>首先从核心对象区选择“生成记录组件，编辑：</p>\n<p>然后选择Excel输出组件到工作区，创建生成记录步骤到Excel输出步骤的连接线，编辑excel输出目录和字段</p>\n<p>最后生成如下，点击运行：</p>\n<p>运行后的结果是输出excel文件，并可以查看每个步骤的处理情况，读、写、输入、输出等</p>\n<p>其它参考链接<br>kettle源码下载，可以选择各个版本下载，自己编译。<br><a href=\"https://github.com/pentaho/pentaho-kettle\" target=\"_blank\" rel=\"noopener\">https://github.com/pentaho/pentaho-kettle</a><br>大数据插件源码<br><a href=\"https://github.com/pentaho/big-data-plugin\" target=\"_blank\" rel=\"noopener\">https://github.com/pentaho/big-data-plugin</a><br>kettle支持的大数据环境源码，主要是hdp,cdh。<br><a href=\"https://github.com/pentaho/pentaho-hadoop-shims\" target=\"_blank\" rel=\"noopener\">https://github.com/pentaho/pentaho-hadoop-shims</a><br>kettle nexus<br><a href=\"http://repo.pentaho.org/content/groups/omni/pentaho/\" target=\"_blank\" rel=\"noopener\">http://repo.pentaho.org/content/groups/omni/pentaho/</a><br><a href=\"http://repository.pentaho.org/artifactory/repo/\" target=\"_blank\" rel=\"noopener\">http://repository.pentaho.org/artifactory/repo/</a><br>所有组件实现说明<br><a href=\"http://wiki.pentaho.com/display/EAI/Pentaho+Data+Integration+Steps\" target=\"_blank\" rel=\"noopener\">http://wiki.pentaho.com/display/EAI/Pentaho+Data+Integration+Steps</a><br>所有组件测试说明<br><a href=\"http://wiki.pentaho.com/display/EAI/test\" target=\"_blank\" rel=\"noopener\">http://wiki.pentaho.com/display/EAI/test</a><br>帮助<br><a href=\"http://help.pentaho.com/Documentation\" target=\"_blank\" rel=\"noopener\">http://help.pentaho.com/Documentation</a> </p>\n","site":{"data":{}},"excerpt":"","more":"<p>#开源ETL工具-kettle<br>说明：本文部分内容参考网络的资料，如果侵权之处请告知一下，不胜感激！</p>\n<p>Kettle是Pentaho公司开发的一款ETL产品，以工作流为核心，强调面向解决方案而非工具的，基于java平台的商业智能(Business Intelligence,BI)套件。Kettle的开源协议是LGPL，该协议来自GNU，因功能强大，被FSF(Free Software Foundation)列为首选协议。LGPL协议允许Kettle作为商业（非开源）代码的链接库，使用Kettle的商业代码无须开源。LGPL带来的不仅是Kettle API，你还可以对它进行拓展对外提供商业软件或服务。</p>\n<h2 id=\"ETL是什么\"><a href=\"#ETL是什么\" class=\"headerlink\" title=\"##ETL是什么\"></a>##ETL是什么</h2><p>ETL早期作为数据仓库的关键环节，负责将分布的、异构数据源中的数据如关系数据、平面数据文件等抽取到临时中间层后进行清洗、转换、集成，最后加载到数据仓库（Data Warehouse）或数据集市（Data Mart）中，成为联机分析处理（On-Line Analytical Processing，OLAP）、数据挖掘（Data Mining）的基础。</p>\n<p>Exract：从多种异构数据源中抽取数据</p>\n<p>Transform：经过清洗，统一化和转换</p>\n<p>Load：将数据加载到目的数据源中</p>\n<h2 id=\"Kettle产品特点\"><a href=\"#Kettle产品特点\" class=\"headerlink\" title=\"##Kettle产品特点\"></a>##Kettle产品特点</h2><p>适用于将多个应用系统的大批量的、异构的数据进行整合，有强大的数据转换功能。<br>高效适配多种类型的异构数据库、文件和应用系统。<br>快速构建复杂数据大集中应用、无需编码。</p>\n<p>Kettle构成</p>\n<p>TODO以Github上的名词进行定义Spoon，Cart等</p>\n<p>左边是集成开发工具（Spoon），可以进行流程的开发、配置、调试、部署、执行(转换、任务)，也可以对运行情况进行监控、对处理过程的日志进行查看、也可以通过接口调用方式进行远程管理。</p>\n<p>中间是服务器(Carte)，包括实际执行转换和任务的ETL引擎、监控管理的接口、认证授权接口，还有一个可以拓展的接口。</p>\n<p>下面是在开发过程中，用于保存集成开发工具中创建的转换、任务、数据库等项的，资源库包含两类，一个是数据库资源库，一个是文件资源库。</p>\n<p>右边个是是第三方平台，可以基于kettle提供的接口实现相应的功能包括状态监控、启停控制、日志查看等功能。</p>\n<p>组成部分<br>名称<br>描述<br>Spoon<br>一个基于swt开发的流式处理客户端，用户开发转换、任务、创建数据库、集群、分区等<br>Pan<br>一个独立的命令行程序，支持通过命令行实现界面的功能，如果转换启停、任务启停。状态查看等<br>Kitchen<br>一个独立的命令行程序，用于执行由Spoon编辑的作业。<br>Carte<br>Carte是一个轻量级的Web容器，用于建立专用、远程的ETL Server。</p>\n<p>PDI相关术语和概念<br>Job(任务)、Transformation(转换)是kettle的两个最重要的概念。任务做的一件完整的事，包含开始、结束等整个生命周期；而转换是要做这件事的某一个小的功能。比如你要从A数据源中解析数据后放入B数据源，那么你可以创建两个转换，一个是从A数据源加载数据-&gt;处理数据-&gt;放入存储中；另一个是把数据放入B数据源，然后在一个任务中处理他们。</p>\n<p>下面我们通过集成开发工具去了解一个转换和任务</p>\n<p>Transformation（转换）<br>Transformation（转换)是由step(步骤)和hops(节点连接线)组成，一个转换，可以看成一段数据流，每一个步骤完成一项数据处理的工作，节点连接线用于数据的流动。</p>\n<p>转换可以单独运行完成某一项工作，文件的扩展名为.ktr</p>\n<p>Steps（步骤）<br>Steps（步骤）是转换的重要组件部分，在Spoon中步骤根据功能分为输入类、输出类、脚本类等，每一个步骤完成一种特定的功能，比如excel输出组件，用于把数据流输出为excel文件格式。参考如下：</p>\n<p>Hops（节点连接）<br>Hops（节点连接）是数据传输的通道，用于连接两个步骤，使数据从一个步骤传递到另一个步骤，支持分发、复制等方式。注意数据处理的顺序并不是按照节点连接箭头的顺序，因为第个步骤都是单独的线程。</p>\n<p>Jobs（工作）<br>Jobs（工作）是基于工作流模型的，顺序处理。把步骤、转换组织在一起完成一件完整的事情。<br>文件扩展名为.kjb</p>\n<p>下载使用<br>kettle下载 目前最新版7.0<br><a href=\"https://sourceforge.net/projects/pentaho/files/Data%20Integration/\" target=\"_blank\" rel=\"noopener\">https://sourceforge.net/projects/pentaho/files/Data%20Integration/</a></p>\n<p>下载解压后是一个如：pdi-ce-7.0.0.0-25的文件，目录内容如下</p>\n<p>windown下直接双拼Spoon.bat、linux下直接运行./spoon.sh即可。<br>注： could not find the main class:org.pentaho.commons.launcher.Launcher. Program will exit. 表示jdk版本错误 。7.0版本只支持jdk1.8，可以单独配置kettle的jdk，添加配置到系统中即可：<br>名称：PENTAHO_JAVA_HOME<br>值：C:\\Program Files\\Java\\jdk1.8.0_45<br>    mac系统下/Library/Java/JavaVirtualMachines/jdk1.8.0_111.jdk/Contents/Home/<br>启动后界面如下</p>\n<p>新建转换：快捷键Ctrl+N</p>\n<p>首先从核心对象区选择“生成记录组件，编辑：</p>\n<p>然后选择Excel输出组件到工作区，创建生成记录步骤到Excel输出步骤的连接线，编辑excel输出目录和字段</p>\n<p>最后生成如下，点击运行：</p>\n<p>运行后的结果是输出excel文件，并可以查看每个步骤的处理情况，读、写、输入、输出等</p>\n<p>其它参考链接<br>kettle源码下载，可以选择各个版本下载，自己编译。<br><a href=\"https://github.com/pentaho/pentaho-kettle\" target=\"_blank\" rel=\"noopener\">https://github.com/pentaho/pentaho-kettle</a><br>大数据插件源码<br><a href=\"https://github.com/pentaho/big-data-plugin\" target=\"_blank\" rel=\"noopener\">https://github.com/pentaho/big-data-plugin</a><br>kettle支持的大数据环境源码，主要是hdp,cdh。<br><a href=\"https://github.com/pentaho/pentaho-hadoop-shims\" target=\"_blank\" rel=\"noopener\">https://github.com/pentaho/pentaho-hadoop-shims</a><br>kettle nexus<br><a href=\"http://repo.pentaho.org/content/groups/omni/pentaho/\" target=\"_blank\" rel=\"noopener\">http://repo.pentaho.org/content/groups/omni/pentaho/</a><br><a href=\"http://repository.pentaho.org/artifactory/repo/\" target=\"_blank\" rel=\"noopener\">http://repository.pentaho.org/artifactory/repo/</a><br>所有组件实现说明<br><a href=\"http://wiki.pentaho.com/display/EAI/Pentaho+Data+Integration+Steps\" target=\"_blank\" rel=\"noopener\">http://wiki.pentaho.com/display/EAI/Pentaho+Data+Integration+Steps</a><br>所有组件测试说明<br><a href=\"http://wiki.pentaho.com/display/EAI/test\" target=\"_blank\" rel=\"noopener\">http://wiki.pentaho.com/display/EAI/test</a><br>帮助<br><a href=\"http://help.pentaho.com/Documentation\" target=\"_blank\" rel=\"noopener\">http://help.pentaho.com/Documentation</a> </p>\n"},{"title":"Apache Spark与Apache Hadoop的关系","date":"2017-04-16T15:43:49.000Z","_content":"数据采集\n本文主要是讲外部系统与Hadoop之间的数据传递，包括从外部系统采集数据导入到hadoop，以及从Hadoop中提取数据导入外部系统中。\n\n#数据采集考量\n虽然Hadoop提出了文件客户端，便于在Hadoop中和Hadoop外复制文件，但是大多数据 Hadoop应用需要从不同来源导入数据，而且对不同的导入频率也提出了要求，Hadoop常用的数据来源包括以下：\n1.传统数据管理系统，如果关系型数据库与主机\n2.日志、机器生成的数据，以及其他类型的事件数据。\n3.从现有的企业数据存储中输入的文件。\n\n将数据从不同的系统输入Hadoop时需要考虑很多因素。如下：\n1.数据采集的时效性与可访问性\n需要采集数据在采集频率方面有哪些要求？下游的处理要求数据多长时间准备完毕？\n\n2.增量更新\n如何添加新数据？需要将数据添加到现有数据库吗？需要重写现有数据吗？\n\n3.数据访问和处理\n数据会用于处理过程吗？如果会，数据会用于批处理任务吗？需要的数据是不是随机获取的？\n\n4.数据分区及数据分片\n数据采集后应该如何分区？需要将数据导入到多个目录系统（如HDFS与HBase）吗？\n\n5.数据存储格式\n数据存储的格式是哪一种？\n\n6.数据变换\n需要变换尚未落地的数据吗？\n\n下面简单列举一下这几点考量？\n\n##1.数据采集的时效性\n这里的时效性是指可进行数据采集的时间与Hadoop中工具可访问数据的时间之间的间隔。采集架构的时间分类会对存储媒介和采集方法造成很大的影响。一般来说数据采集构架，可以使用以下分类中的一个\na.大型批处理\n通常指15分钟到数据小时的任务，有时可能指时间跨度达到一天的任务\n\nb.小型批处理\n通常指大约2分钟发送一次任务，但是总的来说不会超过15分钟\n\nc.近实时决策支持\n指接受信息后“立即作出反应”，并在2秒到2分钟内发送数据\n\nd.近实时事件处理\n指在2秒内处理任务，速度可达到100毫秒\n\ne.实时\n这里指不超过100毫秒\n\n可以注意到随着实现时间到达实时，实现的复杂度和成本会大大增加。从批处理出发（比如使用简单文件传输）通常是个不错的选择。HDFS对时效性的要求比较宽松，所以可能更加适合成为主要存储位置。而一个简单文件传输或Sqoop任务则适合作为采集数据的工具。比如，执行hadoop fs -put命令将复制一个文件，并进行全面的校验，以确定正确地复制数据。\n使用hadoop fs -put命令与Sqoop时，你需要明白一点：HDFS上的数据存储格式可能并不适合数据的长期存储和处理。因此，在使用这些工具的时候，可能需要通过额外的批处理操作，以将数据存储为需要的格式。\n当用户需要从简单的批处理转向更高频率的更新时，就应该考虑Flume或kafka之类的工具了。这里时间要求不起过2分钟，所以Sqoop与文件转换器不适用。而且，因为要求时间不起过2分钟，所以存储层可能需要变成HBase或Solr，这样插入与读取操作会获得更细的粒度。当要求到实时水平时，我们首先需要考虑内存，然后是永久性存储。全世界所有的平行化处理都不会有助于将反应要求控制在500毫秒以内，只要硬盘驱动器保持处理操作的状态。基于这一点，我们开始进入流处理领域，采用Storm或Spark Streaming之类的工具。这里需要强调的是，这些工具应该真正用于大数据处理，而不是像Flume或Sqoop那样用于数据采集。\n\n##2.增量更新\n新的数据是要添加到已有数据集中，还是要修改已有数据集。如果仅要求添加数据，那么HDFS对于大部分实现都很适用。HDFS能够并行 化多个驱动器的I/O操作，所以读写性能很高。HDFS的缺点是无法添加或者随机写入创建后的文件。\n\n#数据采集的选择\n##1.文件传输\n将数据导入导出到Hadoop最简单的方法就是文件传输，就是hadoop fs -put与hadoop fs -get命令。这有时也是最快的方法，所以在设计Hadoop新的数据处理流水线时，首先应该考虑选择文件传输。\n\n下面列一下文件传输的特点：\na.这是一种all-or-nothing批处理方法，所以如果文件传输过程中出现错误，则不会写入或读取任何数据。这种方法与Flume、Kafka之类的采集方法不同，后者提供一定程度的错误处理功能，并且有传输保障。\nb.文件传输默认为单线程，不能并行 文件传输。\nc.文件传输将文件从传统的文件系统导入HDFS\nd.不支持数据转换，数据按原样导入HDFS。数据导入HDFS后才能进行处理，这一点与传输过程中的数据转换截然相反。类似于Flume的系统支持传输过程中的数据转换。\ne.这种加载是逐字进行的，所以能传输任何类型的文件（文件、二进制、图书等）\n\n##文件传输与其他采集方法的考量\n简单文件传输在某些情况下是适用的，尤其是在需要将已存在 的一系列文件输入到HDFS中，而且可以接受保持源文件格式的情况下。否则，在决定是否可以接受文件传输或者是否使用类似于Flume的工具时，需要考虑以下因素。\na.需要将数据采集到多个位置吗？比如，是需要将数据同时输入HDFS和Solr，还是需要将数据同时输入HDFS和HBase？这种情况下，如果使用文件传输，那么在文件采集完成之后将需要额外的工作，因些采用Flume更合适。\nb.对可靠性的要求高不高？如果高，那么一旦传输时出现错误，文件传输就必须重新开始，这时，Fluem同样是更好的选择。\ne.数据采集之前需要转换操作吗？如果需要Flume无疑是适合的工具。\n如果需要采集文件，可以考虑使用Flume Spooling  Directory源。采用这种方法，用户将文件放置到磁盘特定的目录就可以采集文件。这种采集文件的方法简单可靠，而且需要时能够实现传输过程中的数据转换。\n\n##Sqoop：Hadoop与关系数据库的批量传输\nSqoop是一种工具，能批量地将数据从关系型数据管理系统导出到Hadoop中，也能批量地将数据从Hadoop导出至关系型数据库中。\n\nFlume:基于事件的数据收集及处理\nFlume是一种分布式的可靠开源系统，用于流数据的高效收集、聚焦和移动。Flume通常用于移动日志数据，但是也能移动大量事件数据，如社交媒体订阅、消息队列事件或网络流量数据。\n\n##Kafka\nApache Kafka是一种发布订单消息的分布式系统，能够将消息归类为不同主题。应用程序能在Kafka上发布信息，或订阅主题进而接受特定主要下发布的消息。Producer发布消息，而Consumer收集并处理消息。作为分布式系统，Kafka在集群中运行，每个节点被称为Broker。\nKafka维护每个主题的分区日志。消息会发布到相应的主题中，每个分区都是一个有序的消息子集。同一个主题的多个分区能够通过集群中的多个Broker传送，这种方法提高了主题的容量与吞吐量，使其超越了单一机器所能提供的容量与吞吐量。消息在分区内被有序排列，每个消息都包含一个特定的偏移量。Kafka中消息可以通过一个包含主题、分区以及偏移量的组合来确定。Producer能够根据消息的主键选择消息应该写入哪一个分区，也能够简单地用循环的方式，让消息分布在各分区之间。\nConsumer会在Consumer组中注册，每个组包括一个或多个Consumer，每个Consumer读取一个或多个主题分区。每组中的每条消息只能传送给一个Consumer。但是，如果多个组订阅了同一个主题，那么每个组都将得到所有的消息。一个组中包含多个Consumer有助于获得加载平衡（可以支持高于单个Consumer处理能力的吞吐量）与高可用性（如果一个Consumer出现错误，它所读取的分区将重新分配给组中其它Consumer）。\n前面提到，对于应用层面的数据分类，主要单位是主题。一个Consumer或Consumer组将读取其订阅主题的所有数据，所以如果一个应用只关注一个数据子集，那么就应该将该数据子集与其他数据放在两个不同的主题中。如果多个信息集总是一起读取和处理，那么应该将它们归在同一个主题中。\n\n#数据导出\n数据导出的思路与导入类似。\n\n\n\n\n\n\n","source":"_posts/hadoop/Apache Spark与Apache Hadoop的关系.md","raw":"---\ntitle:  Apache Spark与Apache Hadoop的关系\ndate: 2017-04-16 23:43:49\ntags: [大数据,spark,hadoop]\ncategories: [大数据,hadoop]\n---\n数据采集\n本文主要是讲外部系统与Hadoop之间的数据传递，包括从外部系统采集数据导入到hadoop，以及从Hadoop中提取数据导入外部系统中。\n\n#数据采集考量\n虽然Hadoop提出了文件客户端，便于在Hadoop中和Hadoop外复制文件，但是大多数据 Hadoop应用需要从不同来源导入数据，而且对不同的导入频率也提出了要求，Hadoop常用的数据来源包括以下：\n1.传统数据管理系统，如果关系型数据库与主机\n2.日志、机器生成的数据，以及其他类型的事件数据。\n3.从现有的企业数据存储中输入的文件。\n\n将数据从不同的系统输入Hadoop时需要考虑很多因素。如下：\n1.数据采集的时效性与可访问性\n需要采集数据在采集频率方面有哪些要求？下游的处理要求数据多长时间准备完毕？\n\n2.增量更新\n如何添加新数据？需要将数据添加到现有数据库吗？需要重写现有数据吗？\n\n3.数据访问和处理\n数据会用于处理过程吗？如果会，数据会用于批处理任务吗？需要的数据是不是随机获取的？\n\n4.数据分区及数据分片\n数据采集后应该如何分区？需要将数据导入到多个目录系统（如HDFS与HBase）吗？\n\n5.数据存储格式\n数据存储的格式是哪一种？\n\n6.数据变换\n需要变换尚未落地的数据吗？\n\n下面简单列举一下这几点考量？\n\n##1.数据采集的时效性\n这里的时效性是指可进行数据采集的时间与Hadoop中工具可访问数据的时间之间的间隔。采集架构的时间分类会对存储媒介和采集方法造成很大的影响。一般来说数据采集构架，可以使用以下分类中的一个\na.大型批处理\n通常指15分钟到数据小时的任务，有时可能指时间跨度达到一天的任务\n\nb.小型批处理\n通常指大约2分钟发送一次任务，但是总的来说不会超过15分钟\n\nc.近实时决策支持\n指接受信息后“立即作出反应”，并在2秒到2分钟内发送数据\n\nd.近实时事件处理\n指在2秒内处理任务，速度可达到100毫秒\n\ne.实时\n这里指不超过100毫秒\n\n可以注意到随着实现时间到达实时，实现的复杂度和成本会大大增加。从批处理出发（比如使用简单文件传输）通常是个不错的选择。HDFS对时效性的要求比较宽松，所以可能更加适合成为主要存储位置。而一个简单文件传输或Sqoop任务则适合作为采集数据的工具。比如，执行hadoop fs -put命令将复制一个文件，并进行全面的校验，以确定正确地复制数据。\n使用hadoop fs -put命令与Sqoop时，你需要明白一点：HDFS上的数据存储格式可能并不适合数据的长期存储和处理。因此，在使用这些工具的时候，可能需要通过额外的批处理操作，以将数据存储为需要的格式。\n当用户需要从简单的批处理转向更高频率的更新时，就应该考虑Flume或kafka之类的工具了。这里时间要求不起过2分钟，所以Sqoop与文件转换器不适用。而且，因为要求时间不起过2分钟，所以存储层可能需要变成HBase或Solr，这样插入与读取操作会获得更细的粒度。当要求到实时水平时，我们首先需要考虑内存，然后是永久性存储。全世界所有的平行化处理都不会有助于将反应要求控制在500毫秒以内，只要硬盘驱动器保持处理操作的状态。基于这一点，我们开始进入流处理领域，采用Storm或Spark Streaming之类的工具。这里需要强调的是，这些工具应该真正用于大数据处理，而不是像Flume或Sqoop那样用于数据采集。\n\n##2.增量更新\n新的数据是要添加到已有数据集中，还是要修改已有数据集。如果仅要求添加数据，那么HDFS对于大部分实现都很适用。HDFS能够并行 化多个驱动器的I/O操作，所以读写性能很高。HDFS的缺点是无法添加或者随机写入创建后的文件。\n\n#数据采集的选择\n##1.文件传输\n将数据导入导出到Hadoop最简单的方法就是文件传输，就是hadoop fs -put与hadoop fs -get命令。这有时也是最快的方法，所以在设计Hadoop新的数据处理流水线时，首先应该考虑选择文件传输。\n\n下面列一下文件传输的特点：\na.这是一种all-or-nothing批处理方法，所以如果文件传输过程中出现错误，则不会写入或读取任何数据。这种方法与Flume、Kafka之类的采集方法不同，后者提供一定程度的错误处理功能，并且有传输保障。\nb.文件传输默认为单线程，不能并行 文件传输。\nc.文件传输将文件从传统的文件系统导入HDFS\nd.不支持数据转换，数据按原样导入HDFS。数据导入HDFS后才能进行处理，这一点与传输过程中的数据转换截然相反。类似于Flume的系统支持传输过程中的数据转换。\ne.这种加载是逐字进行的，所以能传输任何类型的文件（文件、二进制、图书等）\n\n##文件传输与其他采集方法的考量\n简单文件传输在某些情况下是适用的，尤其是在需要将已存在 的一系列文件输入到HDFS中，而且可以接受保持源文件格式的情况下。否则，在决定是否可以接受文件传输或者是否使用类似于Flume的工具时，需要考虑以下因素。\na.需要将数据采集到多个位置吗？比如，是需要将数据同时输入HDFS和Solr，还是需要将数据同时输入HDFS和HBase？这种情况下，如果使用文件传输，那么在文件采集完成之后将需要额外的工作，因些采用Flume更合适。\nb.对可靠性的要求高不高？如果高，那么一旦传输时出现错误，文件传输就必须重新开始，这时，Fluem同样是更好的选择。\ne.数据采集之前需要转换操作吗？如果需要Flume无疑是适合的工具。\n如果需要采集文件，可以考虑使用Flume Spooling  Directory源。采用这种方法，用户将文件放置到磁盘特定的目录就可以采集文件。这种采集文件的方法简单可靠，而且需要时能够实现传输过程中的数据转换。\n\n##Sqoop：Hadoop与关系数据库的批量传输\nSqoop是一种工具，能批量地将数据从关系型数据管理系统导出到Hadoop中，也能批量地将数据从Hadoop导出至关系型数据库中。\n\nFlume:基于事件的数据收集及处理\nFlume是一种分布式的可靠开源系统，用于流数据的高效收集、聚焦和移动。Flume通常用于移动日志数据，但是也能移动大量事件数据，如社交媒体订阅、消息队列事件或网络流量数据。\n\n##Kafka\nApache Kafka是一种发布订单消息的分布式系统，能够将消息归类为不同主题。应用程序能在Kafka上发布信息，或订阅主题进而接受特定主要下发布的消息。Producer发布消息，而Consumer收集并处理消息。作为分布式系统，Kafka在集群中运行，每个节点被称为Broker。\nKafka维护每个主题的分区日志。消息会发布到相应的主题中，每个分区都是一个有序的消息子集。同一个主题的多个分区能够通过集群中的多个Broker传送，这种方法提高了主题的容量与吞吐量，使其超越了单一机器所能提供的容量与吞吐量。消息在分区内被有序排列，每个消息都包含一个特定的偏移量。Kafka中消息可以通过一个包含主题、分区以及偏移量的组合来确定。Producer能够根据消息的主键选择消息应该写入哪一个分区，也能够简单地用循环的方式，让消息分布在各分区之间。\nConsumer会在Consumer组中注册，每个组包括一个或多个Consumer，每个Consumer读取一个或多个主题分区。每组中的每条消息只能传送给一个Consumer。但是，如果多个组订阅了同一个主题，那么每个组都将得到所有的消息。一个组中包含多个Consumer有助于获得加载平衡（可以支持高于单个Consumer处理能力的吞吐量）与高可用性（如果一个Consumer出现错误，它所读取的分区将重新分配给组中其它Consumer）。\n前面提到，对于应用层面的数据分类，主要单位是主题。一个Consumer或Consumer组将读取其订阅主题的所有数据，所以如果一个应用只关注一个数据子集，那么就应该将该数据子集与其他数据放在两个不同的主题中。如果多个信息集总是一起读取和处理，那么应该将它们归在同一个主题中。\n\n#数据导出\n数据导出的思路与导入类似。\n\n\n\n\n\n\n","slug":"hadoop/Apache Spark与Apache Hadoop的关系","published":1,"updated":"2017-08-18T01:44:35.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjh7polso0011bp7rgneerpd7","content":"<p>数据采集<br>本文主要是讲外部系统与Hadoop之间的数据传递，包括从外部系统采集数据导入到hadoop，以及从Hadoop中提取数据导入外部系统中。</p>\n<p>#数据采集考量<br>虽然Hadoop提出了文件客户端，便于在Hadoop中和Hadoop外复制文件，但是大多数据 Hadoop应用需要从不同来源导入数据，而且对不同的导入频率也提出了要求，Hadoop常用的数据来源包括以下：<br>1.传统数据管理系统，如果关系型数据库与主机<br>2.日志、机器生成的数据，以及其他类型的事件数据。<br>3.从现有的企业数据存储中输入的文件。</p>\n<p>将数据从不同的系统输入Hadoop时需要考虑很多因素。如下：<br>1.数据采集的时效性与可访问性<br>需要采集数据在采集频率方面有哪些要求？下游的处理要求数据多长时间准备完毕？</p>\n<p>2.增量更新<br>如何添加新数据？需要将数据添加到现有数据库吗？需要重写现有数据吗？</p>\n<p>3.数据访问和处理<br>数据会用于处理过程吗？如果会，数据会用于批处理任务吗？需要的数据是不是随机获取的？</p>\n<p>4.数据分区及数据分片<br>数据采集后应该如何分区？需要将数据导入到多个目录系统（如HDFS与HBase）吗？</p>\n<p>5.数据存储格式<br>数据存储的格式是哪一种？</p>\n<p>6.数据变换<br>需要变换尚未落地的数据吗？</p>\n<p>下面简单列举一下这几点考量？</p>\n<p>##1.数据采集的时效性<br>这里的时效性是指可进行数据采集的时间与Hadoop中工具可访问数据的时间之间的间隔。采集架构的时间分类会对存储媒介和采集方法造成很大的影响。一般来说数据采集构架，可以使用以下分类中的一个<br>a.大型批处理<br>通常指15分钟到数据小时的任务，有时可能指时间跨度达到一天的任务</p>\n<p>b.小型批处理<br>通常指大约2分钟发送一次任务，但是总的来说不会超过15分钟</p>\n<p>c.近实时决策支持<br>指接受信息后“立即作出反应”，并在2秒到2分钟内发送数据</p>\n<p>d.近实时事件处理<br>指在2秒内处理任务，速度可达到100毫秒</p>\n<p>e.实时<br>这里指不超过100毫秒</p>\n<p>可以注意到随着实现时间到达实时，实现的复杂度和成本会大大增加。从批处理出发（比如使用简单文件传输）通常是个不错的选择。HDFS对时效性的要求比较宽松，所以可能更加适合成为主要存储位置。而一个简单文件传输或Sqoop任务则适合作为采集数据的工具。比如，执行hadoop fs -put命令将复制一个文件，并进行全面的校验，以确定正确地复制数据。<br>使用hadoop fs -put命令与Sqoop时，你需要明白一点：HDFS上的数据存储格式可能并不适合数据的长期存储和处理。因此，在使用这些工具的时候，可能需要通过额外的批处理操作，以将数据存储为需要的格式。<br>当用户需要从简单的批处理转向更高频率的更新时，就应该考虑Flume或kafka之类的工具了。这里时间要求不起过2分钟，所以Sqoop与文件转换器不适用。而且，因为要求时间不起过2分钟，所以存储层可能需要变成HBase或Solr，这样插入与读取操作会获得更细的粒度。当要求到实时水平时，我们首先需要考虑内存，然后是永久性存储。全世界所有的平行化处理都不会有助于将反应要求控制在500毫秒以内，只要硬盘驱动器保持处理操作的状态。基于这一点，我们开始进入流处理领域，采用Storm或Spark Streaming之类的工具。这里需要强调的是，这些工具应该真正用于大数据处理，而不是像Flume或Sqoop那样用于数据采集。</p>\n<p>##2.增量更新<br>新的数据是要添加到已有数据集中，还是要修改已有数据集。如果仅要求添加数据，那么HDFS对于大部分实现都很适用。HDFS能够并行 化多个驱动器的I/O操作，所以读写性能很高。HDFS的缺点是无法添加或者随机写入创建后的文件。</p>\n<p>#数据采集的选择</p>\n<p>##1.文件传输<br>将数据导入导出到Hadoop最简单的方法就是文件传输，就是hadoop fs -put与hadoop fs -get命令。这有时也是最快的方法，所以在设计Hadoop新的数据处理流水线时，首先应该考虑选择文件传输。</p>\n<p>下面列一下文件传输的特点：<br>a.这是一种all-or-nothing批处理方法，所以如果文件传输过程中出现错误，则不会写入或读取任何数据。这种方法与Flume、Kafka之类的采集方法不同，后者提供一定程度的错误处理功能，并且有传输保障。<br>b.文件传输默认为单线程，不能并行 文件传输。<br>c.文件传输将文件从传统的文件系统导入HDFS<br>d.不支持数据转换，数据按原样导入HDFS。数据导入HDFS后才能进行处理，这一点与传输过程中的数据转换截然相反。类似于Flume的系统支持传输过程中的数据转换。<br>e.这种加载是逐字进行的，所以能传输任何类型的文件（文件、二进制、图书等）</p>\n<p>##文件传输与其他采集方法的考量<br>简单文件传输在某些情况下是适用的，尤其是在需要将已存在 的一系列文件输入到HDFS中，而且可以接受保持源文件格式的情况下。否则，在决定是否可以接受文件传输或者是否使用类似于Flume的工具时，需要考虑以下因素。<br>a.需要将数据采集到多个位置吗？比如，是需要将数据同时输入HDFS和Solr，还是需要将数据同时输入HDFS和HBase？这种情况下，如果使用文件传输，那么在文件采集完成之后将需要额外的工作，因些采用Flume更合适。<br>b.对可靠性的要求高不高？如果高，那么一旦传输时出现错误，文件传输就必须重新开始，这时，Fluem同样是更好的选择。<br>e.数据采集之前需要转换操作吗？如果需要Flume无疑是适合的工具。<br>如果需要采集文件，可以考虑使用Flume Spooling  Directory源。采用这种方法，用户将文件放置到磁盘特定的目录就可以采集文件。这种采集文件的方法简单可靠，而且需要时能够实现传输过程中的数据转换。</p>\n<p>##Sqoop：Hadoop与关系数据库的批量传输<br>Sqoop是一种工具，能批量地将数据从关系型数据管理系统导出到Hadoop中，也能批量地将数据从Hadoop导出至关系型数据库中。</p>\n<p>Flume:基于事件的数据收集及处理<br>Flume是一种分布式的可靠开源系统，用于流数据的高效收集、聚焦和移动。Flume通常用于移动日志数据，但是也能移动大量事件数据，如社交媒体订阅、消息队列事件或网络流量数据。</p>\n<p>##Kafka<br>Apache Kafka是一种发布订单消息的分布式系统，能够将消息归类为不同主题。应用程序能在Kafka上发布信息，或订阅主题进而接受特定主要下发布的消息。Producer发布消息，而Consumer收集并处理消息。作为分布式系统，Kafka在集群中运行，每个节点被称为Broker。<br>Kafka维护每个主题的分区日志。消息会发布到相应的主题中，每个分区都是一个有序的消息子集。同一个主题的多个分区能够通过集群中的多个Broker传送，这种方法提高了主题的容量与吞吐量，使其超越了单一机器所能提供的容量与吞吐量。消息在分区内被有序排列，每个消息都包含一个特定的偏移量。Kafka中消息可以通过一个包含主题、分区以及偏移量的组合来确定。Producer能够根据消息的主键选择消息应该写入哪一个分区，也能够简单地用循环的方式，让消息分布在各分区之间。<br>Consumer会在Consumer组中注册，每个组包括一个或多个Consumer，每个Consumer读取一个或多个主题分区。每组中的每条消息只能传送给一个Consumer。但是，如果多个组订阅了同一个主题，那么每个组都将得到所有的消息。一个组中包含多个Consumer有助于获得加载平衡（可以支持高于单个Consumer处理能力的吞吐量）与高可用性（如果一个Consumer出现错误，它所读取的分区将重新分配给组中其它Consumer）。<br>前面提到，对于应用层面的数据分类，主要单位是主题。一个Consumer或Consumer组将读取其订阅主题的所有数据，所以如果一个应用只关注一个数据子集，那么就应该将该数据子集与其他数据放在两个不同的主题中。如果多个信息集总是一起读取和处理，那么应该将它们归在同一个主题中。</p>\n<p>#数据导出<br>数据导出的思路与导入类似。</p>\n","site":{"data":{}},"excerpt":"","more":"<p>数据采集<br>本文主要是讲外部系统与Hadoop之间的数据传递，包括从外部系统采集数据导入到hadoop，以及从Hadoop中提取数据导入外部系统中。</p>\n<p>#数据采集考量<br>虽然Hadoop提出了文件客户端，便于在Hadoop中和Hadoop外复制文件，但是大多数据 Hadoop应用需要从不同来源导入数据，而且对不同的导入频率也提出了要求，Hadoop常用的数据来源包括以下：<br>1.传统数据管理系统，如果关系型数据库与主机<br>2.日志、机器生成的数据，以及其他类型的事件数据。<br>3.从现有的企业数据存储中输入的文件。</p>\n<p>将数据从不同的系统输入Hadoop时需要考虑很多因素。如下：<br>1.数据采集的时效性与可访问性<br>需要采集数据在采集频率方面有哪些要求？下游的处理要求数据多长时间准备完毕？</p>\n<p>2.增量更新<br>如何添加新数据？需要将数据添加到现有数据库吗？需要重写现有数据吗？</p>\n<p>3.数据访问和处理<br>数据会用于处理过程吗？如果会，数据会用于批处理任务吗？需要的数据是不是随机获取的？</p>\n<p>4.数据分区及数据分片<br>数据采集后应该如何分区？需要将数据导入到多个目录系统（如HDFS与HBase）吗？</p>\n<p>5.数据存储格式<br>数据存储的格式是哪一种？</p>\n<p>6.数据变换<br>需要变换尚未落地的数据吗？</p>\n<p>下面简单列举一下这几点考量？</p>\n<p>##1.数据采集的时效性<br>这里的时效性是指可进行数据采集的时间与Hadoop中工具可访问数据的时间之间的间隔。采集架构的时间分类会对存储媒介和采集方法造成很大的影响。一般来说数据采集构架，可以使用以下分类中的一个<br>a.大型批处理<br>通常指15分钟到数据小时的任务，有时可能指时间跨度达到一天的任务</p>\n<p>b.小型批处理<br>通常指大约2分钟发送一次任务，但是总的来说不会超过15分钟</p>\n<p>c.近实时决策支持<br>指接受信息后“立即作出反应”，并在2秒到2分钟内发送数据</p>\n<p>d.近实时事件处理<br>指在2秒内处理任务，速度可达到100毫秒</p>\n<p>e.实时<br>这里指不超过100毫秒</p>\n<p>可以注意到随着实现时间到达实时，实现的复杂度和成本会大大增加。从批处理出发（比如使用简单文件传输）通常是个不错的选择。HDFS对时效性的要求比较宽松，所以可能更加适合成为主要存储位置。而一个简单文件传输或Sqoop任务则适合作为采集数据的工具。比如，执行hadoop fs -put命令将复制一个文件，并进行全面的校验，以确定正确地复制数据。<br>使用hadoop fs -put命令与Sqoop时，你需要明白一点：HDFS上的数据存储格式可能并不适合数据的长期存储和处理。因此，在使用这些工具的时候，可能需要通过额外的批处理操作，以将数据存储为需要的格式。<br>当用户需要从简单的批处理转向更高频率的更新时，就应该考虑Flume或kafka之类的工具了。这里时间要求不起过2分钟，所以Sqoop与文件转换器不适用。而且，因为要求时间不起过2分钟，所以存储层可能需要变成HBase或Solr，这样插入与读取操作会获得更细的粒度。当要求到实时水平时，我们首先需要考虑内存，然后是永久性存储。全世界所有的平行化处理都不会有助于将反应要求控制在500毫秒以内，只要硬盘驱动器保持处理操作的状态。基于这一点，我们开始进入流处理领域，采用Storm或Spark Streaming之类的工具。这里需要强调的是，这些工具应该真正用于大数据处理，而不是像Flume或Sqoop那样用于数据采集。</p>\n<p>##2.增量更新<br>新的数据是要添加到已有数据集中，还是要修改已有数据集。如果仅要求添加数据，那么HDFS对于大部分实现都很适用。HDFS能够并行 化多个驱动器的I/O操作，所以读写性能很高。HDFS的缺点是无法添加或者随机写入创建后的文件。</p>\n<p>#数据采集的选择</p>\n<p>##1.文件传输<br>将数据导入导出到Hadoop最简单的方法就是文件传输，就是hadoop fs -put与hadoop fs -get命令。这有时也是最快的方法，所以在设计Hadoop新的数据处理流水线时，首先应该考虑选择文件传输。</p>\n<p>下面列一下文件传输的特点：<br>a.这是一种all-or-nothing批处理方法，所以如果文件传输过程中出现错误，则不会写入或读取任何数据。这种方法与Flume、Kafka之类的采集方法不同，后者提供一定程度的错误处理功能，并且有传输保障。<br>b.文件传输默认为单线程，不能并行 文件传输。<br>c.文件传输将文件从传统的文件系统导入HDFS<br>d.不支持数据转换，数据按原样导入HDFS。数据导入HDFS后才能进行处理，这一点与传输过程中的数据转换截然相反。类似于Flume的系统支持传输过程中的数据转换。<br>e.这种加载是逐字进行的，所以能传输任何类型的文件（文件、二进制、图书等）</p>\n<p>##文件传输与其他采集方法的考量<br>简单文件传输在某些情况下是适用的，尤其是在需要将已存在 的一系列文件输入到HDFS中，而且可以接受保持源文件格式的情况下。否则，在决定是否可以接受文件传输或者是否使用类似于Flume的工具时，需要考虑以下因素。<br>a.需要将数据采集到多个位置吗？比如，是需要将数据同时输入HDFS和Solr，还是需要将数据同时输入HDFS和HBase？这种情况下，如果使用文件传输，那么在文件采集完成之后将需要额外的工作，因些采用Flume更合适。<br>b.对可靠性的要求高不高？如果高，那么一旦传输时出现错误，文件传输就必须重新开始，这时，Fluem同样是更好的选择。<br>e.数据采集之前需要转换操作吗？如果需要Flume无疑是适合的工具。<br>如果需要采集文件，可以考虑使用Flume Spooling  Directory源。采用这种方法，用户将文件放置到磁盘特定的目录就可以采集文件。这种采集文件的方法简单可靠，而且需要时能够实现传输过程中的数据转换。</p>\n<p>##Sqoop：Hadoop与关系数据库的批量传输<br>Sqoop是一种工具，能批量地将数据从关系型数据管理系统导出到Hadoop中，也能批量地将数据从Hadoop导出至关系型数据库中。</p>\n<p>Flume:基于事件的数据收集及处理<br>Flume是一种分布式的可靠开源系统，用于流数据的高效收集、聚焦和移动。Flume通常用于移动日志数据，但是也能移动大量事件数据，如社交媒体订阅、消息队列事件或网络流量数据。</p>\n<p>##Kafka<br>Apache Kafka是一种发布订单消息的分布式系统，能够将消息归类为不同主题。应用程序能在Kafka上发布信息，或订阅主题进而接受特定主要下发布的消息。Producer发布消息，而Consumer收集并处理消息。作为分布式系统，Kafka在集群中运行，每个节点被称为Broker。<br>Kafka维护每个主题的分区日志。消息会发布到相应的主题中，每个分区都是一个有序的消息子集。同一个主题的多个分区能够通过集群中的多个Broker传送，这种方法提高了主题的容量与吞吐量，使其超越了单一机器所能提供的容量与吞吐量。消息在分区内被有序排列，每个消息都包含一个特定的偏移量。Kafka中消息可以通过一个包含主题、分区以及偏移量的组合来确定。Producer能够根据消息的主键选择消息应该写入哪一个分区，也能够简单地用循环的方式，让消息分布在各分区之间。<br>Consumer会在Consumer组中注册，每个组包括一个或多个Consumer，每个Consumer读取一个或多个主题分区。每组中的每条消息只能传送给一个Consumer。但是，如果多个组订阅了同一个主题，那么每个组都将得到所有的消息。一个组中包含多个Consumer有助于获得加载平衡（可以支持高于单个Consumer处理能力的吞吐量）与高可用性（如果一个Consumer出现错误，它所读取的分区将重新分配给组中其它Consumer）。<br>前面提到，对于应用层面的数据分类，主要单位是主题。一个Consumer或Consumer组将读取其订阅主题的所有数据，所以如果一个应用只关注一个数据子集，那么就应该将该数据子集与其他数据放在两个不同的主题中。如果多个信息集总是一起读取和处理，那么应该将它们归在同一个主题中。</p>\n<p>#数据导出<br>数据导出的思路与导入类似。</p>\n"},{"title":"HBase入门概念","date":"2017-05-02T02:30:00.000Z","_content":"#Hbase概念\nHBase是一个分布式的、面向列的开源数据库。  \n\n##Hbase术语\n**行键Row Key**：主键是用来检索记录的主键，访问hbasetable中的行。  \n**列族Column Family**：Table在水平方向有一个或者多个ColumnFamily组成，一个ColumnFamily中可以由任意多个Column组成，即ColumnFamily支持动态扩展，无需预先定义Column的数量以及类型，所有Column均以二进制格式存储，用户需要自行进行类型转换。  \n**列column**：由Hbase中的列族ColumnFamily + 列的名称（cell）组成列。  \n**单元格cell**：HBase中通过row和columns确定的为一个存贮单元称为cell。  \n**版本version**：每个 cell都保存着同一份数据的多个版本。版本通过时间戳来索引。  \n\n\n#Hbase安装\n三种方式：单机、伪分布式、分布式\n##单机模式\nHbase安装文件下载解压后，直接运行，在单机模式下HBase不使用HDFS。\n##伪分布式\n运行在单个节点上的分布式模式  \n##全分布式\n全分面式模式下的HBase集群需要ZooKeeper实例运行，并且需要所有的HBase节点都能够与ZooKeeper实例通信，默认情况下HBase自身维护着一组默认的ZooKeeper实例，不过用户也可以配置独立的ZooKeeper实例，这样能够使HBase更加健壮。\n\n#运行HBase\n##单机模式\nstart-hbase.sh\nstop-hbase.sh\n##伪分分面式\n由于伪分布式运行基于HDFS，因此在期待运行HBase之前首先需要启动HDFS。\nstart-dfs.sh\n然后start-hbase.sh\n##全分布式\n与伪分布式相同\n\n#Hbase Shell\nHbase Shell提供了HBae命令，可以方便创建、删除及修改表，还可以向表中添加数据、列出表中相关信息等。  \n在启动hbase之后，用户可以通过下面的命令进入Hbase Shell：  \nhbase shell   \n输入help获取帮助    \nalter:修改列族模式  \ncount:统计表中行的数量  \ncreate：创建表  \ndescribe:显示表相关的详细信息  \ndelete:删除指定对象的值（可以为表、行、列对应的值，另外也可以指定时间戳的值）  \ndeleteall:删除指定行的所有元素值  \ndisable:使表无效  \ndrop:删除表  \nenable:使表有效  \nexists：测试表是否存在  \nexit:退出Hbase Shell  \nget:获取行或单元(cell)的值  \nincr:增加指定表、行或列的值  \nlist:列出HBase中存在的所有表  \nput:向指定的表单元添加值  \ntools:列出HBase所支持的工具  \nscan：通过对表的扫描来获取对应的值  \nstatus:返回HBase集群的状态信息  \nshutdown:关闭HBase集群  \ntruncate:重新创建指定表  \nversion:返回HBase版本信息  \n下面介绍几个详细的：  \n（1）create  \n通过表名及用逗号做好事开的列族信息来创建表    \n1）hbase>create 't1',{NAME=>'f1',VERSIONS=>5}  \n2)hbase>create 't1',{NAME=>'f1'},{NAME=>'f2'},{NAME=>'f3'}  \nhbase>#上面的命令可以简写为下面所示的格式：  \nhbase>create 't1','f1','f2','f3'  \n3)hbase>create 't1',{NAME='f1',VERSIONS=>1,TTL=>2592000,BLOCKCACHE=>true}  \n以\"NAME=>'f1'举例说明，其中，列族参数的格式是箭头左侧为参数变量，右侧为参数对应的值，并用“=>”分开。  \n\n（2）list  \n列出HBase中包含的表名称  \nhbase>list   \n\n(3)put  \n向指定的HBase表单元添加值，例如向表t1的行r1、列c1:1添加值v1，并指定时间戳为ts的操作如下：  \nhbase>put 't1','r1','c1:1','value',ta1  \n\n(4)scan  \n获取指定表的相关信息，可以通过逗号分隔来指定扫描参数  \n例如：获取表test的所有值  \nhbase>scan 'test'  \n获取表test的c1列的所有值  \nhbase>scan 'test',{COLUMNS=>'c1'}   \n获取表test的c1列的前一行的所有值   \nhbase>scan 'test',{COLUMNS=>'c1',limit=>1}  \n\n(5)get  \n获取行或单元的值，此命令可以指定表名、行值、以及可选的列值和时间戳。   \n获取表test行r1的值  \nhbase>get 'test','r1'  \n获取表test行r1列c1:1的值  \nhbase>get 'test','r1'{COLUMN=>'c1:1'}  \n需要注意的是，COLUMN和COLUMNS是不同的，scan操作中的COLUMNS指定的是表的列族，get操作中的COLUMN指定的是特定的列，COLUMN的值实质上为“列族+：+列修饰符”。  \n另外，在shell中，常量不需要用引号引起来，但二进制的值需要用双引号引起来，而其他值则用单引号引起来。  \nHBase Shell的常量可以通过shell中输入“Object.constants”命令来查看。  \n\n\n\n\n","source":"_posts/hadoop/HBase入门概念.md","raw":"---\ntitle: HBase入门概念\ndate: 2017-05-02 10:30:00\ntags: [分布式,hbase]\ncategories: [分布式,hbase]\n---\n#Hbase概念\nHBase是一个分布式的、面向列的开源数据库。  \n\n##Hbase术语\n**行键Row Key**：主键是用来检索记录的主键，访问hbasetable中的行。  \n**列族Column Family**：Table在水平方向有一个或者多个ColumnFamily组成，一个ColumnFamily中可以由任意多个Column组成，即ColumnFamily支持动态扩展，无需预先定义Column的数量以及类型，所有Column均以二进制格式存储，用户需要自行进行类型转换。  \n**列column**：由Hbase中的列族ColumnFamily + 列的名称（cell）组成列。  \n**单元格cell**：HBase中通过row和columns确定的为一个存贮单元称为cell。  \n**版本version**：每个 cell都保存着同一份数据的多个版本。版本通过时间戳来索引。  \n\n\n#Hbase安装\n三种方式：单机、伪分布式、分布式\n##单机模式\nHbase安装文件下载解压后，直接运行，在单机模式下HBase不使用HDFS。\n##伪分布式\n运行在单个节点上的分布式模式  \n##全分布式\n全分面式模式下的HBase集群需要ZooKeeper实例运行，并且需要所有的HBase节点都能够与ZooKeeper实例通信，默认情况下HBase自身维护着一组默认的ZooKeeper实例，不过用户也可以配置独立的ZooKeeper实例，这样能够使HBase更加健壮。\n\n#运行HBase\n##单机模式\nstart-hbase.sh\nstop-hbase.sh\n##伪分分面式\n由于伪分布式运行基于HDFS，因此在期待运行HBase之前首先需要启动HDFS。\nstart-dfs.sh\n然后start-hbase.sh\n##全分布式\n与伪分布式相同\n\n#Hbase Shell\nHbase Shell提供了HBae命令，可以方便创建、删除及修改表，还可以向表中添加数据、列出表中相关信息等。  \n在启动hbase之后，用户可以通过下面的命令进入Hbase Shell：  \nhbase shell   \n输入help获取帮助    \nalter:修改列族模式  \ncount:统计表中行的数量  \ncreate：创建表  \ndescribe:显示表相关的详细信息  \ndelete:删除指定对象的值（可以为表、行、列对应的值，另外也可以指定时间戳的值）  \ndeleteall:删除指定行的所有元素值  \ndisable:使表无效  \ndrop:删除表  \nenable:使表有效  \nexists：测试表是否存在  \nexit:退出Hbase Shell  \nget:获取行或单元(cell)的值  \nincr:增加指定表、行或列的值  \nlist:列出HBase中存在的所有表  \nput:向指定的表单元添加值  \ntools:列出HBase所支持的工具  \nscan：通过对表的扫描来获取对应的值  \nstatus:返回HBase集群的状态信息  \nshutdown:关闭HBase集群  \ntruncate:重新创建指定表  \nversion:返回HBase版本信息  \n下面介绍几个详细的：  \n（1）create  \n通过表名及用逗号做好事开的列族信息来创建表    \n1）hbase>create 't1',{NAME=>'f1',VERSIONS=>5}  \n2)hbase>create 't1',{NAME=>'f1'},{NAME=>'f2'},{NAME=>'f3'}  \nhbase>#上面的命令可以简写为下面所示的格式：  \nhbase>create 't1','f1','f2','f3'  \n3)hbase>create 't1',{NAME='f1',VERSIONS=>1,TTL=>2592000,BLOCKCACHE=>true}  \n以\"NAME=>'f1'举例说明，其中，列族参数的格式是箭头左侧为参数变量，右侧为参数对应的值，并用“=>”分开。  \n\n（2）list  \n列出HBase中包含的表名称  \nhbase>list   \n\n(3)put  \n向指定的HBase表单元添加值，例如向表t1的行r1、列c1:1添加值v1，并指定时间戳为ts的操作如下：  \nhbase>put 't1','r1','c1:1','value',ta1  \n\n(4)scan  \n获取指定表的相关信息，可以通过逗号分隔来指定扫描参数  \n例如：获取表test的所有值  \nhbase>scan 'test'  \n获取表test的c1列的所有值  \nhbase>scan 'test',{COLUMNS=>'c1'}   \n获取表test的c1列的前一行的所有值   \nhbase>scan 'test',{COLUMNS=>'c1',limit=>1}  \n\n(5)get  \n获取行或单元的值，此命令可以指定表名、行值、以及可选的列值和时间戳。   \n获取表test行r1的值  \nhbase>get 'test','r1'  \n获取表test行r1列c1:1的值  \nhbase>get 'test','r1'{COLUMN=>'c1:1'}  \n需要注意的是，COLUMN和COLUMNS是不同的，scan操作中的COLUMNS指定的是表的列族，get操作中的COLUMN指定的是特定的列，COLUMN的值实质上为“列族+：+列修饰符”。  \n另外，在shell中，常量不需要用引号引起来，但二进制的值需要用双引号引起来，而其他值则用单引号引起来。  \nHBase Shell的常量可以通过shell中输入“Object.constants”命令来查看。  \n\n\n\n\n","slug":"hadoop/HBase入门概念","published":1,"updated":"2017-08-18T01:44:35.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjh7polsv0016bp7rpelqoglo","content":"<p>#Hbase概念<br>HBase是一个分布式的、面向列的开源数据库。  </p>\n<p>##Hbase术语<br><strong>行键Row Key</strong>：主键是用来检索记录的主键，访问hbasetable中的行。<br><strong>列族Column Family</strong>：Table在水平方向有一个或者多个ColumnFamily组成，一个ColumnFamily中可以由任意多个Column组成，即ColumnFamily支持动态扩展，无需预先定义Column的数量以及类型，所有Column均以二进制格式存储，用户需要自行进行类型转换。<br><strong>列column</strong>：由Hbase中的列族ColumnFamily + 列的名称（cell）组成列。<br><strong>单元格cell</strong>：HBase中通过row和columns确定的为一个存贮单元称为cell。<br><strong>版本version</strong>：每个 cell都保存着同一份数据的多个版本。版本通过时间戳来索引。  </p>\n<p>#Hbase安装<br>三种方式：单机、伪分布式、分布式</p>\n<p>##单机模式<br>Hbase安装文件下载解压后，直接运行，在单机模式下HBase不使用HDFS。</p>\n<p>##伪分布式<br>运行在单个节点上的分布式模式  </p>\n<p>##全分布式<br>全分面式模式下的HBase集群需要ZooKeeper实例运行，并且需要所有的HBase节点都能够与ZooKeeper实例通信，默认情况下HBase自身维护着一组默认的ZooKeeper实例，不过用户也可以配置独立的ZooKeeper实例，这样能够使HBase更加健壮。</p>\n<p>#运行HBase</p>\n<p>##单机模式<br>start-hbase.sh<br>stop-hbase.sh</p>\n<p>##伪分分面式<br>由于伪分布式运行基于HDFS，因此在期待运行HBase之前首先需要启动HDFS。<br>start-dfs.sh<br>然后start-hbase.sh</p>\n<p>##全分布式<br>与伪分布式相同</p>\n<p>#Hbase Shell<br>Hbase Shell提供了HBae命令，可以方便创建、删除及修改表，还可以向表中添加数据、列出表中相关信息等。<br>在启动hbase之后，用户可以通过下面的命令进入Hbase Shell：<br>hbase shell<br>输入help获取帮助<br>alter:修改列族模式<br>count:统计表中行的数量<br>create：创建表<br>describe:显示表相关的详细信息<br>delete:删除指定对象的值（可以为表、行、列对应的值，另外也可以指定时间戳的值）<br>deleteall:删除指定行的所有元素值<br>disable:使表无效<br>drop:删除表<br>enable:使表有效<br>exists：测试表是否存在<br>exit:退出Hbase Shell<br>get:获取行或单元(cell)的值<br>incr:增加指定表、行或列的值<br>list:列出HBase中存在的所有表<br>put:向指定的表单元添加值<br>tools:列出HBase所支持的工具<br>scan：通过对表的扫描来获取对应的值<br>status:返回HBase集群的状态信息<br>shutdown:关闭HBase集群<br>truncate:重新创建指定表<br>version:返回HBase版本信息<br>下面介绍几个详细的：<br>（1）create<br>通过表名及用逗号做好事开的列族信息来创建表<br>1）hbase&gt;create ‘t1’,{NAME=&gt;’f1’,VERSIONS=&gt;5}<br>2)hbase&gt;create ‘t1’,{NAME=&gt;’f1’},{NAME=&gt;’f2’},{NAME=&gt;’f3’}<br>hbase&gt;#上面的命令可以简写为下面所示的格式：<br>hbase&gt;create ‘t1’,’f1’,’f2’,’f3’<br>3)hbase&gt;create ‘t1’,{NAME=’f1’,VERSIONS=&gt;1,TTL=&gt;2592000,BLOCKCACHE=&gt;true}<br>以”NAME=&gt;’f1’举例说明，其中，列族参数的格式是箭头左侧为参数变量，右侧为参数对应的值，并用“=&gt;”分开。  </p>\n<p>（2）list<br>列出HBase中包含的表名称<br>hbase&gt;list   </p>\n<p>(3)put<br>向指定的HBase表单元添加值，例如向表t1的行r1、列c1:1添加值v1，并指定时间戳为ts的操作如下：<br>hbase&gt;put ‘t1’,’r1’,’c1:1’,’value’,ta1  </p>\n<p>(4)scan<br>获取指定表的相关信息，可以通过逗号分隔来指定扫描参数<br>例如：获取表test的所有值<br>hbase&gt;scan ‘test’<br>获取表test的c1列的所有值<br>hbase&gt;scan ‘test’,{COLUMNS=&gt;’c1’}<br>获取表test的c1列的前一行的所有值<br>hbase&gt;scan ‘test’,{COLUMNS=&gt;’c1’,limit=&gt;1}  </p>\n<p>(5)get<br>获取行或单元的值，此命令可以指定表名、行值、以及可选的列值和时间戳。<br>获取表test行r1的值<br>hbase&gt;get ‘test’,’r1’<br>获取表test行r1列c1:1的值<br>hbase&gt;get ‘test’,’r1’{COLUMN=&gt;’c1:1’}<br>需要注意的是，COLUMN和COLUMNS是不同的，scan操作中的COLUMNS指定的是表的列族，get操作中的COLUMN指定的是特定的列，COLUMN的值实质上为“列族+：+列修饰符”。<br>另外，在shell中，常量不需要用引号引起来，但二进制的值需要用双引号引起来，而其他值则用单引号引起来。<br>HBase Shell的常量可以通过shell中输入“Object.constants”命令来查看。  </p>\n","site":{"data":{}},"excerpt":"","more":"<p>#Hbase概念<br>HBase是一个分布式的、面向列的开源数据库。  </p>\n<p>##Hbase术语<br><strong>行键Row Key</strong>：主键是用来检索记录的主键，访问hbasetable中的行。<br><strong>列族Column Family</strong>：Table在水平方向有一个或者多个ColumnFamily组成，一个ColumnFamily中可以由任意多个Column组成，即ColumnFamily支持动态扩展，无需预先定义Column的数量以及类型，所有Column均以二进制格式存储，用户需要自行进行类型转换。<br><strong>列column</strong>：由Hbase中的列族ColumnFamily + 列的名称（cell）组成列。<br><strong>单元格cell</strong>：HBase中通过row和columns确定的为一个存贮单元称为cell。<br><strong>版本version</strong>：每个 cell都保存着同一份数据的多个版本。版本通过时间戳来索引。  </p>\n<p>#Hbase安装<br>三种方式：单机、伪分布式、分布式</p>\n<p>##单机模式<br>Hbase安装文件下载解压后，直接运行，在单机模式下HBase不使用HDFS。</p>\n<p>##伪分布式<br>运行在单个节点上的分布式模式  </p>\n<p>##全分布式<br>全分面式模式下的HBase集群需要ZooKeeper实例运行，并且需要所有的HBase节点都能够与ZooKeeper实例通信，默认情况下HBase自身维护着一组默认的ZooKeeper实例，不过用户也可以配置独立的ZooKeeper实例，这样能够使HBase更加健壮。</p>\n<p>#运行HBase</p>\n<p>##单机模式<br>start-hbase.sh<br>stop-hbase.sh</p>\n<p>##伪分分面式<br>由于伪分布式运行基于HDFS，因此在期待运行HBase之前首先需要启动HDFS。<br>start-dfs.sh<br>然后start-hbase.sh</p>\n<p>##全分布式<br>与伪分布式相同</p>\n<p>#Hbase Shell<br>Hbase Shell提供了HBae命令，可以方便创建、删除及修改表，还可以向表中添加数据、列出表中相关信息等。<br>在启动hbase之后，用户可以通过下面的命令进入Hbase Shell：<br>hbase shell<br>输入help获取帮助<br>alter:修改列族模式<br>count:统计表中行的数量<br>create：创建表<br>describe:显示表相关的详细信息<br>delete:删除指定对象的值（可以为表、行、列对应的值，另外也可以指定时间戳的值）<br>deleteall:删除指定行的所有元素值<br>disable:使表无效<br>drop:删除表<br>enable:使表有效<br>exists：测试表是否存在<br>exit:退出Hbase Shell<br>get:获取行或单元(cell)的值<br>incr:增加指定表、行或列的值<br>list:列出HBase中存在的所有表<br>put:向指定的表单元添加值<br>tools:列出HBase所支持的工具<br>scan：通过对表的扫描来获取对应的值<br>status:返回HBase集群的状态信息<br>shutdown:关闭HBase集群<br>truncate:重新创建指定表<br>version:返回HBase版本信息<br>下面介绍几个详细的：<br>（1）create<br>通过表名及用逗号做好事开的列族信息来创建表<br>1）hbase&gt;create ‘t1’,{NAME=&gt;’f1’,VERSIONS=&gt;5}<br>2)hbase&gt;create ‘t1’,{NAME=&gt;’f1’},{NAME=&gt;’f2’},{NAME=&gt;’f3’}<br>hbase&gt;#上面的命令可以简写为下面所示的格式：<br>hbase&gt;create ‘t1’,’f1’,’f2’,’f3’<br>3)hbase&gt;create ‘t1’,{NAME=’f1’,VERSIONS=&gt;1,TTL=&gt;2592000,BLOCKCACHE=&gt;true}<br>以”NAME=&gt;’f1’举例说明，其中，列族参数的格式是箭头左侧为参数变量，右侧为参数对应的值，并用“=&gt;”分开。  </p>\n<p>（2）list<br>列出HBase中包含的表名称<br>hbase&gt;list   </p>\n<p>(3)put<br>向指定的HBase表单元添加值，例如向表t1的行r1、列c1:1添加值v1，并指定时间戳为ts的操作如下：<br>hbase&gt;put ‘t1’,’r1’,’c1:1’,’value’,ta1  </p>\n<p>(4)scan<br>获取指定表的相关信息，可以通过逗号分隔来指定扫描参数<br>例如：获取表test的所有值<br>hbase&gt;scan ‘test’<br>获取表test的c1列的所有值<br>hbase&gt;scan ‘test’,{COLUMNS=&gt;’c1’}<br>获取表test的c1列的前一行的所有值<br>hbase&gt;scan ‘test’,{COLUMNS=&gt;’c1’,limit=&gt;1}  </p>\n<p>(5)get<br>获取行或单元的值，此命令可以指定表名、行值、以及可选的列值和时间戳。<br>获取表test行r1的值<br>hbase&gt;get ‘test’,’r1’<br>获取表test行r1列c1:1的值<br>hbase&gt;get ‘test’,’r1’{COLUMN=&gt;’c1:1’}<br>需要注意的是，COLUMN和COLUMNS是不同的，scan操作中的COLUMNS指定的是表的列族，get操作中的COLUMN指定的是特定的列，COLUMN的值实质上为“列族+：+列修饰符”。<br>另外，在shell中，常量不需要用引号引起来，但二进制的值需要用双引号引起来，而其他值则用单引号引起来。<br>HBase Shell的常量可以通过shell中输入“Object.constants”命令来查看。  </p>\n"},{"title":"HDFS入门概念","date":"2017-04-16T15:43:49.000Z","_content":"#数据块\n 每个磁盘都有默认的数据块大小，这是磁盘进行数据读/写的最小单位。构建于单个磁盘之上的文件系统通过磁盘块来管理该文件系统的块，该文件系统块的大小的可以是磁盘块的整数倍。文件系统块一般为几千字节，而磁盘块一般为512字节。这些信息--文件系统块大小---对于需要读/写文件的文件系统用户来说是透明的。\n\n\n\n\n","source":"_posts/hadoop/HDFS入门概念.md","raw":"---\ntitle: HDFS入门概念\ndate: 2017-04-16 23:43:49\ntags: [大数据,hadoop]\ncategories: [大数据,hdfs]\n---\n#数据块\n 每个磁盘都有默认的数据块大小，这是磁盘进行数据读/写的最小单位。构建于单个磁盘之上的文件系统通过磁盘块来管理该文件系统的块，该文件系统块的大小的可以是磁盘块的整数倍。文件系统块一般为几千字节，而磁盘块一般为512字节。这些信息--文件系统块大小---对于需要读/写文件的文件系统用户来说是透明的。\n\n\n\n\n","slug":"hadoop/HDFS入门概念","published":1,"updated":"2017-08-18T01:44:35.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjh7polsx0018bp7r1yp7smox","content":"<p>#数据块<br> 每个磁盘都有默认的数据块大小，这是磁盘进行数据读/写的最小单位。构建于单个磁盘之上的文件系统通过磁盘块来管理该文件系统的块，该文件系统块的大小的可以是磁盘块的整数倍。文件系统块一般为几千字节，而磁盘块一般为512字节。这些信息–文件系统块大小—对于需要读/写文件的文件系统用户来说是透明的。</p>\n","site":{"data":{}},"excerpt":"","more":"<p>#数据块<br> 每个磁盘都有默认的数据块大小，这是磁盘进行数据读/写的最小单位。构建于单个磁盘之上的文件系统通过磁盘块来管理该文件系统的块，该文件系统块的大小的可以是磁盘块的整数倍。文件系统块一般为几千字节，而磁盘块一般为512字节。这些信息–文件系统块大小—对于需要读/写文件的文件系统用户来说是透明的。</p>\n"},{"title":"分布式存储","date":"2017-04-16T15:43:49.000Z","_content":"\n\n##Bigtable\nBigtable是非关系型数据库，是一个稀疏的、分布式的、持久化存储的多维度排序map。Bigtable设计的目的是快速且可靠地处理PB级别的数据，并且能够部署到上千台机器上。\n\nBigtable是闭源的，Cloud Bigtable是Google提供的大数据存储云服务。业界相关的Bigtable模型的开源实现为Apache HBase。\n\n##HBase\nHBase是一个高可靠、高性能、面向列、可伸缩的分布式存储系统，利用HBase技术可以在廉价PC上搭建起大规模结构化存储集群。\nHBase是Google Bigtable的开源实现，类似于Google Bigtable利用GFS作为其文件存储系统，HBase利用Hadoop HDFS作为其文件存储系统；Google运行MapRecue来处理Bigtable中的海量数据，HBase同样利用Hadoop MapRecue来处理HBase中的海量数据；Google Bigtable利用Chubby作为协同服务，HBase利用Zookeeper作为对应。\n###特性：\n强一致性读写：HBase不是“Eventual Consistentcy（最终一致性）”数据存储，这让它很适合高速计数聚合类任务；\n自动分片（Automatic sharding）：HBase表通过region分布在集群中，数据增长时，region会自动分割并重新分布；\nRegionServer自动故障转移\nHadoop/HDFS集成：HBase支持开箱即用HDFS作为它的分布式文件系统；\nMapRecue：HBase通过MapRecue支持大并发处理；\nJava客户端API：HBase支持易于使用的Java API进行编程访问;\nThrift/REST API：HBase也支持Thrift和Rest作为非Java前端访问；\nBlock Cache和Bloom Filter：对于大容量查询优化，HBase支持Block Cache和Bloom Filter;\n运维管理：HBase支持JMX提供内置网页用于运维。\n###HBase应用场景\nHBase不适合所有场景。\n首先，确信有足够多数据，如果有上亿或上千亿行数据，HBase是很好的备选。如果只有上千或上百万行，则用传统的RDBMS可能是更好的选择。因为所有数据如果只需要在一两个节点进行存储，会导致集群其他节点闲置。\n其次，确信可以不依赖于RDBMS的额外特性。例如，列数据类型、第二索引、事务、高级查询语言等\n最后，确保有足够的硬件。因为HDFS在小于5个数据节点时，基本上体现不出来它的优势。\n虽然HBase能在单独的笔记本上运行良好，但这应仅当成是开发阶段的配置 。\n###HBase的优点\n列可以动态增加，并且列为空就不存储数据，节省存储空间；\nHBase可以自动切分数据，使得数据存储自动具有水平扩展功能；\nHBase可以提供高并发读写操作的支持；\n与Hadoop MapRecue相结合有利于数据分析；\n容错性；\n版权免费；\n非常灵活的模式设计（或者说没有固定模式的限制）；\n可以跟Hive集成，使用类SQL查询；\n自动故障转移；\n客户端接口易于使用；\n行级别原子性，即PUT操作一定是完全成功或者完全失败。\n###HBase的缺点\n不能支持条件查询，只支持按照row key来查询；\n容易产生单点故障（在只使用一个HMaster的时候）；\n不支持事务；\nJOIN不是数据库层支持的，而需要用MapRecue；\n只能在主键上索引和排序；\n没有内置的身份和权限认证；\n###HBase与Hadoop/HDFS的差异\nHDFS是分布式文件系统，适合保存大文件。官方宣称它并非普通用途的文件系统，不提供文件的个别记录的快速查询。另一方面，HBase基于HDFS，并能够提供大表的记录快速查询和更新。HBase内部将数据放到索引好的“StoreFiles”存储文件中，以便提供高速查询，而存储文件位于HDFS中。\n\n\n##Cassandra\nCassandra是Facebook于2008年7月在Google Code上开源的项目。Cassandra实现了Dynamo风格的副本复制模型和没有单点失效的架构，增加了更加强大的column family数据模型。\n\n\n##Memcached\nMemcached可以更好利用内存\n\n\n##Redis\nRedis是一个key-value模型的内在数据存储系统。\n\n\n##MongoDB\nMongoDB是一个介于关系型数据库和非关系性数据库之间的产品，是非关系型 数据库中功能最丰富、最像关系型 数据库的，旨在为Web应用提供可扩展的高性能数据存储解决方案。\n\n\n\n\n\n\n\n\n\n\n\n\n\n","source":"_posts/hadoop/Hadoop之分布式存储.md","raw":"---\ntitle: 分布式存储 \ndate: 2017-04-16 23:43:49\ntags: [分布式,分布式存储]\ncategories: [分布式,分布式存储]\n---\n\n\n##Bigtable\nBigtable是非关系型数据库，是一个稀疏的、分布式的、持久化存储的多维度排序map。Bigtable设计的目的是快速且可靠地处理PB级别的数据，并且能够部署到上千台机器上。\n\nBigtable是闭源的，Cloud Bigtable是Google提供的大数据存储云服务。业界相关的Bigtable模型的开源实现为Apache HBase。\n\n##HBase\nHBase是一个高可靠、高性能、面向列、可伸缩的分布式存储系统，利用HBase技术可以在廉价PC上搭建起大规模结构化存储集群。\nHBase是Google Bigtable的开源实现，类似于Google Bigtable利用GFS作为其文件存储系统，HBase利用Hadoop HDFS作为其文件存储系统；Google运行MapRecue来处理Bigtable中的海量数据，HBase同样利用Hadoop MapRecue来处理HBase中的海量数据；Google Bigtable利用Chubby作为协同服务，HBase利用Zookeeper作为对应。\n###特性：\n强一致性读写：HBase不是“Eventual Consistentcy（最终一致性）”数据存储，这让它很适合高速计数聚合类任务；\n自动分片（Automatic sharding）：HBase表通过region分布在集群中，数据增长时，region会自动分割并重新分布；\nRegionServer自动故障转移\nHadoop/HDFS集成：HBase支持开箱即用HDFS作为它的分布式文件系统；\nMapRecue：HBase通过MapRecue支持大并发处理；\nJava客户端API：HBase支持易于使用的Java API进行编程访问;\nThrift/REST API：HBase也支持Thrift和Rest作为非Java前端访问；\nBlock Cache和Bloom Filter：对于大容量查询优化，HBase支持Block Cache和Bloom Filter;\n运维管理：HBase支持JMX提供内置网页用于运维。\n###HBase应用场景\nHBase不适合所有场景。\n首先，确信有足够多数据，如果有上亿或上千亿行数据，HBase是很好的备选。如果只有上千或上百万行，则用传统的RDBMS可能是更好的选择。因为所有数据如果只需要在一两个节点进行存储，会导致集群其他节点闲置。\n其次，确信可以不依赖于RDBMS的额外特性。例如，列数据类型、第二索引、事务、高级查询语言等\n最后，确保有足够的硬件。因为HDFS在小于5个数据节点时，基本上体现不出来它的优势。\n虽然HBase能在单独的笔记本上运行良好，但这应仅当成是开发阶段的配置 。\n###HBase的优点\n列可以动态增加，并且列为空就不存储数据，节省存储空间；\nHBase可以自动切分数据，使得数据存储自动具有水平扩展功能；\nHBase可以提供高并发读写操作的支持；\n与Hadoop MapRecue相结合有利于数据分析；\n容错性；\n版权免费；\n非常灵活的模式设计（或者说没有固定模式的限制）；\n可以跟Hive集成，使用类SQL查询；\n自动故障转移；\n客户端接口易于使用；\n行级别原子性，即PUT操作一定是完全成功或者完全失败。\n###HBase的缺点\n不能支持条件查询，只支持按照row key来查询；\n容易产生单点故障（在只使用一个HMaster的时候）；\n不支持事务；\nJOIN不是数据库层支持的，而需要用MapRecue；\n只能在主键上索引和排序；\n没有内置的身份和权限认证；\n###HBase与Hadoop/HDFS的差异\nHDFS是分布式文件系统，适合保存大文件。官方宣称它并非普通用途的文件系统，不提供文件的个别记录的快速查询。另一方面，HBase基于HDFS，并能够提供大表的记录快速查询和更新。HBase内部将数据放到索引好的“StoreFiles”存储文件中，以便提供高速查询，而存储文件位于HDFS中。\n\n\n##Cassandra\nCassandra是Facebook于2008年7月在Google Code上开源的项目。Cassandra实现了Dynamo风格的副本复制模型和没有单点失效的架构，增加了更加强大的column family数据模型。\n\n\n##Memcached\nMemcached可以更好利用内存\n\n\n##Redis\nRedis是一个key-value模型的内在数据存储系统。\n\n\n##MongoDB\nMongoDB是一个介于关系型数据库和非关系性数据库之间的产品，是非关系型 数据库中功能最丰富、最像关系型 数据库的，旨在为Web应用提供可扩展的高性能数据存储解决方案。\n\n\n\n\n\n\n\n\n\n\n\n\n\n","slug":"hadoop/Hadoop之分布式存储","published":1,"updated":"2017-08-18T01:44:35.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjh7polsz001cbp7rw4ewou1m","content":"<p>##Bigtable<br>Bigtable是非关系型数据库，是一个稀疏的、分布式的、持久化存储的多维度排序map。Bigtable设计的目的是快速且可靠地处理PB级别的数据，并且能够部署到上千台机器上。</p>\n<p>Bigtable是闭源的，Cloud Bigtable是Google提供的大数据存储云服务。业界相关的Bigtable模型的开源实现为Apache HBase。</p>\n<p>##HBase<br>HBase是一个高可靠、高性能、面向列、可伸缩的分布式存储系统，利用HBase技术可以在廉价PC上搭建起大规模结构化存储集群。<br>HBase是Google Bigtable的开源实现，类似于Google Bigtable利用GFS作为其文件存储系统，HBase利用Hadoop HDFS作为其文件存储系统；Google运行MapRecue来处理Bigtable中的海量数据，HBase同样利用Hadoop MapRecue来处理HBase中的海量数据；Google Bigtable利用Chubby作为协同服务，HBase利用Zookeeper作为对应。</p>\n<p>###特性：<br>强一致性读写：HBase不是“Eventual Consistentcy（最终一致性）”数据存储，这让它很适合高速计数聚合类任务；<br>自动分片（Automatic sharding）：HBase表通过region分布在集群中，数据增长时，region会自动分割并重新分布；<br>RegionServer自动故障转移<br>Hadoop/HDFS集成：HBase支持开箱即用HDFS作为它的分布式文件系统；<br>MapRecue：HBase通过MapRecue支持大并发处理；<br>Java客户端API：HBase支持易于使用的Java API进行编程访问;<br>Thrift/REST API：HBase也支持Thrift和Rest作为非Java前端访问；<br>Block Cache和Bloom Filter：对于大容量查询优化，HBase支持Block Cache和Bloom Filter;<br>运维管理：HBase支持JMX提供内置网页用于运维。</p>\n<p>###HBase应用场景<br>HBase不适合所有场景。<br>首先，确信有足够多数据，如果有上亿或上千亿行数据，HBase是很好的备选。如果只有上千或上百万行，则用传统的RDBMS可能是更好的选择。因为所有数据如果只需要在一两个节点进行存储，会导致集群其他节点闲置。<br>其次，确信可以不依赖于RDBMS的额外特性。例如，列数据类型、第二索引、事务、高级查询语言等<br>最后，确保有足够的硬件。因为HDFS在小于5个数据节点时，基本上体现不出来它的优势。<br>虽然HBase能在单独的笔记本上运行良好，但这应仅当成是开发阶段的配置 。</p>\n<p>###HBase的优点<br>列可以动态增加，并且列为空就不存储数据，节省存储空间；<br>HBase可以自动切分数据，使得数据存储自动具有水平扩展功能；<br>HBase可以提供高并发读写操作的支持；<br>与Hadoop MapRecue相结合有利于数据分析；<br>容错性；<br>版权免费；<br>非常灵活的模式设计（或者说没有固定模式的限制）；<br>可以跟Hive集成，使用类SQL查询；<br>自动故障转移；<br>客户端接口易于使用；<br>行级别原子性，即PUT操作一定是完全成功或者完全失败。</p>\n<p>###HBase的缺点<br>不能支持条件查询，只支持按照row key来查询；<br>容易产生单点故障（在只使用一个HMaster的时候）；<br>不支持事务；<br>JOIN不是数据库层支持的，而需要用MapRecue；<br>只能在主键上索引和排序；<br>没有内置的身份和权限认证；</p>\n<p>###HBase与Hadoop/HDFS的差异<br>HDFS是分布式文件系统，适合保存大文件。官方宣称它并非普通用途的文件系统，不提供文件的个别记录的快速查询。另一方面，HBase基于HDFS，并能够提供大表的记录快速查询和更新。HBase内部将数据放到索引好的“StoreFiles”存储文件中，以便提供高速查询，而存储文件位于HDFS中。</p>\n<p>##Cassandra<br>Cassandra是Facebook于2008年7月在Google Code上开源的项目。Cassandra实现了Dynamo风格的副本复制模型和没有单点失效的架构，增加了更加强大的column family数据模型。</p>\n<p>##Memcached<br>Memcached可以更好利用内存</p>\n<p>##Redis<br>Redis是一个key-value模型的内在数据存储系统。</p>\n<p>##MongoDB<br>MongoDB是一个介于关系型数据库和非关系性数据库之间的产品，是非关系型 数据库中功能最丰富、最像关系型 数据库的，旨在为Web应用提供可扩展的高性能数据存储解决方案。</p>\n","site":{"data":{}},"excerpt":"","more":"<p>##Bigtable<br>Bigtable是非关系型数据库，是一个稀疏的、分布式的、持久化存储的多维度排序map。Bigtable设计的目的是快速且可靠地处理PB级别的数据，并且能够部署到上千台机器上。</p>\n<p>Bigtable是闭源的，Cloud Bigtable是Google提供的大数据存储云服务。业界相关的Bigtable模型的开源实现为Apache HBase。</p>\n<p>##HBase<br>HBase是一个高可靠、高性能、面向列、可伸缩的分布式存储系统，利用HBase技术可以在廉价PC上搭建起大规模结构化存储集群。<br>HBase是Google Bigtable的开源实现，类似于Google Bigtable利用GFS作为其文件存储系统，HBase利用Hadoop HDFS作为其文件存储系统；Google运行MapRecue来处理Bigtable中的海量数据，HBase同样利用Hadoop MapRecue来处理HBase中的海量数据；Google Bigtable利用Chubby作为协同服务，HBase利用Zookeeper作为对应。</p>\n<p>###特性：<br>强一致性读写：HBase不是“Eventual Consistentcy（最终一致性）”数据存储，这让它很适合高速计数聚合类任务；<br>自动分片（Automatic sharding）：HBase表通过region分布在集群中，数据增长时，region会自动分割并重新分布；<br>RegionServer自动故障转移<br>Hadoop/HDFS集成：HBase支持开箱即用HDFS作为它的分布式文件系统；<br>MapRecue：HBase通过MapRecue支持大并发处理；<br>Java客户端API：HBase支持易于使用的Java API进行编程访问;<br>Thrift/REST API：HBase也支持Thrift和Rest作为非Java前端访问；<br>Block Cache和Bloom Filter：对于大容量查询优化，HBase支持Block Cache和Bloom Filter;<br>运维管理：HBase支持JMX提供内置网页用于运维。</p>\n<p>###HBase应用场景<br>HBase不适合所有场景。<br>首先，确信有足够多数据，如果有上亿或上千亿行数据，HBase是很好的备选。如果只有上千或上百万行，则用传统的RDBMS可能是更好的选择。因为所有数据如果只需要在一两个节点进行存储，会导致集群其他节点闲置。<br>其次，确信可以不依赖于RDBMS的额外特性。例如，列数据类型、第二索引、事务、高级查询语言等<br>最后，确保有足够的硬件。因为HDFS在小于5个数据节点时，基本上体现不出来它的优势。<br>虽然HBase能在单独的笔记本上运行良好，但这应仅当成是开发阶段的配置 。</p>\n<p>###HBase的优点<br>列可以动态增加，并且列为空就不存储数据，节省存储空间；<br>HBase可以自动切分数据，使得数据存储自动具有水平扩展功能；<br>HBase可以提供高并发读写操作的支持；<br>与Hadoop MapRecue相结合有利于数据分析；<br>容错性；<br>版权免费；<br>非常灵活的模式设计（或者说没有固定模式的限制）；<br>可以跟Hive集成，使用类SQL查询；<br>自动故障转移；<br>客户端接口易于使用；<br>行级别原子性，即PUT操作一定是完全成功或者完全失败。</p>\n<p>###HBase的缺点<br>不能支持条件查询，只支持按照row key来查询；<br>容易产生单点故障（在只使用一个HMaster的时候）；<br>不支持事务；<br>JOIN不是数据库层支持的，而需要用MapRecue；<br>只能在主键上索引和排序；<br>没有内置的身份和权限认证；</p>\n<p>###HBase与Hadoop/HDFS的差异<br>HDFS是分布式文件系统，适合保存大文件。官方宣称它并非普通用途的文件系统，不提供文件的个别记录的快速查询。另一方面，HBase基于HDFS，并能够提供大表的记录快速查询和更新。HBase内部将数据放到索引好的“StoreFiles”存储文件中，以便提供高速查询，而存储文件位于HDFS中。</p>\n<p>##Cassandra<br>Cassandra是Facebook于2008年7月在Google Code上开源的项目。Cassandra实现了Dynamo风格的副本复制模型和没有单点失效的架构，增加了更加强大的column family数据模型。</p>\n<p>##Memcached<br>Memcached可以更好利用内存</p>\n<p>##Redis<br>Redis是一个key-value模型的内在数据存储系统。</p>\n<p>##MongoDB<br>MongoDB是一个介于关系型数据库和非关系性数据库之间的产品，是非关系型 数据库中功能最丰富、最像关系型 数据库的，旨在为Web应用提供可扩展的高性能数据存储解决方案。</p>\n"},{"title":"分布式计算","date":"2017-04-16T15:43:49.000Z","_content":"#MapRecue\n在过去的20年里，互联网产生了大量的数据，比如爬虫文档、Web讲求日志等；也包括了计算各种类型的派生数据，比如，倒排索引、Web文档的图结构的各种表示、每台主机页面数量概要、每天被请求数量最多的集合，等等。很多这样的计算在概念上很容易理解的。然而，当输入的数据量很大时，这些计算必须要被分割到成百上千的机器上才有可能在可以接受的时间内完成。怎样来实现并行计算？如何分发数据？如何进行错误处理？所有这些问题综合在一起，使得原来很简洁的计算，因为要大量的复杂代码来处理这些问题，而变得让人难以处理。 Google公司为了应对大数据的处理，内部已经实现了数据以百计的为专门目的而写的计算程序，其中MapRecue就是其著名的计算框架之王，与GFS、Bigtable一起被称为Google技术的“三宝”。 \n\n##MapRecue简介 \nMapRecue是一个编程模型，用于大规模数据集（TB级）的并行运算。有关MapRecue的论文介绍，最早可以追溯到由Google的Jeffrey Dean和Sanjay Ghemawat发表在2004年OSDI（USENIX Symposium on Operationg Systems Design and Implementation）的《MapRecue：Simplified Data Processing on LargeClusters》。这篇文章描述了Google如何分割、处理、整合他们令人难以置信的大数据集。读者有兴趣可以在线阅读该论文https://www.usenix.org/legacy/events/osdi04/tech/full_papers/dean/dean.pdf。 随后，开源软件先驱Doug Cutting等人受到该论文的启发，开始尝试实现MapRecue计算框架，并将它与NDFS（Nutch Distributed File System）结合，用以支持Nutch引擎的主要算法。由于NDFS与MapRecue在Nutch引擎中有着良好的应用，所以它们于2006年2月被分离出来，成为一套完整而独立的软件，并命名为Hadoop。 MapRecue程序模型应用成功要归功于以下几个方面。首先，由于该模型隐藏了并行、容错、本地优化以及负载平衡的细节，所以即便是那些没有并行和分布式系统经验的程序员也易于使用该模型。其次MapRecue计算可以很容易地表达大数据的各种问题。比如，MapRecue用于为Google的网页搜索服务生成数据，用于排序，用于数据挖掘，用于机器学习以及其他许多系统。再次，MapRecue的实现符合“由数千机器组成的大集群”的尺度，有效地利用了机器资源，所以非常适合解决大型计算问题。 \n\n##MapRecue的编程模型\nMapRecue是一个用于大规模数据集（TB级）并行运算的编程模型，其基本原理就是将大的数据分成小块逐个分析，最后再将提取出来的数据汇总分析，最终获得我们想要的内容。从名字可以看出，“Map(映射)”和“Reduce（归纳）”是MapRecue模型的核心，其灵感来源于函数式语言（比如Lisp）中的内置函数map和reduce：用户通过定义一个Map函数，处理key/value（键值对）以生成一个中间key/value集合，MapRecue库将所有拥有相同的key(key I)的中间状态key合并起来传递到Redure职数；一个叫作Reduce的函数用以合并所有先前Map过后的有相同key（Key I）的中间量。map(k1,v1) -> list<k2,v2)reduce(k2,list(v2)) -> list(k3,v3)但上面的定义显然还是过于抽象。现实世界中的许多任务在这个模型中得到了很好的表达。Shekhar Gulati就在他的博客里《How I explained MapReduce to my Wife?》举了一个辣椒酱制作过程的例子，来形象地描述MapRecue的原理，如下所述。1.MapRecue制作辣椒酱的过程辣椒酱制作的过程是这样的，先取一个洋葱，把它切碎，然后拌入盐和水，最后放进混合研磨机研磨。这样就能得到洋葱辣椒酱了。 现在，假设你想用薄荷、洋葱、番茄、辣椒、大蒜弄一瓶混合辣椒酱。你会怎么做呢？你会取薄荷叶一撮，洋葱一个，番茄一个，辣椒一根，大蒜一根，切碎后加入适量的盐和水，再放入混合研磨机里研磨，这样你就可以得到一瓶混合辣椒酱了。 现在把MapRecue的概念应用到食谱上，Map和Reduce其实就是两种操作。 Map：把洋葱、番茄、辣椒和大蒜切磋，是各自作用在这些物体上的一个Map操作。所以你给Map一个洋葱，Map就会把洋葱切碎。同样地，你把辣椒、大蒜和番茄一一地拿给Map，你也会得到各种碎块。所以，当你在切像洋葱这样的蔬菜时，你执行的就是一个Map操作。Map操作适用于每一种蔬菜，它会相应地生产出一种或多种碎块，在我们的例子中生产的是蔬菜块。在Map操作中可能会出现有个洋葱坏掉了的情况，你只要把洋葱丢了就行了。所以，如果出现坏洋葱了，Map操作就会过滤掉这个坏洋葱而不会生产出任何的坏洋葱块。 Reduce：在这一阶段，你将各种蔬菜都放入研磨机时在进行研磨，你就可以得到一瓶辣椒酱了。这意味要制成一瓶辣椒酱，你得研磨所有的原料。因此，研磨机通常将Map操作的蔬菜聚焦在了一起。 当然上面内容只是MapRecue的一部分，MapRecue的强大在于分布式计算。假设你每天需要生产10000瓶辣椒酱，你会怎么办？这个时候你就不得不雇佣更多的人和研磨机来完成这项工作了，你需要几个人一起切蔬菜。每个人都要处理满满一袋子的蔬菜，而每一个人都相当于在执行一个简单的Map操作。每一个人都将不断地从袋子里拿出蔬菜来，并且每次只对一种蔬菜进行处理，也就是将它们切碎，直到袋子空了为止。这样，当所有的工人都切完以后，工作台（每个人工作的地方）上就有了洋葱块、番茄块和蒜蓉，等等。 MapRecue将所有输出的蔬菜都搅拌在了一起，这些蔬菜都在以key为基础的Map操作下产生的。搅拌将自动完成，你可以假设key是一种原料的名字，你像洋葱一样。所以全部的洋葱key都搅拌在一起，并转移到研磨洋葱的研磨器里。这样，你就能得到洋葱辣椒酱了。同样地，所有的番茄也会被转移地标记着番茄的研磨器里，并制造出番茄辣椒酱。 \n\n\n\n#Apache Hadoop\nApache Hadoop是一个由Apache基金会开发的分布式系统基础架构，它可以让用户在不了解分布式底层细节的情况下，开发出可靠、可扩展的分布式计算应用。\nApache Hadoop框架允许用户使用简单的编程模型来实现计算机集群的大型数据集的分布式处理。它的目的是支持从单一服务器到上千台机器的扩展，充分利用了每台机器所提供本地计算和存储，而不是依靠硬件来提高高可用性。其本身被设计成在应用层检测和处理故障的库，对于计算机集群来说，其中每台机器的顶层都被设计成可以容错的，以便提供一个高可用的服务。\nApache Hadoop的框架最核心的设计就是HDFS和MapRecue。HDFS为海量的数据提供了存储，而MapRecue则为海量的数据提供了计算。\n\n##Apache Hadoop核心组件\nApache Hadoop包含以下模块：\nHadoop Common---常见实用工具，用来支持其他hadoop模块。\nHadoop Distributed File System（HDFS）---分布式文件系统，它提供对应用程序数据的高吞吐量访问\nHadoop YARN----一个作业调度和集群资源管理框架\nHadoop MapRecue--基于YARN的大型数据集的并行处理系统\n\n###其它Apache Hadoop 相关的项目包括：\nAmbari----一个基于Web的工具，用于配置、管理和监控的Apache Hadoop 集群，其中包括支持Hadoop  HDFS、Hadoop  MapRecue、Hive、HCatalog、HBase、ZooKeeperOozie、Pig和Sqoop。Ambari还提供了仪表盘用于查看集群的健康，如热图，并能够以用户友好的方式来查看MapRecue、Pig和Hive应用，方便诊断其性能。\nAvro--数据序列化系统\nCassandra--可扩展的、无单点故障的多主数据库\nChukwa--数据采集系统，用于管理大型分布式系统。\nHbase--一个可扩展的分布式数据库，支持结构化数据的大表存储\nHive--数据仓库基础设施，提供数据汇总以及特定的查询\nMahout---一种可扩展的机器学习和数据挖掘库\nPig--一个高层次的数据流并行计算语言和执行框架\nSpark---Hadoop数据的快速和通用计算引擎。Spark提供了简单和强大的编程模型用于支持广泛的应用，其中包括ETL、机器学习、流处理和图形处理。\nTEZ--通用的数据流编程框架，建立在Hadoop YARN之上。它提供了一个强大而灵活的引擎来执行任意DAG任务，以实现批量和交互式数据的处理。TEZ正在被Hive、Pig和Hadoop生态系统中的其他框架所采用，也可以通过其他商业软件（例如，ETL工具），以取代hadoop mapreduce作为底层执行引擎。\nZooKeeper--一个高性能的分布式应用程序协调服务。\n\n\n##Apache Spark\nSpark是一个快速和通用的集群计算系统。特别：\n1. 快速 Spack具有支持循环数据流和内存计算的先进的DAG执行引擎，所以比Hadoop MapRecue在内存计算上快100倍，在硬盘计算上快10倍。\n2. 易于使用 Spark提供了Java，Scala，Python和R等语言的高级API，可以用于快速开发相关语言应用。Spark提供了超过80个高级的操作，可以轻松构建并行应用程序。\n3. 全面 Spark提供了Spark SQL，机器学习的MLlib，进行图形处理的GraphX，以及Spark Streaming等库。你可以在同一应用程序无缝地合并这些库。\n4. 到处运行 可以standalone cluster mode运行EC2、Hadoop YARN、或者Apache Mesos中。可以访问HDFS、Cassandra、HBase、Hive、Tachyon，以及任意的Hadoop数据源。\n\n\n\n##Apache Mesos\n在传统上，物理机和虚拟机是数据中心的典型的计算单元。当应用部署后，这些机器需要安装各种配置工具来管理这些应用。机器通常被组织成集群，提供独立的服务，而系统管理员则监督其日常的日常动作。当这些集群达到其最大容量时，需要多机联网来处理负载，这就是集群的扩展带来了挑战。\n在2010年，UC Berkeley大学就对上述问题提出了解决方案，这就是现在的Apache Mesos，Mesos抽象了CPU、内存、硬盘资源，让数据中心的功能对外就像是一个大的机器。Mesos创建一个单独的底层集群来提供应用程序所需要的资源，而不会超出虚拟机和操作系统性能限制。\n\n###Apache Mesos简介\nMesos是Apache下的开源分布式资源管理框架，它被称为分布式系统的内核，使用内置Linux内核相同的原理，只是在不同的抽象层次。该 Mesos内核运行在每个机器上，在整个数据中心和云环境内应用程序（例如Hadoop、Spark、Kafka、Elaborate等）提供资源管理和资源负载的API接口。\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n","source":"_posts/hadoop/Hadoop之分布式计算 .md","raw":"---\ntitle: 分布式计算 \ndate: 2017-04-16 23:43:49\ntags: [分布式,分布式计算]\ncategories: [分布式,分布式计算]\n---\n#MapRecue\n在过去的20年里，互联网产生了大量的数据，比如爬虫文档、Web讲求日志等；也包括了计算各种类型的派生数据，比如，倒排索引、Web文档的图结构的各种表示、每台主机页面数量概要、每天被请求数量最多的集合，等等。很多这样的计算在概念上很容易理解的。然而，当输入的数据量很大时，这些计算必须要被分割到成百上千的机器上才有可能在可以接受的时间内完成。怎样来实现并行计算？如何分发数据？如何进行错误处理？所有这些问题综合在一起，使得原来很简洁的计算，因为要大量的复杂代码来处理这些问题，而变得让人难以处理。 Google公司为了应对大数据的处理，内部已经实现了数据以百计的为专门目的而写的计算程序，其中MapRecue就是其著名的计算框架之王，与GFS、Bigtable一起被称为Google技术的“三宝”。 \n\n##MapRecue简介 \nMapRecue是一个编程模型，用于大规模数据集（TB级）的并行运算。有关MapRecue的论文介绍，最早可以追溯到由Google的Jeffrey Dean和Sanjay Ghemawat发表在2004年OSDI（USENIX Symposium on Operationg Systems Design and Implementation）的《MapRecue：Simplified Data Processing on LargeClusters》。这篇文章描述了Google如何分割、处理、整合他们令人难以置信的大数据集。读者有兴趣可以在线阅读该论文https://www.usenix.org/legacy/events/osdi04/tech/full_papers/dean/dean.pdf。 随后，开源软件先驱Doug Cutting等人受到该论文的启发，开始尝试实现MapRecue计算框架，并将它与NDFS（Nutch Distributed File System）结合，用以支持Nutch引擎的主要算法。由于NDFS与MapRecue在Nutch引擎中有着良好的应用，所以它们于2006年2月被分离出来，成为一套完整而独立的软件，并命名为Hadoop。 MapRecue程序模型应用成功要归功于以下几个方面。首先，由于该模型隐藏了并行、容错、本地优化以及负载平衡的细节，所以即便是那些没有并行和分布式系统经验的程序员也易于使用该模型。其次MapRecue计算可以很容易地表达大数据的各种问题。比如，MapRecue用于为Google的网页搜索服务生成数据，用于排序，用于数据挖掘，用于机器学习以及其他许多系统。再次，MapRecue的实现符合“由数千机器组成的大集群”的尺度，有效地利用了机器资源，所以非常适合解决大型计算问题。 \n\n##MapRecue的编程模型\nMapRecue是一个用于大规模数据集（TB级）并行运算的编程模型，其基本原理就是将大的数据分成小块逐个分析，最后再将提取出来的数据汇总分析，最终获得我们想要的内容。从名字可以看出，“Map(映射)”和“Reduce（归纳）”是MapRecue模型的核心，其灵感来源于函数式语言（比如Lisp）中的内置函数map和reduce：用户通过定义一个Map函数，处理key/value（键值对）以生成一个中间key/value集合，MapRecue库将所有拥有相同的key(key I)的中间状态key合并起来传递到Redure职数；一个叫作Reduce的函数用以合并所有先前Map过后的有相同key（Key I）的中间量。map(k1,v1) -> list<k2,v2)reduce(k2,list(v2)) -> list(k3,v3)但上面的定义显然还是过于抽象。现实世界中的许多任务在这个模型中得到了很好的表达。Shekhar Gulati就在他的博客里《How I explained MapReduce to my Wife?》举了一个辣椒酱制作过程的例子，来形象地描述MapRecue的原理，如下所述。1.MapRecue制作辣椒酱的过程辣椒酱制作的过程是这样的，先取一个洋葱，把它切碎，然后拌入盐和水，最后放进混合研磨机研磨。这样就能得到洋葱辣椒酱了。 现在，假设你想用薄荷、洋葱、番茄、辣椒、大蒜弄一瓶混合辣椒酱。你会怎么做呢？你会取薄荷叶一撮，洋葱一个，番茄一个，辣椒一根，大蒜一根，切碎后加入适量的盐和水，再放入混合研磨机里研磨，这样你就可以得到一瓶混合辣椒酱了。 现在把MapRecue的概念应用到食谱上，Map和Reduce其实就是两种操作。 Map：把洋葱、番茄、辣椒和大蒜切磋，是各自作用在这些物体上的一个Map操作。所以你给Map一个洋葱，Map就会把洋葱切碎。同样地，你把辣椒、大蒜和番茄一一地拿给Map，你也会得到各种碎块。所以，当你在切像洋葱这样的蔬菜时，你执行的就是一个Map操作。Map操作适用于每一种蔬菜，它会相应地生产出一种或多种碎块，在我们的例子中生产的是蔬菜块。在Map操作中可能会出现有个洋葱坏掉了的情况，你只要把洋葱丢了就行了。所以，如果出现坏洋葱了，Map操作就会过滤掉这个坏洋葱而不会生产出任何的坏洋葱块。 Reduce：在这一阶段，你将各种蔬菜都放入研磨机时在进行研磨，你就可以得到一瓶辣椒酱了。这意味要制成一瓶辣椒酱，你得研磨所有的原料。因此，研磨机通常将Map操作的蔬菜聚焦在了一起。 当然上面内容只是MapRecue的一部分，MapRecue的强大在于分布式计算。假设你每天需要生产10000瓶辣椒酱，你会怎么办？这个时候你就不得不雇佣更多的人和研磨机来完成这项工作了，你需要几个人一起切蔬菜。每个人都要处理满满一袋子的蔬菜，而每一个人都相当于在执行一个简单的Map操作。每一个人都将不断地从袋子里拿出蔬菜来，并且每次只对一种蔬菜进行处理，也就是将它们切碎，直到袋子空了为止。这样，当所有的工人都切完以后，工作台（每个人工作的地方）上就有了洋葱块、番茄块和蒜蓉，等等。 MapRecue将所有输出的蔬菜都搅拌在了一起，这些蔬菜都在以key为基础的Map操作下产生的。搅拌将自动完成，你可以假设key是一种原料的名字，你像洋葱一样。所以全部的洋葱key都搅拌在一起，并转移到研磨洋葱的研磨器里。这样，你就能得到洋葱辣椒酱了。同样地，所有的番茄也会被转移地标记着番茄的研磨器里，并制造出番茄辣椒酱。 \n\n\n\n#Apache Hadoop\nApache Hadoop是一个由Apache基金会开发的分布式系统基础架构，它可以让用户在不了解分布式底层细节的情况下，开发出可靠、可扩展的分布式计算应用。\nApache Hadoop框架允许用户使用简单的编程模型来实现计算机集群的大型数据集的分布式处理。它的目的是支持从单一服务器到上千台机器的扩展，充分利用了每台机器所提供本地计算和存储，而不是依靠硬件来提高高可用性。其本身被设计成在应用层检测和处理故障的库，对于计算机集群来说，其中每台机器的顶层都被设计成可以容错的，以便提供一个高可用的服务。\nApache Hadoop的框架最核心的设计就是HDFS和MapRecue。HDFS为海量的数据提供了存储，而MapRecue则为海量的数据提供了计算。\n\n##Apache Hadoop核心组件\nApache Hadoop包含以下模块：\nHadoop Common---常见实用工具，用来支持其他hadoop模块。\nHadoop Distributed File System（HDFS）---分布式文件系统，它提供对应用程序数据的高吞吐量访问\nHadoop YARN----一个作业调度和集群资源管理框架\nHadoop MapRecue--基于YARN的大型数据集的并行处理系统\n\n###其它Apache Hadoop 相关的项目包括：\nAmbari----一个基于Web的工具，用于配置、管理和监控的Apache Hadoop 集群，其中包括支持Hadoop  HDFS、Hadoop  MapRecue、Hive、HCatalog、HBase、ZooKeeperOozie、Pig和Sqoop。Ambari还提供了仪表盘用于查看集群的健康，如热图，并能够以用户友好的方式来查看MapRecue、Pig和Hive应用，方便诊断其性能。\nAvro--数据序列化系统\nCassandra--可扩展的、无单点故障的多主数据库\nChukwa--数据采集系统，用于管理大型分布式系统。\nHbase--一个可扩展的分布式数据库，支持结构化数据的大表存储\nHive--数据仓库基础设施，提供数据汇总以及特定的查询\nMahout---一种可扩展的机器学习和数据挖掘库\nPig--一个高层次的数据流并行计算语言和执行框架\nSpark---Hadoop数据的快速和通用计算引擎。Spark提供了简单和强大的编程模型用于支持广泛的应用，其中包括ETL、机器学习、流处理和图形处理。\nTEZ--通用的数据流编程框架，建立在Hadoop YARN之上。它提供了一个强大而灵活的引擎来执行任意DAG任务，以实现批量和交互式数据的处理。TEZ正在被Hive、Pig和Hadoop生态系统中的其他框架所采用，也可以通过其他商业软件（例如，ETL工具），以取代hadoop mapreduce作为底层执行引擎。\nZooKeeper--一个高性能的分布式应用程序协调服务。\n\n\n##Apache Spark\nSpark是一个快速和通用的集群计算系统。特别：\n1. 快速 Spack具有支持循环数据流和内存计算的先进的DAG执行引擎，所以比Hadoop MapRecue在内存计算上快100倍，在硬盘计算上快10倍。\n2. 易于使用 Spark提供了Java，Scala，Python和R等语言的高级API，可以用于快速开发相关语言应用。Spark提供了超过80个高级的操作，可以轻松构建并行应用程序。\n3. 全面 Spark提供了Spark SQL，机器学习的MLlib，进行图形处理的GraphX，以及Spark Streaming等库。你可以在同一应用程序无缝地合并这些库。\n4. 到处运行 可以standalone cluster mode运行EC2、Hadoop YARN、或者Apache Mesos中。可以访问HDFS、Cassandra、HBase、Hive、Tachyon，以及任意的Hadoop数据源。\n\n\n\n##Apache Mesos\n在传统上，物理机和虚拟机是数据中心的典型的计算单元。当应用部署后，这些机器需要安装各种配置工具来管理这些应用。机器通常被组织成集群，提供独立的服务，而系统管理员则监督其日常的日常动作。当这些集群达到其最大容量时，需要多机联网来处理负载，这就是集群的扩展带来了挑战。\n在2010年，UC Berkeley大学就对上述问题提出了解决方案，这就是现在的Apache Mesos，Mesos抽象了CPU、内存、硬盘资源，让数据中心的功能对外就像是一个大的机器。Mesos创建一个单独的底层集群来提供应用程序所需要的资源，而不会超出虚拟机和操作系统性能限制。\n\n###Apache Mesos简介\nMesos是Apache下的开源分布式资源管理框架，它被称为分布式系统的内核，使用内置Linux内核相同的原理，只是在不同的抽象层次。该 Mesos内核运行在每个机器上，在整个数据中心和云环境内应用程序（例如Hadoop、Spark、Kafka、Elaborate等）提供资源管理和资源负载的API接口。\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n","slug":"hadoop/Hadoop之分布式计算 ","published":1,"updated":"2017-08-18T01:44:35.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjh7polt0001fbp7rdui2a9xy","content":"<p>#MapRecue<br>在过去的20年里，互联网产生了大量的数据，比如爬虫文档、Web讲求日志等；也包括了计算各种类型的派生数据，比如，倒排索引、Web文档的图结构的各种表示、每台主机页面数量概要、每天被请求数量最多的集合，等等。很多这样的计算在概念上很容易理解的。然而，当输入的数据量很大时，这些计算必须要被分割到成百上千的机器上才有可能在可以接受的时间内完成。怎样来实现并行计算？如何分发数据？如何进行错误处理？所有这些问题综合在一起，使得原来很简洁的计算，因为要大量的复杂代码来处理这些问题，而变得让人难以处理。 Google公司为了应对大数据的处理，内部已经实现了数据以百计的为专门目的而写的计算程序，其中MapRecue就是其著名的计算框架之王，与GFS、Bigtable一起被称为Google技术的“三宝”。 </p>\n<p>##MapRecue简介<br>MapRecue是一个编程模型，用于大规模数据集（TB级）的并行运算。有关MapRecue的论文介绍，最早可以追溯到由Google的Jeffrey Dean和Sanjay Ghemawat发表在2004年OSDI（USENIX Symposium on Operationg Systems Design and Implementation）的《MapRecue：Simplified Data Processing on LargeClusters》。这篇文章描述了Google如何分割、处理、整合他们令人难以置信的大数据集。读者有兴趣可以在线阅读该论文<a href=\"https://www.usenix.org/legacy/events/osdi04/tech/full_papers/dean/dean.pdf。\" target=\"_blank\" rel=\"noopener\">https://www.usenix.org/legacy/events/osdi04/tech/full_papers/dean/dean.pdf。</a> 随后，开源软件先驱Doug Cutting等人受到该论文的启发，开始尝试实现MapRecue计算框架，并将它与NDFS（Nutch Distributed File System）结合，用以支持Nutch引擎的主要算法。由于NDFS与MapRecue在Nutch引擎中有着良好的应用，所以它们于2006年2月被分离出来，成为一套完整而独立的软件，并命名为Hadoop。 MapRecue程序模型应用成功要归功于以下几个方面。首先，由于该模型隐藏了并行、容错、本地优化以及负载平衡的细节，所以即便是那些没有并行和分布式系统经验的程序员也易于使用该模型。其次MapRecue计算可以很容易地表达大数据的各种问题。比如，MapRecue用于为Google的网页搜索服务生成数据，用于排序，用于数据挖掘，用于机器学习以及其他许多系统。再次，MapRecue的实现符合“由数千机器组成的大集群”的尺度，有效地利用了机器资源，所以非常适合解决大型计算问题。 </p>\n<p>##MapRecue的编程模型<br>MapRecue是一个用于大规模数据集（TB级）并行运算的编程模型，其基本原理就是将大的数据分成小块逐个分析，最后再将提取出来的数据汇总分析，最终获得我们想要的内容。从名字可以看出，“Map(映射)”和“Reduce（归纳）”是MapRecue模型的核心，其灵感来源于函数式语言（比如Lisp）中的内置函数map和reduce：用户通过定义一个Map函数，处理key/value（键值对）以生成一个中间key/value集合，MapRecue库将所有拥有相同的key(key I)的中间状态key合并起来传递到Redure职数；一个叫作Reduce的函数用以合并所有先前Map过后的有相同key（Key I）的中间量。map(k1,v1) -&gt; list&lt;k2,v2)reduce(k2,list(v2)) -&gt; list(k3,v3)但上面的定义显然还是过于抽象。现实世界中的许多任务在这个模型中得到了很好的表达。Shekhar Gulati就在他的博客里《How I explained MapReduce to my Wife?》举了一个辣椒酱制作过程的例子，来形象地描述MapRecue的原理，如下所述。1.MapRecue制作辣椒酱的过程辣椒酱制作的过程是这样的，先取一个洋葱，把它切碎，然后拌入盐和水，最后放进混合研磨机研磨。这样就能得到洋葱辣椒酱了。 现在，假设你想用薄荷、洋葱、番茄、辣椒、大蒜弄一瓶混合辣椒酱。你会怎么做呢？你会取薄荷叶一撮，洋葱一个，番茄一个，辣椒一根，大蒜一根，切碎后加入适量的盐和水，再放入混合研磨机里研磨，这样你就可以得到一瓶混合辣椒酱了。 现在把MapRecue的概念应用到食谱上，Map和Reduce其实就是两种操作。 Map：把洋葱、番茄、辣椒和大蒜切磋，是各自作用在这些物体上的一个Map操作。所以你给Map一个洋葱，Map就会把洋葱切碎。同样地，你把辣椒、大蒜和番茄一一地拿给Map，你也会得到各种碎块。所以，当你在切像洋葱这样的蔬菜时，你执行的就是一个Map操作。Map操作适用于每一种蔬菜，它会相应地生产出一种或多种碎块，在我们的例子中生产的是蔬菜块。在Map操作中可能会出现有个洋葱坏掉了的情况，你只要把洋葱丢了就行了。所以，如果出现坏洋葱了，Map操作就会过滤掉这个坏洋葱而不会生产出任何的坏洋葱块。 Reduce：在这一阶段，你将各种蔬菜都放入研磨机时在进行研磨，你就可以得到一瓶辣椒酱了。这意味要制成一瓶辣椒酱，你得研磨所有的原料。因此，研磨机通常将Map操作的蔬菜聚焦在了一起。 当然上面内容只是MapRecue的一部分，MapRecue的强大在于分布式计算。假设你每天需要生产10000瓶辣椒酱，你会怎么办？这个时候你就不得不雇佣更多的人和研磨机来完成这项工作了，你需要几个人一起切蔬菜。每个人都要处理满满一袋子的蔬菜，而每一个人都相当于在执行一个简单的Map操作。每一个人都将不断地从袋子里拿出蔬菜来，并且每次只对一种蔬菜进行处理，也就是将它们切碎，直到袋子空了为止。这样，当所有的工人都切完以后，工作台（每个人工作的地方）上就有了洋葱块、番茄块和蒜蓉，等等。 MapRecue将所有输出的蔬菜都搅拌在了一起，这些蔬菜都在以key为基础的Map操作下产生的。搅拌将自动完成，你可以假设key是一种原料的名字，你像洋葱一样。所以全部的洋葱key都搅拌在一起，并转移到研磨洋葱的研磨器里。这样，你就能得到洋葱辣椒酱了。同样地，所有的番茄也会被转移地标记着番茄的研磨器里，并制造出番茄辣椒酱。 </p>\n<p>#Apache Hadoop<br>Apache Hadoop是一个由Apache基金会开发的分布式系统基础架构，它可以让用户在不了解分布式底层细节的情况下，开发出可靠、可扩展的分布式计算应用。<br>Apache Hadoop框架允许用户使用简单的编程模型来实现计算机集群的大型数据集的分布式处理。它的目的是支持从单一服务器到上千台机器的扩展，充分利用了每台机器所提供本地计算和存储，而不是依靠硬件来提高高可用性。其本身被设计成在应用层检测和处理故障的库，对于计算机集群来说，其中每台机器的顶层都被设计成可以容错的，以便提供一个高可用的服务。<br>Apache Hadoop的框架最核心的设计就是HDFS和MapRecue。HDFS为海量的数据提供了存储，而MapRecue则为海量的数据提供了计算。</p>\n<p>##Apache Hadoop核心组件<br>Apache Hadoop包含以下模块：<br>Hadoop Common—常见实用工具，用来支持其他hadoop模块。<br>Hadoop Distributed File System（HDFS）—分布式文件系统，它提供对应用程序数据的高吞吐量访问<br>Hadoop YARN—-一个作业调度和集群资源管理框架<br>Hadoop MapRecue–基于YARN的大型数据集的并行处理系统</p>\n<p>###其它Apache Hadoop 相关的项目包括：<br>Ambari—-一个基于Web的工具，用于配置、管理和监控的Apache Hadoop 集群，其中包括支持Hadoop  HDFS、Hadoop  MapRecue、Hive、HCatalog、HBase、ZooKeeperOozie、Pig和Sqoop。Ambari还提供了仪表盘用于查看集群的健康，如热图，并能够以用户友好的方式来查看MapRecue、Pig和Hive应用，方便诊断其性能。<br>Avro–数据序列化系统<br>Cassandra–可扩展的、无单点故障的多主数据库<br>Chukwa–数据采集系统，用于管理大型分布式系统。<br>Hbase–一个可扩展的分布式数据库，支持结构化数据的大表存储<br>Hive–数据仓库基础设施，提供数据汇总以及特定的查询<br>Mahout—一种可扩展的机器学习和数据挖掘库<br>Pig–一个高层次的数据流并行计算语言和执行框架<br>Spark—Hadoop数据的快速和通用计算引擎。Spark提供了简单和强大的编程模型用于支持广泛的应用，其中包括ETL、机器学习、流处理和图形处理。<br>TEZ–通用的数据流编程框架，建立在Hadoop YARN之上。它提供了一个强大而灵活的引擎来执行任意DAG任务，以实现批量和交互式数据的处理。TEZ正在被Hive、Pig和Hadoop生态系统中的其他框架所采用，也可以通过其他商业软件（例如，ETL工具），以取代hadoop mapreduce作为底层执行引擎。<br>ZooKeeper–一个高性能的分布式应用程序协调服务。</p>\n<p>##Apache Spark<br>Spark是一个快速和通用的集群计算系统。特别：</p>\n<ol>\n<li>快速 Spack具有支持循环数据流和内存计算的先进的DAG执行引擎，所以比Hadoop MapRecue在内存计算上快100倍，在硬盘计算上快10倍。</li>\n<li>易于使用 Spark提供了Java，Scala，Python和R等语言的高级API，可以用于快速开发相关语言应用。Spark提供了超过80个高级的操作，可以轻松构建并行应用程序。</li>\n<li>全面 Spark提供了Spark SQL，机器学习的MLlib，进行图形处理的GraphX，以及Spark Streaming等库。你可以在同一应用程序无缝地合并这些库。</li>\n<li>到处运行 可以standalone cluster mode运行EC2、Hadoop YARN、或者Apache Mesos中。可以访问HDFS、Cassandra、HBase、Hive、Tachyon，以及任意的Hadoop数据源。</li>\n</ol>\n<p>##Apache Mesos<br>在传统上，物理机和虚拟机是数据中心的典型的计算单元。当应用部署后，这些机器需要安装各种配置工具来管理这些应用。机器通常被组织成集群，提供独立的服务，而系统管理员则监督其日常的日常动作。当这些集群达到其最大容量时，需要多机联网来处理负载，这就是集群的扩展带来了挑战。<br>在2010年，UC Berkeley大学就对上述问题提出了解决方案，这就是现在的Apache Mesos，Mesos抽象了CPU、内存、硬盘资源，让数据中心的功能对外就像是一个大的机器。Mesos创建一个单独的底层集群来提供应用程序所需要的资源，而不会超出虚拟机和操作系统性能限制。</p>\n<p>###Apache Mesos简介<br>Mesos是Apache下的开源分布式资源管理框架，它被称为分布式系统的内核，使用内置Linux内核相同的原理，只是在不同的抽象层次。该 Mesos内核运行在每个机器上，在整个数据中心和云环境内应用程序（例如Hadoop、Spark、Kafka、Elaborate等）提供资源管理和资源负载的API接口。</p>\n","site":{"data":{}},"excerpt":"","more":"<p>#MapRecue<br>在过去的20年里，互联网产生了大量的数据，比如爬虫文档、Web讲求日志等；也包括了计算各种类型的派生数据，比如，倒排索引、Web文档的图结构的各种表示、每台主机页面数量概要、每天被请求数量最多的集合，等等。很多这样的计算在概念上很容易理解的。然而，当输入的数据量很大时，这些计算必须要被分割到成百上千的机器上才有可能在可以接受的时间内完成。怎样来实现并行计算？如何分发数据？如何进行错误处理？所有这些问题综合在一起，使得原来很简洁的计算，因为要大量的复杂代码来处理这些问题，而变得让人难以处理。 Google公司为了应对大数据的处理，内部已经实现了数据以百计的为专门目的而写的计算程序，其中MapRecue就是其著名的计算框架之王，与GFS、Bigtable一起被称为Google技术的“三宝”。 </p>\n<p>##MapRecue简介<br>MapRecue是一个编程模型，用于大规模数据集（TB级）的并行运算。有关MapRecue的论文介绍，最早可以追溯到由Google的Jeffrey Dean和Sanjay Ghemawat发表在2004年OSDI（USENIX Symposium on Operationg Systems Design and Implementation）的《MapRecue：Simplified Data Processing on LargeClusters》。这篇文章描述了Google如何分割、处理、整合他们令人难以置信的大数据集。读者有兴趣可以在线阅读该论文<a href=\"https://www.usenix.org/legacy/events/osdi04/tech/full_papers/dean/dean.pdf。\" target=\"_blank\" rel=\"noopener\">https://www.usenix.org/legacy/events/osdi04/tech/full_papers/dean/dean.pdf。</a> 随后，开源软件先驱Doug Cutting等人受到该论文的启发，开始尝试实现MapRecue计算框架，并将它与NDFS（Nutch Distributed File System）结合，用以支持Nutch引擎的主要算法。由于NDFS与MapRecue在Nutch引擎中有着良好的应用，所以它们于2006年2月被分离出来，成为一套完整而独立的软件，并命名为Hadoop。 MapRecue程序模型应用成功要归功于以下几个方面。首先，由于该模型隐藏了并行、容错、本地优化以及负载平衡的细节，所以即便是那些没有并行和分布式系统经验的程序员也易于使用该模型。其次MapRecue计算可以很容易地表达大数据的各种问题。比如，MapRecue用于为Google的网页搜索服务生成数据，用于排序，用于数据挖掘，用于机器学习以及其他许多系统。再次，MapRecue的实现符合“由数千机器组成的大集群”的尺度，有效地利用了机器资源，所以非常适合解决大型计算问题。 </p>\n<p>##MapRecue的编程模型<br>MapRecue是一个用于大规模数据集（TB级）并行运算的编程模型，其基本原理就是将大的数据分成小块逐个分析，最后再将提取出来的数据汇总分析，最终获得我们想要的内容。从名字可以看出，“Map(映射)”和“Reduce（归纳）”是MapRecue模型的核心，其灵感来源于函数式语言（比如Lisp）中的内置函数map和reduce：用户通过定义一个Map函数，处理key/value（键值对）以生成一个中间key/value集合，MapRecue库将所有拥有相同的key(key I)的中间状态key合并起来传递到Redure职数；一个叫作Reduce的函数用以合并所有先前Map过后的有相同key（Key I）的中间量。map(k1,v1) -&gt; list&lt;k2,v2)reduce(k2,list(v2)) -&gt; list(k3,v3)但上面的定义显然还是过于抽象。现实世界中的许多任务在这个模型中得到了很好的表达。Shekhar Gulati就在他的博客里《How I explained MapReduce to my Wife?》举了一个辣椒酱制作过程的例子，来形象地描述MapRecue的原理，如下所述。1.MapRecue制作辣椒酱的过程辣椒酱制作的过程是这样的，先取一个洋葱，把它切碎，然后拌入盐和水，最后放进混合研磨机研磨。这样就能得到洋葱辣椒酱了。 现在，假设你想用薄荷、洋葱、番茄、辣椒、大蒜弄一瓶混合辣椒酱。你会怎么做呢？你会取薄荷叶一撮，洋葱一个，番茄一个，辣椒一根，大蒜一根，切碎后加入适量的盐和水，再放入混合研磨机里研磨，这样你就可以得到一瓶混合辣椒酱了。 现在把MapRecue的概念应用到食谱上，Map和Reduce其实就是两种操作。 Map：把洋葱、番茄、辣椒和大蒜切磋，是各自作用在这些物体上的一个Map操作。所以你给Map一个洋葱，Map就会把洋葱切碎。同样地，你把辣椒、大蒜和番茄一一地拿给Map，你也会得到各种碎块。所以，当你在切像洋葱这样的蔬菜时，你执行的就是一个Map操作。Map操作适用于每一种蔬菜，它会相应地生产出一种或多种碎块，在我们的例子中生产的是蔬菜块。在Map操作中可能会出现有个洋葱坏掉了的情况，你只要把洋葱丢了就行了。所以，如果出现坏洋葱了，Map操作就会过滤掉这个坏洋葱而不会生产出任何的坏洋葱块。 Reduce：在这一阶段，你将各种蔬菜都放入研磨机时在进行研磨，你就可以得到一瓶辣椒酱了。这意味要制成一瓶辣椒酱，你得研磨所有的原料。因此，研磨机通常将Map操作的蔬菜聚焦在了一起。 当然上面内容只是MapRecue的一部分，MapRecue的强大在于分布式计算。假设你每天需要生产10000瓶辣椒酱，你会怎么办？这个时候你就不得不雇佣更多的人和研磨机来完成这项工作了，你需要几个人一起切蔬菜。每个人都要处理满满一袋子的蔬菜，而每一个人都相当于在执行一个简单的Map操作。每一个人都将不断地从袋子里拿出蔬菜来，并且每次只对一种蔬菜进行处理，也就是将它们切碎，直到袋子空了为止。这样，当所有的工人都切完以后，工作台（每个人工作的地方）上就有了洋葱块、番茄块和蒜蓉，等等。 MapRecue将所有输出的蔬菜都搅拌在了一起，这些蔬菜都在以key为基础的Map操作下产生的。搅拌将自动完成，你可以假设key是一种原料的名字，你像洋葱一样。所以全部的洋葱key都搅拌在一起，并转移到研磨洋葱的研磨器里。这样，你就能得到洋葱辣椒酱了。同样地，所有的番茄也会被转移地标记着番茄的研磨器里，并制造出番茄辣椒酱。 </p>\n<p>#Apache Hadoop<br>Apache Hadoop是一个由Apache基金会开发的分布式系统基础架构，它可以让用户在不了解分布式底层细节的情况下，开发出可靠、可扩展的分布式计算应用。<br>Apache Hadoop框架允许用户使用简单的编程模型来实现计算机集群的大型数据集的分布式处理。它的目的是支持从单一服务器到上千台机器的扩展，充分利用了每台机器所提供本地计算和存储，而不是依靠硬件来提高高可用性。其本身被设计成在应用层检测和处理故障的库，对于计算机集群来说，其中每台机器的顶层都被设计成可以容错的，以便提供一个高可用的服务。<br>Apache Hadoop的框架最核心的设计就是HDFS和MapRecue。HDFS为海量的数据提供了存储，而MapRecue则为海量的数据提供了计算。</p>\n<p>##Apache Hadoop核心组件<br>Apache Hadoop包含以下模块：<br>Hadoop Common—常见实用工具，用来支持其他hadoop模块。<br>Hadoop Distributed File System（HDFS）—分布式文件系统，它提供对应用程序数据的高吞吐量访问<br>Hadoop YARN—-一个作业调度和集群资源管理框架<br>Hadoop MapRecue–基于YARN的大型数据集的并行处理系统</p>\n<p>###其它Apache Hadoop 相关的项目包括：<br>Ambari—-一个基于Web的工具，用于配置、管理和监控的Apache Hadoop 集群，其中包括支持Hadoop  HDFS、Hadoop  MapRecue、Hive、HCatalog、HBase、ZooKeeperOozie、Pig和Sqoop。Ambari还提供了仪表盘用于查看集群的健康，如热图，并能够以用户友好的方式来查看MapRecue、Pig和Hive应用，方便诊断其性能。<br>Avro–数据序列化系统<br>Cassandra–可扩展的、无单点故障的多主数据库<br>Chukwa–数据采集系统，用于管理大型分布式系统。<br>Hbase–一个可扩展的分布式数据库，支持结构化数据的大表存储<br>Hive–数据仓库基础设施，提供数据汇总以及特定的查询<br>Mahout—一种可扩展的机器学习和数据挖掘库<br>Pig–一个高层次的数据流并行计算语言和执行框架<br>Spark—Hadoop数据的快速和通用计算引擎。Spark提供了简单和强大的编程模型用于支持广泛的应用，其中包括ETL、机器学习、流处理和图形处理。<br>TEZ–通用的数据流编程框架，建立在Hadoop YARN之上。它提供了一个强大而灵活的引擎来执行任意DAG任务，以实现批量和交互式数据的处理。TEZ正在被Hive、Pig和Hadoop生态系统中的其他框架所采用，也可以通过其他商业软件（例如，ETL工具），以取代hadoop mapreduce作为底层执行引擎。<br>ZooKeeper–一个高性能的分布式应用程序协调服务。</p>\n<p>##Apache Spark<br>Spark是一个快速和通用的集群计算系统。特别：</p>\n<ol>\n<li>快速 Spack具有支持循环数据流和内存计算的先进的DAG执行引擎，所以比Hadoop MapRecue在内存计算上快100倍，在硬盘计算上快10倍。</li>\n<li>易于使用 Spark提供了Java，Scala，Python和R等语言的高级API，可以用于快速开发相关语言应用。Spark提供了超过80个高级的操作，可以轻松构建并行应用程序。</li>\n<li>全面 Spark提供了Spark SQL，机器学习的MLlib，进行图形处理的GraphX，以及Spark Streaming等库。你可以在同一应用程序无缝地合并这些库。</li>\n<li>到处运行 可以standalone cluster mode运行EC2、Hadoop YARN、或者Apache Mesos中。可以访问HDFS、Cassandra、HBase、Hive、Tachyon，以及任意的Hadoop数据源。</li>\n</ol>\n<p>##Apache Mesos<br>在传统上，物理机和虚拟机是数据中心的典型的计算单元。当应用部署后，这些机器需要安装各种配置工具来管理这些应用。机器通常被组织成集群，提供独立的服务，而系统管理员则监督其日常的日常动作。当这些集群达到其最大容量时，需要多机联网来处理负载，这就是集群的扩展带来了挑战。<br>在2010年，UC Berkeley大学就对上述问题提出了解决方案，这就是现在的Apache Mesos，Mesos抽象了CPU、内存、硬盘资源，让数据中心的功能对外就像是一个大的机器。Mesos创建一个单独的底层集群来提供应用程序所需要的资源，而不会超出虚拟机和操作系统性能限制。</p>\n<p>###Apache Mesos简介<br>Mesos是Apache下的开源分布式资源管理框架，它被称为分布式系统的内核，使用内置Linux内核相同的原理，只是在不同的抽象层次。该 Mesos内核运行在每个机器上，在整个数据中心和云环境内应用程序（例如Hadoop、Spark、Kafka、Elaborate等）提供资源管理和资源负载的API接口。</p>\n"},{"title":"实时分析","date":"2017-04-16T15:43:49.000Z","_content":"少量数据离线分析对于MapRecue这样的批处理系统挑战并不大，如果要求时实而又分为两种情况：如果查询模式单一，那么，可以通过MapRecue预处理后将最终结果导入到在线系统提供实时查询；如果查询模式复杂，例如涉及多个列任意组合查询，那么，只能通过实时分析系统解决。实时分析系统融合了并行数据库和云计算这两类技术，能够从海量数据中快速分析出汇总结果。\n\n\n#MPP架构\n并行数据库往往采用MPP（Massively Parallel Processing，大规模并行处理）架构。MPP架构是一种不共享的结果，每个节点可以运行自己的操作系统、数据库等，每个节点内的CPU不能访问另一个节点的内存，节点之间的信息交互是通过节点互联网络实现的。\n\n#EMC Greenplum\nGreenplum是EMC公司研发的一款采用MPP架构的OLAP产品，底层基于开源的PostgreSQL数据库。\n\n\n#HP Vertica\nVertica是商业版。\n\n\n#Google Dremel\n\n\n\n\n\n\n","source":"_posts/hadoop/Hadoop之实时分析.md","raw":"---\ntitle: 实时分析\ndate: 2017-04-16 23:43:49\ntags: [大数据,实时分析]\ncategories: [大数据,实时分析]\n---\n少量数据离线分析对于MapRecue这样的批处理系统挑战并不大，如果要求时实而又分为两种情况：如果查询模式单一，那么，可以通过MapRecue预处理后将最终结果导入到在线系统提供实时查询；如果查询模式复杂，例如涉及多个列任意组合查询，那么，只能通过实时分析系统解决。实时分析系统融合了并行数据库和云计算这两类技术，能够从海量数据中快速分析出汇总结果。\n\n\n#MPP架构\n并行数据库往往采用MPP（Massively Parallel Processing，大规模并行处理）架构。MPP架构是一种不共享的结果，每个节点可以运行自己的操作系统、数据库等，每个节点内的CPU不能访问另一个节点的内存，节点之间的信息交互是通过节点互联网络实现的。\n\n#EMC Greenplum\nGreenplum是EMC公司研发的一款采用MPP架构的OLAP产品，底层基于开源的PostgreSQL数据库。\n\n\n#HP Vertica\nVertica是商业版。\n\n\n#Google Dremel\n\n\n\n\n\n\n","slug":"hadoop/Hadoop之实时分析","published":1,"updated":"2017-08-18T01:44:35.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjh7polt1001ibp7ri7gid1ig","content":"<p>少量数据离线分析对于MapRecue这样的批处理系统挑战并不大，如果要求时实而又分为两种情况：如果查询模式单一，那么，可以通过MapRecue预处理后将最终结果导入到在线系统提供实时查询；如果查询模式复杂，例如涉及多个列任意组合查询，那么，只能通过实时分析系统解决。实时分析系统融合了并行数据库和云计算这两类技术，能够从海量数据中快速分析出汇总结果。</p>\n<p>#MPP架构<br>并行数据库往往采用MPP（Massively Parallel Processing，大规模并行处理）架构。MPP架构是一种不共享的结果，每个节点可以运行自己的操作系统、数据库等，每个节点内的CPU不能访问另一个节点的内存，节点之间的信息交互是通过节点互联网络实现的。</p>\n<p>#EMC Greenplum<br>Greenplum是EMC公司研发的一款采用MPP架构的OLAP产品，底层基于开源的PostgreSQL数据库。</p>\n<p>#HP Vertica<br>Vertica是商业版。</p>\n<p>#Google Dremel</p>\n","site":{"data":{}},"excerpt":"","more":"<p>少量数据离线分析对于MapRecue这样的批处理系统挑战并不大，如果要求时实而又分为两种情况：如果查询模式单一，那么，可以通过MapRecue预处理后将最终结果导入到在线系统提供实时查询；如果查询模式复杂，例如涉及多个列任意组合查询，那么，只能通过实时分析系统解决。实时分析系统融合了并行数据库和云计算这两类技术，能够从海量数据中快速分析出汇总结果。</p>\n<p>#MPP架构<br>并行数据库往往采用MPP（Massively Parallel Processing，大规模并行处理）架构。MPP架构是一种不共享的结果，每个节点可以运行自己的操作系统、数据库等，每个节点内的CPU不能访问另一个节点的内存，节点之间的信息交互是通过节点互联网络实现的。</p>\n<p>#EMC Greenplum<br>Greenplum是EMC公司研发的一款采用MPP架构的OLAP产品，底层基于开源的PostgreSQL数据库。</p>\n<p>#HP Vertica<br>Vertica是商业版。</p>\n<p>#Google Dremel</p>\n"},{"title":"Hadoop数据移动","date":"2017-04-16T15:43:49.000Z","_content":"数据采集\n本文主要是讲外部系统与Hadoop之间的数据传递，包括从外部系统采集数据导入到hadoop，以及从Hadoop中提取数据导入外部系统中。\n\n#数据采集考量\n虽然Hadoop提出了文件客户端，便于在Hadoop中和Hadoop外复制文件，但是大多数据 Hadoop应用需要从不同来源导入数据，而且对不同的导入频率也提出了要求，Hadoop常用的数据来源包括以下：\n1.传统数据管理系统，如果关系型数据库与主机\n2.日志、机器生成的数据，以及其他类型的事件数据。\n3.从现有的企业数据存储中输入的文件。\n\n将数据从不同的系统输入Hadoop时需要考虑很多因素。如下：\n1.数据采集的时效性与可访问性\n需要采集数据在采集频率方面有哪些要求？下游的处理要求数据多长时间准备完毕？\n\n2.增量更新\n如何添加新数据？需要将数据添加到现有数据库吗？需要重写现有数据吗？\n\n3.数据访问和处理\n数据会用于处理过程吗？如果会，数据会用于批处理任务吗？需要的数据是不是随机获取的？\n\n4.数据分区及数据分片\n数据采集后应该如何分区？需要将数据导入到多个目录系统（如HDFS与HBase）吗？\n\n5.数据存储格式\n数据存储的格式是哪一种？\n\n6.数据变换\n需要变换尚未落地的数据吗？\n\n下面简单列举一下这几点考量？\n\n##1.数据采集的时效性\n这里的时效性是指可进行数据采集的时间与Hadoop中工具可访问数据的时间之间的间隔。采集架构的时间分类会对存储媒介和采集方法造成很大的影响。一般来说数据采集构架，可以使用以下分类中的一个\na.大型批处理\n通常指15分钟到数据小时的任务，有时可能指时间跨度达到一天的任务\n\nb.小型批处理\n通常指大约2分钟发送一次任务，但是总的来说不会超过15分钟\n\nc.近实时决策支持\n指接受信息后“立即作出反应”，并在2秒到2分钟内发送数据\n\nd.近实时事件处理\n指在2秒内处理任务，速度可达到100毫秒\n\ne.实时\n这里指不超过100毫秒\n\n可以注意到随着实现时间到达实时，实现的复杂度和成本会大大增加。从批处理出发（比如使用简单文件传输）通常是个不错的选择。HDFS对时效性的要求比较宽松，所以可能更加适合成为主要存储位置。而一个简单文件传输或Sqoop任务则适合作为采集数据的工具。比如，执行hadoop fs -put命令将复制一个文件，并进行全面的校验，以确定正确地复制数据。\n使用hadoop fs -put命令与Sqoop时，你需要明白一点：HDFS上的数据存储格式可能并不适合数据的长期存储和处理。因此，在使用这些工具的时候，可能需要通过额外的批处理操作，以将数据存储为需要的格式。\n当用户需要从简单的批处理转向更高频率的更新时，就应该考虑Flume或kafka之类的工具了。这里时间要求不起过2分钟，所以Sqoop与文件转换器不适用。而且，因为要求时间不起过2分钟，所以存储层可能需要变成HBase或Solr，这样插入与读取操作会获得更细的粒度。当要求到实时水平时，我们首先需要考虑内存，然后是永久性存储。全世界所有的平行化处理都不会有助于将反应要求控制在500毫秒以内，只要硬盘驱动器保持处理操作的状态。基于这一点，我们开始进入流处理领域，采用Storm或Spark Streaming之类的工具。这里需要强调的是，这些工具应该真正用于大数据处理，而不是像Flume或Sqoop那样用于数据采集。\n\n##2.增量更新\n新的数据是要添加到已有数据集中，还是要修改已有数据集。如果仅要求添加数据，那么HDFS对于大部分实现都很适用。HDFS能够并行 化多个驱动器的I/O操作，所以读写性能很高。HDFS的缺点是无法添加或者随机写入创建后的文件。\n\n#数据采集的选择\n##1.文件传输\n将数据导入导出到Hadoop最简单的方法就是文件传输，就是hadoop fs -put与hadoop fs -get命令。这有时也是最快的方法，所以在设计Hadoop新的数据处理流水线时，首先应该考虑选择文件传输。\n\n下面列一下文件传输的特点：\na.这是一种all-or-nothing批处理方法，所以如果文件传输过程中出现错误，则不会写入或读取任何数据。这种方法与Flume、Kafka之类的采集方法不同，后者提供一定程度的错误处理功能，并且有传输保障。\nb.文件传输默认为单线程，不能并行 文件传输。\nc.文件传输将文件从传统的文件系统导入HDFS\nd.不支持数据转换，数据按原样导入HDFS。数据导入HDFS后才能进行处理，这一点与传输过程中的数据转换截然相反。类似于Flume的系统支持传输过程中的数据转换。\ne.这种加载是逐字进行的，所以能传输任何类型的文件（文件、二进制、图书等）\n\n##文件传输与其他采集方法的考量\n简单文件传输在某些情况下是适用的，尤其是在需要将已存在 的一系列文件输入到HDFS中，而且可以接受保持源文件格式的情况下。否则，在决定是否可以接受文件传输或者是否使用类似于Flume的工具时，需要考虑以下因素。\na.需要将数据采集到多个位置吗？比如，是需要将数据同时输入HDFS和Solr，还是需要将数据同时输入HDFS和HBase？这种情况下，如果使用文件传输，那么在文件采集完成之后将需要额外的工作，因些采用Flume更合适。\nb.对可靠性的要求高不高？如果高，那么一旦传输时出现错误，文件传输就必须重新开始，这时，Fluem同样是更好的选择。\ne.数据采集之前需要转换操作吗？如果需要Flume无疑是适合的工具。\n如果需要采集文件，可以考虑使用Flume Spooling  Directory源。采用这种方法，用户将文件放置到磁盘特定的目录就可以采集文件。这种采集文件的方法简单可靠，而且需要时能够实现传输过程中的数据转换。\n\n##Sqoop：Hadoop与关系数据库的批量传输\nSqoop是一种工具，能批量地将数据从关系型数据管理系统导出到Hadoop中，也能批量地将数据从Hadoop导出至关系型数据库中。\n\nFlume:基于事件的数据收集及处理\nFlume是一种分布式的可靠开源系统，用于流数据的高效收集、聚焦和移动。Flume通常用于移动日志数据，但是也能移动大量事件数据，如社交媒体订阅、消息队列事件或网络流量数据。\n\n##Kafka\nApache Kafka是一种发布订单消息的分布式系统，能够将消息归类为不同主题。应用程序能在Kafka上发布信息，或订阅主题进而接受特定主要下发布的消息。Producer发布消息，而Consumer收集并处理消息。作为分布式系统，Kafka在集群中运行，每个节点被称为Broker。\nKafka维护每个主题的分区日志。消息会发布到相应的主题中，每个分区都是一个有序的消息子集。同一个主题的多个分区能够通过集群中的多个Broker传送，这种方法提高了主题的容量与吞吐量，使其超越了单一机器所能提供的容量与吞吐量。消息在分区内被有序排列，每个消息都包含一个特定的偏移量。Kafka中消息可以通过一个包含主题、分区以及偏移量的组合来确定。Producer能够根据消息的主键选择消息应该写入哪一个分区，也能够简单地用循环的方式，让消息分布在各分区之间。\nConsumer会在Consumer组中注册，每个组包括一个或多个Consumer，每个Consumer读取一个或多个主题分区。每组中的每条消息只能传送给一个Consumer。但是，如果多个组订阅了同一个主题，那么每个组都将得到所有的消息。一个组中包含多个Consumer有助于获得加载平衡（可以支持高于单个Consumer处理能力的吞吐量）与高可用性（如果一个Consumer出现错误，它所读取的分区将重新分配给组中其它Consumer）。\n前面提到，对于应用层面的数据分类，主要单位是主题。一个Consumer或Consumer组将读取其订阅主题的所有数据，所以如果一个应用只关注一个数据子集，那么就应该将该数据子集与其他数据放在两个不同的主题中。如果多个信息集总是一起读取和处理，那么应该将它们归在同一个主题中。\n\n#数据导出\n数据导出的思路与导入类似。\n\n\n\n\n\n\n","source":"_posts/hadoop/Hadoop之数据采集.md","raw":"---\ntitle: Hadoop数据移动\ndate: 2017-04-16 23:43:49\ntags: [大数据,数据仓库,数据采集]\ncategories: [大数据,数据仓库]\n---\n数据采集\n本文主要是讲外部系统与Hadoop之间的数据传递，包括从外部系统采集数据导入到hadoop，以及从Hadoop中提取数据导入外部系统中。\n\n#数据采集考量\n虽然Hadoop提出了文件客户端，便于在Hadoop中和Hadoop外复制文件，但是大多数据 Hadoop应用需要从不同来源导入数据，而且对不同的导入频率也提出了要求，Hadoop常用的数据来源包括以下：\n1.传统数据管理系统，如果关系型数据库与主机\n2.日志、机器生成的数据，以及其他类型的事件数据。\n3.从现有的企业数据存储中输入的文件。\n\n将数据从不同的系统输入Hadoop时需要考虑很多因素。如下：\n1.数据采集的时效性与可访问性\n需要采集数据在采集频率方面有哪些要求？下游的处理要求数据多长时间准备完毕？\n\n2.增量更新\n如何添加新数据？需要将数据添加到现有数据库吗？需要重写现有数据吗？\n\n3.数据访问和处理\n数据会用于处理过程吗？如果会，数据会用于批处理任务吗？需要的数据是不是随机获取的？\n\n4.数据分区及数据分片\n数据采集后应该如何分区？需要将数据导入到多个目录系统（如HDFS与HBase）吗？\n\n5.数据存储格式\n数据存储的格式是哪一种？\n\n6.数据变换\n需要变换尚未落地的数据吗？\n\n下面简单列举一下这几点考量？\n\n##1.数据采集的时效性\n这里的时效性是指可进行数据采集的时间与Hadoop中工具可访问数据的时间之间的间隔。采集架构的时间分类会对存储媒介和采集方法造成很大的影响。一般来说数据采集构架，可以使用以下分类中的一个\na.大型批处理\n通常指15分钟到数据小时的任务，有时可能指时间跨度达到一天的任务\n\nb.小型批处理\n通常指大约2分钟发送一次任务，但是总的来说不会超过15分钟\n\nc.近实时决策支持\n指接受信息后“立即作出反应”，并在2秒到2分钟内发送数据\n\nd.近实时事件处理\n指在2秒内处理任务，速度可达到100毫秒\n\ne.实时\n这里指不超过100毫秒\n\n可以注意到随着实现时间到达实时，实现的复杂度和成本会大大增加。从批处理出发（比如使用简单文件传输）通常是个不错的选择。HDFS对时效性的要求比较宽松，所以可能更加适合成为主要存储位置。而一个简单文件传输或Sqoop任务则适合作为采集数据的工具。比如，执行hadoop fs -put命令将复制一个文件，并进行全面的校验，以确定正确地复制数据。\n使用hadoop fs -put命令与Sqoop时，你需要明白一点：HDFS上的数据存储格式可能并不适合数据的长期存储和处理。因此，在使用这些工具的时候，可能需要通过额外的批处理操作，以将数据存储为需要的格式。\n当用户需要从简单的批处理转向更高频率的更新时，就应该考虑Flume或kafka之类的工具了。这里时间要求不起过2分钟，所以Sqoop与文件转换器不适用。而且，因为要求时间不起过2分钟，所以存储层可能需要变成HBase或Solr，这样插入与读取操作会获得更细的粒度。当要求到实时水平时，我们首先需要考虑内存，然后是永久性存储。全世界所有的平行化处理都不会有助于将反应要求控制在500毫秒以内，只要硬盘驱动器保持处理操作的状态。基于这一点，我们开始进入流处理领域，采用Storm或Spark Streaming之类的工具。这里需要强调的是，这些工具应该真正用于大数据处理，而不是像Flume或Sqoop那样用于数据采集。\n\n##2.增量更新\n新的数据是要添加到已有数据集中，还是要修改已有数据集。如果仅要求添加数据，那么HDFS对于大部分实现都很适用。HDFS能够并行 化多个驱动器的I/O操作，所以读写性能很高。HDFS的缺点是无法添加或者随机写入创建后的文件。\n\n#数据采集的选择\n##1.文件传输\n将数据导入导出到Hadoop最简单的方法就是文件传输，就是hadoop fs -put与hadoop fs -get命令。这有时也是最快的方法，所以在设计Hadoop新的数据处理流水线时，首先应该考虑选择文件传输。\n\n下面列一下文件传输的特点：\na.这是一种all-or-nothing批处理方法，所以如果文件传输过程中出现错误，则不会写入或读取任何数据。这种方法与Flume、Kafka之类的采集方法不同，后者提供一定程度的错误处理功能，并且有传输保障。\nb.文件传输默认为单线程，不能并行 文件传输。\nc.文件传输将文件从传统的文件系统导入HDFS\nd.不支持数据转换，数据按原样导入HDFS。数据导入HDFS后才能进行处理，这一点与传输过程中的数据转换截然相反。类似于Flume的系统支持传输过程中的数据转换。\ne.这种加载是逐字进行的，所以能传输任何类型的文件（文件、二进制、图书等）\n\n##文件传输与其他采集方法的考量\n简单文件传输在某些情况下是适用的，尤其是在需要将已存在 的一系列文件输入到HDFS中，而且可以接受保持源文件格式的情况下。否则，在决定是否可以接受文件传输或者是否使用类似于Flume的工具时，需要考虑以下因素。\na.需要将数据采集到多个位置吗？比如，是需要将数据同时输入HDFS和Solr，还是需要将数据同时输入HDFS和HBase？这种情况下，如果使用文件传输，那么在文件采集完成之后将需要额外的工作，因些采用Flume更合适。\nb.对可靠性的要求高不高？如果高，那么一旦传输时出现错误，文件传输就必须重新开始，这时，Fluem同样是更好的选择。\ne.数据采集之前需要转换操作吗？如果需要Flume无疑是适合的工具。\n如果需要采集文件，可以考虑使用Flume Spooling  Directory源。采用这种方法，用户将文件放置到磁盘特定的目录就可以采集文件。这种采集文件的方法简单可靠，而且需要时能够实现传输过程中的数据转换。\n\n##Sqoop：Hadoop与关系数据库的批量传输\nSqoop是一种工具，能批量地将数据从关系型数据管理系统导出到Hadoop中，也能批量地将数据从Hadoop导出至关系型数据库中。\n\nFlume:基于事件的数据收集及处理\nFlume是一种分布式的可靠开源系统，用于流数据的高效收集、聚焦和移动。Flume通常用于移动日志数据，但是也能移动大量事件数据，如社交媒体订阅、消息队列事件或网络流量数据。\n\n##Kafka\nApache Kafka是一种发布订单消息的分布式系统，能够将消息归类为不同主题。应用程序能在Kafka上发布信息，或订阅主题进而接受特定主要下发布的消息。Producer发布消息，而Consumer收集并处理消息。作为分布式系统，Kafka在集群中运行，每个节点被称为Broker。\nKafka维护每个主题的分区日志。消息会发布到相应的主题中，每个分区都是一个有序的消息子集。同一个主题的多个分区能够通过集群中的多个Broker传送，这种方法提高了主题的容量与吞吐量，使其超越了单一机器所能提供的容量与吞吐量。消息在分区内被有序排列，每个消息都包含一个特定的偏移量。Kafka中消息可以通过一个包含主题、分区以及偏移量的组合来确定。Producer能够根据消息的主键选择消息应该写入哪一个分区，也能够简单地用循环的方式，让消息分布在各分区之间。\nConsumer会在Consumer组中注册，每个组包括一个或多个Consumer，每个Consumer读取一个或多个主题分区。每组中的每条消息只能传送给一个Consumer。但是，如果多个组订阅了同一个主题，那么每个组都将得到所有的消息。一个组中包含多个Consumer有助于获得加载平衡（可以支持高于单个Consumer处理能力的吞吐量）与高可用性（如果一个Consumer出现错误，它所读取的分区将重新分配给组中其它Consumer）。\n前面提到，对于应用层面的数据分类，主要单位是主题。一个Consumer或Consumer组将读取其订阅主题的所有数据，所以如果一个应用只关注一个数据子集，那么就应该将该数据子集与其他数据放在两个不同的主题中。如果多个信息集总是一起读取和处理，那么应该将它们归在同一个主题中。\n\n#数据导出\n数据导出的思路与导入类似。\n\n\n\n\n\n\n","slug":"hadoop/Hadoop之数据采集","published":1,"updated":"2017-08-18T01:44:35.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjh7polt3001lbp7r344ba31i","content":"<p>数据采集<br>本文主要是讲外部系统与Hadoop之间的数据传递，包括从外部系统采集数据导入到hadoop，以及从Hadoop中提取数据导入外部系统中。</p>\n<p>#数据采集考量<br>虽然Hadoop提出了文件客户端，便于在Hadoop中和Hadoop外复制文件，但是大多数据 Hadoop应用需要从不同来源导入数据，而且对不同的导入频率也提出了要求，Hadoop常用的数据来源包括以下：<br>1.传统数据管理系统，如果关系型数据库与主机<br>2.日志、机器生成的数据，以及其他类型的事件数据。<br>3.从现有的企业数据存储中输入的文件。</p>\n<p>将数据从不同的系统输入Hadoop时需要考虑很多因素。如下：<br>1.数据采集的时效性与可访问性<br>需要采集数据在采集频率方面有哪些要求？下游的处理要求数据多长时间准备完毕？</p>\n<p>2.增量更新<br>如何添加新数据？需要将数据添加到现有数据库吗？需要重写现有数据吗？</p>\n<p>3.数据访问和处理<br>数据会用于处理过程吗？如果会，数据会用于批处理任务吗？需要的数据是不是随机获取的？</p>\n<p>4.数据分区及数据分片<br>数据采集后应该如何分区？需要将数据导入到多个目录系统（如HDFS与HBase）吗？</p>\n<p>5.数据存储格式<br>数据存储的格式是哪一种？</p>\n<p>6.数据变换<br>需要变换尚未落地的数据吗？</p>\n<p>下面简单列举一下这几点考量？</p>\n<p>##1.数据采集的时效性<br>这里的时效性是指可进行数据采集的时间与Hadoop中工具可访问数据的时间之间的间隔。采集架构的时间分类会对存储媒介和采集方法造成很大的影响。一般来说数据采集构架，可以使用以下分类中的一个<br>a.大型批处理<br>通常指15分钟到数据小时的任务，有时可能指时间跨度达到一天的任务</p>\n<p>b.小型批处理<br>通常指大约2分钟发送一次任务，但是总的来说不会超过15分钟</p>\n<p>c.近实时决策支持<br>指接受信息后“立即作出反应”，并在2秒到2分钟内发送数据</p>\n<p>d.近实时事件处理<br>指在2秒内处理任务，速度可达到100毫秒</p>\n<p>e.实时<br>这里指不超过100毫秒</p>\n<p>可以注意到随着实现时间到达实时，实现的复杂度和成本会大大增加。从批处理出发（比如使用简单文件传输）通常是个不错的选择。HDFS对时效性的要求比较宽松，所以可能更加适合成为主要存储位置。而一个简单文件传输或Sqoop任务则适合作为采集数据的工具。比如，执行hadoop fs -put命令将复制一个文件，并进行全面的校验，以确定正确地复制数据。<br>使用hadoop fs -put命令与Sqoop时，你需要明白一点：HDFS上的数据存储格式可能并不适合数据的长期存储和处理。因此，在使用这些工具的时候，可能需要通过额外的批处理操作，以将数据存储为需要的格式。<br>当用户需要从简单的批处理转向更高频率的更新时，就应该考虑Flume或kafka之类的工具了。这里时间要求不起过2分钟，所以Sqoop与文件转换器不适用。而且，因为要求时间不起过2分钟，所以存储层可能需要变成HBase或Solr，这样插入与读取操作会获得更细的粒度。当要求到实时水平时，我们首先需要考虑内存，然后是永久性存储。全世界所有的平行化处理都不会有助于将反应要求控制在500毫秒以内，只要硬盘驱动器保持处理操作的状态。基于这一点，我们开始进入流处理领域，采用Storm或Spark Streaming之类的工具。这里需要强调的是，这些工具应该真正用于大数据处理，而不是像Flume或Sqoop那样用于数据采集。</p>\n<p>##2.增量更新<br>新的数据是要添加到已有数据集中，还是要修改已有数据集。如果仅要求添加数据，那么HDFS对于大部分实现都很适用。HDFS能够并行 化多个驱动器的I/O操作，所以读写性能很高。HDFS的缺点是无法添加或者随机写入创建后的文件。</p>\n<p>#数据采集的选择</p>\n<p>##1.文件传输<br>将数据导入导出到Hadoop最简单的方法就是文件传输，就是hadoop fs -put与hadoop fs -get命令。这有时也是最快的方法，所以在设计Hadoop新的数据处理流水线时，首先应该考虑选择文件传输。</p>\n<p>下面列一下文件传输的特点：<br>a.这是一种all-or-nothing批处理方法，所以如果文件传输过程中出现错误，则不会写入或读取任何数据。这种方法与Flume、Kafka之类的采集方法不同，后者提供一定程度的错误处理功能，并且有传输保障。<br>b.文件传输默认为单线程，不能并行 文件传输。<br>c.文件传输将文件从传统的文件系统导入HDFS<br>d.不支持数据转换，数据按原样导入HDFS。数据导入HDFS后才能进行处理，这一点与传输过程中的数据转换截然相反。类似于Flume的系统支持传输过程中的数据转换。<br>e.这种加载是逐字进行的，所以能传输任何类型的文件（文件、二进制、图书等）</p>\n<p>##文件传输与其他采集方法的考量<br>简单文件传输在某些情况下是适用的，尤其是在需要将已存在 的一系列文件输入到HDFS中，而且可以接受保持源文件格式的情况下。否则，在决定是否可以接受文件传输或者是否使用类似于Flume的工具时，需要考虑以下因素。<br>a.需要将数据采集到多个位置吗？比如，是需要将数据同时输入HDFS和Solr，还是需要将数据同时输入HDFS和HBase？这种情况下，如果使用文件传输，那么在文件采集完成之后将需要额外的工作，因些采用Flume更合适。<br>b.对可靠性的要求高不高？如果高，那么一旦传输时出现错误，文件传输就必须重新开始，这时，Fluem同样是更好的选择。<br>e.数据采集之前需要转换操作吗？如果需要Flume无疑是适合的工具。<br>如果需要采集文件，可以考虑使用Flume Spooling  Directory源。采用这种方法，用户将文件放置到磁盘特定的目录就可以采集文件。这种采集文件的方法简单可靠，而且需要时能够实现传输过程中的数据转换。</p>\n<p>##Sqoop：Hadoop与关系数据库的批量传输<br>Sqoop是一种工具，能批量地将数据从关系型数据管理系统导出到Hadoop中，也能批量地将数据从Hadoop导出至关系型数据库中。</p>\n<p>Flume:基于事件的数据收集及处理<br>Flume是一种分布式的可靠开源系统，用于流数据的高效收集、聚焦和移动。Flume通常用于移动日志数据，但是也能移动大量事件数据，如社交媒体订阅、消息队列事件或网络流量数据。</p>\n<p>##Kafka<br>Apache Kafka是一种发布订单消息的分布式系统，能够将消息归类为不同主题。应用程序能在Kafka上发布信息，或订阅主题进而接受特定主要下发布的消息。Producer发布消息，而Consumer收集并处理消息。作为分布式系统，Kafka在集群中运行，每个节点被称为Broker。<br>Kafka维护每个主题的分区日志。消息会发布到相应的主题中，每个分区都是一个有序的消息子集。同一个主题的多个分区能够通过集群中的多个Broker传送，这种方法提高了主题的容量与吞吐量，使其超越了单一机器所能提供的容量与吞吐量。消息在分区内被有序排列，每个消息都包含一个特定的偏移量。Kafka中消息可以通过一个包含主题、分区以及偏移量的组合来确定。Producer能够根据消息的主键选择消息应该写入哪一个分区，也能够简单地用循环的方式，让消息分布在各分区之间。<br>Consumer会在Consumer组中注册，每个组包括一个或多个Consumer，每个Consumer读取一个或多个主题分区。每组中的每条消息只能传送给一个Consumer。但是，如果多个组订阅了同一个主题，那么每个组都将得到所有的消息。一个组中包含多个Consumer有助于获得加载平衡（可以支持高于单个Consumer处理能力的吞吐量）与高可用性（如果一个Consumer出现错误，它所读取的分区将重新分配给组中其它Consumer）。<br>前面提到，对于应用层面的数据分类，主要单位是主题。一个Consumer或Consumer组将读取其订阅主题的所有数据，所以如果一个应用只关注一个数据子集，那么就应该将该数据子集与其他数据放在两个不同的主题中。如果多个信息集总是一起读取和处理，那么应该将它们归在同一个主题中。</p>\n<p>#数据导出<br>数据导出的思路与导入类似。</p>\n","site":{"data":{}},"excerpt":"","more":"<p>数据采集<br>本文主要是讲外部系统与Hadoop之间的数据传递，包括从外部系统采集数据导入到hadoop，以及从Hadoop中提取数据导入外部系统中。</p>\n<p>#数据采集考量<br>虽然Hadoop提出了文件客户端，便于在Hadoop中和Hadoop外复制文件，但是大多数据 Hadoop应用需要从不同来源导入数据，而且对不同的导入频率也提出了要求，Hadoop常用的数据来源包括以下：<br>1.传统数据管理系统，如果关系型数据库与主机<br>2.日志、机器生成的数据，以及其他类型的事件数据。<br>3.从现有的企业数据存储中输入的文件。</p>\n<p>将数据从不同的系统输入Hadoop时需要考虑很多因素。如下：<br>1.数据采集的时效性与可访问性<br>需要采集数据在采集频率方面有哪些要求？下游的处理要求数据多长时间准备完毕？</p>\n<p>2.增量更新<br>如何添加新数据？需要将数据添加到现有数据库吗？需要重写现有数据吗？</p>\n<p>3.数据访问和处理<br>数据会用于处理过程吗？如果会，数据会用于批处理任务吗？需要的数据是不是随机获取的？</p>\n<p>4.数据分区及数据分片<br>数据采集后应该如何分区？需要将数据导入到多个目录系统（如HDFS与HBase）吗？</p>\n<p>5.数据存储格式<br>数据存储的格式是哪一种？</p>\n<p>6.数据变换<br>需要变换尚未落地的数据吗？</p>\n<p>下面简单列举一下这几点考量？</p>\n<p>##1.数据采集的时效性<br>这里的时效性是指可进行数据采集的时间与Hadoop中工具可访问数据的时间之间的间隔。采集架构的时间分类会对存储媒介和采集方法造成很大的影响。一般来说数据采集构架，可以使用以下分类中的一个<br>a.大型批处理<br>通常指15分钟到数据小时的任务，有时可能指时间跨度达到一天的任务</p>\n<p>b.小型批处理<br>通常指大约2分钟发送一次任务，但是总的来说不会超过15分钟</p>\n<p>c.近实时决策支持<br>指接受信息后“立即作出反应”，并在2秒到2分钟内发送数据</p>\n<p>d.近实时事件处理<br>指在2秒内处理任务，速度可达到100毫秒</p>\n<p>e.实时<br>这里指不超过100毫秒</p>\n<p>可以注意到随着实现时间到达实时，实现的复杂度和成本会大大增加。从批处理出发（比如使用简单文件传输）通常是个不错的选择。HDFS对时效性的要求比较宽松，所以可能更加适合成为主要存储位置。而一个简单文件传输或Sqoop任务则适合作为采集数据的工具。比如，执行hadoop fs -put命令将复制一个文件，并进行全面的校验，以确定正确地复制数据。<br>使用hadoop fs -put命令与Sqoop时，你需要明白一点：HDFS上的数据存储格式可能并不适合数据的长期存储和处理。因此，在使用这些工具的时候，可能需要通过额外的批处理操作，以将数据存储为需要的格式。<br>当用户需要从简单的批处理转向更高频率的更新时，就应该考虑Flume或kafka之类的工具了。这里时间要求不起过2分钟，所以Sqoop与文件转换器不适用。而且，因为要求时间不起过2分钟，所以存储层可能需要变成HBase或Solr，这样插入与读取操作会获得更细的粒度。当要求到实时水平时，我们首先需要考虑内存，然后是永久性存储。全世界所有的平行化处理都不会有助于将反应要求控制在500毫秒以内，只要硬盘驱动器保持处理操作的状态。基于这一点，我们开始进入流处理领域，采用Storm或Spark Streaming之类的工具。这里需要强调的是，这些工具应该真正用于大数据处理，而不是像Flume或Sqoop那样用于数据采集。</p>\n<p>##2.增量更新<br>新的数据是要添加到已有数据集中，还是要修改已有数据集。如果仅要求添加数据，那么HDFS对于大部分实现都很适用。HDFS能够并行 化多个驱动器的I/O操作，所以读写性能很高。HDFS的缺点是无法添加或者随机写入创建后的文件。</p>\n<p>#数据采集的选择</p>\n<p>##1.文件传输<br>将数据导入导出到Hadoop最简单的方法就是文件传输，就是hadoop fs -put与hadoop fs -get命令。这有时也是最快的方法，所以在设计Hadoop新的数据处理流水线时，首先应该考虑选择文件传输。</p>\n<p>下面列一下文件传输的特点：<br>a.这是一种all-or-nothing批处理方法，所以如果文件传输过程中出现错误，则不会写入或读取任何数据。这种方法与Flume、Kafka之类的采集方法不同，后者提供一定程度的错误处理功能，并且有传输保障。<br>b.文件传输默认为单线程，不能并行 文件传输。<br>c.文件传输将文件从传统的文件系统导入HDFS<br>d.不支持数据转换，数据按原样导入HDFS。数据导入HDFS后才能进行处理，这一点与传输过程中的数据转换截然相反。类似于Flume的系统支持传输过程中的数据转换。<br>e.这种加载是逐字进行的，所以能传输任何类型的文件（文件、二进制、图书等）</p>\n<p>##文件传输与其他采集方法的考量<br>简单文件传输在某些情况下是适用的，尤其是在需要将已存在 的一系列文件输入到HDFS中，而且可以接受保持源文件格式的情况下。否则，在决定是否可以接受文件传输或者是否使用类似于Flume的工具时，需要考虑以下因素。<br>a.需要将数据采集到多个位置吗？比如，是需要将数据同时输入HDFS和Solr，还是需要将数据同时输入HDFS和HBase？这种情况下，如果使用文件传输，那么在文件采集完成之后将需要额外的工作，因些采用Flume更合适。<br>b.对可靠性的要求高不高？如果高，那么一旦传输时出现错误，文件传输就必须重新开始，这时，Fluem同样是更好的选择。<br>e.数据采集之前需要转换操作吗？如果需要Flume无疑是适合的工具。<br>如果需要采集文件，可以考虑使用Flume Spooling  Directory源。采用这种方法，用户将文件放置到磁盘特定的目录就可以采集文件。这种采集文件的方法简单可靠，而且需要时能够实现传输过程中的数据转换。</p>\n<p>##Sqoop：Hadoop与关系数据库的批量传输<br>Sqoop是一种工具，能批量地将数据从关系型数据管理系统导出到Hadoop中，也能批量地将数据从Hadoop导出至关系型数据库中。</p>\n<p>Flume:基于事件的数据收集及处理<br>Flume是一种分布式的可靠开源系统，用于流数据的高效收集、聚焦和移动。Flume通常用于移动日志数据，但是也能移动大量事件数据，如社交媒体订阅、消息队列事件或网络流量数据。</p>\n<p>##Kafka<br>Apache Kafka是一种发布订单消息的分布式系统，能够将消息归类为不同主题。应用程序能在Kafka上发布信息，或订阅主题进而接受特定主要下发布的消息。Producer发布消息，而Consumer收集并处理消息。作为分布式系统，Kafka在集群中运行，每个节点被称为Broker。<br>Kafka维护每个主题的分区日志。消息会发布到相应的主题中，每个分区都是一个有序的消息子集。同一个主题的多个分区能够通过集群中的多个Broker传送，这种方法提高了主题的容量与吞吐量，使其超越了单一机器所能提供的容量与吞吐量。消息在分区内被有序排列，每个消息都包含一个特定的偏移量。Kafka中消息可以通过一个包含主题、分区以及偏移量的组合来确定。Producer能够根据消息的主键选择消息应该写入哪一个分区，也能够简单地用循环的方式，让消息分布在各分区之间。<br>Consumer会在Consumer组中注册，每个组包括一个或多个Consumer，每个Consumer读取一个或多个主题分区。每组中的每条消息只能传送给一个Consumer。但是，如果多个组订阅了同一个主题，那么每个组都将得到所有的消息。一个组中包含多个Consumer有助于获得加载平衡（可以支持高于单个Consumer处理能力的吞吐量）与高可用性（如果一个Consumer出现错误，它所读取的分区将重新分配给组中其它Consumer）。<br>前面提到，对于应用层面的数据分类，主要单位是主题。一个Consumer或Consumer组将读取其订阅主题的所有数据，所以如果一个应用只关注一个数据子集，那么就应该将该数据子集与其他数据放在两个不同的主题中。如果多个信息集总是一起读取和处理，那么应该将它们归在同一个主题中。</p>\n<p>#数据导出<br>数据导出的思路与导入类似。</p>\n"},{"title":"流式计算","date":"2017-04-16T15:43:49.000Z","_content":"MapRecue及其扩展解决了离线批处理问题，但是无法保证实时性。对于实时性要求高的场景，可以采用流式计算或者实时分析系统进行处理。\n流式计算（Stream Processing）解决在线聚合（Online Aggregation）、在线过滤（Online Filter）等问题，流式计算同时具有存储系统和计算系统的特点，经常应用在一些类似于反作弊、交易异常监控等场景。流式计算的操作算子和时间相关，处理最近一段时间窗口内的数据。\n\n#原理\n流式计算强调的是数据流的实时性。MapRecue系统主要解决的是对静态数据的批量处理，当MapRecue作业启动时，已经准备好了输入数据，比如保存在分布式文件系统上。而流式计算系统在启动时，输入数据一般并没有完全到位，而是经由外部数据流源源不断地流入。另外，流式计算并不像批处理系统那样，重视数据处理的总吞吐量，而是更加重视对数据处理的延迟。\nMapRecue及其扩展采用的是一种比较静态的模型，如果用它来做数据流的处理，首先需要将数据流缓存并分块，然后放入集群计算。如果MapRecue每次处理的数据量较小，缓存数据流的时间较短，但是，MapRecue框架造成的额外开销将会占用很大比重；如果MapRecue每次处理的数据量较大，缓存数据流的时间会很长，无法满足实时性的要求。\n\n#Yahoo S4\n\n\n#Twitter Stoorm\n\n\n\n\n\n\n\n","source":"_posts/hadoop/Hadoop之流式计算.md","raw":"---\ntitle: 流式计算\ndate: 2017-04-16 23:43:49\ntags: [大数据,流式计算]\ncategories: [大数据,流式计算]\n---\nMapRecue及其扩展解决了离线批处理问题，但是无法保证实时性。对于实时性要求高的场景，可以采用流式计算或者实时分析系统进行处理。\n流式计算（Stream Processing）解决在线聚合（Online Aggregation）、在线过滤（Online Filter）等问题，流式计算同时具有存储系统和计算系统的特点，经常应用在一些类似于反作弊、交易异常监控等场景。流式计算的操作算子和时间相关，处理最近一段时间窗口内的数据。\n\n#原理\n流式计算强调的是数据流的实时性。MapRecue系统主要解决的是对静态数据的批量处理，当MapRecue作业启动时，已经准备好了输入数据，比如保存在分布式文件系统上。而流式计算系统在启动时，输入数据一般并没有完全到位，而是经由外部数据流源源不断地流入。另外，流式计算并不像批处理系统那样，重视数据处理的总吞吐量，而是更加重视对数据处理的延迟。\nMapRecue及其扩展采用的是一种比较静态的模型，如果用它来做数据流的处理，首先需要将数据流缓存并分块，然后放入集群计算。如果MapRecue每次处理的数据量较小，缓存数据流的时间较短，但是，MapRecue框架造成的额外开销将会占用很大比重；如果MapRecue每次处理的数据量较大，缓存数据流的时间会很长，无法满足实时性的要求。\n\n#Yahoo S4\n\n\n#Twitter Stoorm\n\n\n\n\n\n\n\n","slug":"hadoop/Hadoop之流式计算","published":1,"updated":"2017-08-18T01:44:35.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjh7polt4001nbp7rgk74wma7","content":"<p>MapRecue及其扩展解决了离线批处理问题，但是无法保证实时性。对于实时性要求高的场景，可以采用流式计算或者实时分析系统进行处理。<br>流式计算（Stream Processing）解决在线聚合（Online Aggregation）、在线过滤（Online Filter）等问题，流式计算同时具有存储系统和计算系统的特点，经常应用在一些类似于反作弊、交易异常监控等场景。流式计算的操作算子和时间相关，处理最近一段时间窗口内的数据。</p>\n<p>#原理<br>流式计算强调的是数据流的实时性。MapRecue系统主要解决的是对静态数据的批量处理，当MapRecue作业启动时，已经准备好了输入数据，比如保存在分布式文件系统上。而流式计算系统在启动时，输入数据一般并没有完全到位，而是经由外部数据流源源不断地流入。另外，流式计算并不像批处理系统那样，重视数据处理的总吞吐量，而是更加重视对数据处理的延迟。<br>MapRecue及其扩展采用的是一种比较静态的模型，如果用它来做数据流的处理，首先需要将数据流缓存并分块，然后放入集群计算。如果MapRecue每次处理的数据量较小，缓存数据流的时间较短，但是，MapRecue框架造成的额外开销将会占用很大比重；如果MapRecue每次处理的数据量较大，缓存数据流的时间会很长，无法满足实时性的要求。</p>\n<p>#Yahoo S4</p>\n<p>#Twitter Stoorm</p>\n","site":{"data":{}},"excerpt":"","more":"<p>MapRecue及其扩展解决了离线批处理问题，但是无法保证实时性。对于实时性要求高的场景，可以采用流式计算或者实时分析系统进行处理。<br>流式计算（Stream Processing）解决在线聚合（Online Aggregation）、在线过滤（Online Filter）等问题，流式计算同时具有存储系统和计算系统的特点，经常应用在一些类似于反作弊、交易异常监控等场景。流式计算的操作算子和时间相关，处理最近一段时间窗口内的数据。</p>\n<p>#原理<br>流式计算强调的是数据流的实时性。MapRecue系统主要解决的是对静态数据的批量处理，当MapRecue作业启动时，已经准备好了输入数据，比如保存在分布式文件系统上。而流式计算系统在启动时，输入数据一般并没有完全到位，而是经由外部数据流源源不断地流入。另外，流式计算并不像批处理系统那样，重视数据处理的总吞吐量，而是更加重视对数据处理的延迟。<br>MapRecue及其扩展采用的是一种比较静态的模型，如果用它来做数据流的处理，首先需要将数据流缓存并分块，然后放入集群计算。如果MapRecue每次处理的数据量较小，缓存数据流的时间较短，但是，MapRecue框架造成的额外开销将会占用很大比重；如果MapRecue每次处理的数据量较大，缓存数据流的时间会很长，无法满足实时性的要求。</p>\n<p>#Yahoo S4</p>\n<p>#Twitter Stoorm</p>\n"},{"title":"Hadoop知识点","date":"2017-05-08T15:43:49.000Z","_content":"#Hadoop数据管理\n主要包括Hadoop的分布式文件系统HDFS、分布式数据库HBase和数据仓库工具Hive\n\n##HDFS的数据管理\nHDFS是分布式计算的存储基石，Hadoop分布式文件系统和其它文件系统有很多类似的特性；  \n1）对于整个集群有单一的命名空间；  \n2）具有数据一致性，都适合一次写入多次读取的模型，客户端在文件没有被成功创建之前是无法看到文件存在的。  \n3）文件会被分割成多个文件块，每个文件块被分配存储到数据节点上，而且会根据配置由复制文件块来保证数据的安全性。  \nHDFS有三个重要的角色来进行文件系统的管理：NameNode、DataNode和Client。NameNode可以看做是分布式文件系统的管理者，主要负责管理文件系统的命名空间、集群配置信息和存储块的复制等。NameNode会将文件系统的Metadata存储在内存中，这些信息主要包括文件信息、每一个文件对应的文件块的信息和每一个文件块在DataNode中的信息等。DataNode是文件存储的基本单元，它将文件块（Block）存储在本地文件系统中，保存了所有Block的Metadata，同时周期性地将所有存在的Block信息发送给NameNode。Client就是需要获取分布式文件系统文件的应用程序。接下来三个具体的操作来说明HDFS对数据的管理。  \n（1）文件写入\n1）Client向NameNode发起文件写入的请求。  \n2）NameNode根据文件的大小和文件块配置情况，返回给Client所管理的DataNode的信息。  \n3）Client将文件划分为多个Block，根据DataNode的地址信息，按顺序将其写入到每一个DataNode块中。  \n（2）文件读取  \n1）Client向NameNode发起文件读取的请求。  \n2）NameNode返回文件存储的DataNode信息。  \n3）Client读取文件信息。  \n（3）文件块（Block）复制  \n1）NameNode发现部分文件的Block不符合最小复制数这一要求或部分DataNode失效。  \n2）通知DataNode相互复制Block。  \n3）DataNode开始直接相互复制。  \n\n作为分布式文件系统，HDFS在数据管理方面还有值得借鉴的几个功能：  \na.文件块（Block）的放置：一个Block会有三份备份，一份放在NameNode指定的DataNode上，另一份放在与指定DataNode不在同一机器上的DataNode上，最后一份放在与指定DataNode同一Rack的DataNode上。备份的目的是为了数据安全，采用这种配置方式主要是考虑同一Rack失败的情况，以及不同Rack之间进行数据复制会带来的性能问题。  \nb.心跳检测：用心跳检测DataNode的健康状况，如果发现问题就采取数据备份的方式来保证数据的安全性。  \nc.数据复制（场景为DataNode失败、需要平衡DataNode的存储利用率和平衡DataNode数据交互压力等情况）；使用Hadoop时可以用HDFS的balancer命令配置Threshold来平衡每一个DataNode的磁盘利用率。假设设置了Threshold为10%，那么执行balancer命令时，首先会统计所有的DataNode的磁盘利用率的平均值，然后判断如果某一个DataNode的磁盘利用率超过这个平均值，那么将会把这个DataNode的Block转移到磁盘利用率低的DataNode上，这对于新的节点为加入十分有用。  \nd.数据校验：采用CRC32做数据校验。在写入文件块的时候，除了会写入数据外还会写入校验信息，在读取的时候则需要先校验后读入。  \ne.数据管道性的写入：当客户端要写入文件到DataNode上时，首先会读取一个Block，然后将其写到每一个DataNode上，接着由第一个DataNode将其传递到备份的DataNode上，直到所有需要写入这个Block的DataNode都成功写入后，客户端才会开始写下一个Block。  \nf.安全模型：分布式文件系统启动时会进入安全模式（系统运行期间也可以通过命令进入安全模式），当分布式文件处于安全模式时，文件系统中的内容不允许修改也不允许删除，直到安DataNode上数据块的有效性，同时根据策略进行必要的复制或删除部分数据块。在实际操作过程中，如果在系统启动时修改和删除文件会出现安全模式不允许修改的错误提示，只需要等待一会即可。  \n\n\n##HBase的数据管理\nHBase是一个类似Bigtable的分布式数据库，它的大部分特性和Bigtable一样，是一个稀疏的、长期存储的（存在硬盘上）、多维度的排序映射表，这张表的索引是行关键字、列关键字和时间戳。表中的每个值是一个纯字符数组，数据都是字符串，没有类型，所以同一张表中的每一行数据都可以有截然不同的列。列名字的格式是“<family>:<label>\"，它是由字符串组成的，每一张表有一个family集合，这个集合是固定不变的，相当于表的结构，只能通过改变表结构来改变表的family集合。但是label值相对于每一行来说都是可以改变的。  \nHBase把同一个family中的数据存储在同一个目录下，而HB的写操作是锁行的。每一行都是一个原子元素，都可以加锁。所有数据库的更新都有一个时间戳标记，每次更新都会生成一个新的版本，而HBase会保留一定数量的版本，这个值是可以设定的。客户端可以选择获取距离某个时间点最近的版本，或者一次获取所有版本。  \n\n以上从微观上介绍了HBase的一些数据管理措施，那么HBase作为分布式数据为顺整体上从集群出发又是如何管理数据的呢？  \nHBase在分布式集群上主要依赖于HRegion、HMaster、HClient组成的体系结构从整体上管理数据。  \nHBase体系结构有三大重要组成部分：  \na.HBaseMaster：HBase主服务器，与Bigtable的主服务器类似。  \nb.HRegionServer：HBase域服务器，与Bigtable的Tablet服务器类似。  \nc.Hbase Client：HBase客户端是由org.apache.hadoop.Hbase.client.HTable定义的。  \n下面将对这三个组件进行详细的介绍。  \n（1）HBaseMaster  \n一个HBase只部署一台主服务器，它通过领导选举算法确保只有唯一的主服务器是活跃的，ZooKeeper保存主服务器的服务器地址信息。如果主服务器瘫痪，可以通过领导选举算法从备用服务器中选择新的主服务器。  \n主服务器承担着初始化集群的任务。当主服务器每一次启动时，会试图从HDFS获取根或根域目录，如果获取失败则创建根或根域目录，以及第一个元域目录。在下次启动时，主服务器就可以获取集群和集群中所有域 的信息了。同时主服务器还负责集群中域的分配、域服务器运行状态的监控、表格的管理等工作。  \n\n\n（2）HRegionServer  \nHBase域服务器的主要职责有服务于主服务器分配的域、处理端的读写请求、本地缓冲回写、本地数据压缩和分割域等功能。  \n每个域只能由一台域服务器来提供服务。当它开始服务于某域时，它会从HDFS文件系统中读取该域的日志和所有存储文件，同时还会管理操作HDFS文件的持久性存储工作。客户端通过与主服务器通信获取域或域服务器的列表信息后，就可以直接向域服务器发送域读写请求，来完成操作。  \n\n（3）HBaseClient  \n HBase客户端负责查找用户域所在的域服务器地址。HBase客户端会与HBase主机交换消息以查找根域的位置，这是两者之间唯一的交流。  \n定位根域后，客户端连接根域所在的服务器，并扫描根域获取元域信息。元域信息中包含所需用户域的域服务器地址。客户端再连接元域所在的服务器，扫描元域以获取所需用户域所在的域服务器地址。客户端再连接元域所在的域服务器，扫描元域以获取所需用户域所有的域服务器地址。定位用户域后，客户端连接用户域所在的域服务器并发出读写请求。用户域的地址将在客户端被缓存，后续的请求无须重复上述过程。  \n\n综上所述，HBase的体系结构中，HBase主要由主服务器、域服务器和客户端三部分组成。主服务器作为HBase的中心，管理整个集群中的所有域，监控每台域服务器的运行情况等；域服务器接收来自服务器的分配域，处理管理端的域读写请求并回写映射文件等；客户端主要用来查找用户域所在的域服务器地址信息。  \n\n\n##Hive的数据管理  \nHive是建立在Hadoop上的数据仓库基础架构。它提供了一系列的工具，用来进行数据提取、转化、加载，这是一种可以存储、查询和分析存储在Hadoop中的大规模数据的机制。Hive定义了简单的类SQL的查询语言，称为HiveQL，它允许熟悉SQL的用户用SQL语言查询数据。作为一个数据仓库，Hive的数据管理按照使用层次可以从元数据存储、数据存储和数据交换三方面来介绍。  \n（1）元数据存储  \nHive将元数据存储在RDBMS中，有三种模式可以连接到数据库。  \n1）Single User Mode:此模式连接到一个In-memory的数据库Derby，一般用于Unit Test。  \n2）Multi User Mode：通过网络连接到一个数据库中，这是最常用的模式。  \n3）Remote Server Mode：用于非Java客户端访问元数据，在服务器端启动一个MetaStoreServer，客户端利用Thrift协议通过MetaStoreServer来访问元数据库。  \n（2）数据存储  \n首先，Hive没有专门的数据存储格式，也没有为数据建立索引，用户可以非常自由地组织Hive中的表，只需要在创建表的时候告诉Hive数据中的列分隔符和行分隔符，它就可以解析数据了。  \n其次，Hive中所有的数据都存储在HDFS中，Hive中包含4种数据模型：Table、External Table、Partition和Bucket。  \nHive中的Table和数据库中的Table在概念上是类似的，每一个Table在Hive中都有一个相应的目录来存储数据。例如，一个表pvs，它在HDFS中的路径为:/wh/pvs，其中wh是在hive-site.xml中由${hive.metastore.warehouse.dir}指定的数据仓库的目录，所有的Table数据（不包括External Table）都保存在这个目录中。  \n（3）数据交换\n数据交换主要分为以下部分，如图：\n\n\n\na）用户接口：包括客户端、Web界面和数据库接口。  \nb)元数据存储：通常存在在关系型数据库中，如MYSQL、Derby中。  \nc)解释器、编译器、优化器、执行器。  \nd)Hadoop：利用HDFS进行存储，利用MapRecue进行计算。  \n用户接口主要有三个：客户端、数据库接口和Web界面，其中最常用的是客户端。Client是Hive的客户端，当启动Client模式时，用户会想要连接Hive Server，这时需要指出Hive Server所在的节点，并且在该节点启动HiveServer。Web界面是通过浏览器访问Hive的。  \nHive元数据存储在数据库中，如MYSQL、Derby中。Hive中的元数据包括表的名字、表的列、表的分区、表分区的属性、表的属性、表的数据所在目录等。    \n解释器、编译器、优化器完成HiveQL查询语句从记法分析、语法分析、编译、优化到查询计划的生成。生成的查询计划存储在HDFS中，并且随后由MapRecue调用执行。  \nHive的数据存储在HDFS中，大部分的查询由MapRecue完成（包括*的查询不会生成MapRecue任务，比如select * from tbl).  \n\n\n\n\n#安装并运行Hadoop\n介绍Hadoop安装之前，先介绍一下Hadoop对各个节点的角色定义。\nHadoop分别从三个角度将主机划分为两种角色。第一，最基本的划分为Master和Slave，即主人和奴隶；第二，从HDFS的角度，将主机划分为NameNode和DataNode（在分布式文件系统中，目录的管理很重要，管理目录相当于主人，而NameNode就是目录管理者）；第三，从MapRecue的角度，将主机划分为JobTracker和TaskTracker（一个Job经常被划分为多个Task，从这个角度不难理解它们之间的关系）。\n","source":"_posts/hadoop/Hadoop知识点.md","raw":"---\ntitle: Hadoop知识点\ndate: 2017-05-08 23:43:49\ntags: [大数据,hadoop]\ncategories: [大数据,hadoop]\n---\n#Hadoop数据管理\n主要包括Hadoop的分布式文件系统HDFS、分布式数据库HBase和数据仓库工具Hive\n\n##HDFS的数据管理\nHDFS是分布式计算的存储基石，Hadoop分布式文件系统和其它文件系统有很多类似的特性；  \n1）对于整个集群有单一的命名空间；  \n2）具有数据一致性，都适合一次写入多次读取的模型，客户端在文件没有被成功创建之前是无法看到文件存在的。  \n3）文件会被分割成多个文件块，每个文件块被分配存储到数据节点上，而且会根据配置由复制文件块来保证数据的安全性。  \nHDFS有三个重要的角色来进行文件系统的管理：NameNode、DataNode和Client。NameNode可以看做是分布式文件系统的管理者，主要负责管理文件系统的命名空间、集群配置信息和存储块的复制等。NameNode会将文件系统的Metadata存储在内存中，这些信息主要包括文件信息、每一个文件对应的文件块的信息和每一个文件块在DataNode中的信息等。DataNode是文件存储的基本单元，它将文件块（Block）存储在本地文件系统中，保存了所有Block的Metadata，同时周期性地将所有存在的Block信息发送给NameNode。Client就是需要获取分布式文件系统文件的应用程序。接下来三个具体的操作来说明HDFS对数据的管理。  \n（1）文件写入\n1）Client向NameNode发起文件写入的请求。  \n2）NameNode根据文件的大小和文件块配置情况，返回给Client所管理的DataNode的信息。  \n3）Client将文件划分为多个Block，根据DataNode的地址信息，按顺序将其写入到每一个DataNode块中。  \n（2）文件读取  \n1）Client向NameNode发起文件读取的请求。  \n2）NameNode返回文件存储的DataNode信息。  \n3）Client读取文件信息。  \n（3）文件块（Block）复制  \n1）NameNode发现部分文件的Block不符合最小复制数这一要求或部分DataNode失效。  \n2）通知DataNode相互复制Block。  \n3）DataNode开始直接相互复制。  \n\n作为分布式文件系统，HDFS在数据管理方面还有值得借鉴的几个功能：  \na.文件块（Block）的放置：一个Block会有三份备份，一份放在NameNode指定的DataNode上，另一份放在与指定DataNode不在同一机器上的DataNode上，最后一份放在与指定DataNode同一Rack的DataNode上。备份的目的是为了数据安全，采用这种配置方式主要是考虑同一Rack失败的情况，以及不同Rack之间进行数据复制会带来的性能问题。  \nb.心跳检测：用心跳检测DataNode的健康状况，如果发现问题就采取数据备份的方式来保证数据的安全性。  \nc.数据复制（场景为DataNode失败、需要平衡DataNode的存储利用率和平衡DataNode数据交互压力等情况）；使用Hadoop时可以用HDFS的balancer命令配置Threshold来平衡每一个DataNode的磁盘利用率。假设设置了Threshold为10%，那么执行balancer命令时，首先会统计所有的DataNode的磁盘利用率的平均值，然后判断如果某一个DataNode的磁盘利用率超过这个平均值，那么将会把这个DataNode的Block转移到磁盘利用率低的DataNode上，这对于新的节点为加入十分有用。  \nd.数据校验：采用CRC32做数据校验。在写入文件块的时候，除了会写入数据外还会写入校验信息，在读取的时候则需要先校验后读入。  \ne.数据管道性的写入：当客户端要写入文件到DataNode上时，首先会读取一个Block，然后将其写到每一个DataNode上，接着由第一个DataNode将其传递到备份的DataNode上，直到所有需要写入这个Block的DataNode都成功写入后，客户端才会开始写下一个Block。  \nf.安全模型：分布式文件系统启动时会进入安全模式（系统运行期间也可以通过命令进入安全模式），当分布式文件处于安全模式时，文件系统中的内容不允许修改也不允许删除，直到安DataNode上数据块的有效性，同时根据策略进行必要的复制或删除部分数据块。在实际操作过程中，如果在系统启动时修改和删除文件会出现安全模式不允许修改的错误提示，只需要等待一会即可。  \n\n\n##HBase的数据管理\nHBase是一个类似Bigtable的分布式数据库，它的大部分特性和Bigtable一样，是一个稀疏的、长期存储的（存在硬盘上）、多维度的排序映射表，这张表的索引是行关键字、列关键字和时间戳。表中的每个值是一个纯字符数组，数据都是字符串，没有类型，所以同一张表中的每一行数据都可以有截然不同的列。列名字的格式是“<family>:<label>\"，它是由字符串组成的，每一张表有一个family集合，这个集合是固定不变的，相当于表的结构，只能通过改变表结构来改变表的family集合。但是label值相对于每一行来说都是可以改变的。  \nHBase把同一个family中的数据存储在同一个目录下，而HB的写操作是锁行的。每一行都是一个原子元素，都可以加锁。所有数据库的更新都有一个时间戳标记，每次更新都会生成一个新的版本，而HBase会保留一定数量的版本，这个值是可以设定的。客户端可以选择获取距离某个时间点最近的版本，或者一次获取所有版本。  \n\n以上从微观上介绍了HBase的一些数据管理措施，那么HBase作为分布式数据为顺整体上从集群出发又是如何管理数据的呢？  \nHBase在分布式集群上主要依赖于HRegion、HMaster、HClient组成的体系结构从整体上管理数据。  \nHBase体系结构有三大重要组成部分：  \na.HBaseMaster：HBase主服务器，与Bigtable的主服务器类似。  \nb.HRegionServer：HBase域服务器，与Bigtable的Tablet服务器类似。  \nc.Hbase Client：HBase客户端是由org.apache.hadoop.Hbase.client.HTable定义的。  \n下面将对这三个组件进行详细的介绍。  \n（1）HBaseMaster  \n一个HBase只部署一台主服务器，它通过领导选举算法确保只有唯一的主服务器是活跃的，ZooKeeper保存主服务器的服务器地址信息。如果主服务器瘫痪，可以通过领导选举算法从备用服务器中选择新的主服务器。  \n主服务器承担着初始化集群的任务。当主服务器每一次启动时，会试图从HDFS获取根或根域目录，如果获取失败则创建根或根域目录，以及第一个元域目录。在下次启动时，主服务器就可以获取集群和集群中所有域 的信息了。同时主服务器还负责集群中域的分配、域服务器运行状态的监控、表格的管理等工作。  \n\n\n（2）HRegionServer  \nHBase域服务器的主要职责有服务于主服务器分配的域、处理端的读写请求、本地缓冲回写、本地数据压缩和分割域等功能。  \n每个域只能由一台域服务器来提供服务。当它开始服务于某域时，它会从HDFS文件系统中读取该域的日志和所有存储文件，同时还会管理操作HDFS文件的持久性存储工作。客户端通过与主服务器通信获取域或域服务器的列表信息后，就可以直接向域服务器发送域读写请求，来完成操作。  \n\n（3）HBaseClient  \n HBase客户端负责查找用户域所在的域服务器地址。HBase客户端会与HBase主机交换消息以查找根域的位置，这是两者之间唯一的交流。  \n定位根域后，客户端连接根域所在的服务器，并扫描根域获取元域信息。元域信息中包含所需用户域的域服务器地址。客户端再连接元域所在的服务器，扫描元域以获取所需用户域所在的域服务器地址。客户端再连接元域所在的域服务器，扫描元域以获取所需用户域所有的域服务器地址。定位用户域后，客户端连接用户域所在的域服务器并发出读写请求。用户域的地址将在客户端被缓存，后续的请求无须重复上述过程。  \n\n综上所述，HBase的体系结构中，HBase主要由主服务器、域服务器和客户端三部分组成。主服务器作为HBase的中心，管理整个集群中的所有域，监控每台域服务器的运行情况等；域服务器接收来自服务器的分配域，处理管理端的域读写请求并回写映射文件等；客户端主要用来查找用户域所在的域服务器地址信息。  \n\n\n##Hive的数据管理  \nHive是建立在Hadoop上的数据仓库基础架构。它提供了一系列的工具，用来进行数据提取、转化、加载，这是一种可以存储、查询和分析存储在Hadoop中的大规模数据的机制。Hive定义了简单的类SQL的查询语言，称为HiveQL，它允许熟悉SQL的用户用SQL语言查询数据。作为一个数据仓库，Hive的数据管理按照使用层次可以从元数据存储、数据存储和数据交换三方面来介绍。  \n（1）元数据存储  \nHive将元数据存储在RDBMS中，有三种模式可以连接到数据库。  \n1）Single User Mode:此模式连接到一个In-memory的数据库Derby，一般用于Unit Test。  \n2）Multi User Mode：通过网络连接到一个数据库中，这是最常用的模式。  \n3）Remote Server Mode：用于非Java客户端访问元数据，在服务器端启动一个MetaStoreServer，客户端利用Thrift协议通过MetaStoreServer来访问元数据库。  \n（2）数据存储  \n首先，Hive没有专门的数据存储格式，也没有为数据建立索引，用户可以非常自由地组织Hive中的表，只需要在创建表的时候告诉Hive数据中的列分隔符和行分隔符，它就可以解析数据了。  \n其次，Hive中所有的数据都存储在HDFS中，Hive中包含4种数据模型：Table、External Table、Partition和Bucket。  \nHive中的Table和数据库中的Table在概念上是类似的，每一个Table在Hive中都有一个相应的目录来存储数据。例如，一个表pvs，它在HDFS中的路径为:/wh/pvs，其中wh是在hive-site.xml中由${hive.metastore.warehouse.dir}指定的数据仓库的目录，所有的Table数据（不包括External Table）都保存在这个目录中。  \n（3）数据交换\n数据交换主要分为以下部分，如图：\n\n\n\na）用户接口：包括客户端、Web界面和数据库接口。  \nb)元数据存储：通常存在在关系型数据库中，如MYSQL、Derby中。  \nc)解释器、编译器、优化器、执行器。  \nd)Hadoop：利用HDFS进行存储，利用MapRecue进行计算。  \n用户接口主要有三个：客户端、数据库接口和Web界面，其中最常用的是客户端。Client是Hive的客户端，当启动Client模式时，用户会想要连接Hive Server，这时需要指出Hive Server所在的节点，并且在该节点启动HiveServer。Web界面是通过浏览器访问Hive的。  \nHive元数据存储在数据库中，如MYSQL、Derby中。Hive中的元数据包括表的名字、表的列、表的分区、表分区的属性、表的属性、表的数据所在目录等。    \n解释器、编译器、优化器完成HiveQL查询语句从记法分析、语法分析、编译、优化到查询计划的生成。生成的查询计划存储在HDFS中，并且随后由MapRecue调用执行。  \nHive的数据存储在HDFS中，大部分的查询由MapRecue完成（包括*的查询不会生成MapRecue任务，比如select * from tbl).  \n\n\n\n\n#安装并运行Hadoop\n介绍Hadoop安装之前，先介绍一下Hadoop对各个节点的角色定义。\nHadoop分别从三个角度将主机划分为两种角色。第一，最基本的划分为Master和Slave，即主人和奴隶；第二，从HDFS的角度，将主机划分为NameNode和DataNode（在分布式文件系统中，目录的管理很重要，管理目录相当于主人，而NameNode就是目录管理者）；第三，从MapRecue的角度，将主机划分为JobTracker和TaskTracker（一个Job经常被划分为多个Task，从这个角度不难理解它们之间的关系）。\n","slug":"hadoop/Hadoop知识点","published":1,"updated":"2017-08-18T01:44:35.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjh7polt6001rbp7racatjt8y","content":"<p>#Hadoop数据管理<br>主要包括Hadoop的分布式文件系统HDFS、分布式数据库HBase和数据仓库工具Hive</p>\n<p>##HDFS的数据管理<br>HDFS是分布式计算的存储基石，Hadoop分布式文件系统和其它文件系统有很多类似的特性；<br>1）对于整个集群有单一的命名空间；<br>2）具有数据一致性，都适合一次写入多次读取的模型，客户端在文件没有被成功创建之前是无法看到文件存在的。<br>3）文件会被分割成多个文件块，每个文件块被分配存储到数据节点上，而且会根据配置由复制文件块来保证数据的安全性。<br>HDFS有三个重要的角色来进行文件系统的管理：NameNode、DataNode和Client。NameNode可以看做是分布式文件系统的管理者，主要负责管理文件系统的命名空间、集群配置信息和存储块的复制等。NameNode会将文件系统的Metadata存储在内存中，这些信息主要包括文件信息、每一个文件对应的文件块的信息和每一个文件块在DataNode中的信息等。DataNode是文件存储的基本单元，它将文件块（Block）存储在本地文件系统中，保存了所有Block的Metadata，同时周期性地将所有存在的Block信息发送给NameNode。Client就是需要获取分布式文件系统文件的应用程序。接下来三个具体的操作来说明HDFS对数据的管理。<br>（1）文件写入<br>1）Client向NameNode发起文件写入的请求。<br>2）NameNode根据文件的大小和文件块配置情况，返回给Client所管理的DataNode的信息。<br>3）Client将文件划分为多个Block，根据DataNode的地址信息，按顺序将其写入到每一个DataNode块中。<br>（2）文件读取<br>1）Client向NameNode发起文件读取的请求。<br>2）NameNode返回文件存储的DataNode信息。<br>3）Client读取文件信息。<br>（3）文件块（Block）复制<br>1）NameNode发现部分文件的Block不符合最小复制数这一要求或部分DataNode失效。<br>2）通知DataNode相互复制Block。<br>3）DataNode开始直接相互复制。  </p>\n<p>作为分布式文件系统，HDFS在数据管理方面还有值得借鉴的几个功能：<br>a.文件块（Block）的放置：一个Block会有三份备份，一份放在NameNode指定的DataNode上，另一份放在与指定DataNode不在同一机器上的DataNode上，最后一份放在与指定DataNode同一Rack的DataNode上。备份的目的是为了数据安全，采用这种配置方式主要是考虑同一Rack失败的情况，以及不同Rack之间进行数据复制会带来的性能问题。<br>b.心跳检测：用心跳检测DataNode的健康状况，如果发现问题就采取数据备份的方式来保证数据的安全性。<br>c.数据复制（场景为DataNode失败、需要平衡DataNode的存储利用率和平衡DataNode数据交互压力等情况）；使用Hadoop时可以用HDFS的balancer命令配置Threshold来平衡每一个DataNode的磁盘利用率。假设设置了Threshold为10%，那么执行balancer命令时，首先会统计所有的DataNode的磁盘利用率的平均值，然后判断如果某一个DataNode的磁盘利用率超过这个平均值，那么将会把这个DataNode的Block转移到磁盘利用率低的DataNode上，这对于新的节点为加入十分有用。<br>d.数据校验：采用CRC32做数据校验。在写入文件块的时候，除了会写入数据外还会写入校验信息，在读取的时候则需要先校验后读入。<br>e.数据管道性的写入：当客户端要写入文件到DataNode上时，首先会读取一个Block，然后将其写到每一个DataNode上，接着由第一个DataNode将其传递到备份的DataNode上，直到所有需要写入这个Block的DataNode都成功写入后，客户端才会开始写下一个Block。<br>f.安全模型：分布式文件系统启动时会进入安全模式（系统运行期间也可以通过命令进入安全模式），当分布式文件处于安全模式时，文件系统中的内容不允许修改也不允许删除，直到安DataNode上数据块的有效性，同时根据策略进行必要的复制或删除部分数据块。在实际操作过程中，如果在系统启动时修改和删除文件会出现安全模式不允许修改的错误提示，只需要等待一会即可。  </p>\n<p>##HBase的数据管理<br>HBase是一个类似Bigtable的分布式数据库，它的大部分特性和Bigtable一样，是一个稀疏的、长期存储的（存在硬盘上）、多维度的排序映射表，这张表的索引是行关键字、列关键字和时间戳。表中的每个值是一个纯字符数组，数据都是字符串，没有类型，所以同一张表中的每一行数据都可以有截然不同的列。列名字的格式是“<family>:<label>“，它是由字符串组成的，每一张表有一个family集合，这个集合是固定不变的，相当于表的结构，只能通过改变表结构来改变表的family集合。但是label值相对于每一行来说都是可以改变的。<br>HBase把同一个family中的数据存储在同一个目录下，而HB的写操作是锁行的。每一行都是一个原子元素，都可以加锁。所有数据库的更新都有一个时间戳标记，每次更新都会生成一个新的版本，而HBase会保留一定数量的版本，这个值是可以设定的。客户端可以选择获取距离某个时间点最近的版本，或者一次获取所有版本。  </label></family></p>\n<p>以上从微观上介绍了HBase的一些数据管理措施，那么HBase作为分布式数据为顺整体上从集群出发又是如何管理数据的呢？<br>HBase在分布式集群上主要依赖于HRegion、HMaster、HClient组成的体系结构从整体上管理数据。<br>HBase体系结构有三大重要组成部分：<br>a.HBaseMaster：HBase主服务器，与Bigtable的主服务器类似。<br>b.HRegionServer：HBase域服务器，与Bigtable的Tablet服务器类似。<br>c.Hbase Client：HBase客户端是由org.apache.hadoop.Hbase.client.HTable定义的。<br>下面将对这三个组件进行详细的介绍。<br>（1）HBaseMaster<br>一个HBase只部署一台主服务器，它通过领导选举算法确保只有唯一的主服务器是活跃的，ZooKeeper保存主服务器的服务器地址信息。如果主服务器瘫痪，可以通过领导选举算法从备用服务器中选择新的主服务器。<br>主服务器承担着初始化集群的任务。当主服务器每一次启动时，会试图从HDFS获取根或根域目录，如果获取失败则创建根或根域目录，以及第一个元域目录。在下次启动时，主服务器就可以获取集群和集群中所有域 的信息了。同时主服务器还负责集群中域的分配、域服务器运行状态的监控、表格的管理等工作。  </p>\n<p>（2）HRegionServer<br>HBase域服务器的主要职责有服务于主服务器分配的域、处理端的读写请求、本地缓冲回写、本地数据压缩和分割域等功能。<br>每个域只能由一台域服务器来提供服务。当它开始服务于某域时，它会从HDFS文件系统中读取该域的日志和所有存储文件，同时还会管理操作HDFS文件的持久性存储工作。客户端通过与主服务器通信获取域或域服务器的列表信息后，就可以直接向域服务器发送域读写请求，来完成操作。  </p>\n<p>（3）HBaseClient<br> HBase客户端负责查找用户域所在的域服务器地址。HBase客户端会与HBase主机交换消息以查找根域的位置，这是两者之间唯一的交流。<br>定位根域后，客户端连接根域所在的服务器，并扫描根域获取元域信息。元域信息中包含所需用户域的域服务器地址。客户端再连接元域所在的服务器，扫描元域以获取所需用户域所在的域服务器地址。客户端再连接元域所在的域服务器，扫描元域以获取所需用户域所有的域服务器地址。定位用户域后，客户端连接用户域所在的域服务器并发出读写请求。用户域的地址将在客户端被缓存，后续的请求无须重复上述过程。  </p>\n<p>综上所述，HBase的体系结构中，HBase主要由主服务器、域服务器和客户端三部分组成。主服务器作为HBase的中心，管理整个集群中的所有域，监控每台域服务器的运行情况等；域服务器接收来自服务器的分配域，处理管理端的域读写请求并回写映射文件等；客户端主要用来查找用户域所在的域服务器地址信息。  </p>\n<p>##Hive的数据管理<br>Hive是建立在Hadoop上的数据仓库基础架构。它提供了一系列的工具，用来进行数据提取、转化、加载，这是一种可以存储、查询和分析存储在Hadoop中的大规模数据的机制。Hive定义了简单的类SQL的查询语言，称为HiveQL，它允许熟悉SQL的用户用SQL语言查询数据。作为一个数据仓库，Hive的数据管理按照使用层次可以从元数据存储、数据存储和数据交换三方面来介绍。<br>（1）元数据存储<br>Hive将元数据存储在RDBMS中，有三种模式可以连接到数据库。<br>1）Single User Mode:此模式连接到一个In-memory的数据库Derby，一般用于Unit Test。<br>2）Multi User Mode：通过网络连接到一个数据库中，这是最常用的模式。<br>3）Remote Server Mode：用于非Java客户端访问元数据，在服务器端启动一个MetaStoreServer，客户端利用Thrift协议通过MetaStoreServer来访问元数据库。<br>（2）数据存储<br>首先，Hive没有专门的数据存储格式，也没有为数据建立索引，用户可以非常自由地组织Hive中的表，只需要在创建表的时候告诉Hive数据中的列分隔符和行分隔符，它就可以解析数据了。<br>其次，Hive中所有的数据都存储在HDFS中，Hive中包含4种数据模型：Table、External Table、Partition和Bucket。<br>Hive中的Table和数据库中的Table在概念上是类似的，每一个Table在Hive中都有一个相应的目录来存储数据。例如，一个表pvs，它在HDFS中的路径为:/wh/pvs，其中wh是在hive-site.xml中由${hive.metastore.warehouse.dir}指定的数据仓库的目录，所有的Table数据（不包括External Table）都保存在这个目录中。<br>（3）数据交换<br>数据交换主要分为以下部分，如图：</p>\n<p>a）用户接口：包括客户端、Web界面和数据库接口。<br>b)元数据存储：通常存在在关系型数据库中，如MYSQL、Derby中。<br>c)解释器、编译器、优化器、执行器。<br>d)Hadoop：利用HDFS进行存储，利用MapRecue进行计算。<br>用户接口主要有三个：客户端、数据库接口和Web界面，其中最常用的是客户端。Client是Hive的客户端，当启动Client模式时，用户会想要连接Hive Server，这时需要指出Hive Server所在的节点，并且在该节点启动HiveServer。Web界面是通过浏览器访问Hive的。<br>Hive元数据存储在数据库中，如MYSQL、Derby中。Hive中的元数据包括表的名字、表的列、表的分区、表分区的属性、表的属性、表的数据所在目录等。<br>解释器、编译器、优化器完成HiveQL查询语句从记法分析、语法分析、编译、优化到查询计划的生成。生成的查询计划存储在HDFS中，并且随后由MapRecue调用执行。<br>Hive的数据存储在HDFS中，大部分的查询由MapRecue完成（包括<em>的查询不会生成MapRecue任务，比如select </em> from tbl).  </p>\n<p>#安装并运行Hadoop<br>介绍Hadoop安装之前，先介绍一下Hadoop对各个节点的角色定义。<br>Hadoop分别从三个角度将主机划分为两种角色。第一，最基本的划分为Master和Slave，即主人和奴隶；第二，从HDFS的角度，将主机划分为NameNode和DataNode（在分布式文件系统中，目录的管理很重要，管理目录相当于主人，而NameNode就是目录管理者）；第三，从MapRecue的角度，将主机划分为JobTracker和TaskTracker（一个Job经常被划分为多个Task，从这个角度不难理解它们之间的关系）。</p>\n","site":{"data":{}},"excerpt":"","more":"<p>#Hadoop数据管理<br>主要包括Hadoop的分布式文件系统HDFS、分布式数据库HBase和数据仓库工具Hive</p>\n<p>##HDFS的数据管理<br>HDFS是分布式计算的存储基石，Hadoop分布式文件系统和其它文件系统有很多类似的特性；<br>1）对于整个集群有单一的命名空间；<br>2）具有数据一致性，都适合一次写入多次读取的模型，客户端在文件没有被成功创建之前是无法看到文件存在的。<br>3）文件会被分割成多个文件块，每个文件块被分配存储到数据节点上，而且会根据配置由复制文件块来保证数据的安全性。<br>HDFS有三个重要的角色来进行文件系统的管理：NameNode、DataNode和Client。NameNode可以看做是分布式文件系统的管理者，主要负责管理文件系统的命名空间、集群配置信息和存储块的复制等。NameNode会将文件系统的Metadata存储在内存中，这些信息主要包括文件信息、每一个文件对应的文件块的信息和每一个文件块在DataNode中的信息等。DataNode是文件存储的基本单元，它将文件块（Block）存储在本地文件系统中，保存了所有Block的Metadata，同时周期性地将所有存在的Block信息发送给NameNode。Client就是需要获取分布式文件系统文件的应用程序。接下来三个具体的操作来说明HDFS对数据的管理。<br>（1）文件写入<br>1）Client向NameNode发起文件写入的请求。<br>2）NameNode根据文件的大小和文件块配置情况，返回给Client所管理的DataNode的信息。<br>3）Client将文件划分为多个Block，根据DataNode的地址信息，按顺序将其写入到每一个DataNode块中。<br>（2）文件读取<br>1）Client向NameNode发起文件读取的请求。<br>2）NameNode返回文件存储的DataNode信息。<br>3）Client读取文件信息。<br>（3）文件块（Block）复制<br>1）NameNode发现部分文件的Block不符合最小复制数这一要求或部分DataNode失效。<br>2）通知DataNode相互复制Block。<br>3）DataNode开始直接相互复制。  </p>\n<p>作为分布式文件系统，HDFS在数据管理方面还有值得借鉴的几个功能：<br>a.文件块（Block）的放置：一个Block会有三份备份，一份放在NameNode指定的DataNode上，另一份放在与指定DataNode不在同一机器上的DataNode上，最后一份放在与指定DataNode同一Rack的DataNode上。备份的目的是为了数据安全，采用这种配置方式主要是考虑同一Rack失败的情况，以及不同Rack之间进行数据复制会带来的性能问题。<br>b.心跳检测：用心跳检测DataNode的健康状况，如果发现问题就采取数据备份的方式来保证数据的安全性。<br>c.数据复制（场景为DataNode失败、需要平衡DataNode的存储利用率和平衡DataNode数据交互压力等情况）；使用Hadoop时可以用HDFS的balancer命令配置Threshold来平衡每一个DataNode的磁盘利用率。假设设置了Threshold为10%，那么执行balancer命令时，首先会统计所有的DataNode的磁盘利用率的平均值，然后判断如果某一个DataNode的磁盘利用率超过这个平均值，那么将会把这个DataNode的Block转移到磁盘利用率低的DataNode上，这对于新的节点为加入十分有用。<br>d.数据校验：采用CRC32做数据校验。在写入文件块的时候，除了会写入数据外还会写入校验信息，在读取的时候则需要先校验后读入。<br>e.数据管道性的写入：当客户端要写入文件到DataNode上时，首先会读取一个Block，然后将其写到每一个DataNode上，接着由第一个DataNode将其传递到备份的DataNode上，直到所有需要写入这个Block的DataNode都成功写入后，客户端才会开始写下一个Block。<br>f.安全模型：分布式文件系统启动时会进入安全模式（系统运行期间也可以通过命令进入安全模式），当分布式文件处于安全模式时，文件系统中的内容不允许修改也不允许删除，直到安DataNode上数据块的有效性，同时根据策略进行必要的复制或删除部分数据块。在实际操作过程中，如果在系统启动时修改和删除文件会出现安全模式不允许修改的错误提示，只需要等待一会即可。  </p>\n<p>##HBase的数据管理<br>HBase是一个类似Bigtable的分布式数据库，它的大部分特性和Bigtable一样，是一个稀疏的、长期存储的（存在硬盘上）、多维度的排序映射表，这张表的索引是行关键字、列关键字和时间戳。表中的每个值是一个纯字符数组，数据都是字符串，没有类型，所以同一张表中的每一行数据都可以有截然不同的列。列名字的格式是“<family>:<label>“，它是由字符串组成的，每一张表有一个family集合，这个集合是固定不变的，相当于表的结构，只能通过改变表结构来改变表的family集合。但是label值相对于每一行来说都是可以改变的。<br>HBase把同一个family中的数据存储在同一个目录下，而HB的写操作是锁行的。每一行都是一个原子元素，都可以加锁。所有数据库的更新都有一个时间戳标记，每次更新都会生成一个新的版本，而HBase会保留一定数量的版本，这个值是可以设定的。客户端可以选择获取距离某个时间点最近的版本，或者一次获取所有版本。  </label></family></p>\n<p>以上从微观上介绍了HBase的一些数据管理措施，那么HBase作为分布式数据为顺整体上从集群出发又是如何管理数据的呢？<br>HBase在分布式集群上主要依赖于HRegion、HMaster、HClient组成的体系结构从整体上管理数据。<br>HBase体系结构有三大重要组成部分：<br>a.HBaseMaster：HBase主服务器，与Bigtable的主服务器类似。<br>b.HRegionServer：HBase域服务器，与Bigtable的Tablet服务器类似。<br>c.Hbase Client：HBase客户端是由org.apache.hadoop.Hbase.client.HTable定义的。<br>下面将对这三个组件进行详细的介绍。<br>（1）HBaseMaster<br>一个HBase只部署一台主服务器，它通过领导选举算法确保只有唯一的主服务器是活跃的，ZooKeeper保存主服务器的服务器地址信息。如果主服务器瘫痪，可以通过领导选举算法从备用服务器中选择新的主服务器。<br>主服务器承担着初始化集群的任务。当主服务器每一次启动时，会试图从HDFS获取根或根域目录，如果获取失败则创建根或根域目录，以及第一个元域目录。在下次启动时，主服务器就可以获取集群和集群中所有域 的信息了。同时主服务器还负责集群中域的分配、域服务器运行状态的监控、表格的管理等工作。  </p>\n<p>（2）HRegionServer<br>HBase域服务器的主要职责有服务于主服务器分配的域、处理端的读写请求、本地缓冲回写、本地数据压缩和分割域等功能。<br>每个域只能由一台域服务器来提供服务。当它开始服务于某域时，它会从HDFS文件系统中读取该域的日志和所有存储文件，同时还会管理操作HDFS文件的持久性存储工作。客户端通过与主服务器通信获取域或域服务器的列表信息后，就可以直接向域服务器发送域读写请求，来完成操作。  </p>\n<p>（3）HBaseClient<br> HBase客户端负责查找用户域所在的域服务器地址。HBase客户端会与HBase主机交换消息以查找根域的位置，这是两者之间唯一的交流。<br>定位根域后，客户端连接根域所在的服务器，并扫描根域获取元域信息。元域信息中包含所需用户域的域服务器地址。客户端再连接元域所在的服务器，扫描元域以获取所需用户域所在的域服务器地址。客户端再连接元域所在的域服务器，扫描元域以获取所需用户域所有的域服务器地址。定位用户域后，客户端连接用户域所在的域服务器并发出读写请求。用户域的地址将在客户端被缓存，后续的请求无须重复上述过程。  </p>\n<p>综上所述，HBase的体系结构中，HBase主要由主服务器、域服务器和客户端三部分组成。主服务器作为HBase的中心，管理整个集群中的所有域，监控每台域服务器的运行情况等；域服务器接收来自服务器的分配域，处理管理端的域读写请求并回写映射文件等；客户端主要用来查找用户域所在的域服务器地址信息。  </p>\n<p>##Hive的数据管理<br>Hive是建立在Hadoop上的数据仓库基础架构。它提供了一系列的工具，用来进行数据提取、转化、加载，这是一种可以存储、查询和分析存储在Hadoop中的大规模数据的机制。Hive定义了简单的类SQL的查询语言，称为HiveQL，它允许熟悉SQL的用户用SQL语言查询数据。作为一个数据仓库，Hive的数据管理按照使用层次可以从元数据存储、数据存储和数据交换三方面来介绍。<br>（1）元数据存储<br>Hive将元数据存储在RDBMS中，有三种模式可以连接到数据库。<br>1）Single User Mode:此模式连接到一个In-memory的数据库Derby，一般用于Unit Test。<br>2）Multi User Mode：通过网络连接到一个数据库中，这是最常用的模式。<br>3）Remote Server Mode：用于非Java客户端访问元数据，在服务器端启动一个MetaStoreServer，客户端利用Thrift协议通过MetaStoreServer来访问元数据库。<br>（2）数据存储<br>首先，Hive没有专门的数据存储格式，也没有为数据建立索引，用户可以非常自由地组织Hive中的表，只需要在创建表的时候告诉Hive数据中的列分隔符和行分隔符，它就可以解析数据了。<br>其次，Hive中所有的数据都存储在HDFS中，Hive中包含4种数据模型：Table、External Table、Partition和Bucket。<br>Hive中的Table和数据库中的Table在概念上是类似的，每一个Table在Hive中都有一个相应的目录来存储数据。例如，一个表pvs，它在HDFS中的路径为:/wh/pvs，其中wh是在hive-site.xml中由${hive.metastore.warehouse.dir}指定的数据仓库的目录，所有的Table数据（不包括External Table）都保存在这个目录中。<br>（3）数据交换<br>数据交换主要分为以下部分，如图：</p>\n<p>a）用户接口：包括客户端、Web界面和数据库接口。<br>b)元数据存储：通常存在在关系型数据库中，如MYSQL、Derby中。<br>c)解释器、编译器、优化器、执行器。<br>d)Hadoop：利用HDFS进行存储，利用MapRecue进行计算。<br>用户接口主要有三个：客户端、数据库接口和Web界面，其中最常用的是客户端。Client是Hive的客户端，当启动Client模式时，用户会想要连接Hive Server，这时需要指出Hive Server所在的节点，并且在该节点启动HiveServer。Web界面是通过浏览器访问Hive的。<br>Hive元数据存储在数据库中，如MYSQL、Derby中。Hive中的元数据包括表的名字、表的列、表的分区、表分区的属性、表的属性、表的数据所在目录等。<br>解释器、编译器、优化器完成HiveQL查询语句从记法分析、语法分析、编译、优化到查询计划的生成。生成的查询计划存储在HDFS中，并且随后由MapRecue调用执行。<br>Hive的数据存储在HDFS中，大部分的查询由MapRecue完成（包括<em>的查询不会生成MapRecue任务，比如select </em> from tbl).  </p>\n<p>#安装并运行Hadoop<br>介绍Hadoop安装之前，先介绍一下Hadoop对各个节点的角色定义。<br>Hadoop分别从三个角度将主机划分为两种角色。第一，最基本的划分为Master和Slave，即主人和奴隶；第二，从HDFS的角度，将主机划分为NameNode和DataNode（在分布式文件系统中，目录的管理很重要，管理目录相当于主人，而NameNode就是目录管理者）；第三，从MapRecue的角度，将主机划分为JobTracker和TaskTracker（一个Job经常被划分为多个Task，从这个角度不难理解它们之间的关系）。</p>\n"},{"title":"Hive入门概念","date":"2017-05-02T02:30:00.000Z","_content":"#Hive\n大数据生态下，通过Hadoop MapReduce，实现将计算分割成多个处理单元，然后分散到一群家用或服务器级别的硬件上，从而降低成本并提供可伸缩性；这个计算模型下是HDFS，这是个“可插拔的“文件系统。不过，这里存在一个问题，就是用户如何从一个现有的数据基础架构转移到Hadoop上，而这个基础架构是基于关系型数据库和结构化查询语句（SQL）？\n这就是Hive出现的原因，Hive提供了被称为Hive查询语言的（或称为HiveQL或HQL）的SQL方言，来查询存储在Hadoop集群中的数据。Hive将大多数据的查询转换为MapRecue任务（ｊｏｂ）。\n\n#Hive安装\nHive使用环境变量HADOOP_HOME来指定Hadoop的所有相关JAR和配置文件，因此在安装之前请确认下是否设置好了这个环境变量。\n$cd ~\n$curl -o http://archive.apache.org/dis/hive/hive-0.9.0/hive-0.9.0-bin.tar.gz\n$tar -xzf hive-0.9.0.tar.gz\n$sudo mkdir -p /user/hive/warehouse\n$sudo chmod a+rwx /user/hive/warehouse\n\n可以定义HIVE_HOME环境变量\n$sudo echo \"export HIVE_HOME=$PWD/hive-0.9.0\" > /etc/profile.d/hive.sh\n$sudo echo \"PATH=$PATH:$HIVE_HOME/bin\" >> /etc/profile.d/hive.sh\n$. /etc/profile\n\n#Hive组成\n主要包含三个部分：\n1.代码本身，在$HIVE_HOME/lib下可以看到许多jar，例如hive-exec*.jar，hive-metastore*.ja，每个jar文件都实现了hive功能中某个特定的部分。\n2.可执行文件，在$HIVE_HOME/bin下，包含hive的命令行界面CLI，CLI是使用hive最常用的方式，一般会使用小写的hive代替。CLI用于提供交互式的界面供输入语句或用户执行hive语句的脚本。\n3.metastoreservice（元数据服务），所有的hive客户端都需要元数据服务，hive使用这个服务来存储表模式信息和其他元数据信息。通常会使用关系型数据库来存储这些信息，默认使用内置的DerbySQL服务器，其可以提供有限的、单进程的存储服务。例如，当使用Derby时，用户不能执行2个并发的Hive CLI实例，然而，如果是在个人计算机上或某些开发任务上使用的话这样也没有问题。对于集群来说，需要使用MYSQL或类似的关系型数据库。\n另外，hive还有一些组件，Thrift服务提供可远程访问的其他进程的功能，也提供JDBC和ODBC访问Hive的功能。Hive还提供了一个简单的网页界面HWI，提供远程访问Hive服务。\n\n#Hive启动\n使用$HIVE_HOME/bin/hive命令\n$cd $HIVE_HOME\n$bin/hive\nhive>CREATE TABLE x (a INT);\nhive>SELECT * from x;\nhive>DROP TABLE x; \nhive>exit;\n\n#Hive命令\n[root@cdhmaster~]#hive--help  \nUsage./hive<parameters>--serviceserviceName<serviceparameters>  \nServiceList:beelinecleardanglingscratchdirclihelphiveburninclienthiveserver2hiveserverhwijarlineagemetastoremetatoolorcfiledumprcfilecatschemaToolversion  \nParametersparsed:  \n--auxpath:Auxillaryjars  \n--config:Hiveconfigurationdirectory  \n--service:Startsspecificservice/component.cliisdefault  \nParametersused:  \nHADOOP_HOMEorHADOOP_PREFIX:Hadoopinstalldirectory  \nHIVE_OPT:Hiveoptions  \nForhelponaparticularservice:  \n./hive--serviceserviceName--help  \nDebughelp:./hive--debug--help  \nYouhavenewmailin/var/spool/mail/root  \n需要注意ServiceList:后面的内容，这里提供了几个服务，包括我们绝大多数据时间将要使用的CLI。用户可以通过--servicename服务名称来启用某个服务。  \n\n#常用SQL\n显示数据库  \nhive>showdatabases;  \nOK  \nDefault  \nhive>showdatabaselike'h.*';  \n创建数据库  \nhive>createdatabasetest_test001;  \nuse命令用于将某个数据库设置为用户当前的工作数据库  \nhive>usetest_test001;  \n设置当前工作数据库后，即可查询所有表  \nhive>showtables；  \n删除数据库  \nhive>dropdatabaseifexiststest_test001;  \n\n创建数据  \ncreatetableifnotexistsmydb.employees(  \nnamestringcomment'emplyeename',  \nSalaryfloat  \n)  \n\n删除表  \ndroptableifexiststest_test001;  \n\n修改表  \naltertable只会修改元数据  \n\n表重命名  \naltertabletest_test001renametotes;  \n\nset hive.cli.print.header=true; // 打印列名  \nset hive.cli.print.row.to.vertical=true; // 开启行转列功能, 前提必须开启打印列名功能  \nset hive.cli.print.row.to.vertical.num=1; // 设置每行显示的列数  \n\n\n\n","source":"_posts/hadoop/Hive入门概念.md","raw":"---\ntitle: Hive入门概念\ndate: 2017-05-02 10:30:00\ntags: [大数据,hive]\ncategories: [大数据,hive]\n---\n#Hive\n大数据生态下，通过Hadoop MapReduce，实现将计算分割成多个处理单元，然后分散到一群家用或服务器级别的硬件上，从而降低成本并提供可伸缩性；这个计算模型下是HDFS，这是个“可插拔的“文件系统。不过，这里存在一个问题，就是用户如何从一个现有的数据基础架构转移到Hadoop上，而这个基础架构是基于关系型数据库和结构化查询语句（SQL）？\n这就是Hive出现的原因，Hive提供了被称为Hive查询语言的（或称为HiveQL或HQL）的SQL方言，来查询存储在Hadoop集群中的数据。Hive将大多数据的查询转换为MapRecue任务（ｊｏｂ）。\n\n#Hive安装\nHive使用环境变量HADOOP_HOME来指定Hadoop的所有相关JAR和配置文件，因此在安装之前请确认下是否设置好了这个环境变量。\n$cd ~\n$curl -o http://archive.apache.org/dis/hive/hive-0.9.0/hive-0.9.0-bin.tar.gz\n$tar -xzf hive-0.9.0.tar.gz\n$sudo mkdir -p /user/hive/warehouse\n$sudo chmod a+rwx /user/hive/warehouse\n\n可以定义HIVE_HOME环境变量\n$sudo echo \"export HIVE_HOME=$PWD/hive-0.9.0\" > /etc/profile.d/hive.sh\n$sudo echo \"PATH=$PATH:$HIVE_HOME/bin\" >> /etc/profile.d/hive.sh\n$. /etc/profile\n\n#Hive组成\n主要包含三个部分：\n1.代码本身，在$HIVE_HOME/lib下可以看到许多jar，例如hive-exec*.jar，hive-metastore*.ja，每个jar文件都实现了hive功能中某个特定的部分。\n2.可执行文件，在$HIVE_HOME/bin下，包含hive的命令行界面CLI，CLI是使用hive最常用的方式，一般会使用小写的hive代替。CLI用于提供交互式的界面供输入语句或用户执行hive语句的脚本。\n3.metastoreservice（元数据服务），所有的hive客户端都需要元数据服务，hive使用这个服务来存储表模式信息和其他元数据信息。通常会使用关系型数据库来存储这些信息，默认使用内置的DerbySQL服务器，其可以提供有限的、单进程的存储服务。例如，当使用Derby时，用户不能执行2个并发的Hive CLI实例，然而，如果是在个人计算机上或某些开发任务上使用的话这样也没有问题。对于集群来说，需要使用MYSQL或类似的关系型数据库。\n另外，hive还有一些组件，Thrift服务提供可远程访问的其他进程的功能，也提供JDBC和ODBC访问Hive的功能。Hive还提供了一个简单的网页界面HWI，提供远程访问Hive服务。\n\n#Hive启动\n使用$HIVE_HOME/bin/hive命令\n$cd $HIVE_HOME\n$bin/hive\nhive>CREATE TABLE x (a INT);\nhive>SELECT * from x;\nhive>DROP TABLE x; \nhive>exit;\n\n#Hive命令\n[root@cdhmaster~]#hive--help  \nUsage./hive<parameters>--serviceserviceName<serviceparameters>  \nServiceList:beelinecleardanglingscratchdirclihelphiveburninclienthiveserver2hiveserverhwijarlineagemetastoremetatoolorcfiledumprcfilecatschemaToolversion  \nParametersparsed:  \n--auxpath:Auxillaryjars  \n--config:Hiveconfigurationdirectory  \n--service:Startsspecificservice/component.cliisdefault  \nParametersused:  \nHADOOP_HOMEorHADOOP_PREFIX:Hadoopinstalldirectory  \nHIVE_OPT:Hiveoptions  \nForhelponaparticularservice:  \n./hive--serviceserviceName--help  \nDebughelp:./hive--debug--help  \nYouhavenewmailin/var/spool/mail/root  \n需要注意ServiceList:后面的内容，这里提供了几个服务，包括我们绝大多数据时间将要使用的CLI。用户可以通过--servicename服务名称来启用某个服务。  \n\n#常用SQL\n显示数据库  \nhive>showdatabases;  \nOK  \nDefault  \nhive>showdatabaselike'h.*';  \n创建数据库  \nhive>createdatabasetest_test001;  \nuse命令用于将某个数据库设置为用户当前的工作数据库  \nhive>usetest_test001;  \n设置当前工作数据库后，即可查询所有表  \nhive>showtables；  \n删除数据库  \nhive>dropdatabaseifexiststest_test001;  \n\n创建数据  \ncreatetableifnotexistsmydb.employees(  \nnamestringcomment'emplyeename',  \nSalaryfloat  \n)  \n\n删除表  \ndroptableifexiststest_test001;  \n\n修改表  \naltertable只会修改元数据  \n\n表重命名  \naltertabletest_test001renametotes;  \n\nset hive.cli.print.header=true; // 打印列名  \nset hive.cli.print.row.to.vertical=true; // 开启行转列功能, 前提必须开启打印列名功能  \nset hive.cli.print.row.to.vertical.num=1; // 设置每行显示的列数  \n\n\n\n","slug":"hadoop/Hive入门概念","published":1,"updated":"2017-08-18T01:44:35.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjh7polt8001tbp7rr2bj806d","content":"<p>#Hive<br>大数据生态下，通过Hadoop MapReduce，实现将计算分割成多个处理单元，然后分散到一群家用或服务器级别的硬件上，从而降低成本并提供可伸缩性；这个计算模型下是HDFS，这是个“可插拔的“文件系统。不过，这里存在一个问题，就是用户如何从一个现有的数据基础架构转移到Hadoop上，而这个基础架构是基于关系型数据库和结构化查询语句（SQL）？<br>这就是Hive出现的原因，Hive提供了被称为Hive查询语言的（或称为HiveQL或HQL）的SQL方言，来查询存储在Hadoop集群中的数据。Hive将大多数据的查询转换为MapRecue任务（ｊｏｂ）。</p>\n<p>#Hive安装<br>Hive使用环境变量HADOOP_HOME来指定Hadoop的所有相关JAR和配置文件，因此在安装之前请确认下是否设置好了这个环境变量。<br>$cd ~<br>$curl -o <a href=\"http://archive.apache.org/dis/hive/hive-0.9.0/hive-0.9.0-bin.tar.gz\" target=\"_blank\" rel=\"noopener\">http://archive.apache.org/dis/hive/hive-0.9.0/hive-0.9.0-bin.tar.gz</a><br>$tar -xzf hive-0.9.0.tar.gz<br>$sudo mkdir -p /user/hive/warehouse<br>$sudo chmod a+rwx /user/hive/warehouse</p>\n<p>可以定义HIVE_HOME环境变量<br>$sudo echo “export HIVE_HOME=$PWD/hive-0.9.0” &gt; /etc/profile.d/hive.sh<br>$sudo echo “PATH=$PATH:$HIVE_HOME/bin” &gt;&gt; /etc/profile.d/hive.sh<br>$. /etc/profile</p>\n<p>#Hive组成<br>主要包含三个部分：<br>1.代码本身，在$HIVE_HOME/lib下可以看到许多jar，例如hive-exec<em>.jar，hive-metastore</em>.ja，每个jar文件都实现了hive功能中某个特定的部分。<br>2.可执行文件，在$HIVE_HOME/bin下，包含hive的命令行界面CLI，CLI是使用hive最常用的方式，一般会使用小写的hive代替。CLI用于提供交互式的界面供输入语句或用户执行hive语句的脚本。<br>3.metastoreservice（元数据服务），所有的hive客户端都需要元数据服务，hive使用这个服务来存储表模式信息和其他元数据信息。通常会使用关系型数据库来存储这些信息，默认使用内置的DerbySQL服务器，其可以提供有限的、单进程的存储服务。例如，当使用Derby时，用户不能执行2个并发的Hive CLI实例，然而，如果是在个人计算机上或某些开发任务上使用的话这样也没有问题。对于集群来说，需要使用MYSQL或类似的关系型数据库。<br>另外，hive还有一些组件，Thrift服务提供可远程访问的其他进程的功能，也提供JDBC和ODBC访问Hive的功能。Hive还提供了一个简单的网页界面HWI，提供远程访问Hive服务。</p>\n<p>#Hive启动<br>使用$HIVE_HOME/bin/hive命令<br>$cd $HIVE_HOME<br>$bin/hive<br>hive&gt;CREATE TABLE x (a INT);<br>hive&gt;SELECT * from x;<br>hive&gt;DROP TABLE x;<br>hive&gt;exit;</p>\n<p>#Hive命令<br>[root@cdhmaster~]#hive–help<br>Usage./hive<parameters>–serviceserviceName<serviceparameters><br>ServiceList:beelinecleardanglingscratchdirclihelphiveburninclienthiveserver2hiveserverhwijarlineagemetastoremetatoolorcfiledumprcfilecatschemaToolversion<br>Parametersparsed:<br>–auxpath:Auxillaryjars<br>–config:Hiveconfigurationdirectory<br>–service:Startsspecificservice/component.cliisdefault<br>Parametersused:<br>HADOOP_HOMEorHADOOP_PREFIX:Hadoopinstalldirectory<br>HIVE_OPT:Hiveoptions<br>Forhelponaparticularservice:<br>./hive–serviceserviceName–help<br>Debughelp:./hive–debug–help<br>Youhavenewmailin/var/spool/mail/root<br>需要注意ServiceList:后面的内容，这里提供了几个服务，包括我们绝大多数据时间将要使用的CLI。用户可以通过–servicename服务名称来启用某个服务。  </serviceparameters></parameters></p>\n<p>#常用SQL<br>显示数据库<br>hive&gt;showdatabases;<br>OK<br>Default<br>hive&gt;showdatabaselike’h.*’;<br>创建数据库<br>hive&gt;createdatabasetest_test001;<br>use命令用于将某个数据库设置为用户当前的工作数据库<br>hive&gt;usetest_test001;<br>设置当前工作数据库后，即可查询所有表<br>hive&gt;showtables；<br>删除数据库<br>hive&gt;dropdatabaseifexiststest_test001;  </p>\n<p>创建数据<br>createtableifnotexistsmydb.employees(<br>namestringcomment’emplyeename’,<br>Salaryfloat<br>)  </p>\n<p>删除表<br>droptableifexiststest_test001;  </p>\n<p>修改表<br>altertable只会修改元数据  </p>\n<p>表重命名<br>altertabletest_test001renametotes;  </p>\n<p>set hive.cli.print.header=true; // 打印列名<br>set hive.cli.print.row.to.vertical=true; // 开启行转列功能, 前提必须开启打印列名功能<br>set hive.cli.print.row.to.vertical.num=1; // 设置每行显示的列数  </p>\n","site":{"data":{}},"excerpt":"","more":"<p>#Hive<br>大数据生态下，通过Hadoop MapReduce，实现将计算分割成多个处理单元，然后分散到一群家用或服务器级别的硬件上，从而降低成本并提供可伸缩性；这个计算模型下是HDFS，这是个“可插拔的“文件系统。不过，这里存在一个问题，就是用户如何从一个现有的数据基础架构转移到Hadoop上，而这个基础架构是基于关系型数据库和结构化查询语句（SQL）？<br>这就是Hive出现的原因，Hive提供了被称为Hive查询语言的（或称为HiveQL或HQL）的SQL方言，来查询存储在Hadoop集群中的数据。Hive将大多数据的查询转换为MapRecue任务（ｊｏｂ）。</p>\n<p>#Hive安装<br>Hive使用环境变量HADOOP_HOME来指定Hadoop的所有相关JAR和配置文件，因此在安装之前请确认下是否设置好了这个环境变量。<br>$cd ~<br>$curl -o <a href=\"http://archive.apache.org/dis/hive/hive-0.9.0/hive-0.9.0-bin.tar.gz\" target=\"_blank\" rel=\"noopener\">http://archive.apache.org/dis/hive/hive-0.9.0/hive-0.9.0-bin.tar.gz</a><br>$tar -xzf hive-0.9.0.tar.gz<br>$sudo mkdir -p /user/hive/warehouse<br>$sudo chmod a+rwx /user/hive/warehouse</p>\n<p>可以定义HIVE_HOME环境变量<br>$sudo echo “export HIVE_HOME=$PWD/hive-0.9.0” &gt; /etc/profile.d/hive.sh<br>$sudo echo “PATH=$PATH:$HIVE_HOME/bin” &gt;&gt; /etc/profile.d/hive.sh<br>$. /etc/profile</p>\n<p>#Hive组成<br>主要包含三个部分：<br>1.代码本身，在$HIVE_HOME/lib下可以看到许多jar，例如hive-exec<em>.jar，hive-metastore</em>.ja，每个jar文件都实现了hive功能中某个特定的部分。<br>2.可执行文件，在$HIVE_HOME/bin下，包含hive的命令行界面CLI，CLI是使用hive最常用的方式，一般会使用小写的hive代替。CLI用于提供交互式的界面供输入语句或用户执行hive语句的脚本。<br>3.metastoreservice（元数据服务），所有的hive客户端都需要元数据服务，hive使用这个服务来存储表模式信息和其他元数据信息。通常会使用关系型数据库来存储这些信息，默认使用内置的DerbySQL服务器，其可以提供有限的、单进程的存储服务。例如，当使用Derby时，用户不能执行2个并发的Hive CLI实例，然而，如果是在个人计算机上或某些开发任务上使用的话这样也没有问题。对于集群来说，需要使用MYSQL或类似的关系型数据库。<br>另外，hive还有一些组件，Thrift服务提供可远程访问的其他进程的功能，也提供JDBC和ODBC访问Hive的功能。Hive还提供了一个简单的网页界面HWI，提供远程访问Hive服务。</p>\n<p>#Hive启动<br>使用$HIVE_HOME/bin/hive命令<br>$cd $HIVE_HOME<br>$bin/hive<br>hive&gt;CREATE TABLE x (a INT);<br>hive&gt;SELECT * from x;<br>hive&gt;DROP TABLE x;<br>hive&gt;exit;</p>\n<p>#Hive命令<br>[root@cdhmaster~]#hive–help<br>Usage./hive<parameters>–serviceserviceName<serviceparameters><br>ServiceList:beelinecleardanglingscratchdirclihelphiveburninclienthiveserver2hiveserverhwijarlineagemetastoremetatoolorcfiledumprcfilecatschemaToolversion<br>Parametersparsed:<br>–auxpath:Auxillaryjars<br>–config:Hiveconfigurationdirectory<br>–service:Startsspecificservice/component.cliisdefault<br>Parametersused:<br>HADOOP_HOMEorHADOOP_PREFIX:Hadoopinstalldirectory<br>HIVE_OPT:Hiveoptions<br>Forhelponaparticularservice:<br>./hive–serviceserviceName–help<br>Debughelp:./hive–debug–help<br>Youhavenewmailin/var/spool/mail/root<br>需要注意ServiceList:后面的内容，这里提供了几个服务，包括我们绝大多数据时间将要使用的CLI。用户可以通过–servicename服务名称来启用某个服务。  </serviceparameters></parameters></p>\n<p>#常用SQL<br>显示数据库<br>hive&gt;showdatabases;<br>OK<br>Default<br>hive&gt;showdatabaselike’h.*’;<br>创建数据库<br>hive&gt;createdatabasetest_test001;<br>use命令用于将某个数据库设置为用户当前的工作数据库<br>hive&gt;usetest_test001;<br>设置当前工作数据库后，即可查询所有表<br>hive&gt;showtables；<br>删除数据库<br>hive&gt;dropdatabaseifexiststest_test001;  </p>\n<p>创建数据<br>createtableifnotexistsmydb.employees(<br>namestringcomment’emplyeename’,<br>Salaryfloat<br>)  </p>\n<p>删除表<br>droptableifexiststest_test001;  </p>\n<p>修改表<br>altertable只会修改元数据  </p>\n<p>表重命名<br>altertabletest_test001renametotes;  </p>\n<p>set hive.cli.print.header=true; // 打印列名<br>set hive.cli.print.row.to.vertical=true; // 开启行转列功能, 前提必须开启打印列名功能<br>set hive.cli.print.row.to.vertical.num=1; // 设置每行显示的列数  </p>\n"},{"title":"Kylin入门概念","date":"2017-04-16T15:43:49.000Z","_content":"\n#Apache Kylin的工作原理\nApache Kylin的工作原理本质上是MOLAP（Multidimensional　Online　Analytical　Processing）Cube，也就是多维　立方体分析。这是数据分析中相当经典的理论，在关系数据库年代就已经有了广泛的应用，下面将其做简要的介绍。  \n\n##维度和度量\n简单来讲，维度就是观察数据的角度。比如电商的销售数据，可以从时间的维度来观察，也可以进一步细化，从时间和地区的维度来观察。维度一般是一组离散的值，比如时间维度上的每一个独立的日期，或者商品维度上的每一件独立的商品。因此统计时可以把维度值 相同的记录聚合在一起，然后应用聚合函数做累加、平均、去重计数等聚合计算。  ![\"维度和度量的例子\"](/images/hadoop/kylin/维度和度量的例子.jpg)\n    \n度量就是被聚合的统计值，也是聚合运算的结果，它一般是连续的值，如图1-2中的销售额，抑或是销售商品的总件数据 。通过比较和测量试题，分析师可以对数据进行评估，比如今年的销售额相比去年有多大的增长，增长的速度是否达到预期，不同商品类别的增长比例是否合理等。  \n\n##Cube和Cuboid\n有了维度和度量，一个数据表或数据模型上的所有字段就可以分类了，它们要么是维度，要么是度量（可以被聚合）。于是就有了根据维度和度量来做预计算的Cube理论。  \n给定一个数据模型，我们可以对其上的所有维度进行组合。对于N个维度来说，组合的所有可能共2的n次方种。对于每一种维度的组合，将度量做聚合运算，然后将运算的结果保存为一个物化视图，称为Cuboid。所有维度组合的Coboid作为一个整体，被称为Cube。所以简单来说一个Cube就是许多按维度聚合的物化视图的集合。  \n下面来举一个具体的例子。假定有一个电商的销售数据集，其中维度包括时间（Time）、商品（Item）、地点（Location）和供应商（Supplier），度量为销售额（GMV）。那么所有维度的组合就有2的4次方=16种，比如一维度（ID）的组合有[Time]、[Item]、[Location]、[Supplier]4种；二维度（3D）的组合有[Time,Item]、[Time，Location]、[Time,Supplier]、[Item,Location]、[Item,Supplier]、[Location,Supplier]6种；三维度（3D）的组合也有4种；最后零维度（0D）和四维度（4D）的组合各有1种，总共有16种组合。\n![\"一个四维Cube的例子\"](/images/hadoop/kylin/一个四维Cube的例子.jpg)\n\t\t\n计算Cuboid，即按维度来聚合销售额。如果用SQL语句来表达计算Cuboid[Time,Location]，那么SQL语句如下：  \nSelect Time,Location,Sum(GMV) as GMV from Sales group by Time,Location.  \n将计算的结果保存为物化视图，所有Cuboid物化视图的总称是Cube。\n\n##工作原理\nApache Kylin的工作原理就是对数据模型做Cube预计算，并利用计算的结果加速查询，具体工作过程如下：\n1）指定数据模型，定义维度和度量\n2）预计算Cube，计算所有Cuboid并保存为物化视图。\n3）执行查询时，读取Cuboid，运算，产生查询结果。\n由于Kylin的查询过程不会扫描原始记录，而是通过预计算预先完成表的关联、聚合等复杂运算，并利用预计算的结果来执行查询，因此相比非预计算的查询技术，其速度一般要快一到两个数据级，并且这点在超磊的数据集上优势更加明显。当数据集达到千亿及至万亿级别时，Kylin的速度甚至可以超越其他非预计算技术1000倍以上。\n\n#技术架构\nApache Kylin系统可以分为在线查询和离线构建两部分，技术架构如下图所示，在线查询的模块主要处于上半区，而离线构建则处于下半区。\n![\"Kylin的技术架构\"](/images/hadoop/kylin/Kylin的技术架构.jpg)\t\n\n我们首先看看离线构建的部分。从图1-4可以看出，数据源在左侧，目前主要是Hadoop Hive，保存着待分析的用户数据。根据元数据的字义，下方构建引擎从数据源抽取数据，并构建Cube。数据以关系表的形式输入，且必须符合星形模型（Star Schema）（更复杂的雪花模型在成文时还不支持，可以通过视图将雪花模型转化为星形模型，再使用Kylin）。MapRecue是当前主要的构建技术。构建后的Cube保存在右侧的存储引擎中，一般选用HBase作为存储。  \n完成了离线构建之后，用户可以从上方查询系统发送SQL进行查询分析。Kylin提供了各种Rest　API、ＪＤＢＣ／ＯＤＢＣ接口。无论从哪个接口进入，SQL最终都会来到Rest服务层，再转交给查询引擎进行处理。这里需要注意的是，SQL语句是基于数据源的关系模型书写的，而不是Cube。Kylin在设计时刻意对查询用户屏蔽了Cube的概念，分析师只需要理解简单的关系模型就可以使用Kylin，没有额外的学习门槛，传统的SQL应用也很容易迁移。查询引擎解析SQL，生成基于关系表的逻辑执行计划，然后将其转义为基于Cube的物理执行计划，最后查询预计算生成的Cube并产生结果。整个过程不会访问原始数据源。  \n\n\n**注意**：对于查询引擎下方的路由选择，在最初设计时曾考虑过将Kylin不能执行查询引导去Hive中继续执行，但在实践后发现Hive与Kylin的速度差异过大，导致用户无法对查询的速度有一致的期望，很可能大多数据查询几秒内就返回结果了，而有些查询则要等几分钟到几十分钟，因此体验非常糟糕。最后这个路由功能在发行版中默认关闭。\n\n\nApache Kylin 1.5版本引入了“可扩展架构”的概念。在图1-4中显示为三个粗虚框，表示的抽象层。可扩展指Kylin可以对其主要依赖的三个模块做任意的扩展和替换。Kylin的三大依赖模型分别是数据源、构建引擎和存储引擎。在设计之初，作为Hadoop家族 一员，这三者分别是Hive、MapRecue和HBase。但随着推广和使用的深入，渐渐有用户发现它们均存在不足之处。比如，实时分析可能会希望从Kafka导入数据而不是Hive；而Spark的迅速崛起，又使我们不得不考虑将MapRecue替换为Spark，以期大幅提高Cube的构建速度；至于HBase，它的读性能可能还不如Cassandra或Kudu等 。可见，是否可以将一种技术替换为另一种技术已成为一个常见的问题。于在1.5版本的系统架构进行了重构，将数据源、构建引擎、存储引擎三大依赖抽象为接口，而Hive、MapRecue、HBase只是默认实现。深度用户可以根据自己的需要做二次开发，将其中的一个或多个替换为更适合的技术。  \n\n#核心概念\n##数据仓库\n数据仓库（Data Warehouse）是一种系统的资料储存理论，此理论强调的是利用某些特殊的资料储存方式，让所包含的资料特别有利于分析和处理，从而产生有价值的资讯，并可依此做出决策。\n利用数据仓库的方式存放资料，具有一旦存入，便不会随时间发生变动的特性，此外，存入的资料必定包含时间属性，通常一个数据仓库中会含有大量的历史性资料，并且它可利用特定的分析方式，从其中发掘特定的资讯。\n\n##OLAP\nOLAP（Online Analytical Process），联机分析处理，以多维度的方式分析数据，而且能够弹性地提供上卷（Roll-up）、下钻（Drill-down）和透视分析（Pivot）等操作，它呈现集成性决策信息的方法，多用于决策支持系统、商务智能或数据仓库。其主要的功能在于方便大规模数据分析及统计计算，可对决策提供参考和支持。与之相区别的是取机交易处理（OLTP），联机交易处理，更侧重于基本的、日常的事务处理，包括数据的增删改查。\nOLAP需要以大量历史数据为基础，再配合时间点的差异，对多维度及汇整型的信息进行复杂的分析。\nOLAP需要用户有主观的信息需求定义，因此系统效率较佳。\nOLAP的概念，在实际应用中存在广义和狭义两种不同的理解方式。广义上的理解与字面上的意义相同，泛指一切不会对数据进行更新的分析处理。但更多的情况下OLAP被理解为其狭义上的含义，即与多维分析相关，基于立方体（Cube）计算而进行的分析。\n\n##BI\nBI（Business Intelligence），即商务智能，指现代数据仓库技术、在线分析技术、数据挖掘和数据展现技术进行数据分析以实现商业价值。\n\n\n##维度和度量\n维度和度量是数据分析中的两个基本的概念\n**维度**是指审视数据的角度，它通常是数据记录的一个属性，例如时间、地点等。  \n**度量**是基于数据所计算出来的考量值；它通常是一个数值，如总销售额、不同的用户数等。分析人员往往要结合若干个维度来审查度量值，以便在其中找到变化规律。在一个SQL查询中，Group By的属性通常就是维度，而所计算的值则是度量。如下面的示例：  \n    select part_dt,lstg_iste_id,sum(price) as total_selled,count(distinct seller_id) as sellers from kylin_sales group by part_dt,lstg_site_id\n\n##事实表和维度表\n**事实表**（Fact Table）是指存储有事实记录的表，如系统日志、销售记录等；事实表的记录在不断地动态增长，所以它的体积通常远大于其他表。\n\n**维度表**（Dimension Table）或维表，有时也称查找表（Lookup Table），是与事实表相对应的一种表；它保存了维度的属性值，可以跟事实表做关联；相当于将事实表上经常重复出现的属性抽取、规范出来用一张表进行管理。常见的维度表有：日期表（存储与日期对应的周、月、季度等属性）、地点表（包含国家、省、城市等属性）。使用维度表有诸多好处，具体如下：\na.缩小了事实表的大小\nb.便于维度的管理和维护，增加、删除和修改维度的属性，不必对事实表的大量记录进行改动。\nc.维度表可以为多个事实表重用，以减少重复工作。\n\n##Cube、Cuboid和Cube Segment\n###Cube\nCube（或Data Cube），即数据立方体，是一种常用于数据分析与索引的技术；它可以对原始数据建立多维度索引。通过Cube对数据进行分析，可以大大加快数据的查询效率。\n\n###Cuboid\nCuboid在Kylin中特指在某一种维度组合下所计算的数据。\n\n##Cube Segment\nCube Segment是指针对源数据中的某一片段，计算出来的Cube数据。通常数据仓库中的数据数量会随着时间的增长而增长，而Cube Segment也是按时间顺序来构建的。\n\n\n\n#在Hive中准备数据\n这里介绍准备Hive数据的一些注意事项。需要被分析的数据必须先保存为Hive表的形式，然后Kylin才能从Hive中导入数据，创建Cube。\nHive是一个基于Hadoop的数据仓库工具，可以将结构化的数据文件映射为数据库表，并可以将SQL语句转换为MapRecue或Tez任务进行运行，从而让用户以类SQL（HiveQL，也称HQL）的方式管理和查询Hadoop上的海量数据。\n此外，Hive还提供了多种方式（如命令行、API和Web服务等）可供第三方方便地获取和使用元数据并进行查询。今天，Hive已经成为Hadoop数据仓库的首选，是Hadoop上不可或缺的一个重要组件，很多项目都已兼容或集成了Hive。基于此情况，Kylin选择Hive作为原始数据的主要来源。\n在Hive中准备待分析的数据是使用Kylin的前提；将数据导入到Hive表中的方法有很多，用户管理数据的技术和工具也各式各样，因此具体步骤不在本书的讨论范围之内。\n\n##星形模型\n数据挖掘有几种常见的多维数据模型，如星形模型（Star Schema）、雪花模型（Snowf lake Schema）、事实星座模型（Fact Constellation）等。  \n星形模型中有一张事实表，以及零个或多个维度表；事实表与维度表通过主键外键相关联，维度表之间没有关联，就像很多星星围绕在一个恒星周围，帮取名为星形模型。\n如果将星形模型中某些维度的表再做规范，抽取成更细的维度表，然后让维度表之间也进行关联，那么这种模型称为雪花模型。\n星形模型是更复杂的模型，其中包含了多个事实表，而维度表是公用的，可以共享。\n不过，Kylin只支持星形模型的数据集，这是基于以下考虑的。  \n\n- 星形模型是最简单，也是最常用的模型  \n- 由于星形模型只有一张大表，因此它相比于其它模型更适合于大数据处理  \n- 其他模型可以通过一定的转换，变成星形模型。  \n\n##维度表的设计\n除了数据模型以外，Kylin还对维度表有一定的要求，具体要求如下。  \n\n\n- 要具有数据一致性，主键值必须是唯一的；Kylin会进行检查，如果有两行的主键值相同则会报错。\n- 维度越小越好，因为Kylin会将维度表加载到内存中供查询；过大的表不适合作为维度表，默认的阈值是300MB。  \n- 改变频率低，Kylin会在每次构建中试图重用维度表的快照，如果维度表经常改变的话，重用就会失效，这就会导致要经常对维度表创建快照。\n- 维度表最好不要是Hive视图（View），虽然在Kylin1.5.3中加入了对维度表是视图这种情况的支持，但每次都需要将视图进行物化，从而导致额外的时间开销。\n\n##Hive表分区\nHive支持多分区（Partition）。简单来说，一个分区就是一个文件目录，存储了特定的数据文件。当有新的数据生成的时候，可以将数据加载到指定的分区，读取数据的时候也可以指定分区。对于 SQL查询，如果查询中指定了分区列的属性条件，则Hive会智能地选择特定分区（也就是目录），从而避免全量数据的扫描，减少读写操作对集群的压力。\n下面举的一组SQL演示了如何使用分区：  \n\nHie>create table invites(id int,name string) partitioned by(ds string) row format delimited fields terminated by 't' stroed as textfile;  \nHive>load data local inpath '/user/hadoop/data.txt' overwrite into table invites partition (ds='2016-08-16');  \nHive>select * from invites where ds = '2016-08-16';  \nKylin支持增量的Cube构建，通常是按时间属性来增量地从Hive表中抽取数据。如果Hive表正好是按此时间属性做分区的话，那么就可以利用到Hive分区的好处，每次在Hive构建的时候都可以直接跳过不相干的日期的数据，节省Cube构建的时间。这样的列在Kylin里也称为分割时间列（Partition Time Column），通常它应该也是Hive表的分区列。\n\n\n##了解维度的基数\n维度的基数（Cardinality）指的是该维度在数据集中出现的不同值的个数；例如“国家”是一个维度，如果有200个不同的值，那么此维度的基数就是200.通常一个维度的基数会从几十到几万个不等，个别维度如“用户ID”的基数会超过百万甚至千万。基数超过一百万的维度通常称为超高维度（Ulta Hight Cardinality，UHC），需要引起设计者的注意。  \nCube中所有维度的基数都可以体现Cube的复杂度，如果一个Cube中有好几个超高基数维度，那么这个Cube膨胀就会很高。在创建Cube前需要对所有维度的基数做一个了解，这样就可以帮助设计合理的Cube。计算基数有多种途径，最简单的方法就是让Hive执行一个count distinct的SQL查询；Kylin也提供计算基数的方法，在导入Hive表定义后可以看到每一个列的基数，参数名为Cardinality\n\n##Sample Data\n如果需要快速体验Kylin，可以用Kylin自带的Sample Data。运行${KYLIN_HOME}/bin/sample.sh来导入Sample Data，然后就能按照下面的流程来创建模型和Cube。  \n具体请执行下面命令，将Sample Data导入到Hive数据库。  \ncd ${KYLIN_HOME}  \nbin/sample.sh  \nSample Data测试的样例数据集总共仅1M左右，共计3张表，其中事实表有10000条数据。数据集是一个规范的星形模型结构，它总包含3个数据表：    \nKYLIN_SALES是事实表，保存了销售订单的明细信息。各列分别保存着卖家、商品、分类、订单金额、商品数据等信息，每一行对应着一笔交易订单。  \nKYLIN_CATEGORY_GROUPINGS是维表，保存了商品分类的详细介绍，例如商品分类名称等。  \nKYLIN_CAL_DT也是维表，保存了时间的扩展信息。如单个日期所在的年始、月始、周始、年份、月份等。  \n这3张表一起构成了整个星形模型。  \n\n#设计Cube\n如果数据已经在Hive中准备好了，就可以开始创建Cube了。\n##导入Hive表定义\n登陆Kylin的Web界面，创建新的或选择一个已有的项目之后，需要做的就是将Hive表的定义导入到Kylin中。  \n单击Web界面的Model->Data Source下的”Local Hive Table“图标，然后输入表的名称（可以一次导入多个表，以逗号分隔表名），单击按钮”Sync“，Kylin就会使用Hive的API从Hive中获取表的属性信息。  \n导入成功后，表的结构信息会以树状的形式显示在页面的左侧，可以单击展开或收缩。\n\n同时Kylin会在后台触发一个MapRecue任务，计算此表的每个列的基数。通常稍过几分钟后再刷新页面，就会看到显示出来 的基数信息Cardinality\n\n需要注意的是，这里Kylin对基数的计算方法采用的是HyperLogLog的近似算法，与精确值略有误差，只做参考值。\n\n##创建数据模型\n有了表信息之后，就可以开始创建数据模型（Data Model）了。数据模型是Cube的基础，它主要用于描述一个星形模型。有了数据模型以后，定义Cube的时候就可以直接从此模型定义的表和列中进行选择了，省去重复指定连接（join）条件的步骤。基于一个数据模型还可以创建多个Cube，以方便减少用户的重复性工作。  \n在Kylin界面中”Models“页面中单击”New\"->\"New Model\"，开始创建数据模型。\n\n\n接下来选择用作维度和度量的列。这里只是选择一个范围，不代表这些列将来一定要用作Cube 的维度或度量，你可以把所有可能会用到表都选进来，后续创建Cube的时候，将只能从这些列中进行选择。   \n\n选择维度列时，维度可以来自事实表或维度表  \n选择度量列时，度量只能来自事实表  \n最后一步，是为模型补充侵害时间列信息和过滤条件。如果此模型中的事实表记录是按时间增长的，那么可以指定一个日期/时间列作为模型的分割时间列，从而可以让Cube按此列做增量构建。\n\n过滤（Filter）条件是指，如果想把一些记录忽略掉，那么这里可以设置一个过滤条件。Kylin在向Hive请求源数据的时候，会带上此过滤条件。\n\n随后“Save”后，出现在“Model”的列表中。\n\n\n##创建Cube\n单击“New”，选择“New Cube”，会开启一个包含若干步骤的向导。\n\n第一页，选择要使用的数据模型，并为此Cube输入一个唯一的名称（必需的）和描述（可选的）；这里还可以输入一个邮件通知列表，用于在构建完成或出错时收到通知。如果不想接收处于某些状态的通知，那么可以从“Notification Events”中将其去掉。\n\n第二页，选择Cube的维度。可以通过以下两个按钮来添加维度。   \n**“Add Mimension”**：逐个添加维度，可以是普通维度也可以是衍生（Derived）维度。\n**“Auto Generator”：**批量选择并添加，让Kylin自动完成其它信息。  \n使用第一种方法的时候需要为每个维度起个名字，然后选择表和列。  \n如果是衍生维度的话，则必须是来自于某个维度表，一次可以选择多个列；由于这些列值都可以从该维度表的主键值中衍生出来，所以实际上只有主键列会被Cube加入计算。而在Kylin 的具体实现中，往往采用事实表上的外键替代主键进行计算和存储。但是在逻辑上可以认为衍生列来自于维度表的主键。  \n使用第二种方法，Kylin会用一个树状结构呈现出所有的列，用户只需要勾选所需要的列即可，Kylin会自动补充其他信息，从而方便用户的操作。请注意，在这里Kylin会把维度表上的列都创建成衍生维度，这也许不是最合适的，在这种情况下请使用第一种方法。\n\n\n第三页，创建度量。Kylin默认会创建一个Count(1)的度量。可以单击“+Measure\"按钮来添加新度量。Kylin支持的度量有：SUM、MIN、MAX、COUNT、COUNT　DISTINCT、ＴＯＰ＿Ｎ、RAW等。请选择需要的度量类型，然后再选择适当的参数（通常为列名）\n\n重复上面的步骤，创建所需要的度量。Kylin可以支持在一个Cube中添加多达上百个度量；添加完成所有度量之后，单击“Next”。\n\n第四页，是关于Cube数据刷新的设置。在这里可以设置自动合并的阈值、数据保留的最短时间，以及第一个Segment的起点时间（如果Cube有分割时间列的话）。\n\n第五页，高级设置。在此页面上可以设置聚合组和Rowkey\nKylin默认会把所有的维度都放在同一个聚合中；如果维度数据较多（例如>10），那么建议用户根据查询的习惯和模式，单击“New Aggregation Group+”，将维度分为多个聚合组。通过使用多个聚合组，可以大大降低Cube中的Cuboid数量。下面来举例说明，如果一个Cube有（M+N)个维度，那么默认它会有2的m+n次方个Cuboid；如果把这些维度分为两个不相交的聚合组，那么Cuboid的数量将被减少为2的m次方+2的n次方。  \n在单个聚合组中，可以对维度设置高级属性，例如Mandatory、Hierarchy、Joint等。这几个属性都是为了优化Cube的计算而设计的，了解这些属性的含义对日后更好地使用Cube至关重要。  \nMandatory维度指的是那些总是会出现在where条件或Group By语句里的维度；通过将某个维度指定为Mandatory，Kylin就可以不用预计算那些不包含此维度的Cuboid，从而减少计算量。  \nHierarchy是一组有层级关系的维度，例如：“国家”“省”“市”，这里的“国家”是高级的维度，“省”“市”依次是低级的维度。用户会按高级别维度进行查询，也会按低级别维度进行查询，但在查询低级别维度时，往往都会带上高级别维度的条件，而不会孤立地审视低级别维度的数据。例如，用户单击“国家”作为维度来查询汇总数据，也可能单击“国家”+“省”或者“国家”+“省”+“市”来查询，但是不会跨越国家直接Group By“省”或“市”。通过指定Hierarchy，Kylin可以省略不满足此模式的cuboid。  \nJoint是将多个维度组合成一个维度，其通常适用于如下两种情况。\n1.总是会在一起查询的维度。  \n2.基数很低的维度  \nKylin以Key-Value的方式将Cube存在到HBase中。HBase的key，也就是Rowkey，是由各维度的值拼接而成的；为了更高效地存储这些值，Kylin会对它们进行编码和压缩；每个维度均可以选择合适的编码（Encoding）方式，默认采用的是字典（Dictionary）编码技术；除了字典以外，还有整数（Int）和固定长度（Fixed Length）的编码。  \n字典编码是将此维度下所有值构建成一个从string到int的映射表；Kylin会将字典序列化保存，在Cube中存储int值，从而大大减小存储的大小。另外，字典是保持顺序的，即如果字符串A比字符串B大的话，那么A的编码后的int值也会比B编码后的值大；这样可以使得在HBase中进行比较查询的时候，依然使用编码后的值，而无需解码。\n\n字典非常适合于非固定长度的string类型值的维度，而且用户无需指定编码后的长度；但是由于使用字典需要维护一张映射表，因些如果此维度的基数很高，那么字典的大小就非常可观，从而不适合于加载到内存中，在这种情况下就要选择其他的编码方式了。Kylin中字典编码允许的基数上限默认是500万（由参数\"kylin.dictioinary.max.cardinality\"配置）。  \n整数（int）编码适合于对int或bigint类型的值进行编码，它无需额外存储，同时还可以支持很大的基数。用户需要根据值域选择编码的长度。例如有一个手机号码的维度，它是一个11位的数字，如13800138000，我们知道它大于2的31次方，但是小于2的39次方减1，那么使用int(5)即可满足要求，每个值占用5字节，比按字符存储（11字节）要少占一半以上的空间。  \n\n当上面几种编码方式都不适合的时候，就使用固定长度的编码了；此编码方式其实只是将原始值截断或补充成相同长度的一组字节，没有额外的转换，所以空间效率较差，通常只是作为一种权宜手段。  \n各维度在Rowkeys中的顺序，对于 查询的性能会产生较明显的影响。在这里用户可以根据查询的模式和习惯，通过拖拽的方式调整各个维度在Rowkeys上的顺序。通常的原则是，将过滤频率较高的列放置在过滤频率较低的列之前，将基数高的列放置在基数低的列之前。这样做的好处是，充分利用过滤条件来缩小在HBase中扫描的范围，从而提高查询的效率。  \n第五页，为Cube配置参数。和其他Hadoop工具一样，Kylin使用了很多配置参数以提高录活性，用户可以根据具体的环境、场景等配置不同的参数进行调优。Kylin全局的参数值可在conf/kylin.properties文件中进行配置；如果Cube需要覆盖全局设置的话，则需要在此页面中指定。单击“+Property”按钮，然后输入参数名和参数值。例如“kylin.hbase.region.cut=1\",这样此Cube在存储的时候，Kylin将会为每个HTbase Region分配1GB来创建一个HTbase Region。\n\n\n\n\n#构建Cube\n新创建的Cube只有定义，而没有计算的数据，它的状态是”DISABLED“，是不会被查询引擎挑中的。要想让Cube有数据，还需要对它进行构建。Cube的构建方式通常有两种：全量构建和增量构建；两者的构建步骤是完全一样的，区别只在于构建时读取的数据源是全集还是子集。  \nCube的构建包含如下步骤，由任务引擎来调度执行。  \n1）创建临时的Hive平表（从Hive读取数据）  \n2）计算各维度的不同值，并收集各Cuboid的统计数据。  \n3）创建并保存字典。  \n4）保存Cuboid统计信息。  \n5）创建HTable。  \n6）计算Cube（一轮或若干轮MapRecue）。  \n7）将Cube的计算结果转成HFile。  \n8）加载HFile到HBase。  \n9）更新Cube元数据。  \n10）垃圾回收。  \n以上步骤中，前5步是计算Cube而做的准备工作，例如遍历维度值来创建字典，对数据做统计秋估算以创建HTable等；第6）步是真正的Cube计算，取决于所使用的Cube算法，它可能是一轮MapRecue任务，也可能是N（在没有优化的情况下，N可以被视作是维度数）轮迭代的MapRecue。由于Cube运算的中间结果是以SequenceFile的格式存储在HDFS上的，所以为了导入到HBase中，还需要第7）步将这些结果转换成HFile（HBase文件存储格式）。第8）步通过使用HBase BulkLoad工具，将HFile导入到HBase集群，这一步完成之后，HTable就可以查询到数据了。第9）步更新Cube的数据，将此次构建 Segment的状态从”NEW“更新为”ＲＥＡＤＹ＂，表示已经可借查询了。最后一步，清理构建过程中生成的临时文件等垃圾，释放集群资源。　　\nMonitor页面会显示当前项目下近期的构建任务。　　\n\n\n\n##全量构建和增量构建\n\n###全量构建\n\n\n###增量构建\n\n\n\n##历史数据刷新\n\n\n##合并\n\n#查询Cube\n\n\n","source":"_posts/hadoop/Kylin入门概念.md","raw":"---\ntitle: Kylin入门概念\ndate: 2017-04-16 23:43:49\ntags: [大数据,kylin]\ncategories: [大数据,kylin]\n---\n\n#Apache Kylin的工作原理\nApache Kylin的工作原理本质上是MOLAP（Multidimensional　Online　Analytical　Processing）Cube，也就是多维　立方体分析。这是数据分析中相当经典的理论，在关系数据库年代就已经有了广泛的应用，下面将其做简要的介绍。  \n\n##维度和度量\n简单来讲，维度就是观察数据的角度。比如电商的销售数据，可以从时间的维度来观察，也可以进一步细化，从时间和地区的维度来观察。维度一般是一组离散的值，比如时间维度上的每一个独立的日期，或者商品维度上的每一件独立的商品。因此统计时可以把维度值 相同的记录聚合在一起，然后应用聚合函数做累加、平均、去重计数等聚合计算。  ![\"维度和度量的例子\"](/images/hadoop/kylin/维度和度量的例子.jpg)\n    \n度量就是被聚合的统计值，也是聚合运算的结果，它一般是连续的值，如图1-2中的销售额，抑或是销售商品的总件数据 。通过比较和测量试题，分析师可以对数据进行评估，比如今年的销售额相比去年有多大的增长，增长的速度是否达到预期，不同商品类别的增长比例是否合理等。  \n\n##Cube和Cuboid\n有了维度和度量，一个数据表或数据模型上的所有字段就可以分类了，它们要么是维度，要么是度量（可以被聚合）。于是就有了根据维度和度量来做预计算的Cube理论。  \n给定一个数据模型，我们可以对其上的所有维度进行组合。对于N个维度来说，组合的所有可能共2的n次方种。对于每一种维度的组合，将度量做聚合运算，然后将运算的结果保存为一个物化视图，称为Cuboid。所有维度组合的Coboid作为一个整体，被称为Cube。所以简单来说一个Cube就是许多按维度聚合的物化视图的集合。  \n下面来举一个具体的例子。假定有一个电商的销售数据集，其中维度包括时间（Time）、商品（Item）、地点（Location）和供应商（Supplier），度量为销售额（GMV）。那么所有维度的组合就有2的4次方=16种，比如一维度（ID）的组合有[Time]、[Item]、[Location]、[Supplier]4种；二维度（3D）的组合有[Time,Item]、[Time，Location]、[Time,Supplier]、[Item,Location]、[Item,Supplier]、[Location,Supplier]6种；三维度（3D）的组合也有4种；最后零维度（0D）和四维度（4D）的组合各有1种，总共有16种组合。\n![\"一个四维Cube的例子\"](/images/hadoop/kylin/一个四维Cube的例子.jpg)\n\t\t\n计算Cuboid，即按维度来聚合销售额。如果用SQL语句来表达计算Cuboid[Time,Location]，那么SQL语句如下：  \nSelect Time,Location,Sum(GMV) as GMV from Sales group by Time,Location.  \n将计算的结果保存为物化视图，所有Cuboid物化视图的总称是Cube。\n\n##工作原理\nApache Kylin的工作原理就是对数据模型做Cube预计算，并利用计算的结果加速查询，具体工作过程如下：\n1）指定数据模型，定义维度和度量\n2）预计算Cube，计算所有Cuboid并保存为物化视图。\n3）执行查询时，读取Cuboid，运算，产生查询结果。\n由于Kylin的查询过程不会扫描原始记录，而是通过预计算预先完成表的关联、聚合等复杂运算，并利用预计算的结果来执行查询，因此相比非预计算的查询技术，其速度一般要快一到两个数据级，并且这点在超磊的数据集上优势更加明显。当数据集达到千亿及至万亿级别时，Kylin的速度甚至可以超越其他非预计算技术1000倍以上。\n\n#技术架构\nApache Kylin系统可以分为在线查询和离线构建两部分，技术架构如下图所示，在线查询的模块主要处于上半区，而离线构建则处于下半区。\n![\"Kylin的技术架构\"](/images/hadoop/kylin/Kylin的技术架构.jpg)\t\n\n我们首先看看离线构建的部分。从图1-4可以看出，数据源在左侧，目前主要是Hadoop Hive，保存着待分析的用户数据。根据元数据的字义，下方构建引擎从数据源抽取数据，并构建Cube。数据以关系表的形式输入，且必须符合星形模型（Star Schema）（更复杂的雪花模型在成文时还不支持，可以通过视图将雪花模型转化为星形模型，再使用Kylin）。MapRecue是当前主要的构建技术。构建后的Cube保存在右侧的存储引擎中，一般选用HBase作为存储。  \n完成了离线构建之后，用户可以从上方查询系统发送SQL进行查询分析。Kylin提供了各种Rest　API、ＪＤＢＣ／ＯＤＢＣ接口。无论从哪个接口进入，SQL最终都会来到Rest服务层，再转交给查询引擎进行处理。这里需要注意的是，SQL语句是基于数据源的关系模型书写的，而不是Cube。Kylin在设计时刻意对查询用户屏蔽了Cube的概念，分析师只需要理解简单的关系模型就可以使用Kylin，没有额外的学习门槛，传统的SQL应用也很容易迁移。查询引擎解析SQL，生成基于关系表的逻辑执行计划，然后将其转义为基于Cube的物理执行计划，最后查询预计算生成的Cube并产生结果。整个过程不会访问原始数据源。  \n\n\n**注意**：对于查询引擎下方的路由选择，在最初设计时曾考虑过将Kylin不能执行查询引导去Hive中继续执行，但在实践后发现Hive与Kylin的速度差异过大，导致用户无法对查询的速度有一致的期望，很可能大多数据查询几秒内就返回结果了，而有些查询则要等几分钟到几十分钟，因此体验非常糟糕。最后这个路由功能在发行版中默认关闭。\n\n\nApache Kylin 1.5版本引入了“可扩展架构”的概念。在图1-4中显示为三个粗虚框，表示的抽象层。可扩展指Kylin可以对其主要依赖的三个模块做任意的扩展和替换。Kylin的三大依赖模型分别是数据源、构建引擎和存储引擎。在设计之初，作为Hadoop家族 一员，这三者分别是Hive、MapRecue和HBase。但随着推广和使用的深入，渐渐有用户发现它们均存在不足之处。比如，实时分析可能会希望从Kafka导入数据而不是Hive；而Spark的迅速崛起，又使我们不得不考虑将MapRecue替换为Spark，以期大幅提高Cube的构建速度；至于HBase，它的读性能可能还不如Cassandra或Kudu等 。可见，是否可以将一种技术替换为另一种技术已成为一个常见的问题。于在1.5版本的系统架构进行了重构，将数据源、构建引擎、存储引擎三大依赖抽象为接口，而Hive、MapRecue、HBase只是默认实现。深度用户可以根据自己的需要做二次开发，将其中的一个或多个替换为更适合的技术。  \n\n#核心概念\n##数据仓库\n数据仓库（Data Warehouse）是一种系统的资料储存理论，此理论强调的是利用某些特殊的资料储存方式，让所包含的资料特别有利于分析和处理，从而产生有价值的资讯，并可依此做出决策。\n利用数据仓库的方式存放资料，具有一旦存入，便不会随时间发生变动的特性，此外，存入的资料必定包含时间属性，通常一个数据仓库中会含有大量的历史性资料，并且它可利用特定的分析方式，从其中发掘特定的资讯。\n\n##OLAP\nOLAP（Online Analytical Process），联机分析处理，以多维度的方式分析数据，而且能够弹性地提供上卷（Roll-up）、下钻（Drill-down）和透视分析（Pivot）等操作，它呈现集成性决策信息的方法，多用于决策支持系统、商务智能或数据仓库。其主要的功能在于方便大规模数据分析及统计计算，可对决策提供参考和支持。与之相区别的是取机交易处理（OLTP），联机交易处理，更侧重于基本的、日常的事务处理，包括数据的增删改查。\nOLAP需要以大量历史数据为基础，再配合时间点的差异，对多维度及汇整型的信息进行复杂的分析。\nOLAP需要用户有主观的信息需求定义，因此系统效率较佳。\nOLAP的概念，在实际应用中存在广义和狭义两种不同的理解方式。广义上的理解与字面上的意义相同，泛指一切不会对数据进行更新的分析处理。但更多的情况下OLAP被理解为其狭义上的含义，即与多维分析相关，基于立方体（Cube）计算而进行的分析。\n\n##BI\nBI（Business Intelligence），即商务智能，指现代数据仓库技术、在线分析技术、数据挖掘和数据展现技术进行数据分析以实现商业价值。\n\n\n##维度和度量\n维度和度量是数据分析中的两个基本的概念\n**维度**是指审视数据的角度，它通常是数据记录的一个属性，例如时间、地点等。  \n**度量**是基于数据所计算出来的考量值；它通常是一个数值，如总销售额、不同的用户数等。分析人员往往要结合若干个维度来审查度量值，以便在其中找到变化规律。在一个SQL查询中，Group By的属性通常就是维度，而所计算的值则是度量。如下面的示例：  \n    select part_dt,lstg_iste_id,sum(price) as total_selled,count(distinct seller_id) as sellers from kylin_sales group by part_dt,lstg_site_id\n\n##事实表和维度表\n**事实表**（Fact Table）是指存储有事实记录的表，如系统日志、销售记录等；事实表的记录在不断地动态增长，所以它的体积通常远大于其他表。\n\n**维度表**（Dimension Table）或维表，有时也称查找表（Lookup Table），是与事实表相对应的一种表；它保存了维度的属性值，可以跟事实表做关联；相当于将事实表上经常重复出现的属性抽取、规范出来用一张表进行管理。常见的维度表有：日期表（存储与日期对应的周、月、季度等属性）、地点表（包含国家、省、城市等属性）。使用维度表有诸多好处，具体如下：\na.缩小了事实表的大小\nb.便于维度的管理和维护，增加、删除和修改维度的属性，不必对事实表的大量记录进行改动。\nc.维度表可以为多个事实表重用，以减少重复工作。\n\n##Cube、Cuboid和Cube Segment\n###Cube\nCube（或Data Cube），即数据立方体，是一种常用于数据分析与索引的技术；它可以对原始数据建立多维度索引。通过Cube对数据进行分析，可以大大加快数据的查询效率。\n\n###Cuboid\nCuboid在Kylin中特指在某一种维度组合下所计算的数据。\n\n##Cube Segment\nCube Segment是指针对源数据中的某一片段，计算出来的Cube数据。通常数据仓库中的数据数量会随着时间的增长而增长，而Cube Segment也是按时间顺序来构建的。\n\n\n\n#在Hive中准备数据\n这里介绍准备Hive数据的一些注意事项。需要被分析的数据必须先保存为Hive表的形式，然后Kylin才能从Hive中导入数据，创建Cube。\nHive是一个基于Hadoop的数据仓库工具，可以将结构化的数据文件映射为数据库表，并可以将SQL语句转换为MapRecue或Tez任务进行运行，从而让用户以类SQL（HiveQL，也称HQL）的方式管理和查询Hadoop上的海量数据。\n此外，Hive还提供了多种方式（如命令行、API和Web服务等）可供第三方方便地获取和使用元数据并进行查询。今天，Hive已经成为Hadoop数据仓库的首选，是Hadoop上不可或缺的一个重要组件，很多项目都已兼容或集成了Hive。基于此情况，Kylin选择Hive作为原始数据的主要来源。\n在Hive中准备待分析的数据是使用Kylin的前提；将数据导入到Hive表中的方法有很多，用户管理数据的技术和工具也各式各样，因此具体步骤不在本书的讨论范围之内。\n\n##星形模型\n数据挖掘有几种常见的多维数据模型，如星形模型（Star Schema）、雪花模型（Snowf lake Schema）、事实星座模型（Fact Constellation）等。  \n星形模型中有一张事实表，以及零个或多个维度表；事实表与维度表通过主键外键相关联，维度表之间没有关联，就像很多星星围绕在一个恒星周围，帮取名为星形模型。\n如果将星形模型中某些维度的表再做规范，抽取成更细的维度表，然后让维度表之间也进行关联，那么这种模型称为雪花模型。\n星形模型是更复杂的模型，其中包含了多个事实表，而维度表是公用的，可以共享。\n不过，Kylin只支持星形模型的数据集，这是基于以下考虑的。  \n\n- 星形模型是最简单，也是最常用的模型  \n- 由于星形模型只有一张大表，因此它相比于其它模型更适合于大数据处理  \n- 其他模型可以通过一定的转换，变成星形模型。  \n\n##维度表的设计\n除了数据模型以外，Kylin还对维度表有一定的要求，具体要求如下。  \n\n\n- 要具有数据一致性，主键值必须是唯一的；Kylin会进行检查，如果有两行的主键值相同则会报错。\n- 维度越小越好，因为Kylin会将维度表加载到内存中供查询；过大的表不适合作为维度表，默认的阈值是300MB。  \n- 改变频率低，Kylin会在每次构建中试图重用维度表的快照，如果维度表经常改变的话，重用就会失效，这就会导致要经常对维度表创建快照。\n- 维度表最好不要是Hive视图（View），虽然在Kylin1.5.3中加入了对维度表是视图这种情况的支持，但每次都需要将视图进行物化，从而导致额外的时间开销。\n\n##Hive表分区\nHive支持多分区（Partition）。简单来说，一个分区就是一个文件目录，存储了特定的数据文件。当有新的数据生成的时候，可以将数据加载到指定的分区，读取数据的时候也可以指定分区。对于 SQL查询，如果查询中指定了分区列的属性条件，则Hive会智能地选择特定分区（也就是目录），从而避免全量数据的扫描，减少读写操作对集群的压力。\n下面举的一组SQL演示了如何使用分区：  \n\nHie>create table invites(id int,name string) partitioned by(ds string) row format delimited fields terminated by 't' stroed as textfile;  \nHive>load data local inpath '/user/hadoop/data.txt' overwrite into table invites partition (ds='2016-08-16');  \nHive>select * from invites where ds = '2016-08-16';  \nKylin支持增量的Cube构建，通常是按时间属性来增量地从Hive表中抽取数据。如果Hive表正好是按此时间属性做分区的话，那么就可以利用到Hive分区的好处，每次在Hive构建的时候都可以直接跳过不相干的日期的数据，节省Cube构建的时间。这样的列在Kylin里也称为分割时间列（Partition Time Column），通常它应该也是Hive表的分区列。\n\n\n##了解维度的基数\n维度的基数（Cardinality）指的是该维度在数据集中出现的不同值的个数；例如“国家”是一个维度，如果有200个不同的值，那么此维度的基数就是200.通常一个维度的基数会从几十到几万个不等，个别维度如“用户ID”的基数会超过百万甚至千万。基数超过一百万的维度通常称为超高维度（Ulta Hight Cardinality，UHC），需要引起设计者的注意。  \nCube中所有维度的基数都可以体现Cube的复杂度，如果一个Cube中有好几个超高基数维度，那么这个Cube膨胀就会很高。在创建Cube前需要对所有维度的基数做一个了解，这样就可以帮助设计合理的Cube。计算基数有多种途径，最简单的方法就是让Hive执行一个count distinct的SQL查询；Kylin也提供计算基数的方法，在导入Hive表定义后可以看到每一个列的基数，参数名为Cardinality\n\n##Sample Data\n如果需要快速体验Kylin，可以用Kylin自带的Sample Data。运行${KYLIN_HOME}/bin/sample.sh来导入Sample Data，然后就能按照下面的流程来创建模型和Cube。  \n具体请执行下面命令，将Sample Data导入到Hive数据库。  \ncd ${KYLIN_HOME}  \nbin/sample.sh  \nSample Data测试的样例数据集总共仅1M左右，共计3张表，其中事实表有10000条数据。数据集是一个规范的星形模型结构，它总包含3个数据表：    \nKYLIN_SALES是事实表，保存了销售订单的明细信息。各列分别保存着卖家、商品、分类、订单金额、商品数据等信息，每一行对应着一笔交易订单。  \nKYLIN_CATEGORY_GROUPINGS是维表，保存了商品分类的详细介绍，例如商品分类名称等。  \nKYLIN_CAL_DT也是维表，保存了时间的扩展信息。如单个日期所在的年始、月始、周始、年份、月份等。  \n这3张表一起构成了整个星形模型。  \n\n#设计Cube\n如果数据已经在Hive中准备好了，就可以开始创建Cube了。\n##导入Hive表定义\n登陆Kylin的Web界面，创建新的或选择一个已有的项目之后，需要做的就是将Hive表的定义导入到Kylin中。  \n单击Web界面的Model->Data Source下的”Local Hive Table“图标，然后输入表的名称（可以一次导入多个表，以逗号分隔表名），单击按钮”Sync“，Kylin就会使用Hive的API从Hive中获取表的属性信息。  \n导入成功后，表的结构信息会以树状的形式显示在页面的左侧，可以单击展开或收缩。\n\n同时Kylin会在后台触发一个MapRecue任务，计算此表的每个列的基数。通常稍过几分钟后再刷新页面，就会看到显示出来 的基数信息Cardinality\n\n需要注意的是，这里Kylin对基数的计算方法采用的是HyperLogLog的近似算法，与精确值略有误差，只做参考值。\n\n##创建数据模型\n有了表信息之后，就可以开始创建数据模型（Data Model）了。数据模型是Cube的基础，它主要用于描述一个星形模型。有了数据模型以后，定义Cube的时候就可以直接从此模型定义的表和列中进行选择了，省去重复指定连接（join）条件的步骤。基于一个数据模型还可以创建多个Cube，以方便减少用户的重复性工作。  \n在Kylin界面中”Models“页面中单击”New\"->\"New Model\"，开始创建数据模型。\n\n\n接下来选择用作维度和度量的列。这里只是选择一个范围，不代表这些列将来一定要用作Cube 的维度或度量，你可以把所有可能会用到表都选进来，后续创建Cube的时候，将只能从这些列中进行选择。   \n\n选择维度列时，维度可以来自事实表或维度表  \n选择度量列时，度量只能来自事实表  \n最后一步，是为模型补充侵害时间列信息和过滤条件。如果此模型中的事实表记录是按时间增长的，那么可以指定一个日期/时间列作为模型的分割时间列，从而可以让Cube按此列做增量构建。\n\n过滤（Filter）条件是指，如果想把一些记录忽略掉，那么这里可以设置一个过滤条件。Kylin在向Hive请求源数据的时候，会带上此过滤条件。\n\n随后“Save”后，出现在“Model”的列表中。\n\n\n##创建Cube\n单击“New”，选择“New Cube”，会开启一个包含若干步骤的向导。\n\n第一页，选择要使用的数据模型，并为此Cube输入一个唯一的名称（必需的）和描述（可选的）；这里还可以输入一个邮件通知列表，用于在构建完成或出错时收到通知。如果不想接收处于某些状态的通知，那么可以从“Notification Events”中将其去掉。\n\n第二页，选择Cube的维度。可以通过以下两个按钮来添加维度。   \n**“Add Mimension”**：逐个添加维度，可以是普通维度也可以是衍生（Derived）维度。\n**“Auto Generator”：**批量选择并添加，让Kylin自动完成其它信息。  \n使用第一种方法的时候需要为每个维度起个名字，然后选择表和列。  \n如果是衍生维度的话，则必须是来自于某个维度表，一次可以选择多个列；由于这些列值都可以从该维度表的主键值中衍生出来，所以实际上只有主键列会被Cube加入计算。而在Kylin 的具体实现中，往往采用事实表上的外键替代主键进行计算和存储。但是在逻辑上可以认为衍生列来自于维度表的主键。  \n使用第二种方法，Kylin会用一个树状结构呈现出所有的列，用户只需要勾选所需要的列即可，Kylin会自动补充其他信息，从而方便用户的操作。请注意，在这里Kylin会把维度表上的列都创建成衍生维度，这也许不是最合适的，在这种情况下请使用第一种方法。\n\n\n第三页，创建度量。Kylin默认会创建一个Count(1)的度量。可以单击“+Measure\"按钮来添加新度量。Kylin支持的度量有：SUM、MIN、MAX、COUNT、COUNT　DISTINCT、ＴＯＰ＿Ｎ、RAW等。请选择需要的度量类型，然后再选择适当的参数（通常为列名）\n\n重复上面的步骤，创建所需要的度量。Kylin可以支持在一个Cube中添加多达上百个度量；添加完成所有度量之后，单击“Next”。\n\n第四页，是关于Cube数据刷新的设置。在这里可以设置自动合并的阈值、数据保留的最短时间，以及第一个Segment的起点时间（如果Cube有分割时间列的话）。\n\n第五页，高级设置。在此页面上可以设置聚合组和Rowkey\nKylin默认会把所有的维度都放在同一个聚合中；如果维度数据较多（例如>10），那么建议用户根据查询的习惯和模式，单击“New Aggregation Group+”，将维度分为多个聚合组。通过使用多个聚合组，可以大大降低Cube中的Cuboid数量。下面来举例说明，如果一个Cube有（M+N)个维度，那么默认它会有2的m+n次方个Cuboid；如果把这些维度分为两个不相交的聚合组，那么Cuboid的数量将被减少为2的m次方+2的n次方。  \n在单个聚合组中，可以对维度设置高级属性，例如Mandatory、Hierarchy、Joint等。这几个属性都是为了优化Cube的计算而设计的，了解这些属性的含义对日后更好地使用Cube至关重要。  \nMandatory维度指的是那些总是会出现在where条件或Group By语句里的维度；通过将某个维度指定为Mandatory，Kylin就可以不用预计算那些不包含此维度的Cuboid，从而减少计算量。  \nHierarchy是一组有层级关系的维度，例如：“国家”“省”“市”，这里的“国家”是高级的维度，“省”“市”依次是低级的维度。用户会按高级别维度进行查询，也会按低级别维度进行查询，但在查询低级别维度时，往往都会带上高级别维度的条件，而不会孤立地审视低级别维度的数据。例如，用户单击“国家”作为维度来查询汇总数据，也可能单击“国家”+“省”或者“国家”+“省”+“市”来查询，但是不会跨越国家直接Group By“省”或“市”。通过指定Hierarchy，Kylin可以省略不满足此模式的cuboid。  \nJoint是将多个维度组合成一个维度，其通常适用于如下两种情况。\n1.总是会在一起查询的维度。  \n2.基数很低的维度  \nKylin以Key-Value的方式将Cube存在到HBase中。HBase的key，也就是Rowkey，是由各维度的值拼接而成的；为了更高效地存储这些值，Kylin会对它们进行编码和压缩；每个维度均可以选择合适的编码（Encoding）方式，默认采用的是字典（Dictionary）编码技术；除了字典以外，还有整数（Int）和固定长度（Fixed Length）的编码。  \n字典编码是将此维度下所有值构建成一个从string到int的映射表；Kylin会将字典序列化保存，在Cube中存储int值，从而大大减小存储的大小。另外，字典是保持顺序的，即如果字符串A比字符串B大的话，那么A的编码后的int值也会比B编码后的值大；这样可以使得在HBase中进行比较查询的时候，依然使用编码后的值，而无需解码。\n\n字典非常适合于非固定长度的string类型值的维度，而且用户无需指定编码后的长度；但是由于使用字典需要维护一张映射表，因些如果此维度的基数很高，那么字典的大小就非常可观，从而不适合于加载到内存中，在这种情况下就要选择其他的编码方式了。Kylin中字典编码允许的基数上限默认是500万（由参数\"kylin.dictioinary.max.cardinality\"配置）。  \n整数（int）编码适合于对int或bigint类型的值进行编码，它无需额外存储，同时还可以支持很大的基数。用户需要根据值域选择编码的长度。例如有一个手机号码的维度，它是一个11位的数字，如13800138000，我们知道它大于2的31次方，但是小于2的39次方减1，那么使用int(5)即可满足要求，每个值占用5字节，比按字符存储（11字节）要少占一半以上的空间。  \n\n当上面几种编码方式都不适合的时候，就使用固定长度的编码了；此编码方式其实只是将原始值截断或补充成相同长度的一组字节，没有额外的转换，所以空间效率较差，通常只是作为一种权宜手段。  \n各维度在Rowkeys中的顺序，对于 查询的性能会产生较明显的影响。在这里用户可以根据查询的模式和习惯，通过拖拽的方式调整各个维度在Rowkeys上的顺序。通常的原则是，将过滤频率较高的列放置在过滤频率较低的列之前，将基数高的列放置在基数低的列之前。这样做的好处是，充分利用过滤条件来缩小在HBase中扫描的范围，从而提高查询的效率。  \n第五页，为Cube配置参数。和其他Hadoop工具一样，Kylin使用了很多配置参数以提高录活性，用户可以根据具体的环境、场景等配置不同的参数进行调优。Kylin全局的参数值可在conf/kylin.properties文件中进行配置；如果Cube需要覆盖全局设置的话，则需要在此页面中指定。单击“+Property”按钮，然后输入参数名和参数值。例如“kylin.hbase.region.cut=1\",这样此Cube在存储的时候，Kylin将会为每个HTbase Region分配1GB来创建一个HTbase Region。\n\n\n\n\n#构建Cube\n新创建的Cube只有定义，而没有计算的数据，它的状态是”DISABLED“，是不会被查询引擎挑中的。要想让Cube有数据，还需要对它进行构建。Cube的构建方式通常有两种：全量构建和增量构建；两者的构建步骤是完全一样的，区别只在于构建时读取的数据源是全集还是子集。  \nCube的构建包含如下步骤，由任务引擎来调度执行。  \n1）创建临时的Hive平表（从Hive读取数据）  \n2）计算各维度的不同值，并收集各Cuboid的统计数据。  \n3）创建并保存字典。  \n4）保存Cuboid统计信息。  \n5）创建HTable。  \n6）计算Cube（一轮或若干轮MapRecue）。  \n7）将Cube的计算结果转成HFile。  \n8）加载HFile到HBase。  \n9）更新Cube元数据。  \n10）垃圾回收。  \n以上步骤中，前5步是计算Cube而做的准备工作，例如遍历维度值来创建字典，对数据做统计秋估算以创建HTable等；第6）步是真正的Cube计算，取决于所使用的Cube算法，它可能是一轮MapRecue任务，也可能是N（在没有优化的情况下，N可以被视作是维度数）轮迭代的MapRecue。由于Cube运算的中间结果是以SequenceFile的格式存储在HDFS上的，所以为了导入到HBase中，还需要第7）步将这些结果转换成HFile（HBase文件存储格式）。第8）步通过使用HBase BulkLoad工具，将HFile导入到HBase集群，这一步完成之后，HTable就可以查询到数据了。第9）步更新Cube的数据，将此次构建 Segment的状态从”NEW“更新为”ＲＥＡＤＹ＂，表示已经可借查询了。最后一步，清理构建过程中生成的临时文件等垃圾，释放集群资源。　　\nMonitor页面会显示当前项目下近期的构建任务。　　\n\n\n\n##全量构建和增量构建\n\n###全量构建\n\n\n###增量构建\n\n\n\n##历史数据刷新\n\n\n##合并\n\n#查询Cube\n\n\n","slug":"hadoop/Kylin入门概念","published":1,"updated":"2017-08-18T01:44:35.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjh7polta001wbp7rg1x5bu2p","content":"<p>#Apache Kylin的工作原理<br>Apache Kylin的工作原理本质上是MOLAP（Multidimensional　Online　Analytical　Processing）Cube，也就是多维　立方体分析。这是数据分析中相当经典的理论，在关系数据库年代就已经有了广泛的应用，下面将其做简要的介绍。  </p>\n<p>##维度和度量<br>简单来讲，维度就是观察数据的角度。比如电商的销售数据，可以从时间的维度来观察，也可以进一步细化，从时间和地区的维度来观察。维度一般是一组离散的值，比如时间维度上的每一个独立的日期，或者商品维度上的每一件独立的商品。因此统计时可以把维度值 相同的记录聚合在一起，然后应用聚合函数做累加、平均、去重计数等聚合计算。  <img src=\"/images/hadoop/kylin/维度和度量的例子.jpg\" alt=\"&quot;维度和度量的例子&quot;\"></p>\n<p>度量就是被聚合的统计值，也是聚合运算的结果，它一般是连续的值，如图1-2中的销售额，抑或是销售商品的总件数据 。通过比较和测量试题，分析师可以对数据进行评估，比如今年的销售额相比去年有多大的增长，增长的速度是否达到预期，不同商品类别的增长比例是否合理等。  </p>\n<p>##Cube和Cuboid<br>有了维度和度量，一个数据表或数据模型上的所有字段就可以分类了，它们要么是维度，要么是度量（可以被聚合）。于是就有了根据维度和度量来做预计算的Cube理论。<br>给定一个数据模型，我们可以对其上的所有维度进行组合。对于N个维度来说，组合的所有可能共2的n次方种。对于每一种维度的组合，将度量做聚合运算，然后将运算的结果保存为一个物化视图，称为Cuboid。所有维度组合的Coboid作为一个整体，被称为Cube。所以简单来说一个Cube就是许多按维度聚合的物化视图的集合。<br>下面来举一个具体的例子。假定有一个电商的销售数据集，其中维度包括时间（Time）、商品（Item）、地点（Location）和供应商（Supplier），度量为销售额（GMV）。那么所有维度的组合就有2的4次方=16种，比如一维度（ID）的组合有[Time]、[Item]、[Location]、[Supplier]4种；二维度（3D）的组合有[Time,Item]、[Time，Location]、[Time,Supplier]、[Item,Location]、[Item,Supplier]、[Location,Supplier]6种；三维度（3D）的组合也有4种；最后零维度（0D）和四维度（4D）的组合各有1种，总共有16种组合。<br><img src=\"/images/hadoop/kylin/一个四维Cube的例子.jpg\" alt=\"&quot;一个四维Cube的例子&quot;\"></p>\n<p>计算Cuboid，即按维度来聚合销售额。如果用SQL语句来表达计算Cuboid[Time,Location]，那么SQL语句如下：<br>Select Time,Location,Sum(GMV) as GMV from Sales group by Time,Location.<br>将计算的结果保存为物化视图，所有Cuboid物化视图的总称是Cube。</p>\n<p>##工作原理<br>Apache Kylin的工作原理就是对数据模型做Cube预计算，并利用计算的结果加速查询，具体工作过程如下：<br>1）指定数据模型，定义维度和度量<br>2）预计算Cube，计算所有Cuboid并保存为物化视图。<br>3）执行查询时，读取Cuboid，运算，产生查询结果。<br>由于Kylin的查询过程不会扫描原始记录，而是通过预计算预先完成表的关联、聚合等复杂运算，并利用预计算的结果来执行查询，因此相比非预计算的查询技术，其速度一般要快一到两个数据级，并且这点在超磊的数据集上优势更加明显。当数据集达到千亿及至万亿级别时，Kylin的速度甚至可以超越其他非预计算技术1000倍以上。</p>\n<p>#技术架构<br>Apache Kylin系统可以分为在线查询和离线构建两部分，技术架构如下图所示，在线查询的模块主要处于上半区，而离线构建则处于下半区。<br><img src=\"/images/hadoop/kylin/Kylin的技术架构.jpg\" alt=\"&quot;Kylin的技术架构&quot;\">    </p>\n<p>我们首先看看离线构建的部分。从图1-4可以看出，数据源在左侧，目前主要是Hadoop Hive，保存着待分析的用户数据。根据元数据的字义，下方构建引擎从数据源抽取数据，并构建Cube。数据以关系表的形式输入，且必须符合星形模型（Star Schema）（更复杂的雪花模型在成文时还不支持，可以通过视图将雪花模型转化为星形模型，再使用Kylin）。MapRecue是当前主要的构建技术。构建后的Cube保存在右侧的存储引擎中，一般选用HBase作为存储。<br>完成了离线构建之后，用户可以从上方查询系统发送SQL进行查询分析。Kylin提供了各种Rest　API、ＪＤＢＣ／ＯＤＢＣ接口。无论从哪个接口进入，SQL最终都会来到Rest服务层，再转交给查询引擎进行处理。这里需要注意的是，SQL语句是基于数据源的关系模型书写的，而不是Cube。Kylin在设计时刻意对查询用户屏蔽了Cube的概念，分析师只需要理解简单的关系模型就可以使用Kylin，没有额外的学习门槛，传统的SQL应用也很容易迁移。查询引擎解析SQL，生成基于关系表的逻辑执行计划，然后将其转义为基于Cube的物理执行计划，最后查询预计算生成的Cube并产生结果。整个过程不会访问原始数据源。  </p>\n<p><strong>注意</strong>：对于查询引擎下方的路由选择，在最初设计时曾考虑过将Kylin不能执行查询引导去Hive中继续执行，但在实践后发现Hive与Kylin的速度差异过大，导致用户无法对查询的速度有一致的期望，很可能大多数据查询几秒内就返回结果了，而有些查询则要等几分钟到几十分钟，因此体验非常糟糕。最后这个路由功能在发行版中默认关闭。</p>\n<p>Apache Kylin 1.5版本引入了“可扩展架构”的概念。在图1-4中显示为三个粗虚框，表示的抽象层。可扩展指Kylin可以对其主要依赖的三个模块做任意的扩展和替换。Kylin的三大依赖模型分别是数据源、构建引擎和存储引擎。在设计之初，作为Hadoop家族 一员，这三者分别是Hive、MapRecue和HBase。但随着推广和使用的深入，渐渐有用户发现它们均存在不足之处。比如，实时分析可能会希望从Kafka导入数据而不是Hive；而Spark的迅速崛起，又使我们不得不考虑将MapRecue替换为Spark，以期大幅提高Cube的构建速度；至于HBase，它的读性能可能还不如Cassandra或Kudu等 。可见，是否可以将一种技术替换为另一种技术已成为一个常见的问题。于在1.5版本的系统架构进行了重构，将数据源、构建引擎、存储引擎三大依赖抽象为接口，而Hive、MapRecue、HBase只是默认实现。深度用户可以根据自己的需要做二次开发，将其中的一个或多个替换为更适合的技术。  </p>\n<p>#核心概念</p>\n<p>##数据仓库<br>数据仓库（Data Warehouse）是一种系统的资料储存理论，此理论强调的是利用某些特殊的资料储存方式，让所包含的资料特别有利于分析和处理，从而产生有价值的资讯，并可依此做出决策。<br>利用数据仓库的方式存放资料，具有一旦存入，便不会随时间发生变动的特性，此外，存入的资料必定包含时间属性，通常一个数据仓库中会含有大量的历史性资料，并且它可利用特定的分析方式，从其中发掘特定的资讯。</p>\n<p>##OLAP<br>OLAP（Online Analytical Process），联机分析处理，以多维度的方式分析数据，而且能够弹性地提供上卷（Roll-up）、下钻（Drill-down）和透视分析（Pivot）等操作，它呈现集成性决策信息的方法，多用于决策支持系统、商务智能或数据仓库。其主要的功能在于方便大规模数据分析及统计计算，可对决策提供参考和支持。与之相区别的是取机交易处理（OLTP），联机交易处理，更侧重于基本的、日常的事务处理，包括数据的增删改查。<br>OLAP需要以大量历史数据为基础，再配合时间点的差异，对多维度及汇整型的信息进行复杂的分析。<br>OLAP需要用户有主观的信息需求定义，因此系统效率较佳。<br>OLAP的概念，在实际应用中存在广义和狭义两种不同的理解方式。广义上的理解与字面上的意义相同，泛指一切不会对数据进行更新的分析处理。但更多的情况下OLAP被理解为其狭义上的含义，即与多维分析相关，基于立方体（Cube）计算而进行的分析。</p>\n<p>##BI<br>BI（Business Intelligence），即商务智能，指现代数据仓库技术、在线分析技术、数据挖掘和数据展现技术进行数据分析以实现商业价值。</p>\n<p>##维度和度量<br>维度和度量是数据分析中的两个基本的概念<br><strong>维度</strong>是指审视数据的角度，它通常是数据记录的一个属性，例如时间、地点等。<br><strong>度量</strong>是基于数据所计算出来的考量值；它通常是一个数值，如总销售额、不同的用户数等。分析人员往往要结合若干个维度来审查度量值，以便在其中找到变化规律。在一个SQL查询中，Group By的属性通常就是维度，而所计算的值则是度量。如下面的示例：<br>    select part_dt,lstg_iste_id,sum(price) as total_selled,count(distinct seller_id) as sellers from kylin_sales group by part_dt,lstg_site_id</p>\n<p>##事实表和维度表<br><strong>事实表</strong>（Fact Table）是指存储有事实记录的表，如系统日志、销售记录等；事实表的记录在不断地动态增长，所以它的体积通常远大于其他表。</p>\n<p><strong>维度表</strong>（Dimension Table）或维表，有时也称查找表（Lookup Table），是与事实表相对应的一种表；它保存了维度的属性值，可以跟事实表做关联；相当于将事实表上经常重复出现的属性抽取、规范出来用一张表进行管理。常见的维度表有：日期表（存储与日期对应的周、月、季度等属性）、地点表（包含国家、省、城市等属性）。使用维度表有诸多好处，具体如下：<br>a.缩小了事实表的大小<br>b.便于维度的管理和维护，增加、删除和修改维度的属性，不必对事实表的大量记录进行改动。<br>c.维度表可以为多个事实表重用，以减少重复工作。</p>\n<p>##Cube、Cuboid和Cube Segment</p>\n<p>###Cube<br>Cube（或Data Cube），即数据立方体，是一种常用于数据分析与索引的技术；它可以对原始数据建立多维度索引。通过Cube对数据进行分析，可以大大加快数据的查询效率。</p>\n<p>###Cuboid<br>Cuboid在Kylin中特指在某一种维度组合下所计算的数据。</p>\n<p>##Cube Segment<br>Cube Segment是指针对源数据中的某一片段，计算出来的Cube数据。通常数据仓库中的数据数量会随着时间的增长而增长，而Cube Segment也是按时间顺序来构建的。</p>\n<p>#在Hive中准备数据<br>这里介绍准备Hive数据的一些注意事项。需要被分析的数据必须先保存为Hive表的形式，然后Kylin才能从Hive中导入数据，创建Cube。<br>Hive是一个基于Hadoop的数据仓库工具，可以将结构化的数据文件映射为数据库表，并可以将SQL语句转换为MapRecue或Tez任务进行运行，从而让用户以类SQL（HiveQL，也称HQL）的方式管理和查询Hadoop上的海量数据。<br>此外，Hive还提供了多种方式（如命令行、API和Web服务等）可供第三方方便地获取和使用元数据并进行查询。今天，Hive已经成为Hadoop数据仓库的首选，是Hadoop上不可或缺的一个重要组件，很多项目都已兼容或集成了Hive。基于此情况，Kylin选择Hive作为原始数据的主要来源。<br>在Hive中准备待分析的数据是使用Kylin的前提；将数据导入到Hive表中的方法有很多，用户管理数据的技术和工具也各式各样，因此具体步骤不在本书的讨论范围之内。</p>\n<p>##星形模型<br>数据挖掘有几种常见的多维数据模型，如星形模型（Star Schema）、雪花模型（Snowf lake Schema）、事实星座模型（Fact Constellation）等。<br>星形模型中有一张事实表，以及零个或多个维度表；事实表与维度表通过主键外键相关联，维度表之间没有关联，就像很多星星围绕在一个恒星周围，帮取名为星形模型。<br>如果将星形模型中某些维度的表再做规范，抽取成更细的维度表，然后让维度表之间也进行关联，那么这种模型称为雪花模型。<br>星形模型是更复杂的模型，其中包含了多个事实表，而维度表是公用的，可以共享。<br>不过，Kylin只支持星形模型的数据集，这是基于以下考虑的。  </p>\n<ul>\n<li>星形模型是最简单，也是最常用的模型  </li>\n<li>由于星形模型只有一张大表，因此它相比于其它模型更适合于大数据处理  </li>\n<li>其他模型可以通过一定的转换，变成星形模型。  </li>\n</ul>\n<p>##维度表的设计<br>除了数据模型以外，Kylin还对维度表有一定的要求，具体要求如下。  </p>\n<ul>\n<li>要具有数据一致性，主键值必须是唯一的；Kylin会进行检查，如果有两行的主键值相同则会报错。</li>\n<li>维度越小越好，因为Kylin会将维度表加载到内存中供查询；过大的表不适合作为维度表，默认的阈值是300MB。  </li>\n<li>改变频率低，Kylin会在每次构建中试图重用维度表的快照，如果维度表经常改变的话，重用就会失效，这就会导致要经常对维度表创建快照。</li>\n<li>维度表最好不要是Hive视图（View），虽然在Kylin1.5.3中加入了对维度表是视图这种情况的支持，但每次都需要将视图进行物化，从而导致额外的时间开销。</li>\n</ul>\n<p>##Hive表分区<br>Hive支持多分区（Partition）。简单来说，一个分区就是一个文件目录，存储了特定的数据文件。当有新的数据生成的时候，可以将数据加载到指定的分区，读取数据的时候也可以指定分区。对于 SQL查询，如果查询中指定了分区列的属性条件，则Hive会智能地选择特定分区（也就是目录），从而避免全量数据的扫描，减少读写操作对集群的压力。<br>下面举的一组SQL演示了如何使用分区：  </p>\n<p>Hie&gt;create table invites(id int,name string) partitioned by(ds string) row format delimited fields terminated by ‘t’ stroed as textfile;<br>Hive&gt;load data local inpath ‘/user/hadoop/data.txt’ overwrite into table invites partition (ds=’2016-08-16’);<br>Hive&gt;select * from invites where ds = ‘2016-08-16’;<br>Kylin支持增量的Cube构建，通常是按时间属性来增量地从Hive表中抽取数据。如果Hive表正好是按此时间属性做分区的话，那么就可以利用到Hive分区的好处，每次在Hive构建的时候都可以直接跳过不相干的日期的数据，节省Cube构建的时间。这样的列在Kylin里也称为分割时间列（Partition Time Column），通常它应该也是Hive表的分区列。</p>\n<p>##了解维度的基数<br>维度的基数（Cardinality）指的是该维度在数据集中出现的不同值的个数；例如“国家”是一个维度，如果有200个不同的值，那么此维度的基数就是200.通常一个维度的基数会从几十到几万个不等，个别维度如“用户ID”的基数会超过百万甚至千万。基数超过一百万的维度通常称为超高维度（Ulta Hight Cardinality，UHC），需要引起设计者的注意。<br>Cube中所有维度的基数都可以体现Cube的复杂度，如果一个Cube中有好几个超高基数维度，那么这个Cube膨胀就会很高。在创建Cube前需要对所有维度的基数做一个了解，这样就可以帮助设计合理的Cube。计算基数有多种途径，最简单的方法就是让Hive执行一个count distinct的SQL查询；Kylin也提供计算基数的方法，在导入Hive表定义后可以看到每一个列的基数，参数名为Cardinality</p>\n<p>##Sample Data<br>如果需要快速体验Kylin，可以用Kylin自带的Sample Data。运行${KYLIN_HOME}/bin/sample.sh来导入Sample Data，然后就能按照下面的流程来创建模型和Cube。<br>具体请执行下面命令，将Sample Data导入到Hive数据库。<br>cd ${KYLIN_HOME}<br>bin/sample.sh<br>Sample Data测试的样例数据集总共仅1M左右，共计3张表，其中事实表有10000条数据。数据集是一个规范的星形模型结构，它总包含3个数据表：<br>KYLIN_SALES是事实表，保存了销售订单的明细信息。各列分别保存着卖家、商品、分类、订单金额、商品数据等信息，每一行对应着一笔交易订单。<br>KYLIN_CATEGORY_GROUPINGS是维表，保存了商品分类的详细介绍，例如商品分类名称等。<br>KYLIN_CAL_DT也是维表，保存了时间的扩展信息。如单个日期所在的年始、月始、周始、年份、月份等。<br>这3张表一起构成了整个星形模型。  </p>\n<p>#设计Cube<br>如果数据已经在Hive中准备好了，就可以开始创建Cube了。</p>\n<p>##导入Hive表定义<br>登陆Kylin的Web界面，创建新的或选择一个已有的项目之后，需要做的就是将Hive表的定义导入到Kylin中。<br>单击Web界面的Model-&gt;Data Source下的”Local Hive Table“图标，然后输入表的名称（可以一次导入多个表，以逗号分隔表名），单击按钮”Sync“，Kylin就会使用Hive的API从Hive中获取表的属性信息。<br>导入成功后，表的结构信息会以树状的形式显示在页面的左侧，可以单击展开或收缩。</p>\n<p>同时Kylin会在后台触发一个MapRecue任务，计算此表的每个列的基数。通常稍过几分钟后再刷新页面，就会看到显示出来 的基数信息Cardinality</p>\n<p>需要注意的是，这里Kylin对基数的计算方法采用的是HyperLogLog的近似算法，与精确值略有误差，只做参考值。</p>\n<p>##创建数据模型<br>有了表信息之后，就可以开始创建数据模型（Data Model）了。数据模型是Cube的基础，它主要用于描述一个星形模型。有了数据模型以后，定义Cube的时候就可以直接从此模型定义的表和列中进行选择了，省去重复指定连接（join）条件的步骤。基于一个数据模型还可以创建多个Cube，以方便减少用户的重复性工作。<br>在Kylin界面中”Models“页面中单击”New”-&gt;”New Model”，开始创建数据模型。</p>\n<p>接下来选择用作维度和度量的列。这里只是选择一个范围，不代表这些列将来一定要用作Cube 的维度或度量，你可以把所有可能会用到表都选进来，后续创建Cube的时候，将只能从这些列中进行选择。   </p>\n<p>选择维度列时，维度可以来自事实表或维度表<br>选择度量列时，度量只能来自事实表<br>最后一步，是为模型补充侵害时间列信息和过滤条件。如果此模型中的事实表记录是按时间增长的，那么可以指定一个日期/时间列作为模型的分割时间列，从而可以让Cube按此列做增量构建。</p>\n<p>过滤（Filter）条件是指，如果想把一些记录忽略掉，那么这里可以设置一个过滤条件。Kylin在向Hive请求源数据的时候，会带上此过滤条件。</p>\n<p>随后“Save”后，出现在“Model”的列表中。</p>\n<p>##创建Cube<br>单击“New”，选择“New Cube”，会开启一个包含若干步骤的向导。</p>\n<p>第一页，选择要使用的数据模型，并为此Cube输入一个唯一的名称（必需的）和描述（可选的）；这里还可以输入一个邮件通知列表，用于在构建完成或出错时收到通知。如果不想接收处于某些状态的通知，那么可以从“Notification Events”中将其去掉。</p>\n<p>第二页，选择Cube的维度。可以通过以下两个按钮来添加维度。<br><strong>“Add Mimension”</strong>：逐个添加维度，可以是普通维度也可以是衍生（Derived）维度。<br><strong>“Auto Generator”：</strong>批量选择并添加，让Kylin自动完成其它信息。<br>使用第一种方法的时候需要为每个维度起个名字，然后选择表和列。<br>如果是衍生维度的话，则必须是来自于某个维度表，一次可以选择多个列；由于这些列值都可以从该维度表的主键值中衍生出来，所以实际上只有主键列会被Cube加入计算。而在Kylin 的具体实现中，往往采用事实表上的外键替代主键进行计算和存储。但是在逻辑上可以认为衍生列来自于维度表的主键。<br>使用第二种方法，Kylin会用一个树状结构呈现出所有的列，用户只需要勾选所需要的列即可，Kylin会自动补充其他信息，从而方便用户的操作。请注意，在这里Kylin会把维度表上的列都创建成衍生维度，这也许不是最合适的，在这种情况下请使用第一种方法。</p>\n<p>第三页，创建度量。Kylin默认会创建一个Count(1)的度量。可以单击“+Measure”按钮来添加新度量。Kylin支持的度量有：SUM、MIN、MAX、COUNT、COUNT　DISTINCT、ＴＯＰ＿Ｎ、RAW等。请选择需要的度量类型，然后再选择适当的参数（通常为列名）</p>\n<p>重复上面的步骤，创建所需要的度量。Kylin可以支持在一个Cube中添加多达上百个度量；添加完成所有度量之后，单击“Next”。</p>\n<p>第四页，是关于Cube数据刷新的设置。在这里可以设置自动合并的阈值、数据保留的最短时间，以及第一个Segment的起点时间（如果Cube有分割时间列的话）。</p>\n<p>第五页，高级设置。在此页面上可以设置聚合组和Rowkey<br>Kylin默认会把所有的维度都放在同一个聚合中；如果维度数据较多（例如&gt;10），那么建议用户根据查询的习惯和模式，单击“New Aggregation Group+”，将维度分为多个聚合组。通过使用多个聚合组，可以大大降低Cube中的Cuboid数量。下面来举例说明，如果一个Cube有（M+N)个维度，那么默认它会有2的m+n次方个Cuboid；如果把这些维度分为两个不相交的聚合组，那么Cuboid的数量将被减少为2的m次方+2的n次方。<br>在单个聚合组中，可以对维度设置高级属性，例如Mandatory、Hierarchy、Joint等。这几个属性都是为了优化Cube的计算而设计的，了解这些属性的含义对日后更好地使用Cube至关重要。<br>Mandatory维度指的是那些总是会出现在where条件或Group By语句里的维度；通过将某个维度指定为Mandatory，Kylin就可以不用预计算那些不包含此维度的Cuboid，从而减少计算量。<br>Hierarchy是一组有层级关系的维度，例如：“国家”“省”“市”，这里的“国家”是高级的维度，“省”“市”依次是低级的维度。用户会按高级别维度进行查询，也会按低级别维度进行查询，但在查询低级别维度时，往往都会带上高级别维度的条件，而不会孤立地审视低级别维度的数据。例如，用户单击“国家”作为维度来查询汇总数据，也可能单击“国家”+“省”或者“国家”+“省”+“市”来查询，但是不会跨越国家直接Group By“省”或“市”。通过指定Hierarchy，Kylin可以省略不满足此模式的cuboid。<br>Joint是将多个维度组合成一个维度，其通常适用于如下两种情况。<br>1.总是会在一起查询的维度。<br>2.基数很低的维度<br>Kylin以Key-Value的方式将Cube存在到HBase中。HBase的key，也就是Rowkey，是由各维度的值拼接而成的；为了更高效地存储这些值，Kylin会对它们进行编码和压缩；每个维度均可以选择合适的编码（Encoding）方式，默认采用的是字典（Dictionary）编码技术；除了字典以外，还有整数（Int）和固定长度（Fixed Length）的编码。<br>字典编码是将此维度下所有值构建成一个从string到int的映射表；Kylin会将字典序列化保存，在Cube中存储int值，从而大大减小存储的大小。另外，字典是保持顺序的，即如果字符串A比字符串B大的话，那么A的编码后的int值也会比B编码后的值大；这样可以使得在HBase中进行比较查询的时候，依然使用编码后的值，而无需解码。</p>\n<p>字典非常适合于非固定长度的string类型值的维度，而且用户无需指定编码后的长度；但是由于使用字典需要维护一张映射表，因些如果此维度的基数很高，那么字典的大小就非常可观，从而不适合于加载到内存中，在这种情况下就要选择其他的编码方式了。Kylin中字典编码允许的基数上限默认是500万（由参数”kylin.dictioinary.max.cardinality”配置）。<br>整数（int）编码适合于对int或bigint类型的值进行编码，它无需额外存储，同时还可以支持很大的基数。用户需要根据值域选择编码的长度。例如有一个手机号码的维度，它是一个11位的数字，如13800138000，我们知道它大于2的31次方，但是小于2的39次方减1，那么使用int(5)即可满足要求，每个值占用5字节，比按字符存储（11字节）要少占一半以上的空间。  </p>\n<p>当上面几种编码方式都不适合的时候，就使用固定长度的编码了；此编码方式其实只是将原始值截断或补充成相同长度的一组字节，没有额外的转换，所以空间效率较差，通常只是作为一种权宜手段。<br>各维度在Rowkeys中的顺序，对于 查询的性能会产生较明显的影响。在这里用户可以根据查询的模式和习惯，通过拖拽的方式调整各个维度在Rowkeys上的顺序。通常的原则是，将过滤频率较高的列放置在过滤频率较低的列之前，将基数高的列放置在基数低的列之前。这样做的好处是，充分利用过滤条件来缩小在HBase中扫描的范围，从而提高查询的效率。<br>第五页，为Cube配置参数。和其他Hadoop工具一样，Kylin使用了很多配置参数以提高录活性，用户可以根据具体的环境、场景等配置不同的参数进行调优。Kylin全局的参数值可在conf/kylin.properties文件中进行配置；如果Cube需要覆盖全局设置的话，则需要在此页面中指定。单击“+Property”按钮，然后输入参数名和参数值。例如“kylin.hbase.region.cut=1”,这样此Cube在存储的时候，Kylin将会为每个HTbase Region分配1GB来创建一个HTbase Region。</p>\n<p>#构建Cube<br>新创建的Cube只有定义，而没有计算的数据，它的状态是”DISABLED“，是不会被查询引擎挑中的。要想让Cube有数据，还需要对它进行构建。Cube的构建方式通常有两种：全量构建和增量构建；两者的构建步骤是完全一样的，区别只在于构建时读取的数据源是全集还是子集。<br>Cube的构建包含如下步骤，由任务引擎来调度执行。<br>1）创建临时的Hive平表（从Hive读取数据）<br>2）计算各维度的不同值，并收集各Cuboid的统计数据。<br>3）创建并保存字典。<br>4）保存Cuboid统计信息。<br>5）创建HTable。<br>6）计算Cube（一轮或若干轮MapRecue）。<br>7）将Cube的计算结果转成HFile。<br>8）加载HFile到HBase。<br>9）更新Cube元数据。<br>10）垃圾回收。<br>以上步骤中，前5步是计算Cube而做的准备工作，例如遍历维度值来创建字典，对数据做统计秋估算以创建HTable等；第6）步是真正的Cube计算，取决于所使用的Cube算法，它可能是一轮MapRecue任务，也可能是N（在没有优化的情况下，N可以被视作是维度数）轮迭代的MapRecue。由于Cube运算的中间结果是以SequenceFile的格式存储在HDFS上的，所以为了导入到HBase中，还需要第7）步将这些结果转换成HFile（HBase文件存储格式）。第8）步通过使用HBase BulkLoad工具，将HFile导入到HBase集群，这一步完成之后，HTable就可以查询到数据了。第9）步更新Cube的数据，将此次构建 Segment的状态从”NEW“更新为”ＲＥＡＤＹ＂，表示已经可借查询了。最后一步，清理构建过程中生成的临时文件等垃圾，释放集群资源。　　<br>Monitor页面会显示当前项目下近期的构建任务。　　</p>\n<p>##全量构建和增量构建</p>\n<p>###全量构建</p>\n<p>###增量构建</p>\n<p>##历史数据刷新</p>\n<p>##合并</p>\n<p>#查询Cube</p>\n","site":{"data":{}},"excerpt":"","more":"<p>#Apache Kylin的工作原理<br>Apache Kylin的工作原理本质上是MOLAP（Multidimensional　Online　Analytical　Processing）Cube，也就是多维　立方体分析。这是数据分析中相当经典的理论，在关系数据库年代就已经有了广泛的应用，下面将其做简要的介绍。  </p>\n<p>##维度和度量<br>简单来讲，维度就是观察数据的角度。比如电商的销售数据，可以从时间的维度来观察，也可以进一步细化，从时间和地区的维度来观察。维度一般是一组离散的值，比如时间维度上的每一个独立的日期，或者商品维度上的每一件独立的商品。因此统计时可以把维度值 相同的记录聚合在一起，然后应用聚合函数做累加、平均、去重计数等聚合计算。  <img src=\"/images/hadoop/kylin/维度和度量的例子.jpg\" alt=\"&quot;维度和度量的例子&quot;\"></p>\n<p>度量就是被聚合的统计值，也是聚合运算的结果，它一般是连续的值，如图1-2中的销售额，抑或是销售商品的总件数据 。通过比较和测量试题，分析师可以对数据进行评估，比如今年的销售额相比去年有多大的增长，增长的速度是否达到预期，不同商品类别的增长比例是否合理等。  </p>\n<p>##Cube和Cuboid<br>有了维度和度量，一个数据表或数据模型上的所有字段就可以分类了，它们要么是维度，要么是度量（可以被聚合）。于是就有了根据维度和度量来做预计算的Cube理论。<br>给定一个数据模型，我们可以对其上的所有维度进行组合。对于N个维度来说，组合的所有可能共2的n次方种。对于每一种维度的组合，将度量做聚合运算，然后将运算的结果保存为一个物化视图，称为Cuboid。所有维度组合的Coboid作为一个整体，被称为Cube。所以简单来说一个Cube就是许多按维度聚合的物化视图的集合。<br>下面来举一个具体的例子。假定有一个电商的销售数据集，其中维度包括时间（Time）、商品（Item）、地点（Location）和供应商（Supplier），度量为销售额（GMV）。那么所有维度的组合就有2的4次方=16种，比如一维度（ID）的组合有[Time]、[Item]、[Location]、[Supplier]4种；二维度（3D）的组合有[Time,Item]、[Time，Location]、[Time,Supplier]、[Item,Location]、[Item,Supplier]、[Location,Supplier]6种；三维度（3D）的组合也有4种；最后零维度（0D）和四维度（4D）的组合各有1种，总共有16种组合。<br><img src=\"/images/hadoop/kylin/一个四维Cube的例子.jpg\" alt=\"&quot;一个四维Cube的例子&quot;\"></p>\n<p>计算Cuboid，即按维度来聚合销售额。如果用SQL语句来表达计算Cuboid[Time,Location]，那么SQL语句如下：<br>Select Time,Location,Sum(GMV) as GMV from Sales group by Time,Location.<br>将计算的结果保存为物化视图，所有Cuboid物化视图的总称是Cube。</p>\n<p>##工作原理<br>Apache Kylin的工作原理就是对数据模型做Cube预计算，并利用计算的结果加速查询，具体工作过程如下：<br>1）指定数据模型，定义维度和度量<br>2）预计算Cube，计算所有Cuboid并保存为物化视图。<br>3）执行查询时，读取Cuboid，运算，产生查询结果。<br>由于Kylin的查询过程不会扫描原始记录，而是通过预计算预先完成表的关联、聚合等复杂运算，并利用预计算的结果来执行查询，因此相比非预计算的查询技术，其速度一般要快一到两个数据级，并且这点在超磊的数据集上优势更加明显。当数据集达到千亿及至万亿级别时，Kylin的速度甚至可以超越其他非预计算技术1000倍以上。</p>\n<p>#技术架构<br>Apache Kylin系统可以分为在线查询和离线构建两部分，技术架构如下图所示，在线查询的模块主要处于上半区，而离线构建则处于下半区。<br><img src=\"/images/hadoop/kylin/Kylin的技术架构.jpg\" alt=\"&quot;Kylin的技术架构&quot;\">    </p>\n<p>我们首先看看离线构建的部分。从图1-4可以看出，数据源在左侧，目前主要是Hadoop Hive，保存着待分析的用户数据。根据元数据的字义，下方构建引擎从数据源抽取数据，并构建Cube。数据以关系表的形式输入，且必须符合星形模型（Star Schema）（更复杂的雪花模型在成文时还不支持，可以通过视图将雪花模型转化为星形模型，再使用Kylin）。MapRecue是当前主要的构建技术。构建后的Cube保存在右侧的存储引擎中，一般选用HBase作为存储。<br>完成了离线构建之后，用户可以从上方查询系统发送SQL进行查询分析。Kylin提供了各种Rest　API、ＪＤＢＣ／ＯＤＢＣ接口。无论从哪个接口进入，SQL最终都会来到Rest服务层，再转交给查询引擎进行处理。这里需要注意的是，SQL语句是基于数据源的关系模型书写的，而不是Cube。Kylin在设计时刻意对查询用户屏蔽了Cube的概念，分析师只需要理解简单的关系模型就可以使用Kylin，没有额外的学习门槛，传统的SQL应用也很容易迁移。查询引擎解析SQL，生成基于关系表的逻辑执行计划，然后将其转义为基于Cube的物理执行计划，最后查询预计算生成的Cube并产生结果。整个过程不会访问原始数据源。  </p>\n<p><strong>注意</strong>：对于查询引擎下方的路由选择，在最初设计时曾考虑过将Kylin不能执行查询引导去Hive中继续执行，但在实践后发现Hive与Kylin的速度差异过大，导致用户无法对查询的速度有一致的期望，很可能大多数据查询几秒内就返回结果了，而有些查询则要等几分钟到几十分钟，因此体验非常糟糕。最后这个路由功能在发行版中默认关闭。</p>\n<p>Apache Kylin 1.5版本引入了“可扩展架构”的概念。在图1-4中显示为三个粗虚框，表示的抽象层。可扩展指Kylin可以对其主要依赖的三个模块做任意的扩展和替换。Kylin的三大依赖模型分别是数据源、构建引擎和存储引擎。在设计之初，作为Hadoop家族 一员，这三者分别是Hive、MapRecue和HBase。但随着推广和使用的深入，渐渐有用户发现它们均存在不足之处。比如，实时分析可能会希望从Kafka导入数据而不是Hive；而Spark的迅速崛起，又使我们不得不考虑将MapRecue替换为Spark，以期大幅提高Cube的构建速度；至于HBase，它的读性能可能还不如Cassandra或Kudu等 。可见，是否可以将一种技术替换为另一种技术已成为一个常见的问题。于在1.5版本的系统架构进行了重构，将数据源、构建引擎、存储引擎三大依赖抽象为接口，而Hive、MapRecue、HBase只是默认实现。深度用户可以根据自己的需要做二次开发，将其中的一个或多个替换为更适合的技术。  </p>\n<p>#核心概念</p>\n<p>##数据仓库<br>数据仓库（Data Warehouse）是一种系统的资料储存理论，此理论强调的是利用某些特殊的资料储存方式，让所包含的资料特别有利于分析和处理，从而产生有价值的资讯，并可依此做出决策。<br>利用数据仓库的方式存放资料，具有一旦存入，便不会随时间发生变动的特性，此外，存入的资料必定包含时间属性，通常一个数据仓库中会含有大量的历史性资料，并且它可利用特定的分析方式，从其中发掘特定的资讯。</p>\n<p>##OLAP<br>OLAP（Online Analytical Process），联机分析处理，以多维度的方式分析数据，而且能够弹性地提供上卷（Roll-up）、下钻（Drill-down）和透视分析（Pivot）等操作，它呈现集成性决策信息的方法，多用于决策支持系统、商务智能或数据仓库。其主要的功能在于方便大规模数据分析及统计计算，可对决策提供参考和支持。与之相区别的是取机交易处理（OLTP），联机交易处理，更侧重于基本的、日常的事务处理，包括数据的增删改查。<br>OLAP需要以大量历史数据为基础，再配合时间点的差异，对多维度及汇整型的信息进行复杂的分析。<br>OLAP需要用户有主观的信息需求定义，因此系统效率较佳。<br>OLAP的概念，在实际应用中存在广义和狭义两种不同的理解方式。广义上的理解与字面上的意义相同，泛指一切不会对数据进行更新的分析处理。但更多的情况下OLAP被理解为其狭义上的含义，即与多维分析相关，基于立方体（Cube）计算而进行的分析。</p>\n<p>##BI<br>BI（Business Intelligence），即商务智能，指现代数据仓库技术、在线分析技术、数据挖掘和数据展现技术进行数据分析以实现商业价值。</p>\n<p>##维度和度量<br>维度和度量是数据分析中的两个基本的概念<br><strong>维度</strong>是指审视数据的角度，它通常是数据记录的一个属性，例如时间、地点等。<br><strong>度量</strong>是基于数据所计算出来的考量值；它通常是一个数值，如总销售额、不同的用户数等。分析人员往往要结合若干个维度来审查度量值，以便在其中找到变化规律。在一个SQL查询中，Group By的属性通常就是维度，而所计算的值则是度量。如下面的示例：<br>    select part_dt,lstg_iste_id,sum(price) as total_selled,count(distinct seller_id) as sellers from kylin_sales group by part_dt,lstg_site_id</p>\n<p>##事实表和维度表<br><strong>事实表</strong>（Fact Table）是指存储有事实记录的表，如系统日志、销售记录等；事实表的记录在不断地动态增长，所以它的体积通常远大于其他表。</p>\n<p><strong>维度表</strong>（Dimension Table）或维表，有时也称查找表（Lookup Table），是与事实表相对应的一种表；它保存了维度的属性值，可以跟事实表做关联；相当于将事实表上经常重复出现的属性抽取、规范出来用一张表进行管理。常见的维度表有：日期表（存储与日期对应的周、月、季度等属性）、地点表（包含国家、省、城市等属性）。使用维度表有诸多好处，具体如下：<br>a.缩小了事实表的大小<br>b.便于维度的管理和维护，增加、删除和修改维度的属性，不必对事实表的大量记录进行改动。<br>c.维度表可以为多个事实表重用，以减少重复工作。</p>\n<p>##Cube、Cuboid和Cube Segment</p>\n<p>###Cube<br>Cube（或Data Cube），即数据立方体，是一种常用于数据分析与索引的技术；它可以对原始数据建立多维度索引。通过Cube对数据进行分析，可以大大加快数据的查询效率。</p>\n<p>###Cuboid<br>Cuboid在Kylin中特指在某一种维度组合下所计算的数据。</p>\n<p>##Cube Segment<br>Cube Segment是指针对源数据中的某一片段，计算出来的Cube数据。通常数据仓库中的数据数量会随着时间的增长而增长，而Cube Segment也是按时间顺序来构建的。</p>\n<p>#在Hive中准备数据<br>这里介绍准备Hive数据的一些注意事项。需要被分析的数据必须先保存为Hive表的形式，然后Kylin才能从Hive中导入数据，创建Cube。<br>Hive是一个基于Hadoop的数据仓库工具，可以将结构化的数据文件映射为数据库表，并可以将SQL语句转换为MapRecue或Tez任务进行运行，从而让用户以类SQL（HiveQL，也称HQL）的方式管理和查询Hadoop上的海量数据。<br>此外，Hive还提供了多种方式（如命令行、API和Web服务等）可供第三方方便地获取和使用元数据并进行查询。今天，Hive已经成为Hadoop数据仓库的首选，是Hadoop上不可或缺的一个重要组件，很多项目都已兼容或集成了Hive。基于此情况，Kylin选择Hive作为原始数据的主要来源。<br>在Hive中准备待分析的数据是使用Kylin的前提；将数据导入到Hive表中的方法有很多，用户管理数据的技术和工具也各式各样，因此具体步骤不在本书的讨论范围之内。</p>\n<p>##星形模型<br>数据挖掘有几种常见的多维数据模型，如星形模型（Star Schema）、雪花模型（Snowf lake Schema）、事实星座模型（Fact Constellation）等。<br>星形模型中有一张事实表，以及零个或多个维度表；事实表与维度表通过主键外键相关联，维度表之间没有关联，就像很多星星围绕在一个恒星周围，帮取名为星形模型。<br>如果将星形模型中某些维度的表再做规范，抽取成更细的维度表，然后让维度表之间也进行关联，那么这种模型称为雪花模型。<br>星形模型是更复杂的模型，其中包含了多个事实表，而维度表是公用的，可以共享。<br>不过，Kylin只支持星形模型的数据集，这是基于以下考虑的。  </p>\n<ul>\n<li>星形模型是最简单，也是最常用的模型  </li>\n<li>由于星形模型只有一张大表，因此它相比于其它模型更适合于大数据处理  </li>\n<li>其他模型可以通过一定的转换，变成星形模型。  </li>\n</ul>\n<p>##维度表的设计<br>除了数据模型以外，Kylin还对维度表有一定的要求，具体要求如下。  </p>\n<ul>\n<li>要具有数据一致性，主键值必须是唯一的；Kylin会进行检查，如果有两行的主键值相同则会报错。</li>\n<li>维度越小越好，因为Kylin会将维度表加载到内存中供查询；过大的表不适合作为维度表，默认的阈值是300MB。  </li>\n<li>改变频率低，Kylin会在每次构建中试图重用维度表的快照，如果维度表经常改变的话，重用就会失效，这就会导致要经常对维度表创建快照。</li>\n<li>维度表最好不要是Hive视图（View），虽然在Kylin1.5.3中加入了对维度表是视图这种情况的支持，但每次都需要将视图进行物化，从而导致额外的时间开销。</li>\n</ul>\n<p>##Hive表分区<br>Hive支持多分区（Partition）。简单来说，一个分区就是一个文件目录，存储了特定的数据文件。当有新的数据生成的时候，可以将数据加载到指定的分区，读取数据的时候也可以指定分区。对于 SQL查询，如果查询中指定了分区列的属性条件，则Hive会智能地选择特定分区（也就是目录），从而避免全量数据的扫描，减少读写操作对集群的压力。<br>下面举的一组SQL演示了如何使用分区：  </p>\n<p>Hie&gt;create table invites(id int,name string) partitioned by(ds string) row format delimited fields terminated by ‘t’ stroed as textfile;<br>Hive&gt;load data local inpath ‘/user/hadoop/data.txt’ overwrite into table invites partition (ds=’2016-08-16’);<br>Hive&gt;select * from invites where ds = ‘2016-08-16’;<br>Kylin支持增量的Cube构建，通常是按时间属性来增量地从Hive表中抽取数据。如果Hive表正好是按此时间属性做分区的话，那么就可以利用到Hive分区的好处，每次在Hive构建的时候都可以直接跳过不相干的日期的数据，节省Cube构建的时间。这样的列在Kylin里也称为分割时间列（Partition Time Column），通常它应该也是Hive表的分区列。</p>\n<p>##了解维度的基数<br>维度的基数（Cardinality）指的是该维度在数据集中出现的不同值的个数；例如“国家”是一个维度，如果有200个不同的值，那么此维度的基数就是200.通常一个维度的基数会从几十到几万个不等，个别维度如“用户ID”的基数会超过百万甚至千万。基数超过一百万的维度通常称为超高维度（Ulta Hight Cardinality，UHC），需要引起设计者的注意。<br>Cube中所有维度的基数都可以体现Cube的复杂度，如果一个Cube中有好几个超高基数维度，那么这个Cube膨胀就会很高。在创建Cube前需要对所有维度的基数做一个了解，这样就可以帮助设计合理的Cube。计算基数有多种途径，最简单的方法就是让Hive执行一个count distinct的SQL查询；Kylin也提供计算基数的方法，在导入Hive表定义后可以看到每一个列的基数，参数名为Cardinality</p>\n<p>##Sample Data<br>如果需要快速体验Kylin，可以用Kylin自带的Sample Data。运行${KYLIN_HOME}/bin/sample.sh来导入Sample Data，然后就能按照下面的流程来创建模型和Cube。<br>具体请执行下面命令，将Sample Data导入到Hive数据库。<br>cd ${KYLIN_HOME}<br>bin/sample.sh<br>Sample Data测试的样例数据集总共仅1M左右，共计3张表，其中事实表有10000条数据。数据集是一个规范的星形模型结构，它总包含3个数据表：<br>KYLIN_SALES是事实表，保存了销售订单的明细信息。各列分别保存着卖家、商品、分类、订单金额、商品数据等信息，每一行对应着一笔交易订单。<br>KYLIN_CATEGORY_GROUPINGS是维表，保存了商品分类的详细介绍，例如商品分类名称等。<br>KYLIN_CAL_DT也是维表，保存了时间的扩展信息。如单个日期所在的年始、月始、周始、年份、月份等。<br>这3张表一起构成了整个星形模型。  </p>\n<p>#设计Cube<br>如果数据已经在Hive中准备好了，就可以开始创建Cube了。</p>\n<p>##导入Hive表定义<br>登陆Kylin的Web界面，创建新的或选择一个已有的项目之后，需要做的就是将Hive表的定义导入到Kylin中。<br>单击Web界面的Model-&gt;Data Source下的”Local Hive Table“图标，然后输入表的名称（可以一次导入多个表，以逗号分隔表名），单击按钮”Sync“，Kylin就会使用Hive的API从Hive中获取表的属性信息。<br>导入成功后，表的结构信息会以树状的形式显示在页面的左侧，可以单击展开或收缩。</p>\n<p>同时Kylin会在后台触发一个MapRecue任务，计算此表的每个列的基数。通常稍过几分钟后再刷新页面，就会看到显示出来 的基数信息Cardinality</p>\n<p>需要注意的是，这里Kylin对基数的计算方法采用的是HyperLogLog的近似算法，与精确值略有误差，只做参考值。</p>\n<p>##创建数据模型<br>有了表信息之后，就可以开始创建数据模型（Data Model）了。数据模型是Cube的基础，它主要用于描述一个星形模型。有了数据模型以后，定义Cube的时候就可以直接从此模型定义的表和列中进行选择了，省去重复指定连接（join）条件的步骤。基于一个数据模型还可以创建多个Cube，以方便减少用户的重复性工作。<br>在Kylin界面中”Models“页面中单击”New”-&gt;”New Model”，开始创建数据模型。</p>\n<p>接下来选择用作维度和度量的列。这里只是选择一个范围，不代表这些列将来一定要用作Cube 的维度或度量，你可以把所有可能会用到表都选进来，后续创建Cube的时候，将只能从这些列中进行选择。   </p>\n<p>选择维度列时，维度可以来自事实表或维度表<br>选择度量列时，度量只能来自事实表<br>最后一步，是为模型补充侵害时间列信息和过滤条件。如果此模型中的事实表记录是按时间增长的，那么可以指定一个日期/时间列作为模型的分割时间列，从而可以让Cube按此列做增量构建。</p>\n<p>过滤（Filter）条件是指，如果想把一些记录忽略掉，那么这里可以设置一个过滤条件。Kylin在向Hive请求源数据的时候，会带上此过滤条件。</p>\n<p>随后“Save”后，出现在“Model”的列表中。</p>\n<p>##创建Cube<br>单击“New”，选择“New Cube”，会开启一个包含若干步骤的向导。</p>\n<p>第一页，选择要使用的数据模型，并为此Cube输入一个唯一的名称（必需的）和描述（可选的）；这里还可以输入一个邮件通知列表，用于在构建完成或出错时收到通知。如果不想接收处于某些状态的通知，那么可以从“Notification Events”中将其去掉。</p>\n<p>第二页，选择Cube的维度。可以通过以下两个按钮来添加维度。<br><strong>“Add Mimension”</strong>：逐个添加维度，可以是普通维度也可以是衍生（Derived）维度。<br><strong>“Auto Generator”：</strong>批量选择并添加，让Kylin自动完成其它信息。<br>使用第一种方法的时候需要为每个维度起个名字，然后选择表和列。<br>如果是衍生维度的话，则必须是来自于某个维度表，一次可以选择多个列；由于这些列值都可以从该维度表的主键值中衍生出来，所以实际上只有主键列会被Cube加入计算。而在Kylin 的具体实现中，往往采用事实表上的外键替代主键进行计算和存储。但是在逻辑上可以认为衍生列来自于维度表的主键。<br>使用第二种方法，Kylin会用一个树状结构呈现出所有的列，用户只需要勾选所需要的列即可，Kylin会自动补充其他信息，从而方便用户的操作。请注意，在这里Kylin会把维度表上的列都创建成衍生维度，这也许不是最合适的，在这种情况下请使用第一种方法。</p>\n<p>第三页，创建度量。Kylin默认会创建一个Count(1)的度量。可以单击“+Measure”按钮来添加新度量。Kylin支持的度量有：SUM、MIN、MAX、COUNT、COUNT　DISTINCT、ＴＯＰ＿Ｎ、RAW等。请选择需要的度量类型，然后再选择适当的参数（通常为列名）</p>\n<p>重复上面的步骤，创建所需要的度量。Kylin可以支持在一个Cube中添加多达上百个度量；添加完成所有度量之后，单击“Next”。</p>\n<p>第四页，是关于Cube数据刷新的设置。在这里可以设置自动合并的阈值、数据保留的最短时间，以及第一个Segment的起点时间（如果Cube有分割时间列的话）。</p>\n<p>第五页，高级设置。在此页面上可以设置聚合组和Rowkey<br>Kylin默认会把所有的维度都放在同一个聚合中；如果维度数据较多（例如&gt;10），那么建议用户根据查询的习惯和模式，单击“New Aggregation Group+”，将维度分为多个聚合组。通过使用多个聚合组，可以大大降低Cube中的Cuboid数量。下面来举例说明，如果一个Cube有（M+N)个维度，那么默认它会有2的m+n次方个Cuboid；如果把这些维度分为两个不相交的聚合组，那么Cuboid的数量将被减少为2的m次方+2的n次方。<br>在单个聚合组中，可以对维度设置高级属性，例如Mandatory、Hierarchy、Joint等。这几个属性都是为了优化Cube的计算而设计的，了解这些属性的含义对日后更好地使用Cube至关重要。<br>Mandatory维度指的是那些总是会出现在where条件或Group By语句里的维度；通过将某个维度指定为Mandatory，Kylin就可以不用预计算那些不包含此维度的Cuboid，从而减少计算量。<br>Hierarchy是一组有层级关系的维度，例如：“国家”“省”“市”，这里的“国家”是高级的维度，“省”“市”依次是低级的维度。用户会按高级别维度进行查询，也会按低级别维度进行查询，但在查询低级别维度时，往往都会带上高级别维度的条件，而不会孤立地审视低级别维度的数据。例如，用户单击“国家”作为维度来查询汇总数据，也可能单击“国家”+“省”或者“国家”+“省”+“市”来查询，但是不会跨越国家直接Group By“省”或“市”。通过指定Hierarchy，Kylin可以省略不满足此模式的cuboid。<br>Joint是将多个维度组合成一个维度，其通常适用于如下两种情况。<br>1.总是会在一起查询的维度。<br>2.基数很低的维度<br>Kylin以Key-Value的方式将Cube存在到HBase中。HBase的key，也就是Rowkey，是由各维度的值拼接而成的；为了更高效地存储这些值，Kylin会对它们进行编码和压缩；每个维度均可以选择合适的编码（Encoding）方式，默认采用的是字典（Dictionary）编码技术；除了字典以外，还有整数（Int）和固定长度（Fixed Length）的编码。<br>字典编码是将此维度下所有值构建成一个从string到int的映射表；Kylin会将字典序列化保存，在Cube中存储int值，从而大大减小存储的大小。另外，字典是保持顺序的，即如果字符串A比字符串B大的话，那么A的编码后的int值也会比B编码后的值大；这样可以使得在HBase中进行比较查询的时候，依然使用编码后的值，而无需解码。</p>\n<p>字典非常适合于非固定长度的string类型值的维度，而且用户无需指定编码后的长度；但是由于使用字典需要维护一张映射表，因些如果此维度的基数很高，那么字典的大小就非常可观，从而不适合于加载到内存中，在这种情况下就要选择其他的编码方式了。Kylin中字典编码允许的基数上限默认是500万（由参数”kylin.dictioinary.max.cardinality”配置）。<br>整数（int）编码适合于对int或bigint类型的值进行编码，它无需额外存储，同时还可以支持很大的基数。用户需要根据值域选择编码的长度。例如有一个手机号码的维度，它是一个11位的数字，如13800138000，我们知道它大于2的31次方，但是小于2的39次方减1，那么使用int(5)即可满足要求，每个值占用5字节，比按字符存储（11字节）要少占一半以上的空间。  </p>\n<p>当上面几种编码方式都不适合的时候，就使用固定长度的编码了；此编码方式其实只是将原始值截断或补充成相同长度的一组字节，没有额外的转换，所以空间效率较差，通常只是作为一种权宜手段。<br>各维度在Rowkeys中的顺序，对于 查询的性能会产生较明显的影响。在这里用户可以根据查询的模式和习惯，通过拖拽的方式调整各个维度在Rowkeys上的顺序。通常的原则是，将过滤频率较高的列放置在过滤频率较低的列之前，将基数高的列放置在基数低的列之前。这样做的好处是，充分利用过滤条件来缩小在HBase中扫描的范围，从而提高查询的效率。<br>第五页，为Cube配置参数。和其他Hadoop工具一样，Kylin使用了很多配置参数以提高录活性，用户可以根据具体的环境、场景等配置不同的参数进行调优。Kylin全局的参数值可在conf/kylin.properties文件中进行配置；如果Cube需要覆盖全局设置的话，则需要在此页面中指定。单击“+Property”按钮，然后输入参数名和参数值。例如“kylin.hbase.region.cut=1”,这样此Cube在存储的时候，Kylin将会为每个HTbase Region分配1GB来创建一个HTbase Region。</p>\n<p>#构建Cube<br>新创建的Cube只有定义，而没有计算的数据，它的状态是”DISABLED“，是不会被查询引擎挑中的。要想让Cube有数据，还需要对它进行构建。Cube的构建方式通常有两种：全量构建和增量构建；两者的构建步骤是完全一样的，区别只在于构建时读取的数据源是全集还是子集。<br>Cube的构建包含如下步骤，由任务引擎来调度执行。<br>1）创建临时的Hive平表（从Hive读取数据）<br>2）计算各维度的不同值，并收集各Cuboid的统计数据。<br>3）创建并保存字典。<br>4）保存Cuboid统计信息。<br>5）创建HTable。<br>6）计算Cube（一轮或若干轮MapRecue）。<br>7）将Cube的计算结果转成HFile。<br>8）加载HFile到HBase。<br>9）更新Cube元数据。<br>10）垃圾回收。<br>以上步骤中，前5步是计算Cube而做的准备工作，例如遍历维度值来创建字典，对数据做统计秋估算以创建HTable等；第6）步是真正的Cube计算，取决于所使用的Cube算法，它可能是一轮MapRecue任务，也可能是N（在没有优化的情况下，N可以被视作是维度数）轮迭代的MapRecue。由于Cube运算的中间结果是以SequenceFile的格式存储在HDFS上的，所以为了导入到HBase中，还需要第7）步将这些结果转换成HFile（HBase文件存储格式）。第8）步通过使用HBase BulkLoad工具，将HFile导入到HBase集群，这一步完成之后，HTable就可以查询到数据了。第9）步更新Cube的数据，将此次构建 Segment的状态从”NEW“更新为”ＲＥＡＤＹ＂，表示已经可借查询了。最后一步，清理构建过程中生成的临时文件等垃圾，释放集群资源。　　<br>Monitor页面会显示当前项目下近期的构建任务。　　</p>\n<p>##全量构建和增量构建</p>\n<p>###全量构建</p>\n<p>###增量构建</p>\n<p>##历史数据刷新</p>\n<p>##合并</p>\n<p>#查询Cube</p>\n"},{"title":"数据分析软件分类","date":"2017-04-16T15:43:49.000Z","_content":"\n#数据分析软件分类  \n下面介绍一些适合大数据分析的存储数据库，或者面向大数据分析，适用于TB级以上的数据库存储和分析任务。分为如下几类介绍：  \n1.商业数据库  \n2.开源时序数据库  \n3.开源计算框架  \n4.开源SQL on hadoop  \n5.云端数据分析SaaS  \n\n##商业软件  \n商业数据库软件种类繁多，但是真正能支持TB级别以上的数据存储和分析并不多，下面介绍几个有特点、支持大数据的商用数据库。  \n\n###HP Vertica\nVertica公司成立于2005年，创立者为数据库巨擘Michael Stonebraker。2011成Vertica被惠普收购。Vertica是能够提供高效数据存储和快速查询数据存储数据库实时分析平台，还支持大规模并行 处理（MPP）。产品广泛应用于高端数据营销、互联网客户分析处理，数据达到PB级别。\nVertical特点如下：\n面向列的存储  \n灵活的压缩算法，根据数据的排序性和基数决定压缩算法。  \n高可用数据库和查询  \nMPP架构，分布式存储和任务负载，Shared nothing架构。\n支持标准SQL查询、ODBC/JDBC等..   \n支持Projection（数据投射）功能。\n\n\n###Oracle Exadata\nOracle Exadata是数据库发展史上一个人传奇，它是数据库软件和最新硬件的完美结合。它提供最快、最可靠的数据库平台，不仅支持常规的数据库应用，也支持联机分析处理（OLAP）和数据仓库（DW）的场景。  \nOracle Exadata采用了多种最新的硬件技术，例如40GB的InfiniBan网络\n\n\n###Teradata\nTeradata（天睿）公司是专注于大数据分析、数据仓库和整合营销管理解决方案的供应商。Teradata采用纯粹的Shared noting架构，支持MPP。对于多维度的查询更加灵活，专注于数据仓库的应用领域。  \n\n\n##时序数据库\n时序数据库用于记录过去时间的各个数据点的信息，典型的应用是服务器的各种性能指标，例如CPU、内存使用情况等 。目前时序数据库也广泛应用于各种传感器的数据收集分析工作中，这些数据的收集都有一个特点，就是对时间的依赖非常大，每天产生的数据量非常大，因此定入的量非常大，一般的关系型数据库无法满足这些场景。因此，时序数据库在设计上需要支持高吞吐、高效数据压缩，支持历史查询、分布式部署等。\n\n###1.OpenTSDB\nOpenTSDB是一个开源的时序数据库，支持存储千亿的数据点，并提供精确查询功能。它采用Java语言编写，通过基于HBaser存储实现横向扩展。\n\n\n###2.InfluxDB\nInfluxDB采用GoLang语言开发，也是一个开源应用，社区非常活跃。其技术特点包含：支持任意数量的列，支持方便、强大的查询语言，集成了数据采集、存储和可视化功能。\n\n##开源分布式计算平台\n一个是Hadoop，另一个是Spark，这里就不一一介绍了。\n\n##开源分析数据库\n###Kylin\nKylin是Apache开源的开源分布式分析引擎。 \n与Kylin一样致力于大数据查询问题的开源产品如Apache Drill、Apache Impala、Druid、Hive、Presto（Facebook）、SparkSQL等。  \n从底层技术角度来看，这些开源产品有很大的共性，一些底层技术几乎被所有的产品一致采用。  \n1）大规模并行处理：可以通过增加机器的方式来扩容处理速度，在相同的时间里处理更多的数据。  \n2）列式存储：通过按列存储提高单位时间里数据的I/O吞吐率，还能跳过不需要访问的列。  \n3）索引：利用索引配合查询条件，可以迅速跳过不符合条件的数据块，仅扫描需要扫描的数据内容。  \n4）压缩：压缩数据然后存储，使得存储的密度更高，在有限的I/O速率下，在单位时间里读取更多的记录。  \n综上所述，我们可以注意到，所有这些方法都只是提高了单位时间内处理数据的能力，当大家都一致采用这些技术时，它们之间的区别将只停留在实现层面的代码细节上。最重要的是，这些技术都不会改变一个事实，那就是处理时间与数据量之间的正比例关系。当数据量翻倍时，MPP（在不扩容的前提下）需要翻倍的时间来完成计算；列式存储需要翻倍的存储空间；索引下符合条件的记录数据数也会翻倍；压缩事的数据大小也还是之间的两倍。因此查询速度也会随之就之前的两倍。当数据量成十倍地增长时，这些技术的查询速度就会成十倍地下降，最终变得不能接受。  \nApache Kylin的特色在于，在上述的底层技术之外，另辟蹊径地使用了独特的Cube预计算技术。预计算将数据按维度组合进行了聚合，将结果保存为物化视图。经过聚合，物化视图的规模就只由维度的基数来决定，而不再随着数据量的增长呈线性增长。以电商为例，如果业务扩张，交易量增长了10倍，只要交易数据的维度不变（供应商/商品数量不变），聚合后的物化视图初依旧是原先的大小，查询的速度也将保持不变。  \n与那些类似产品相比，这一导技术的区别使得Kylin从外在功能上呈现出了不同的特性，具体如下：  \n1）SQL接口：除了Druid以外，所有的产品都支持SQL或类SQL接口。巧合的是Druid也是除了Kylin以外，查询性能相对更好的一个。这点除了Druid有自己的存储引擎之外，可能还利益于其较为受限的查询能力。　　\n2）大数据支持：大数据产品的能力在亿级到十亿级数据量之间，再大的数据量将显著降低查询性能。而Kylin因为采用预计算技术，因此查询速度不受数据量的限制。  \n3）查询速度，不会随着数据量的增加而查询性能下降。  \n4）吞吐量：根据之前的实验数据，Kylin的单例吞吐量一般在每秒70个查询左右，并且可以线性扩展，而普通的产品因为所有计算都在查询时完成，所以需要调动集群的更多资源才能完成查询，通常极限在每秒20个查询左右，而且扩容成本较高，需要扩展整个集群。相对的，Kylin系统因为瓶颈不在整个集群，而在于Kylin服务器，因此只需要增加Kylin服务器就能成倍提高吞率，扩容成本低廉。\n\n###Druid\nDruid是什么？  \nDruid是一个分布式的支持实时分析的数据存储系统（Data Store）,是美国广告技术公司MetaMarkets于2011年创建，2012年开源的项目，Druid设计之初是为分析而生。官方网站是：http://druid.io\n\n\n###Pinot\nPinot是Linkin于2015年开源的一个分布式列式数据存储系统。\n\n\n###神秘的谷歌Dremel\nDremel是谷歌的“交互式”数据分析系统，支持上千台机器的集群部署，处理PB级别的数据，可以对网状数据的只读数据进行随机查询访问，帮助数据分析分提供Ad Hoc查询功能，进行尝试的数据探索（Exploration）。\n\n\n###Apache Drill\nApache Drill通过开源方式实现了谷歌Dremel。Drill架构的整个思想还是通过优化查询引擎，进行快速全表扫描，以快速返回结果，其高层架构示意图如下：\n\n\nApache Drill基于SQL的数据分析和商业智能引入了JSON文件模型，这使得用户能查询固定架构，支持各种格式和数据存储中的模式无关数据。该体系架构中的关系查询引擎和数据库构建是有先决条件的，即假设所有数据都有一个简单的静态架构。\n\nApache Drill的架构是独一无二的，它是唯一一个支持复杂和无模式数据的柱状行引擎，也是唯一一个能在查询执行期间进行的数据驱动查询。\n\n###Elasticsearch\nElasticsearch（ES）是Elastic公司推出一个基于Lucerne的分布式\n\n\n","source":"_posts/hadoop/数据分析软件分类.md","raw":"---\ntitle: 数据分析软件分类\ndate: 2017-04-16 23:43:49\ntags: [大数据,数据分析]\ncategories: [大数据,数据分析]\n---\n\n#数据分析软件分类  \n下面介绍一些适合大数据分析的存储数据库，或者面向大数据分析，适用于TB级以上的数据库存储和分析任务。分为如下几类介绍：  \n1.商业数据库  \n2.开源时序数据库  \n3.开源计算框架  \n4.开源SQL on hadoop  \n5.云端数据分析SaaS  \n\n##商业软件  \n商业数据库软件种类繁多，但是真正能支持TB级别以上的数据存储和分析并不多，下面介绍几个有特点、支持大数据的商用数据库。  \n\n###HP Vertica\nVertica公司成立于2005年，创立者为数据库巨擘Michael Stonebraker。2011成Vertica被惠普收购。Vertica是能够提供高效数据存储和快速查询数据存储数据库实时分析平台，还支持大规模并行 处理（MPP）。产品广泛应用于高端数据营销、互联网客户分析处理，数据达到PB级别。\nVertical特点如下：\n面向列的存储  \n灵活的压缩算法，根据数据的排序性和基数决定压缩算法。  \n高可用数据库和查询  \nMPP架构，分布式存储和任务负载，Shared nothing架构。\n支持标准SQL查询、ODBC/JDBC等..   \n支持Projection（数据投射）功能。\n\n\n###Oracle Exadata\nOracle Exadata是数据库发展史上一个人传奇，它是数据库软件和最新硬件的完美结合。它提供最快、最可靠的数据库平台，不仅支持常规的数据库应用，也支持联机分析处理（OLAP）和数据仓库（DW）的场景。  \nOracle Exadata采用了多种最新的硬件技术，例如40GB的InfiniBan网络\n\n\n###Teradata\nTeradata（天睿）公司是专注于大数据分析、数据仓库和整合营销管理解决方案的供应商。Teradata采用纯粹的Shared noting架构，支持MPP。对于多维度的查询更加灵活，专注于数据仓库的应用领域。  \n\n\n##时序数据库\n时序数据库用于记录过去时间的各个数据点的信息，典型的应用是服务器的各种性能指标，例如CPU、内存使用情况等 。目前时序数据库也广泛应用于各种传感器的数据收集分析工作中，这些数据的收集都有一个特点，就是对时间的依赖非常大，每天产生的数据量非常大，因此定入的量非常大，一般的关系型数据库无法满足这些场景。因此，时序数据库在设计上需要支持高吞吐、高效数据压缩，支持历史查询、分布式部署等。\n\n###1.OpenTSDB\nOpenTSDB是一个开源的时序数据库，支持存储千亿的数据点，并提供精确查询功能。它采用Java语言编写，通过基于HBaser存储实现横向扩展。\n\n\n###2.InfluxDB\nInfluxDB采用GoLang语言开发，也是一个开源应用，社区非常活跃。其技术特点包含：支持任意数量的列，支持方便、强大的查询语言，集成了数据采集、存储和可视化功能。\n\n##开源分布式计算平台\n一个是Hadoop，另一个是Spark，这里就不一一介绍了。\n\n##开源分析数据库\n###Kylin\nKylin是Apache开源的开源分布式分析引擎。 \n与Kylin一样致力于大数据查询问题的开源产品如Apache Drill、Apache Impala、Druid、Hive、Presto（Facebook）、SparkSQL等。  \n从底层技术角度来看，这些开源产品有很大的共性，一些底层技术几乎被所有的产品一致采用。  \n1）大规模并行处理：可以通过增加机器的方式来扩容处理速度，在相同的时间里处理更多的数据。  \n2）列式存储：通过按列存储提高单位时间里数据的I/O吞吐率，还能跳过不需要访问的列。  \n3）索引：利用索引配合查询条件，可以迅速跳过不符合条件的数据块，仅扫描需要扫描的数据内容。  \n4）压缩：压缩数据然后存储，使得存储的密度更高，在有限的I/O速率下，在单位时间里读取更多的记录。  \n综上所述，我们可以注意到，所有这些方法都只是提高了单位时间内处理数据的能力，当大家都一致采用这些技术时，它们之间的区别将只停留在实现层面的代码细节上。最重要的是，这些技术都不会改变一个事实，那就是处理时间与数据量之间的正比例关系。当数据量翻倍时，MPP（在不扩容的前提下）需要翻倍的时间来完成计算；列式存储需要翻倍的存储空间；索引下符合条件的记录数据数也会翻倍；压缩事的数据大小也还是之间的两倍。因此查询速度也会随之就之前的两倍。当数据量成十倍地增长时，这些技术的查询速度就会成十倍地下降，最终变得不能接受。  \nApache Kylin的特色在于，在上述的底层技术之外，另辟蹊径地使用了独特的Cube预计算技术。预计算将数据按维度组合进行了聚合，将结果保存为物化视图。经过聚合，物化视图的规模就只由维度的基数来决定，而不再随着数据量的增长呈线性增长。以电商为例，如果业务扩张，交易量增长了10倍，只要交易数据的维度不变（供应商/商品数量不变），聚合后的物化视图初依旧是原先的大小，查询的速度也将保持不变。  \n与那些类似产品相比，这一导技术的区别使得Kylin从外在功能上呈现出了不同的特性，具体如下：  \n1）SQL接口：除了Druid以外，所有的产品都支持SQL或类SQL接口。巧合的是Druid也是除了Kylin以外，查询性能相对更好的一个。这点除了Druid有自己的存储引擎之外，可能还利益于其较为受限的查询能力。　　\n2）大数据支持：大数据产品的能力在亿级到十亿级数据量之间，再大的数据量将显著降低查询性能。而Kylin因为采用预计算技术，因此查询速度不受数据量的限制。  \n3）查询速度，不会随着数据量的增加而查询性能下降。  \n4）吞吐量：根据之前的实验数据，Kylin的单例吞吐量一般在每秒70个查询左右，并且可以线性扩展，而普通的产品因为所有计算都在查询时完成，所以需要调动集群的更多资源才能完成查询，通常极限在每秒20个查询左右，而且扩容成本较高，需要扩展整个集群。相对的，Kylin系统因为瓶颈不在整个集群，而在于Kylin服务器，因此只需要增加Kylin服务器就能成倍提高吞率，扩容成本低廉。\n\n###Druid\nDruid是什么？  \nDruid是一个分布式的支持实时分析的数据存储系统（Data Store）,是美国广告技术公司MetaMarkets于2011年创建，2012年开源的项目，Druid设计之初是为分析而生。官方网站是：http://druid.io\n\n\n###Pinot\nPinot是Linkin于2015年开源的一个分布式列式数据存储系统。\n\n\n###神秘的谷歌Dremel\nDremel是谷歌的“交互式”数据分析系统，支持上千台机器的集群部署，处理PB级别的数据，可以对网状数据的只读数据进行随机查询访问，帮助数据分析分提供Ad Hoc查询功能，进行尝试的数据探索（Exploration）。\n\n\n###Apache Drill\nApache Drill通过开源方式实现了谷歌Dremel。Drill架构的整个思想还是通过优化查询引擎，进行快速全表扫描，以快速返回结果，其高层架构示意图如下：\n\n\nApache Drill基于SQL的数据分析和商业智能引入了JSON文件模型，这使得用户能查询固定架构，支持各种格式和数据存储中的模式无关数据。该体系架构中的关系查询引擎和数据库构建是有先决条件的，即假设所有数据都有一个简单的静态架构。\n\nApache Drill的架构是独一无二的，它是唯一一个支持复杂和无模式数据的柱状行引擎，也是唯一一个能在查询执行期间进行的数据驱动查询。\n\n###Elasticsearch\nElasticsearch（ES）是Elastic公司推出一个基于Lucerne的分布式\n\n\n","slug":"hadoop/数据分析软件分类","published":1,"updated":"2017-08-18T01:44:35.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjh7poltb001ybp7rbl0w3p14","content":"<p>#数据分析软件分类<br>下面介绍一些适合大数据分析的存储数据库，或者面向大数据分析，适用于TB级以上的数据库存储和分析任务。分为如下几类介绍：<br>1.商业数据库<br>2.开源时序数据库<br>3.开源计算框架<br>4.开源SQL on hadoop<br>5.云端数据分析SaaS  </p>\n<p>##商业软件<br>商业数据库软件种类繁多，但是真正能支持TB级别以上的数据存储和分析并不多，下面介绍几个有特点、支持大数据的商用数据库。  </p>\n<p>###HP Vertica<br>Vertica公司成立于2005年，创立者为数据库巨擘Michael Stonebraker。2011成Vertica被惠普收购。Vertica是能够提供高效数据存储和快速查询数据存储数据库实时分析平台，还支持大规模并行 处理（MPP）。产品广泛应用于高端数据营销、互联网客户分析处理，数据达到PB级别。<br>Vertical特点如下：<br>面向列的存储<br>灵活的压缩算法，根据数据的排序性和基数决定压缩算法。<br>高可用数据库和查询<br>MPP架构，分布式存储和任务负载，Shared nothing架构。<br>支持标准SQL查询、ODBC/JDBC等..<br>支持Projection（数据投射）功能。</p>\n<p>###Oracle Exadata<br>Oracle Exadata是数据库发展史上一个人传奇，它是数据库软件和最新硬件的完美结合。它提供最快、最可靠的数据库平台，不仅支持常规的数据库应用，也支持联机分析处理（OLAP）和数据仓库（DW）的场景。<br>Oracle Exadata采用了多种最新的硬件技术，例如40GB的InfiniBan网络</p>\n<p>###Teradata<br>Teradata（天睿）公司是专注于大数据分析、数据仓库和整合营销管理解决方案的供应商。Teradata采用纯粹的Shared noting架构，支持MPP。对于多维度的查询更加灵活，专注于数据仓库的应用领域。  </p>\n<p>##时序数据库<br>时序数据库用于记录过去时间的各个数据点的信息，典型的应用是服务器的各种性能指标，例如CPU、内存使用情况等 。目前时序数据库也广泛应用于各种传感器的数据收集分析工作中，这些数据的收集都有一个特点，就是对时间的依赖非常大，每天产生的数据量非常大，因此定入的量非常大，一般的关系型数据库无法满足这些场景。因此，时序数据库在设计上需要支持高吞吐、高效数据压缩，支持历史查询、分布式部署等。</p>\n<p>###1.OpenTSDB<br>OpenTSDB是一个开源的时序数据库，支持存储千亿的数据点，并提供精确查询功能。它采用Java语言编写，通过基于HBaser存储实现横向扩展。</p>\n<p>###2.InfluxDB<br>InfluxDB采用GoLang语言开发，也是一个开源应用，社区非常活跃。其技术特点包含：支持任意数量的列，支持方便、强大的查询语言，集成了数据采集、存储和可视化功能。</p>\n<p>##开源分布式计算平台<br>一个是Hadoop，另一个是Spark，这里就不一一介绍了。</p>\n<p>##开源分析数据库</p>\n<p>###Kylin<br>Kylin是Apache开源的开源分布式分析引擎。<br>与Kylin一样致力于大数据查询问题的开源产品如Apache Drill、Apache Impala、Druid、Hive、Presto（Facebook）、SparkSQL等。<br>从底层技术角度来看，这些开源产品有很大的共性，一些底层技术几乎被所有的产品一致采用。<br>1）大规模并行处理：可以通过增加机器的方式来扩容处理速度，在相同的时间里处理更多的数据。<br>2）列式存储：通过按列存储提高单位时间里数据的I/O吞吐率，还能跳过不需要访问的列。<br>3）索引：利用索引配合查询条件，可以迅速跳过不符合条件的数据块，仅扫描需要扫描的数据内容。<br>4）压缩：压缩数据然后存储，使得存储的密度更高，在有限的I/O速率下，在单位时间里读取更多的记录。<br>综上所述，我们可以注意到，所有这些方法都只是提高了单位时间内处理数据的能力，当大家都一致采用这些技术时，它们之间的区别将只停留在实现层面的代码细节上。最重要的是，这些技术都不会改变一个事实，那就是处理时间与数据量之间的正比例关系。当数据量翻倍时，MPP（在不扩容的前提下）需要翻倍的时间来完成计算；列式存储需要翻倍的存储空间；索引下符合条件的记录数据数也会翻倍；压缩事的数据大小也还是之间的两倍。因此查询速度也会随之就之前的两倍。当数据量成十倍地增长时，这些技术的查询速度就会成十倍地下降，最终变得不能接受。<br>Apache Kylin的特色在于，在上述的底层技术之外，另辟蹊径地使用了独特的Cube预计算技术。预计算将数据按维度组合进行了聚合，将结果保存为物化视图。经过聚合，物化视图的规模就只由维度的基数来决定，而不再随着数据量的增长呈线性增长。以电商为例，如果业务扩张，交易量增长了10倍，只要交易数据的维度不变（供应商/商品数量不变），聚合后的物化视图初依旧是原先的大小，查询的速度也将保持不变。<br>与那些类似产品相比，这一导技术的区别使得Kylin从外在功能上呈现出了不同的特性，具体如下：<br>1）SQL接口：除了Druid以外，所有的产品都支持SQL或类SQL接口。巧合的是Druid也是除了Kylin以外，查询性能相对更好的一个。这点除了Druid有自己的存储引擎之外，可能还利益于其较为受限的查询能力。　　<br>2）大数据支持：大数据产品的能力在亿级到十亿级数据量之间，再大的数据量将显著降低查询性能。而Kylin因为采用预计算技术，因此查询速度不受数据量的限制。<br>3）查询速度，不会随着数据量的增加而查询性能下降。<br>4）吞吐量：根据之前的实验数据，Kylin的单例吞吐量一般在每秒70个查询左右，并且可以线性扩展，而普通的产品因为所有计算都在查询时完成，所以需要调动集群的更多资源才能完成查询，通常极限在每秒20个查询左右，而且扩容成本较高，需要扩展整个集群。相对的，Kylin系统因为瓶颈不在整个集群，而在于Kylin服务器，因此只需要增加Kylin服务器就能成倍提高吞率，扩容成本低廉。</p>\n<p>###Druid<br>Druid是什么？<br>Druid是一个分布式的支持实时分析的数据存储系统（Data Store）,是美国广告技术公司MetaMarkets于2011年创建，2012年开源的项目，Druid设计之初是为分析而生。官方网站是：<a href=\"http://druid.io\" target=\"_blank\" rel=\"noopener\">http://druid.io</a></p>\n<p>###Pinot<br>Pinot是Linkin于2015年开源的一个分布式列式数据存储系统。</p>\n<p>###神秘的谷歌Dremel<br>Dremel是谷歌的“交互式”数据分析系统，支持上千台机器的集群部署，处理PB级别的数据，可以对网状数据的只读数据进行随机查询访问，帮助数据分析分提供Ad Hoc查询功能，进行尝试的数据探索（Exploration）。</p>\n<p>###Apache Drill<br>Apache Drill通过开源方式实现了谷歌Dremel。Drill架构的整个思想还是通过优化查询引擎，进行快速全表扫描，以快速返回结果，其高层架构示意图如下：</p>\n<p>Apache Drill基于SQL的数据分析和商业智能引入了JSON文件模型，这使得用户能查询固定架构，支持各种格式和数据存储中的模式无关数据。该体系架构中的关系查询引擎和数据库构建是有先决条件的，即假设所有数据都有一个简单的静态架构。</p>\n<p>Apache Drill的架构是独一无二的，它是唯一一个支持复杂和无模式数据的柱状行引擎，也是唯一一个能在查询执行期间进行的数据驱动查询。</p>\n<p>###Elasticsearch<br>Elasticsearch（ES）是Elastic公司推出一个基于Lucerne的分布式</p>\n","site":{"data":{}},"excerpt":"","more":"<p>#数据分析软件分类<br>下面介绍一些适合大数据分析的存储数据库，或者面向大数据分析，适用于TB级以上的数据库存储和分析任务。分为如下几类介绍：<br>1.商业数据库<br>2.开源时序数据库<br>3.开源计算框架<br>4.开源SQL on hadoop<br>5.云端数据分析SaaS  </p>\n<p>##商业软件<br>商业数据库软件种类繁多，但是真正能支持TB级别以上的数据存储和分析并不多，下面介绍几个有特点、支持大数据的商用数据库。  </p>\n<p>###HP Vertica<br>Vertica公司成立于2005年，创立者为数据库巨擘Michael Stonebraker。2011成Vertica被惠普收购。Vertica是能够提供高效数据存储和快速查询数据存储数据库实时分析平台，还支持大规模并行 处理（MPP）。产品广泛应用于高端数据营销、互联网客户分析处理，数据达到PB级别。<br>Vertical特点如下：<br>面向列的存储<br>灵活的压缩算法，根据数据的排序性和基数决定压缩算法。<br>高可用数据库和查询<br>MPP架构，分布式存储和任务负载，Shared nothing架构。<br>支持标准SQL查询、ODBC/JDBC等..<br>支持Projection（数据投射）功能。</p>\n<p>###Oracle Exadata<br>Oracle Exadata是数据库发展史上一个人传奇，它是数据库软件和最新硬件的完美结合。它提供最快、最可靠的数据库平台，不仅支持常规的数据库应用，也支持联机分析处理（OLAP）和数据仓库（DW）的场景。<br>Oracle Exadata采用了多种最新的硬件技术，例如40GB的InfiniBan网络</p>\n<p>###Teradata<br>Teradata（天睿）公司是专注于大数据分析、数据仓库和整合营销管理解决方案的供应商。Teradata采用纯粹的Shared noting架构，支持MPP。对于多维度的查询更加灵活，专注于数据仓库的应用领域。  </p>\n<p>##时序数据库<br>时序数据库用于记录过去时间的各个数据点的信息，典型的应用是服务器的各种性能指标，例如CPU、内存使用情况等 。目前时序数据库也广泛应用于各种传感器的数据收集分析工作中，这些数据的收集都有一个特点，就是对时间的依赖非常大，每天产生的数据量非常大，因此定入的量非常大，一般的关系型数据库无法满足这些场景。因此，时序数据库在设计上需要支持高吞吐、高效数据压缩，支持历史查询、分布式部署等。</p>\n<p>###1.OpenTSDB<br>OpenTSDB是一个开源的时序数据库，支持存储千亿的数据点，并提供精确查询功能。它采用Java语言编写，通过基于HBaser存储实现横向扩展。</p>\n<p>###2.InfluxDB<br>InfluxDB采用GoLang语言开发，也是一个开源应用，社区非常活跃。其技术特点包含：支持任意数量的列，支持方便、强大的查询语言，集成了数据采集、存储和可视化功能。</p>\n<p>##开源分布式计算平台<br>一个是Hadoop，另一个是Spark，这里就不一一介绍了。</p>\n<p>##开源分析数据库</p>\n<p>###Kylin<br>Kylin是Apache开源的开源分布式分析引擎。<br>与Kylin一样致力于大数据查询问题的开源产品如Apache Drill、Apache Impala、Druid、Hive、Presto（Facebook）、SparkSQL等。<br>从底层技术角度来看，这些开源产品有很大的共性，一些底层技术几乎被所有的产品一致采用。<br>1）大规模并行处理：可以通过增加机器的方式来扩容处理速度，在相同的时间里处理更多的数据。<br>2）列式存储：通过按列存储提高单位时间里数据的I/O吞吐率，还能跳过不需要访问的列。<br>3）索引：利用索引配合查询条件，可以迅速跳过不符合条件的数据块，仅扫描需要扫描的数据内容。<br>4）压缩：压缩数据然后存储，使得存储的密度更高，在有限的I/O速率下，在单位时间里读取更多的记录。<br>综上所述，我们可以注意到，所有这些方法都只是提高了单位时间内处理数据的能力，当大家都一致采用这些技术时，它们之间的区别将只停留在实现层面的代码细节上。最重要的是，这些技术都不会改变一个事实，那就是处理时间与数据量之间的正比例关系。当数据量翻倍时，MPP（在不扩容的前提下）需要翻倍的时间来完成计算；列式存储需要翻倍的存储空间；索引下符合条件的记录数据数也会翻倍；压缩事的数据大小也还是之间的两倍。因此查询速度也会随之就之前的两倍。当数据量成十倍地增长时，这些技术的查询速度就会成十倍地下降，最终变得不能接受。<br>Apache Kylin的特色在于，在上述的底层技术之外，另辟蹊径地使用了独特的Cube预计算技术。预计算将数据按维度组合进行了聚合，将结果保存为物化视图。经过聚合，物化视图的规模就只由维度的基数来决定，而不再随着数据量的增长呈线性增长。以电商为例，如果业务扩张，交易量增长了10倍，只要交易数据的维度不变（供应商/商品数量不变），聚合后的物化视图初依旧是原先的大小，查询的速度也将保持不变。<br>与那些类似产品相比，这一导技术的区别使得Kylin从外在功能上呈现出了不同的特性，具体如下：<br>1）SQL接口：除了Druid以外，所有的产品都支持SQL或类SQL接口。巧合的是Druid也是除了Kylin以外，查询性能相对更好的一个。这点除了Druid有自己的存储引擎之外，可能还利益于其较为受限的查询能力。　　<br>2）大数据支持：大数据产品的能力在亿级到十亿级数据量之间，再大的数据量将显著降低查询性能。而Kylin因为采用预计算技术，因此查询速度不受数据量的限制。<br>3）查询速度，不会随着数据量的增加而查询性能下降。<br>4）吞吐量：根据之前的实验数据，Kylin的单例吞吐量一般在每秒70个查询左右，并且可以线性扩展，而普通的产品因为所有计算都在查询时完成，所以需要调动集群的更多资源才能完成查询，通常极限在每秒20个查询左右，而且扩容成本较高，需要扩展整个集群。相对的，Kylin系统因为瓶颈不在整个集群，而在于Kylin服务器，因此只需要增加Kylin服务器就能成倍提高吞率，扩容成本低廉。</p>\n<p>###Druid<br>Druid是什么？<br>Druid是一个分布式的支持实时分析的数据存储系统（Data Store）,是美国广告技术公司MetaMarkets于2011年创建，2012年开源的项目，Druid设计之初是为分析而生。官方网站是：<a href=\"http://druid.io\" target=\"_blank\" rel=\"noopener\">http://druid.io</a></p>\n<p>###Pinot<br>Pinot是Linkin于2015年开源的一个分布式列式数据存储系统。</p>\n<p>###神秘的谷歌Dremel<br>Dremel是谷歌的“交互式”数据分析系统，支持上千台机器的集群部署，处理PB级别的数据，可以对网状数据的只读数据进行随机查询访问，帮助数据分析分提供Ad Hoc查询功能，进行尝试的数据探索（Exploration）。</p>\n<p>###Apache Drill<br>Apache Drill通过开源方式实现了谷歌Dremel。Drill架构的整个思想还是通过优化查询引擎，进行快速全表扫描，以快速返回结果，其高层架构示意图如下：</p>\n<p>Apache Drill基于SQL的数据分析和商业智能引入了JSON文件模型，这使得用户能查询固定架构，支持各种格式和数据存储中的模式无关数据。该体系架构中的关系查询引擎和数据库构建是有先决条件的，即假设所有数据都有一个简单的静态架构。</p>\n<p>Apache Drill的架构是独一无二的，它是唯一一个支持复杂和无模式数据的柱状行引擎，也是唯一一个能在查询执行期间进行的数据驱动查询。</p>\n<p>###Elasticsearch<br>Elasticsearch（ES）是Elastic公司推出一个基于Lucerne的分布式</p>\n"},{"title":"TODO-Oracle SQL优化","date":"2016-12-31T16:00:00.000Z","_content":"","source":"_posts/oracle/Oracle SQL优化.md","raw":"---\ntitle: TODO-Oracle SQL优化\ndate: 2017-01-01 00:00:00\ntags: [oracle,数据库]\ncategories: [数据库,oracle]\n---\n","slug":"oracle/Oracle SQL优化","published":1,"updated":"2017-08-18T01:44:35.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjh7poltd0022bp7rdjgwtez3","content":"","site":{"data":{}},"excerpt":"","more":""},{"title":"TODO-Oracle数据库系统架构","date":"2016-12-31T16:00:00.000Z","_content":"","source":"_posts/oracle/Oracle数据库系统架构.md","raw":"---\ntitle: TODO-Oracle数据库系统架构\ndate: 2017-01-01 00:00:00\ntags: [oracle,数据库]\ncategories: [数据库,oracle]\n---\n","slug":"oracle/Oracle数据库系统架构","published":1,"updated":"2017-08-18T01:44:35.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjh7polte0024bp7risqf1beq","content":"","site":{"data":{}},"excerpt":"","more":""},{"title":"Oracle网络和数据库连接","date":"2017-04-01T07:30:00.000Z","_content":"内容来自《Oracle Database 11g 数据库管理艺术》\n# 网络概念：Oracle网络如何工作\n在希望从客户机（不管是传统客户机还是基于浏览器的客户机）打开数据库会话时，需要通过网络连接到数据库。假如要将台式电脑通过现有网络连接到UNIX服务器上的一个Oracle数据库，则需要在电脑和Oracle数据库（它使用专门的软件）之间构造一个连接方法。需要某种界面来处理会话（在此例子中为SQL*Plus），并且需要某种与业内标准的网络协议（如TCP/IP）通信的方法。  \n为方便配置和管理网络连接，Oracle提供了Oracle Net Services，它是一套在分布式异构计算环境中提供连接方案的组件。Oracle Net Services由Oracle Net、Oracle Net Listener、Oracle Connection Manager、Oracle Net Configuration Assistant和Oracle Net Manager组成。Oracle Net Services软件是在Oracle Database Server或Oracle Client软件安装的过程中自动安装的。  \nOracle Net是一个初始化、建立及维护客户机和服务器之间的连接的组件。这就是为什么必须在客户机和服务器上都安装Oracle Net的原因。Oracle Net主要由两个组件构成。  \nOracle Network Foundation Layer：负责建立和维护客户机应用程序与服务器之间的连接，以及它们之间的交换信息。  \nOracle Protocol Support： 负责映射Transparent Net Substrate（TNS）功能到连接使用的业内标准协议。\n驻留Oracle数据库的所有服务器还运行一个名为Oracle Net Listener（通常也称为监听器）的服务，其主要功能是监听来自客户机服务登录Oracle数据库的请求。监听器在保证客户机服务具有与数据库匹配的信息（协议、端口和实例名）后，将客户机请求传递到数据库。假如用户名和密码通过认证，则数据库将允许客户机登录。一旦监听器把用户请求交付给数据库，客户机和数据库将直接连接，不再需要监听器的帮助。\nOracle提供了基于GUI的大量的实用程序，以帮助配置数据库的网络连接。这些实用程序包括Oracle Connection Manager、Oracle Net Manager和Oracle Net Configuragion Assistant等。这些工具帮助处理所有网络需求。在结束本章学习后，可单击这些程序的图标，开始测试连接的实验。  \n# Web应用如何连接到Oracle数据库\n为了构造Oracle数据库的一个Internet连接，客户机上的Web浏览器要与Web服务器通信并使用HTTP进行连接请求。Web服务器将此请求传递给一个应用，该应用处理收到的请求并用Oracle Net（配置在数据库服务器和客户机上）与Oracle数据库服务器通信  \n\t下面介绍Oracle网络中几个关键的术语  \n## 数据库实例名\n正如所知，Oracle实例由SGA和一组Oracle进程组成。数据库实例名在初始化文件（init.ora）中作为INSTANCE_NAME参数给出。在谈到Oracle SID（System identifier，系统标识符）时，指的是Oracle实例。  \n\t通常，每个数据库只有一个与其关联的实例。但在Oracle RAC配置中，单个数据库可关联到多个实例。  \n## 全局数据库名\n全局数据库名唯一地标识一个Oracle数据库，其格式为database_name.database_domain，如sales.us.acme.com。在这个全局数据库中，sales为数据库名，us.acme.com为数据库域。因为相同的域中两个数据库不会有相同的数据库名，所以每个全局数据库名都是唯一的。  \n## 数据库服务名\n对于客户机，数据库在逻辑上简单地表现为一个服务。在服务和数据库之间存在一个多对多的关系，因为一个数据库可被一个或多个服务所代表，每个服务都专用于一组不同的客户机，而一个服务可覆盖不止一个数据库实例。我们在自己的系统中用每个数据库的服务名来标识它，用初始化参数SERVICE_NAMES来指定数据库的服务名。服务名参数值默认为全局数据库名。  \n请注意，一个数据库可由多个服务名来访问。如果希望不同的客户机组访问适合于它们的特定需求的不同数据库，应该这样做。例如，可对相同数据库创建如下两个服务名：  \nSales.us.acme.com  \nFinance.us.acme.com  \n销售人员使用sales.us.acme.com服务名，而财务人员则使用finance.us.acme.com服务名。  \n## 连接描述符\n为了将电脑连接到世界上的任何数据库服务，需要提供两个信息：  \n\t数据库服务名；  \n\t地址。   \nOracle使用术语连接描述符(connect descriptor)来表示数据库连接的两个必需的部分：数据库服务名和地址。连接描述符的地址部分包含三个部分，分别是：连接使用的通信协议、主机名和端口号。  \n了解通信协议有助于保证使用合适的网络协议，以便建立连接。标准的协议为TCP/IP或带SSL（Secure Sockets Layer，安全套接层）的TCP/IP。UNIX服务器上的Oracle连接的标准端口为1521或1526.Windows机器上的默认端口为1521.因为任何主机上的数据库具有唯一服务名，所以一个Oracle数据库服务名和一个主机名将唯一地标识任何数据库。下面是一个典型的连接描述符的例子：  \n(DESCRIPTION\n(ADDRESS=(PROTOCOL=tcp)(HOST=sales-server)(PORT=1521))\n(CONNECT_DATA=\n(SERVICE_NAME=sales.us.acme.com)))  \n在此连接描述符中，ADDRESS行指出网络通信将使用TCP协议。HOST指定UNIX（或Windows）服务器，服务器上的Oracle监听器正监听来自端口1521的连接请求。连接描述符的ADDRESS部分也称为协议地址(protocol address)。\n希望连接数据库的客户机首先连接到Oracle监听器进程。监听器接收到达的请求并把它们交给数据库服务器。一旦客户机和数据库服务器通过监听器的引导连接上，它们就直接通信，在此客户机连接的通信过程中不再需要监听器。  \n## 连接标识符\n连接标识符（connect identifier）与连接描述符紧密关联。可把连接描述符作为连接标识符，或者可简单地映射一个数据库服务名为一个连接描述符。例如，可以把一个服务名(如sales)映射为11.2.5节所看到的连接描述符。下面是说明映射sales连接标识符的例子。  \nSales=\n(DESCRIPTION\n(ADDRESS=(PROTOCOL=tcp)(HOST=sales-server)(PORT=1521))\n(CONNECT_DATA=\n(SERVICE_NAME=sales.us.acme.com)))  \n## 连接串 \n通过提供一个连接串(connect string)连接到数据库。连接串包含用户名/密码组合及一个连接标识符。最常见的连接标识符之一是节点服务名，它是一个数据库服务的名字。  \n下面的例子给出一个连接串，它把一个完整的连接描述符作为连接标识符  \n\tCONNECT scott/tiger@(DESCRIPTION=\n\t(ADDRESS=(PROTOCOL=tcp)\n\t(HOST=sales-server)\n\t(PORT=1521))\n\t(CONNECT_DATA=\n\t(SERVICE_NAME=sales.us.acme.com)))\n下面是一个更简单的连接到相同数据库的方法，它使用连接标识符sales：  \n\tCONNECT scott/tiger@sales  \n上面两个例子都能连接到sales数据库，但显然第二个连接串（使用sales连接标识符）简单得多。  \n \n## 使用Oracle网络服务工具\nOracle Net提供了配置客户机与数据库服务之间的连接的几个GUI和命令行工具。最常用的命令行工具是isnrctl实用程序，它帮助管理Oracle监听器服务。下面是帮助管理Oracle Net Servcies的重要GUI工具。  \nOracle NCA（Net Configuration Assistant，Oracle网络配置助手）。此工具主要用于在安装中配置网络组件，它允许在配置客户机连接的几个选项（本章稍后介绍这些选项）中进行选择。其便于使用的GUI界面使你能在所选择的任何命名方法下快速配置客户机连接。在UNIX/Linux系统上，可通过从$ORACLE_HOME/bin目录执行netca来启动NCA。在Window系统上，选择Start|Programs|Oracle-HOME_NAME|Configuration and Migration Tools|Net ConfigurationAssistant。  \nOracle网络配置管理器（Oracle Net Manager）。Oracle Net Manager可在客户机和服务器上运行，它允许配置各种命名方法和监听器。利用此工具，可在本地tnsnames.ora文件或在集中式的OID中配置连接描述符，而且可以方便地增加和修改连接方法。  \n为了从Oracle企业管理器控制台启动Oracle Net Manager，选择Tools|Service Management|Oracle Net Manager。为了在Unix上作为独立的应用启动Oracle Net Manager，在ORACLE_HOME/bin目录执行netmgr。在windown上，选择Start|Programs|Oracle-HOME_NAME|Configuration and Migration Tools|NetManager。  \nOracle企业管理器（Oracle Enterprise Manager）。Oracle Database 11g中的OEM可以完成Oracle Net Manager能完成的所有任务，但不能跨多个文件系统管理多个Oracle主目录。此外，使用OEM可导出目录命名项到tnsnames.ora文件。  \nOracle目录管理器（Oracle Directory Manager）。这个功能强大的工具允许创建使用OID必需的各种域和环境。用此工具还可以执行密码策略管理及完成许多Oracle高级安全任务。在UNIX/LINUX系统上，可从$ORACLE_HOME/bin目录执行oidadmin来启动OID。在Window系统上，选择Start|Programs|Oracle-HOME_NAME|Integrated Manager Tools|Oracle Directory Manager。  \n# 即时客户机\n之前说过Oracle客户机安装需要经历常规的Oracle数据库服务器软件安排的所有预备步骤。幸而为连接到Oracle数据库并不总是需要安装完整的Oracle客户机软件。Oracle的新Instant Client（即时客户机）软件允许执行应用程序而不必安装标准的Oracle客户机也不必具有ORACLE_HOME。不需要为访问Oracle数据库的每台机器安装Oracle客户机软件。所有现有的OCI、ODBC和JDBC应用程序都可以使用Instan Client。如果愿意，甚至可以用Instant Client使用SQL*Plus。  \n\t相对于完整的Oracle客户机，Instant Clients提供以下好处：  \nA. 它是免费的；  \nB. 战胜磁盘空间较少  \nC. 安装更快（5分钟左右）  \nD. 不需要CD  \nE. 它具有Oracle客户机的所有特性，如果有必要甚至包括使用SQL*Plus。  \n\n# 安装Instant Client\n以下是安装新Instant Client软件并快速连接到Oracle数据库的步骤。  \n(1) 从OTN Web站点下载Instant Client软件。你必须安装基本的客户机程序包，还可以包括其他高级可选的程序包。此程序包含以下内容：    \n\ta) Basic：运行OCI、OCCI和JDBC-OCI应用程序所需的文件。  \n\tb) SQL*Plus：为用Instant Client运行SQL*Plus需要的库和可执行文件。  \n\tc) JDBC Supplement：另外支持XA、国际化及JDBC下的RowSet操作。  \n\td) ODBC Supplement：启用带Instant Client的ODBC应用的另外的库（仅对Windows）。  \n\te) SDK：用于Instant Client开发Oracle应用程序所需的其他文件。  \n(2) 将选择的程序包解压到某个目录，将些目录命名为instantclient或其它类似的名称。  \n(3) 在UNIX和Linux系统中，将环境变量LD_LIBRARY_PATH设置为instantclient（从而保证此参数的设置与程序包所有所在的目录名匹配）。在Winddows系统上，将环境变量PATH设置为instantclient。  \n(4) 测试对Oracle服务器的连接。  \n# 监听器和连接\nOracle监听器是一个只运行在服务器上并监听连接请求的服务。Oracle提供一个名为lsnrctl的实用程序来管理监听器进程。以下是监听器如何配合Oracle网络的概述。  \na. 数据库用监听器记录关于服务、实例及服务处理器的信息  \nb. 客户机与监听器进行初步连接  \nc. 监听器接收和验证客户机连接请求并把此请求交给数据库服务的服务处理器。一旦交付了客户机请求，监听器在该连接中不再起作用。  \nListener.ora文件默认位置在UNIX系统上为$ORACLE_HOME/network/admin目录，在Windows系统上为$ORACLE_HOME\\network\\admin目录，它包含监听器的配置信息。因为监听器服务只运行在服务器上，因此在客户机上没有listener.ora文件。代码清单11-1给出了一个典型的listener.ora文件。  \nListener中的所有配置参数都具有默认值，不需要手动配置监听器服务。在服务器上创建了第一个数据库后，监听器服务自动启动，并且将监听器配置文件listener.ora放于默认目录中。新数据库创建后，数据库的网络和服务信息自动添加到监听器的配置文件中。实例启动后，数据库自动向监听器注册，并且监听器开始监听对此数据库的连接请求。  \n代码清单11-1 典型的监听器配置文件  \n代码清单11-1 典型的监听器配置文件   \n\t#LISTENRE.ORA Network Configuration file \n\t/u01/app/oracle/product/11.1.0.6.0/db_1/network/admin/listener.ora\n\tSID_LIST_LISTENER = \n\t(DESCRIPTION_LIST =\n\t\t(DESCRIPTION = \n\t\t\t  (ADDRESS_LIST = \n\t\t\t\t(ADDRESS = (PROTOCOL = IPC)(KEY = EXTPROC4))\n\t\t\t)\n\t\t\t(ADDRESS_LIST = \n\t\t\t\t(ADDRESS = (PROTOCOL = TCP)(HOST = NTL-ALAPATISAM)(PORT = 1521))\n\t\t\t)\n\t\t)\n\t)\n\tSID_LIST_LISTENER = \n\t\t(SID_LIST = \n\t\t(SID_DESC = \n\t\t\t(SID_NAME = PLSExtProc)\n\t\t\t(ORACLE_HOME = /u01/app/oracle/product/11.1.0/db_1)\n\t\t\t(PROGRAM = extproc)\n\t\t)\n\t\t(SID_DESC = \n\t\t\t(GLOBAL_DBNAME = remorse.world)\n\t\t\t(ORACLE_HOME = /u01/app/oracle/product/11.1.0/db_1)\n\t\t\t(SID_NAME = remorse)\n\t\t)\n\t\t(SID_DESC = \n\t\t\t(GLOBAL_DBNAME = finance.world)\n\t\t\t(ORACLE_HOME = /u01/app/oracle/product/11.1.0/db_1)\n\t\t\t(SID_NAME = finance)\n\t\t)\n\t)\n\t \n## 自动服务注册\nOracle PMON进程负责向监听器动态服务注册新Oracle数据库服务名，也就是说，在创建新Oracle数据库时，它们将自动向监听器服务注册。PMON进程将在每个新数据库在服务器上创建之后更新listener.ora文件。\n为自动服务注册，ini.ora文件或SPFILE应该包含如下参数：  \na. SERVICE_NAMES(如sales.us.oracle.com)  \nb. INSTANCE_NAME(如sales)  \n如果不指定SERVICE_NAMES参数的值，它默认为全局数据库名，全局数据库名是DB_NAME和DB_DOMAIN参数的组合。INSTANCE_NAME参数的默认值为Oracle安装或数据库创建时输入的SID。  \n可使用lsnrctl实用程序查看服务器上监听器的状态，如代码清单11-2所示。相应的输出说明监听器启动了多长时间，监听器服务的配置文件位于何处。它还给出监听器为连接请求而监听的数据库的名称。  \n代码清单11-2 使用lsnrctl实用程序查看监听器的状态  \n$ lsnrctl status  \n\t \n在代码清单11-2的Services Summary部分，相应的状态可具有如下的某个值。  \na. READY：此实例可接受连接  \nb. BLOCKED：此实例不能接受连接  \nc. UNKNOWN：此实例在listener.ora文件中注册而不是通过动态服务注册，因而不知道其状态  \n## 监听器命令\n在调用lsnrctl实用程序后，除了status命令外还可以执行其他一些重要的命令。例如，service命令允许查看监听器正为连接请求而监控的是什么服务。  \n注解：还可以从Oracle企业管理器的Net Services Administration页面查看监听器服务的状态。  \n代码清单11-2 使用lsnrctl help列出lsnrctl命令  \n$lsnrctl help  \n可以调用lsnrctl实用程序后，使用start命令启动监听器，使用stop命令停业监听器。如果希望从操作系统命令行发布这些命令，可使用lsnrctl start和lsnrctl stop命令执行这两个任务。  \n如果对listener.ora文件做了更改，为使更改起作用的一种方法是重启监听器。另一种安全的方法是重新装载监听信息，包括对监听器配置文件所做的最新更改。Lsnrctl reload命令允许在运行中重新装载监听器，而不用重新启动它。在监听器重装载（甚至是重启）的过程中，当前连接的客户机将继续保持连接，因为监听器已经将连接“交付”给数据库，在客户和数据库服务之间不起作用。  \n注意：我的忠告是，如非绝对有必要，不要修改listener.ora文件，而且对于动态自动服务注册，几乎没有必要修改此文件。不过，有时可能需要修改监听器文件的某些部分，此文件由监听器监控连接请求的所有服务的网络配置信息组成。  \n## 命名和连接\n在前面连接描述符和连接标识符的例子中，使用sales连接标识符来连接sales服务。连接标识符可以是连接描述符本身，也可以是一个能解析为连接描述符的简单名字(如sales)。一般使用的简单连接标识符称为net service name(网络服务名)。因此前面例子中的sales连接标识符就是一个net service name。  \n因为每次进行连接时都需要提供一下完整的连接描述符非常令人厌烦，使用网络服务名是明智的。但这需要维护网络服务名和连接描述信息之间所有映射的一个中心信息库(central repository)，以便Oracle验证这些网络服务名。因此，在一个用户使用网络服务名sales启动连接进程时，Oracle将搜索中心信息库查找sales的连接描述符。找到连接描述符后，Oracle Net会为指定服务器上的数据库初始化一个连接。  \nOracle允许几种类型的命名信息库，可用下列4种命名方法访问存储在这些位置中的映射信息。  \na. 本地命令(local naming )：使用存储在每个客户机上的名为tnsnames.ora的文件连接到数据库服务器。  \nb. 简易连接命名(easy connect naming)：允许连接而无需任何服务名配置。  \nc. 外部命名(external naming)：使用第三方命名服务来解析服务名。  \nd. 目录命令(directory naming)：使用一个集中式的符合LDAP的目录服务器来解析服务名。  \n\t不管使用何种命名方法，名字解析过程都是相同的。每种命名法都遵循以下步骤将连接描述符解析为网络服务名：\ni. 选择命令方法—本地、简易连接、外部命名或目录服务命名  \nii. 映射连接描述符到服务名；  \niii. 配置客户机以使用步骤1中选择的命名方法  \n## 本地命名方法\n本地命令是建立Oracle连接最简单、最容易的方法。使用这种方法，在名为tnsnames.ora的本地化配置文件中存储服务名及其连接描述符。此文件默认存储在$ORACLE_HOME/network/admin目录中。  \n","source":"_posts/oracle/Oracle网络和数据库连接.md","raw":"---\ntitle: Oracle网络和数据库连接\ndate: 2017-04-01 15:30:00\ntags: [oracle,数据库]\ncategories: [数据库,oracle]\n---\n内容来自《Oracle Database 11g 数据库管理艺术》\n# 网络概念：Oracle网络如何工作\n在希望从客户机（不管是传统客户机还是基于浏览器的客户机）打开数据库会话时，需要通过网络连接到数据库。假如要将台式电脑通过现有网络连接到UNIX服务器上的一个Oracle数据库，则需要在电脑和Oracle数据库（它使用专门的软件）之间构造一个连接方法。需要某种界面来处理会话（在此例子中为SQL*Plus），并且需要某种与业内标准的网络协议（如TCP/IP）通信的方法。  \n为方便配置和管理网络连接，Oracle提供了Oracle Net Services，它是一套在分布式异构计算环境中提供连接方案的组件。Oracle Net Services由Oracle Net、Oracle Net Listener、Oracle Connection Manager、Oracle Net Configuration Assistant和Oracle Net Manager组成。Oracle Net Services软件是在Oracle Database Server或Oracle Client软件安装的过程中自动安装的。  \nOracle Net是一个初始化、建立及维护客户机和服务器之间的连接的组件。这就是为什么必须在客户机和服务器上都安装Oracle Net的原因。Oracle Net主要由两个组件构成。  \nOracle Network Foundation Layer：负责建立和维护客户机应用程序与服务器之间的连接，以及它们之间的交换信息。  \nOracle Protocol Support： 负责映射Transparent Net Substrate（TNS）功能到连接使用的业内标准协议。\n驻留Oracle数据库的所有服务器还运行一个名为Oracle Net Listener（通常也称为监听器）的服务，其主要功能是监听来自客户机服务登录Oracle数据库的请求。监听器在保证客户机服务具有与数据库匹配的信息（协议、端口和实例名）后，将客户机请求传递到数据库。假如用户名和密码通过认证，则数据库将允许客户机登录。一旦监听器把用户请求交付给数据库，客户机和数据库将直接连接，不再需要监听器的帮助。\nOracle提供了基于GUI的大量的实用程序，以帮助配置数据库的网络连接。这些实用程序包括Oracle Connection Manager、Oracle Net Manager和Oracle Net Configuragion Assistant等。这些工具帮助处理所有网络需求。在结束本章学习后，可单击这些程序的图标，开始测试连接的实验。  \n# Web应用如何连接到Oracle数据库\n为了构造Oracle数据库的一个Internet连接，客户机上的Web浏览器要与Web服务器通信并使用HTTP进行连接请求。Web服务器将此请求传递给一个应用，该应用处理收到的请求并用Oracle Net（配置在数据库服务器和客户机上）与Oracle数据库服务器通信  \n\t下面介绍Oracle网络中几个关键的术语  \n## 数据库实例名\n正如所知，Oracle实例由SGA和一组Oracle进程组成。数据库实例名在初始化文件（init.ora）中作为INSTANCE_NAME参数给出。在谈到Oracle SID（System identifier，系统标识符）时，指的是Oracle实例。  \n\t通常，每个数据库只有一个与其关联的实例。但在Oracle RAC配置中，单个数据库可关联到多个实例。  \n## 全局数据库名\n全局数据库名唯一地标识一个Oracle数据库，其格式为database_name.database_domain，如sales.us.acme.com。在这个全局数据库中，sales为数据库名，us.acme.com为数据库域。因为相同的域中两个数据库不会有相同的数据库名，所以每个全局数据库名都是唯一的。  \n## 数据库服务名\n对于客户机，数据库在逻辑上简单地表现为一个服务。在服务和数据库之间存在一个多对多的关系，因为一个数据库可被一个或多个服务所代表，每个服务都专用于一组不同的客户机，而一个服务可覆盖不止一个数据库实例。我们在自己的系统中用每个数据库的服务名来标识它，用初始化参数SERVICE_NAMES来指定数据库的服务名。服务名参数值默认为全局数据库名。  \n请注意，一个数据库可由多个服务名来访问。如果希望不同的客户机组访问适合于它们的特定需求的不同数据库，应该这样做。例如，可对相同数据库创建如下两个服务名：  \nSales.us.acme.com  \nFinance.us.acme.com  \n销售人员使用sales.us.acme.com服务名，而财务人员则使用finance.us.acme.com服务名。  \n## 连接描述符\n为了将电脑连接到世界上的任何数据库服务，需要提供两个信息：  \n\t数据库服务名；  \n\t地址。   \nOracle使用术语连接描述符(connect descriptor)来表示数据库连接的两个必需的部分：数据库服务名和地址。连接描述符的地址部分包含三个部分，分别是：连接使用的通信协议、主机名和端口号。  \n了解通信协议有助于保证使用合适的网络协议，以便建立连接。标准的协议为TCP/IP或带SSL（Secure Sockets Layer，安全套接层）的TCP/IP。UNIX服务器上的Oracle连接的标准端口为1521或1526.Windows机器上的默认端口为1521.因为任何主机上的数据库具有唯一服务名，所以一个Oracle数据库服务名和一个主机名将唯一地标识任何数据库。下面是一个典型的连接描述符的例子：  \n(DESCRIPTION\n(ADDRESS=(PROTOCOL=tcp)(HOST=sales-server)(PORT=1521))\n(CONNECT_DATA=\n(SERVICE_NAME=sales.us.acme.com)))  \n在此连接描述符中，ADDRESS行指出网络通信将使用TCP协议。HOST指定UNIX（或Windows）服务器，服务器上的Oracle监听器正监听来自端口1521的连接请求。连接描述符的ADDRESS部分也称为协议地址(protocol address)。\n希望连接数据库的客户机首先连接到Oracle监听器进程。监听器接收到达的请求并把它们交给数据库服务器。一旦客户机和数据库服务器通过监听器的引导连接上，它们就直接通信，在此客户机连接的通信过程中不再需要监听器。  \n## 连接标识符\n连接标识符（connect identifier）与连接描述符紧密关联。可把连接描述符作为连接标识符，或者可简单地映射一个数据库服务名为一个连接描述符。例如，可以把一个服务名(如sales)映射为11.2.5节所看到的连接描述符。下面是说明映射sales连接标识符的例子。  \nSales=\n(DESCRIPTION\n(ADDRESS=(PROTOCOL=tcp)(HOST=sales-server)(PORT=1521))\n(CONNECT_DATA=\n(SERVICE_NAME=sales.us.acme.com)))  \n## 连接串 \n通过提供一个连接串(connect string)连接到数据库。连接串包含用户名/密码组合及一个连接标识符。最常见的连接标识符之一是节点服务名，它是一个数据库服务的名字。  \n下面的例子给出一个连接串，它把一个完整的连接描述符作为连接标识符  \n\tCONNECT scott/tiger@(DESCRIPTION=\n\t(ADDRESS=(PROTOCOL=tcp)\n\t(HOST=sales-server)\n\t(PORT=1521))\n\t(CONNECT_DATA=\n\t(SERVICE_NAME=sales.us.acme.com)))\n下面是一个更简单的连接到相同数据库的方法，它使用连接标识符sales：  \n\tCONNECT scott/tiger@sales  \n上面两个例子都能连接到sales数据库，但显然第二个连接串（使用sales连接标识符）简单得多。  \n \n## 使用Oracle网络服务工具\nOracle Net提供了配置客户机与数据库服务之间的连接的几个GUI和命令行工具。最常用的命令行工具是isnrctl实用程序，它帮助管理Oracle监听器服务。下面是帮助管理Oracle Net Servcies的重要GUI工具。  \nOracle NCA（Net Configuration Assistant，Oracle网络配置助手）。此工具主要用于在安装中配置网络组件，它允许在配置客户机连接的几个选项（本章稍后介绍这些选项）中进行选择。其便于使用的GUI界面使你能在所选择的任何命名方法下快速配置客户机连接。在UNIX/Linux系统上，可通过从$ORACLE_HOME/bin目录执行netca来启动NCA。在Window系统上，选择Start|Programs|Oracle-HOME_NAME|Configuration and Migration Tools|Net ConfigurationAssistant。  \nOracle网络配置管理器（Oracle Net Manager）。Oracle Net Manager可在客户机和服务器上运行，它允许配置各种命名方法和监听器。利用此工具，可在本地tnsnames.ora文件或在集中式的OID中配置连接描述符，而且可以方便地增加和修改连接方法。  \n为了从Oracle企业管理器控制台启动Oracle Net Manager，选择Tools|Service Management|Oracle Net Manager。为了在Unix上作为独立的应用启动Oracle Net Manager，在ORACLE_HOME/bin目录执行netmgr。在windown上，选择Start|Programs|Oracle-HOME_NAME|Configuration and Migration Tools|NetManager。  \nOracle企业管理器（Oracle Enterprise Manager）。Oracle Database 11g中的OEM可以完成Oracle Net Manager能完成的所有任务，但不能跨多个文件系统管理多个Oracle主目录。此外，使用OEM可导出目录命名项到tnsnames.ora文件。  \nOracle目录管理器（Oracle Directory Manager）。这个功能强大的工具允许创建使用OID必需的各种域和环境。用此工具还可以执行密码策略管理及完成许多Oracle高级安全任务。在UNIX/LINUX系统上，可从$ORACLE_HOME/bin目录执行oidadmin来启动OID。在Window系统上，选择Start|Programs|Oracle-HOME_NAME|Integrated Manager Tools|Oracle Directory Manager。  \n# 即时客户机\n之前说过Oracle客户机安装需要经历常规的Oracle数据库服务器软件安排的所有预备步骤。幸而为连接到Oracle数据库并不总是需要安装完整的Oracle客户机软件。Oracle的新Instant Client（即时客户机）软件允许执行应用程序而不必安装标准的Oracle客户机也不必具有ORACLE_HOME。不需要为访问Oracle数据库的每台机器安装Oracle客户机软件。所有现有的OCI、ODBC和JDBC应用程序都可以使用Instan Client。如果愿意，甚至可以用Instant Client使用SQL*Plus。  \n\t相对于完整的Oracle客户机，Instant Clients提供以下好处：  \nA. 它是免费的；  \nB. 战胜磁盘空间较少  \nC. 安装更快（5分钟左右）  \nD. 不需要CD  \nE. 它具有Oracle客户机的所有特性，如果有必要甚至包括使用SQL*Plus。  \n\n# 安装Instant Client\n以下是安装新Instant Client软件并快速连接到Oracle数据库的步骤。  \n(1) 从OTN Web站点下载Instant Client软件。你必须安装基本的客户机程序包，还可以包括其他高级可选的程序包。此程序包含以下内容：    \n\ta) Basic：运行OCI、OCCI和JDBC-OCI应用程序所需的文件。  \n\tb) SQL*Plus：为用Instant Client运行SQL*Plus需要的库和可执行文件。  \n\tc) JDBC Supplement：另外支持XA、国际化及JDBC下的RowSet操作。  \n\td) ODBC Supplement：启用带Instant Client的ODBC应用的另外的库（仅对Windows）。  \n\te) SDK：用于Instant Client开发Oracle应用程序所需的其他文件。  \n(2) 将选择的程序包解压到某个目录，将些目录命名为instantclient或其它类似的名称。  \n(3) 在UNIX和Linux系统中，将环境变量LD_LIBRARY_PATH设置为instantclient（从而保证此参数的设置与程序包所有所在的目录名匹配）。在Winddows系统上，将环境变量PATH设置为instantclient。  \n(4) 测试对Oracle服务器的连接。  \n# 监听器和连接\nOracle监听器是一个只运行在服务器上并监听连接请求的服务。Oracle提供一个名为lsnrctl的实用程序来管理监听器进程。以下是监听器如何配合Oracle网络的概述。  \na. 数据库用监听器记录关于服务、实例及服务处理器的信息  \nb. 客户机与监听器进行初步连接  \nc. 监听器接收和验证客户机连接请求并把此请求交给数据库服务的服务处理器。一旦交付了客户机请求，监听器在该连接中不再起作用。  \nListener.ora文件默认位置在UNIX系统上为$ORACLE_HOME/network/admin目录，在Windows系统上为$ORACLE_HOME\\network\\admin目录，它包含监听器的配置信息。因为监听器服务只运行在服务器上，因此在客户机上没有listener.ora文件。代码清单11-1给出了一个典型的listener.ora文件。  \nListener中的所有配置参数都具有默认值，不需要手动配置监听器服务。在服务器上创建了第一个数据库后，监听器服务自动启动，并且将监听器配置文件listener.ora放于默认目录中。新数据库创建后，数据库的网络和服务信息自动添加到监听器的配置文件中。实例启动后，数据库自动向监听器注册，并且监听器开始监听对此数据库的连接请求。  \n代码清单11-1 典型的监听器配置文件  \n代码清单11-1 典型的监听器配置文件   \n\t#LISTENRE.ORA Network Configuration file \n\t/u01/app/oracle/product/11.1.0.6.0/db_1/network/admin/listener.ora\n\tSID_LIST_LISTENER = \n\t(DESCRIPTION_LIST =\n\t\t(DESCRIPTION = \n\t\t\t  (ADDRESS_LIST = \n\t\t\t\t(ADDRESS = (PROTOCOL = IPC)(KEY = EXTPROC4))\n\t\t\t)\n\t\t\t(ADDRESS_LIST = \n\t\t\t\t(ADDRESS = (PROTOCOL = TCP)(HOST = NTL-ALAPATISAM)(PORT = 1521))\n\t\t\t)\n\t\t)\n\t)\n\tSID_LIST_LISTENER = \n\t\t(SID_LIST = \n\t\t(SID_DESC = \n\t\t\t(SID_NAME = PLSExtProc)\n\t\t\t(ORACLE_HOME = /u01/app/oracle/product/11.1.0/db_1)\n\t\t\t(PROGRAM = extproc)\n\t\t)\n\t\t(SID_DESC = \n\t\t\t(GLOBAL_DBNAME = remorse.world)\n\t\t\t(ORACLE_HOME = /u01/app/oracle/product/11.1.0/db_1)\n\t\t\t(SID_NAME = remorse)\n\t\t)\n\t\t(SID_DESC = \n\t\t\t(GLOBAL_DBNAME = finance.world)\n\t\t\t(ORACLE_HOME = /u01/app/oracle/product/11.1.0/db_1)\n\t\t\t(SID_NAME = finance)\n\t\t)\n\t)\n\t \n## 自动服务注册\nOracle PMON进程负责向监听器动态服务注册新Oracle数据库服务名，也就是说，在创建新Oracle数据库时，它们将自动向监听器服务注册。PMON进程将在每个新数据库在服务器上创建之后更新listener.ora文件。\n为自动服务注册，ini.ora文件或SPFILE应该包含如下参数：  \na. SERVICE_NAMES(如sales.us.oracle.com)  \nb. INSTANCE_NAME(如sales)  \n如果不指定SERVICE_NAMES参数的值，它默认为全局数据库名，全局数据库名是DB_NAME和DB_DOMAIN参数的组合。INSTANCE_NAME参数的默认值为Oracle安装或数据库创建时输入的SID。  \n可使用lsnrctl实用程序查看服务器上监听器的状态，如代码清单11-2所示。相应的输出说明监听器启动了多长时间，监听器服务的配置文件位于何处。它还给出监听器为连接请求而监听的数据库的名称。  \n代码清单11-2 使用lsnrctl实用程序查看监听器的状态  \n$ lsnrctl status  \n\t \n在代码清单11-2的Services Summary部分，相应的状态可具有如下的某个值。  \na. READY：此实例可接受连接  \nb. BLOCKED：此实例不能接受连接  \nc. UNKNOWN：此实例在listener.ora文件中注册而不是通过动态服务注册，因而不知道其状态  \n## 监听器命令\n在调用lsnrctl实用程序后，除了status命令外还可以执行其他一些重要的命令。例如，service命令允许查看监听器正为连接请求而监控的是什么服务。  \n注解：还可以从Oracle企业管理器的Net Services Administration页面查看监听器服务的状态。  \n代码清单11-2 使用lsnrctl help列出lsnrctl命令  \n$lsnrctl help  \n可以调用lsnrctl实用程序后，使用start命令启动监听器，使用stop命令停业监听器。如果希望从操作系统命令行发布这些命令，可使用lsnrctl start和lsnrctl stop命令执行这两个任务。  \n如果对listener.ora文件做了更改，为使更改起作用的一种方法是重启监听器。另一种安全的方法是重新装载监听信息，包括对监听器配置文件所做的最新更改。Lsnrctl reload命令允许在运行中重新装载监听器，而不用重新启动它。在监听器重装载（甚至是重启）的过程中，当前连接的客户机将继续保持连接，因为监听器已经将连接“交付”给数据库，在客户和数据库服务之间不起作用。  \n注意：我的忠告是，如非绝对有必要，不要修改listener.ora文件，而且对于动态自动服务注册，几乎没有必要修改此文件。不过，有时可能需要修改监听器文件的某些部分，此文件由监听器监控连接请求的所有服务的网络配置信息组成。  \n## 命名和连接\n在前面连接描述符和连接标识符的例子中，使用sales连接标识符来连接sales服务。连接标识符可以是连接描述符本身，也可以是一个能解析为连接描述符的简单名字(如sales)。一般使用的简单连接标识符称为net service name(网络服务名)。因此前面例子中的sales连接标识符就是一个net service name。  \n因为每次进行连接时都需要提供一下完整的连接描述符非常令人厌烦，使用网络服务名是明智的。但这需要维护网络服务名和连接描述信息之间所有映射的一个中心信息库(central repository)，以便Oracle验证这些网络服务名。因此，在一个用户使用网络服务名sales启动连接进程时，Oracle将搜索中心信息库查找sales的连接描述符。找到连接描述符后，Oracle Net会为指定服务器上的数据库初始化一个连接。  \nOracle允许几种类型的命名信息库，可用下列4种命名方法访问存储在这些位置中的映射信息。  \na. 本地命令(local naming )：使用存储在每个客户机上的名为tnsnames.ora的文件连接到数据库服务器。  \nb. 简易连接命名(easy connect naming)：允许连接而无需任何服务名配置。  \nc. 外部命名(external naming)：使用第三方命名服务来解析服务名。  \nd. 目录命令(directory naming)：使用一个集中式的符合LDAP的目录服务器来解析服务名。  \n\t不管使用何种命名方法，名字解析过程都是相同的。每种命名法都遵循以下步骤将连接描述符解析为网络服务名：\ni. 选择命令方法—本地、简易连接、外部命名或目录服务命名  \nii. 映射连接描述符到服务名；  \niii. 配置客户机以使用步骤1中选择的命名方法  \n## 本地命名方法\n本地命令是建立Oracle连接最简单、最容易的方法。使用这种方法，在名为tnsnames.ora的本地化配置文件中存储服务名及其连接描述符。此文件默认存储在$ORACLE_HOME/network/admin目录中。  \n","slug":"oracle/Oracle网络和数据库连接","published":1,"updated":"2017-08-18T01:44:35.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjh7poltf0028bp7rdbmfbb06","content":"<p>内容来自《Oracle Database 11g 数据库管理艺术》</p>\n<h1 id=\"网络概念：Oracle网络如何工作\"><a href=\"#网络概念：Oracle网络如何工作\" class=\"headerlink\" title=\"网络概念：Oracle网络如何工作\"></a>网络概念：Oracle网络如何工作</h1><p>在希望从客户机（不管是传统客户机还是基于浏览器的客户机）打开数据库会话时，需要通过网络连接到数据库。假如要将台式电脑通过现有网络连接到UNIX服务器上的一个Oracle数据库，则需要在电脑和Oracle数据库（它使用专门的软件）之间构造一个连接方法。需要某种界面来处理会话（在此例子中为SQL*Plus），并且需要某种与业内标准的网络协议（如TCP/IP）通信的方法。<br>为方便配置和管理网络连接，Oracle提供了Oracle Net Services，它是一套在分布式异构计算环境中提供连接方案的组件。Oracle Net Services由Oracle Net、Oracle Net Listener、Oracle Connection Manager、Oracle Net Configuration Assistant和Oracle Net Manager组成。Oracle Net Services软件是在Oracle Database Server或Oracle Client软件安装的过程中自动安装的。<br>Oracle Net是一个初始化、建立及维护客户机和服务器之间的连接的组件。这就是为什么必须在客户机和服务器上都安装Oracle Net的原因。Oracle Net主要由两个组件构成。<br>Oracle Network Foundation Layer：负责建立和维护客户机应用程序与服务器之间的连接，以及它们之间的交换信息。<br>Oracle Protocol Support： 负责映射Transparent Net Substrate（TNS）功能到连接使用的业内标准协议。<br>驻留Oracle数据库的所有服务器还运行一个名为Oracle Net Listener（通常也称为监听器）的服务，其主要功能是监听来自客户机服务登录Oracle数据库的请求。监听器在保证客户机服务具有与数据库匹配的信息（协议、端口和实例名）后，将客户机请求传递到数据库。假如用户名和密码通过认证，则数据库将允许客户机登录。一旦监听器把用户请求交付给数据库，客户机和数据库将直接连接，不再需要监听器的帮助。<br>Oracle提供了基于GUI的大量的实用程序，以帮助配置数据库的网络连接。这些实用程序包括Oracle Connection Manager、Oracle Net Manager和Oracle Net Configuragion Assistant等。这些工具帮助处理所有网络需求。在结束本章学习后，可单击这些程序的图标，开始测试连接的实验。  </p>\n<h1 id=\"Web应用如何连接到Oracle数据库\"><a href=\"#Web应用如何连接到Oracle数据库\" class=\"headerlink\" title=\"Web应用如何连接到Oracle数据库\"></a>Web应用如何连接到Oracle数据库</h1><p>为了构造Oracle数据库的一个Internet连接，客户机上的Web浏览器要与Web服务器通信并使用HTTP进行连接请求。Web服务器将此请求传递给一个应用，该应用处理收到的请求并用Oracle Net（配置在数据库服务器和客户机上）与Oracle数据库服务器通信<br>    下面介绍Oracle网络中几个关键的术语  </p>\n<h2 id=\"数据库实例名\"><a href=\"#数据库实例名\" class=\"headerlink\" title=\"数据库实例名\"></a>数据库实例名</h2><p>正如所知，Oracle实例由SGA和一组Oracle进程组成。数据库实例名在初始化文件（init.ora）中作为INSTANCE_NAME参数给出。在谈到Oracle SID（System identifier，系统标识符）时，指的是Oracle实例。<br>    通常，每个数据库只有一个与其关联的实例。但在Oracle RAC配置中，单个数据库可关联到多个实例。  </p>\n<h2 id=\"全局数据库名\"><a href=\"#全局数据库名\" class=\"headerlink\" title=\"全局数据库名\"></a>全局数据库名</h2><p>全局数据库名唯一地标识一个Oracle数据库，其格式为database_name.database_domain，如sales.us.acme.com。在这个全局数据库中，sales为数据库名，us.acme.com为数据库域。因为相同的域中两个数据库不会有相同的数据库名，所以每个全局数据库名都是唯一的。  </p>\n<h2 id=\"数据库服务名\"><a href=\"#数据库服务名\" class=\"headerlink\" title=\"数据库服务名\"></a>数据库服务名</h2><p>对于客户机，数据库在逻辑上简单地表现为一个服务。在服务和数据库之间存在一个多对多的关系，因为一个数据库可被一个或多个服务所代表，每个服务都专用于一组不同的客户机，而一个服务可覆盖不止一个数据库实例。我们在自己的系统中用每个数据库的服务名来标识它，用初始化参数SERVICE_NAMES来指定数据库的服务名。服务名参数值默认为全局数据库名。<br>请注意，一个数据库可由多个服务名来访问。如果希望不同的客户机组访问适合于它们的特定需求的不同数据库，应该这样做。例如，可对相同数据库创建如下两个服务名：<br>Sales.us.acme.com<br>Finance.us.acme.com<br>销售人员使用sales.us.acme.com服务名，而财务人员则使用finance.us.acme.com服务名。  </p>\n<h2 id=\"连接描述符\"><a href=\"#连接描述符\" class=\"headerlink\" title=\"连接描述符\"></a>连接描述符</h2><p>为了将电脑连接到世界上的任何数据库服务，需要提供两个信息：<br>    数据库服务名；<br>    地址。<br>Oracle使用术语连接描述符(connect descriptor)来表示数据库连接的两个必需的部分：数据库服务名和地址。连接描述符的地址部分包含三个部分，分别是：连接使用的通信协议、主机名和端口号。<br>了解通信协议有助于保证使用合适的网络协议，以便建立连接。标准的协议为TCP/IP或带SSL（Secure Sockets Layer，安全套接层）的TCP/IP。UNIX服务器上的Oracle连接的标准端口为1521或1526.Windows机器上的默认端口为1521.因为任何主机上的数据库具有唯一服务名，所以一个Oracle数据库服务名和一个主机名将唯一地标识任何数据库。下面是一个典型的连接描述符的例子：<br>(DESCRIPTION<br>(ADDRESS=(PROTOCOL=tcp)(HOST=sales-server)(PORT=1521))<br>(CONNECT_DATA=<br>(SERVICE_NAME=sales.us.acme.com)))<br>在此连接描述符中，ADDRESS行指出网络通信将使用TCP协议。HOST指定UNIX（或Windows）服务器，服务器上的Oracle监听器正监听来自端口1521的连接请求。连接描述符的ADDRESS部分也称为协议地址(protocol address)。<br>希望连接数据库的客户机首先连接到Oracle监听器进程。监听器接收到达的请求并把它们交给数据库服务器。一旦客户机和数据库服务器通过监听器的引导连接上，它们就直接通信，在此客户机连接的通信过程中不再需要监听器。  </p>\n<h2 id=\"连接标识符\"><a href=\"#连接标识符\" class=\"headerlink\" title=\"连接标识符\"></a>连接标识符</h2><p>连接标识符（connect identifier）与连接描述符紧密关联。可把连接描述符作为连接标识符，或者可简单地映射一个数据库服务名为一个连接描述符。例如，可以把一个服务名(如sales)映射为11.2.5节所看到的连接描述符。下面是说明映射sales连接标识符的例子。<br>Sales=<br>(DESCRIPTION<br>(ADDRESS=(PROTOCOL=tcp)(HOST=sales-server)(PORT=1521))<br>(CONNECT_DATA=<br>(SERVICE_NAME=sales.us.acme.com)))  </p>\n<h2 id=\"连接串\"><a href=\"#连接串\" class=\"headerlink\" title=\"连接串\"></a>连接串</h2><p>通过提供一个连接串(connect string)连接到数据库。连接串包含用户名/密码组合及一个连接标识符。最常见的连接标识符之一是节点服务名，它是一个数据库服务的名字。<br>下面的例子给出一个连接串，它把一个完整的连接描述符作为连接标识符<br>    CONNECT scott/tiger@(DESCRIPTION=<br>    (ADDRESS=(PROTOCOL=tcp)<br>    (HOST=sales-server)<br>    (PORT=1521))<br>    (CONNECT_DATA=<br>    (SERVICE_NAME=sales.us.acme.com)))<br>下面是一个更简单的连接到相同数据库的方法，它使用连接标识符sales：<br>    CONNECT scott/tiger@sales<br>上面两个例子都能连接到sales数据库，但显然第二个连接串（使用sales连接标识符）简单得多。  </p>\n<h2 id=\"使用Oracle网络服务工具\"><a href=\"#使用Oracle网络服务工具\" class=\"headerlink\" title=\"使用Oracle网络服务工具\"></a>使用Oracle网络服务工具</h2><p>Oracle Net提供了配置客户机与数据库服务之间的连接的几个GUI和命令行工具。最常用的命令行工具是isnrctl实用程序，它帮助管理Oracle监听器服务。下面是帮助管理Oracle Net Servcies的重要GUI工具。<br>Oracle NCA（Net Configuration Assistant，Oracle网络配置助手）。此工具主要用于在安装中配置网络组件，它允许在配置客户机连接的几个选项（本章稍后介绍这些选项）中进行选择。其便于使用的GUI界面使你能在所选择的任何命名方法下快速配置客户机连接。在UNIX/Linux系统上，可通过从$ORACLE_HOME/bin目录执行netca来启动NCA。在Window系统上，选择Start|Programs|Oracle-HOME_NAME|Configuration and Migration Tools|Net ConfigurationAssistant。<br>Oracle网络配置管理器（Oracle Net Manager）。Oracle Net Manager可在客户机和服务器上运行，它允许配置各种命名方法和监听器。利用此工具，可在本地tnsnames.ora文件或在集中式的OID中配置连接描述符，而且可以方便地增加和修改连接方法。<br>为了从Oracle企业管理器控制台启动Oracle Net Manager，选择Tools|Service Management|Oracle Net Manager。为了在Unix上作为独立的应用启动Oracle Net Manager，在ORACLE_HOME/bin目录执行netmgr。在windown上，选择Start|Programs|Oracle-HOME_NAME|Configuration and Migration Tools|NetManager。<br>Oracle企业管理器（Oracle Enterprise Manager）。Oracle Database 11g中的OEM可以完成Oracle Net Manager能完成的所有任务，但不能跨多个文件系统管理多个Oracle主目录。此外，使用OEM可导出目录命名项到tnsnames.ora文件。<br>Oracle目录管理器（Oracle Directory Manager）。这个功能强大的工具允许创建使用OID必需的各种域和环境。用此工具还可以执行密码策略管理及完成许多Oracle高级安全任务。在UNIX/LINUX系统上，可从$ORACLE_HOME/bin目录执行oidadmin来启动OID。在Window系统上，选择Start|Programs|Oracle-HOME_NAME|Integrated Manager Tools|Oracle Directory Manager。  </p>\n<h1 id=\"即时客户机\"><a href=\"#即时客户机\" class=\"headerlink\" title=\"即时客户机\"></a>即时客户机</h1><p>之前说过Oracle客户机安装需要经历常规的Oracle数据库服务器软件安排的所有预备步骤。幸而为连接到Oracle数据库并不总是需要安装完整的Oracle客户机软件。Oracle的新Instant Client（即时客户机）软件允许执行应用程序而不必安装标准的Oracle客户机也不必具有ORACLE_HOME。不需要为访问Oracle数据库的每台机器安装Oracle客户机软件。所有现有的OCI、ODBC和JDBC应用程序都可以使用Instan Client。如果愿意，甚至可以用Instant Client使用SQL<em>Plus。<br>    相对于完整的Oracle客户机，Instant Clients提供以下好处：<br>A. 它是免费的；<br>B. 战胜磁盘空间较少<br>C. 安装更快（5分钟左右）<br>D. 不需要CD<br>E. 它具有Oracle客户机的所有特性，如果有必要甚至包括使用SQL</em>Plus。  </p>\n<h1 id=\"安装Instant-Client\"><a href=\"#安装Instant-Client\" class=\"headerlink\" title=\"安装Instant Client\"></a>安装Instant Client</h1><p>以下是安装新Instant Client软件并快速连接到Oracle数据库的步骤。<br>(1) 从OTN Web站点下载Instant Client软件。你必须安装基本的客户机程序包，还可以包括其他高级可选的程序包。此程序包含以下内容：<br>    a) Basic：运行OCI、OCCI和JDBC-OCI应用程序所需的文件。<br>    b) SQL<em>Plus：为用Instant Client运行SQL</em>Plus需要的库和可执行文件。<br>    c) JDBC Supplement：另外支持XA、国际化及JDBC下的RowSet操作。<br>    d) ODBC Supplement：启用带Instant Client的ODBC应用的另外的库（仅对Windows）。<br>    e) SDK：用于Instant Client开发Oracle应用程序所需的其他文件。<br>(2) 将选择的程序包解压到某个目录，将些目录命名为instantclient或其它类似的名称。<br>(3) 在UNIX和Linux系统中，将环境变量LD_LIBRARY_PATH设置为instantclient（从而保证此参数的设置与程序包所有所在的目录名匹配）。在Winddows系统上，将环境变量PATH设置为instantclient。<br>(4) 测试对Oracle服务器的连接。  </p>\n<h1 id=\"监听器和连接\"><a href=\"#监听器和连接\" class=\"headerlink\" title=\"监听器和连接\"></a>监听器和连接</h1><p>Oracle监听器是一个只运行在服务器上并监听连接请求的服务。Oracle提供一个名为lsnrctl的实用程序来管理监听器进程。以下是监听器如何配合Oracle网络的概述。<br>a. 数据库用监听器记录关于服务、实例及服务处理器的信息<br>b. 客户机与监听器进行初步连接<br>c. 监听器接收和验证客户机连接请求并把此请求交给数据库服务的服务处理器。一旦交付了客户机请求，监听器在该连接中不再起作用。<br>Listener.ora文件默认位置在UNIX系统上为$ORACLE_HOME/network/admin目录，在Windows系统上为$ORACLE_HOME\\network\\admin目录，它包含监听器的配置信息。因为监听器服务只运行在服务器上，因此在客户机上没有listener.ora文件。代码清单11-1给出了一个典型的listener.ora文件。<br>Listener中的所有配置参数都具有默认值，不需要手动配置监听器服务。在服务器上创建了第一个数据库后，监听器服务自动启动，并且将监听器配置文件listener.ora放于默认目录中。新数据库创建后，数据库的网络和服务信息自动添加到监听器的配置文件中。实例启动后，数据库自动向监听器注册，并且监听器开始监听对此数据库的连接请求。<br>代码清单11-1 典型的监听器配置文件<br>代码清单11-1 典型的监听器配置文件   </p>\n<pre><code>#LISTENRE.ORA Network Configuration file \n/u01/app/oracle/product/11.1.0.6.0/db_1/network/admin/listener.ora\nSID_LIST_LISTENER = \n(DESCRIPTION_LIST =\n    (DESCRIPTION = \n          (ADDRESS_LIST = \n            (ADDRESS = (PROTOCOL = IPC)(KEY = EXTPROC4))\n        )\n        (ADDRESS_LIST = \n            (ADDRESS = (PROTOCOL = TCP)(HOST = NTL-ALAPATISAM)(PORT = 1521))\n        )\n    )\n)\nSID_LIST_LISTENER = \n    (SID_LIST = \n    (SID_DESC = \n        (SID_NAME = PLSExtProc)\n        (ORACLE_HOME = /u01/app/oracle/product/11.1.0/db_1)\n        (PROGRAM = extproc)\n    )\n    (SID_DESC = \n        (GLOBAL_DBNAME = remorse.world)\n        (ORACLE_HOME = /u01/app/oracle/product/11.1.0/db_1)\n        (SID_NAME = remorse)\n    )\n    (SID_DESC = \n        (GLOBAL_DBNAME = finance.world)\n        (ORACLE_HOME = /u01/app/oracle/product/11.1.0/db_1)\n        (SID_NAME = finance)\n    )\n)\n</code></pre><h2 id=\"自动服务注册\"><a href=\"#自动服务注册\" class=\"headerlink\" title=\"自动服务注册\"></a>自动服务注册</h2><p>Oracle PMON进程负责向监听器动态服务注册新Oracle数据库服务名，也就是说，在创建新Oracle数据库时，它们将自动向监听器服务注册。PMON进程将在每个新数据库在服务器上创建之后更新listener.ora文件。<br>为自动服务注册，ini.ora文件或SPFILE应该包含如下参数：<br>a. SERVICE_NAMES(如sales.us.oracle.com)<br>b. INSTANCE_NAME(如sales)<br>如果不指定SERVICE_NAMES参数的值，它默认为全局数据库名，全局数据库名是DB_NAME和DB_DOMAIN参数的组合。INSTANCE_NAME参数的默认值为Oracle安装或数据库创建时输入的SID。<br>可使用lsnrctl实用程序查看服务器上监听器的状态，如代码清单11-2所示。相应的输出说明监听器启动了多长时间，监听器服务的配置文件位于何处。它还给出监听器为连接请求而监听的数据库的名称。<br>代码清单11-2 使用lsnrctl实用程序查看监听器的状态<br>$ lsnrctl status  </p>\n<p>在代码清单11-2的Services Summary部分，相应的状态可具有如下的某个值。<br>a. READY：此实例可接受连接<br>b. BLOCKED：此实例不能接受连接<br>c. UNKNOWN：此实例在listener.ora文件中注册而不是通过动态服务注册，因而不知道其状态  </p>\n<h2 id=\"监听器命令\"><a href=\"#监听器命令\" class=\"headerlink\" title=\"监听器命令\"></a>监听器命令</h2><p>在调用lsnrctl实用程序后，除了status命令外还可以执行其他一些重要的命令。例如，service命令允许查看监听器正为连接请求而监控的是什么服务。<br>注解：还可以从Oracle企业管理器的Net Services Administration页面查看监听器服务的状态。<br>代码清单11-2 使用lsnrctl help列出lsnrctl命令<br>$lsnrctl help<br>可以调用lsnrctl实用程序后，使用start命令启动监听器，使用stop命令停业监听器。如果希望从操作系统命令行发布这些命令，可使用lsnrctl start和lsnrctl stop命令执行这两个任务。<br>如果对listener.ora文件做了更改，为使更改起作用的一种方法是重启监听器。另一种安全的方法是重新装载监听信息，包括对监听器配置文件所做的最新更改。Lsnrctl reload命令允许在运行中重新装载监听器，而不用重新启动它。在监听器重装载（甚至是重启）的过程中，当前连接的客户机将继续保持连接，因为监听器已经将连接“交付”给数据库，在客户和数据库服务之间不起作用。<br>注意：我的忠告是，如非绝对有必要，不要修改listener.ora文件，而且对于动态自动服务注册，几乎没有必要修改此文件。不过，有时可能需要修改监听器文件的某些部分，此文件由监听器监控连接请求的所有服务的网络配置信息组成。  </p>\n<h2 id=\"命名和连接\"><a href=\"#命名和连接\" class=\"headerlink\" title=\"命名和连接\"></a>命名和连接</h2><p>在前面连接描述符和连接标识符的例子中，使用sales连接标识符来连接sales服务。连接标识符可以是连接描述符本身，也可以是一个能解析为连接描述符的简单名字(如sales)。一般使用的简单连接标识符称为net service name(网络服务名)。因此前面例子中的sales连接标识符就是一个net service name。<br>因为每次进行连接时都需要提供一下完整的连接描述符非常令人厌烦，使用网络服务名是明智的。但这需要维护网络服务名和连接描述信息之间所有映射的一个中心信息库(central repository)，以便Oracle验证这些网络服务名。因此，在一个用户使用网络服务名sales启动连接进程时，Oracle将搜索中心信息库查找sales的连接描述符。找到连接描述符后，Oracle Net会为指定服务器上的数据库初始化一个连接。<br>Oracle允许几种类型的命名信息库，可用下列4种命名方法访问存储在这些位置中的映射信息。<br>a. 本地命令(local naming )：使用存储在每个客户机上的名为tnsnames.ora的文件连接到数据库服务器。<br>b. 简易连接命名(easy connect naming)：允许连接而无需任何服务名配置。<br>c. 外部命名(external naming)：使用第三方命名服务来解析服务名。<br>d. 目录命令(directory naming)：使用一个集中式的符合LDAP的目录服务器来解析服务名。<br>    不管使用何种命名方法，名字解析过程都是相同的。每种命名法都遵循以下步骤将连接描述符解析为网络服务名：<br>i. 选择命令方法—本地、简易连接、外部命名或目录服务命名<br>ii. 映射连接描述符到服务名；<br>iii. 配置客户机以使用步骤1中选择的命名方法  </p>\n<h2 id=\"本地命名方法\"><a href=\"#本地命名方法\" class=\"headerlink\" title=\"本地命名方法\"></a>本地命名方法</h2><p>本地命令是建立Oracle连接最简单、最容易的方法。使用这种方法，在名为tnsnames.ora的本地化配置文件中存储服务名及其连接描述符。此文件默认存储在$ORACLE_HOME/network/admin目录中。  </p>\n","site":{"data":{}},"excerpt":"","more":"<p>内容来自《Oracle Database 11g 数据库管理艺术》</p>\n<h1 id=\"网络概念：Oracle网络如何工作\"><a href=\"#网络概念：Oracle网络如何工作\" class=\"headerlink\" title=\"网络概念：Oracle网络如何工作\"></a>网络概念：Oracle网络如何工作</h1><p>在希望从客户机（不管是传统客户机还是基于浏览器的客户机）打开数据库会话时，需要通过网络连接到数据库。假如要将台式电脑通过现有网络连接到UNIX服务器上的一个Oracle数据库，则需要在电脑和Oracle数据库（它使用专门的软件）之间构造一个连接方法。需要某种界面来处理会话（在此例子中为SQL*Plus），并且需要某种与业内标准的网络协议（如TCP/IP）通信的方法。<br>为方便配置和管理网络连接，Oracle提供了Oracle Net Services，它是一套在分布式异构计算环境中提供连接方案的组件。Oracle Net Services由Oracle Net、Oracle Net Listener、Oracle Connection Manager、Oracle Net Configuration Assistant和Oracle Net Manager组成。Oracle Net Services软件是在Oracle Database Server或Oracle Client软件安装的过程中自动安装的。<br>Oracle Net是一个初始化、建立及维护客户机和服务器之间的连接的组件。这就是为什么必须在客户机和服务器上都安装Oracle Net的原因。Oracle Net主要由两个组件构成。<br>Oracle Network Foundation Layer：负责建立和维护客户机应用程序与服务器之间的连接，以及它们之间的交换信息。<br>Oracle Protocol Support： 负责映射Transparent Net Substrate（TNS）功能到连接使用的业内标准协议。<br>驻留Oracle数据库的所有服务器还运行一个名为Oracle Net Listener（通常也称为监听器）的服务，其主要功能是监听来自客户机服务登录Oracle数据库的请求。监听器在保证客户机服务具有与数据库匹配的信息（协议、端口和实例名）后，将客户机请求传递到数据库。假如用户名和密码通过认证，则数据库将允许客户机登录。一旦监听器把用户请求交付给数据库，客户机和数据库将直接连接，不再需要监听器的帮助。<br>Oracle提供了基于GUI的大量的实用程序，以帮助配置数据库的网络连接。这些实用程序包括Oracle Connection Manager、Oracle Net Manager和Oracle Net Configuragion Assistant等。这些工具帮助处理所有网络需求。在结束本章学习后，可单击这些程序的图标，开始测试连接的实验。  </p>\n<h1 id=\"Web应用如何连接到Oracle数据库\"><a href=\"#Web应用如何连接到Oracle数据库\" class=\"headerlink\" title=\"Web应用如何连接到Oracle数据库\"></a>Web应用如何连接到Oracle数据库</h1><p>为了构造Oracle数据库的一个Internet连接，客户机上的Web浏览器要与Web服务器通信并使用HTTP进行连接请求。Web服务器将此请求传递给一个应用，该应用处理收到的请求并用Oracle Net（配置在数据库服务器和客户机上）与Oracle数据库服务器通信<br>    下面介绍Oracle网络中几个关键的术语  </p>\n<h2 id=\"数据库实例名\"><a href=\"#数据库实例名\" class=\"headerlink\" title=\"数据库实例名\"></a>数据库实例名</h2><p>正如所知，Oracle实例由SGA和一组Oracle进程组成。数据库实例名在初始化文件（init.ora）中作为INSTANCE_NAME参数给出。在谈到Oracle SID（System identifier，系统标识符）时，指的是Oracle实例。<br>    通常，每个数据库只有一个与其关联的实例。但在Oracle RAC配置中，单个数据库可关联到多个实例。  </p>\n<h2 id=\"全局数据库名\"><a href=\"#全局数据库名\" class=\"headerlink\" title=\"全局数据库名\"></a>全局数据库名</h2><p>全局数据库名唯一地标识一个Oracle数据库，其格式为database_name.database_domain，如sales.us.acme.com。在这个全局数据库中，sales为数据库名，us.acme.com为数据库域。因为相同的域中两个数据库不会有相同的数据库名，所以每个全局数据库名都是唯一的。  </p>\n<h2 id=\"数据库服务名\"><a href=\"#数据库服务名\" class=\"headerlink\" title=\"数据库服务名\"></a>数据库服务名</h2><p>对于客户机，数据库在逻辑上简单地表现为一个服务。在服务和数据库之间存在一个多对多的关系，因为一个数据库可被一个或多个服务所代表，每个服务都专用于一组不同的客户机，而一个服务可覆盖不止一个数据库实例。我们在自己的系统中用每个数据库的服务名来标识它，用初始化参数SERVICE_NAMES来指定数据库的服务名。服务名参数值默认为全局数据库名。<br>请注意，一个数据库可由多个服务名来访问。如果希望不同的客户机组访问适合于它们的特定需求的不同数据库，应该这样做。例如，可对相同数据库创建如下两个服务名：<br>Sales.us.acme.com<br>Finance.us.acme.com<br>销售人员使用sales.us.acme.com服务名，而财务人员则使用finance.us.acme.com服务名。  </p>\n<h2 id=\"连接描述符\"><a href=\"#连接描述符\" class=\"headerlink\" title=\"连接描述符\"></a>连接描述符</h2><p>为了将电脑连接到世界上的任何数据库服务，需要提供两个信息：<br>    数据库服务名；<br>    地址。<br>Oracle使用术语连接描述符(connect descriptor)来表示数据库连接的两个必需的部分：数据库服务名和地址。连接描述符的地址部分包含三个部分，分别是：连接使用的通信协议、主机名和端口号。<br>了解通信协议有助于保证使用合适的网络协议，以便建立连接。标准的协议为TCP/IP或带SSL（Secure Sockets Layer，安全套接层）的TCP/IP。UNIX服务器上的Oracle连接的标准端口为1521或1526.Windows机器上的默认端口为1521.因为任何主机上的数据库具有唯一服务名，所以一个Oracle数据库服务名和一个主机名将唯一地标识任何数据库。下面是一个典型的连接描述符的例子：<br>(DESCRIPTION<br>(ADDRESS=(PROTOCOL=tcp)(HOST=sales-server)(PORT=1521))<br>(CONNECT_DATA=<br>(SERVICE_NAME=sales.us.acme.com)))<br>在此连接描述符中，ADDRESS行指出网络通信将使用TCP协议。HOST指定UNIX（或Windows）服务器，服务器上的Oracle监听器正监听来自端口1521的连接请求。连接描述符的ADDRESS部分也称为协议地址(protocol address)。<br>希望连接数据库的客户机首先连接到Oracle监听器进程。监听器接收到达的请求并把它们交给数据库服务器。一旦客户机和数据库服务器通过监听器的引导连接上，它们就直接通信，在此客户机连接的通信过程中不再需要监听器。  </p>\n<h2 id=\"连接标识符\"><a href=\"#连接标识符\" class=\"headerlink\" title=\"连接标识符\"></a>连接标识符</h2><p>连接标识符（connect identifier）与连接描述符紧密关联。可把连接描述符作为连接标识符，或者可简单地映射一个数据库服务名为一个连接描述符。例如，可以把一个服务名(如sales)映射为11.2.5节所看到的连接描述符。下面是说明映射sales连接标识符的例子。<br>Sales=<br>(DESCRIPTION<br>(ADDRESS=(PROTOCOL=tcp)(HOST=sales-server)(PORT=1521))<br>(CONNECT_DATA=<br>(SERVICE_NAME=sales.us.acme.com)))  </p>\n<h2 id=\"连接串\"><a href=\"#连接串\" class=\"headerlink\" title=\"连接串\"></a>连接串</h2><p>通过提供一个连接串(connect string)连接到数据库。连接串包含用户名/密码组合及一个连接标识符。最常见的连接标识符之一是节点服务名，它是一个数据库服务的名字。<br>下面的例子给出一个连接串，它把一个完整的连接描述符作为连接标识符<br>    CONNECT scott/tiger@(DESCRIPTION=<br>    (ADDRESS=(PROTOCOL=tcp)<br>    (HOST=sales-server)<br>    (PORT=1521))<br>    (CONNECT_DATA=<br>    (SERVICE_NAME=sales.us.acme.com)))<br>下面是一个更简单的连接到相同数据库的方法，它使用连接标识符sales：<br>    CONNECT scott/tiger@sales<br>上面两个例子都能连接到sales数据库，但显然第二个连接串（使用sales连接标识符）简单得多。  </p>\n<h2 id=\"使用Oracle网络服务工具\"><a href=\"#使用Oracle网络服务工具\" class=\"headerlink\" title=\"使用Oracle网络服务工具\"></a>使用Oracle网络服务工具</h2><p>Oracle Net提供了配置客户机与数据库服务之间的连接的几个GUI和命令行工具。最常用的命令行工具是isnrctl实用程序，它帮助管理Oracle监听器服务。下面是帮助管理Oracle Net Servcies的重要GUI工具。<br>Oracle NCA（Net Configuration Assistant，Oracle网络配置助手）。此工具主要用于在安装中配置网络组件，它允许在配置客户机连接的几个选项（本章稍后介绍这些选项）中进行选择。其便于使用的GUI界面使你能在所选择的任何命名方法下快速配置客户机连接。在UNIX/Linux系统上，可通过从$ORACLE_HOME/bin目录执行netca来启动NCA。在Window系统上，选择Start|Programs|Oracle-HOME_NAME|Configuration and Migration Tools|Net ConfigurationAssistant。<br>Oracle网络配置管理器（Oracle Net Manager）。Oracle Net Manager可在客户机和服务器上运行，它允许配置各种命名方法和监听器。利用此工具，可在本地tnsnames.ora文件或在集中式的OID中配置连接描述符，而且可以方便地增加和修改连接方法。<br>为了从Oracle企业管理器控制台启动Oracle Net Manager，选择Tools|Service Management|Oracle Net Manager。为了在Unix上作为独立的应用启动Oracle Net Manager，在ORACLE_HOME/bin目录执行netmgr。在windown上，选择Start|Programs|Oracle-HOME_NAME|Configuration and Migration Tools|NetManager。<br>Oracle企业管理器（Oracle Enterprise Manager）。Oracle Database 11g中的OEM可以完成Oracle Net Manager能完成的所有任务，但不能跨多个文件系统管理多个Oracle主目录。此外，使用OEM可导出目录命名项到tnsnames.ora文件。<br>Oracle目录管理器（Oracle Directory Manager）。这个功能强大的工具允许创建使用OID必需的各种域和环境。用此工具还可以执行密码策略管理及完成许多Oracle高级安全任务。在UNIX/LINUX系统上，可从$ORACLE_HOME/bin目录执行oidadmin来启动OID。在Window系统上，选择Start|Programs|Oracle-HOME_NAME|Integrated Manager Tools|Oracle Directory Manager。  </p>\n<h1 id=\"即时客户机\"><a href=\"#即时客户机\" class=\"headerlink\" title=\"即时客户机\"></a>即时客户机</h1><p>之前说过Oracle客户机安装需要经历常规的Oracle数据库服务器软件安排的所有预备步骤。幸而为连接到Oracle数据库并不总是需要安装完整的Oracle客户机软件。Oracle的新Instant Client（即时客户机）软件允许执行应用程序而不必安装标准的Oracle客户机也不必具有ORACLE_HOME。不需要为访问Oracle数据库的每台机器安装Oracle客户机软件。所有现有的OCI、ODBC和JDBC应用程序都可以使用Instan Client。如果愿意，甚至可以用Instant Client使用SQL<em>Plus。<br>    相对于完整的Oracle客户机，Instant Clients提供以下好处：<br>A. 它是免费的；<br>B. 战胜磁盘空间较少<br>C. 安装更快（5分钟左右）<br>D. 不需要CD<br>E. 它具有Oracle客户机的所有特性，如果有必要甚至包括使用SQL</em>Plus。  </p>\n<h1 id=\"安装Instant-Client\"><a href=\"#安装Instant-Client\" class=\"headerlink\" title=\"安装Instant Client\"></a>安装Instant Client</h1><p>以下是安装新Instant Client软件并快速连接到Oracle数据库的步骤。<br>(1) 从OTN Web站点下载Instant Client软件。你必须安装基本的客户机程序包，还可以包括其他高级可选的程序包。此程序包含以下内容：<br>    a) Basic：运行OCI、OCCI和JDBC-OCI应用程序所需的文件。<br>    b) SQL<em>Plus：为用Instant Client运行SQL</em>Plus需要的库和可执行文件。<br>    c) JDBC Supplement：另外支持XA、国际化及JDBC下的RowSet操作。<br>    d) ODBC Supplement：启用带Instant Client的ODBC应用的另外的库（仅对Windows）。<br>    e) SDK：用于Instant Client开发Oracle应用程序所需的其他文件。<br>(2) 将选择的程序包解压到某个目录，将些目录命名为instantclient或其它类似的名称。<br>(3) 在UNIX和Linux系统中，将环境变量LD_LIBRARY_PATH设置为instantclient（从而保证此参数的设置与程序包所有所在的目录名匹配）。在Winddows系统上，将环境变量PATH设置为instantclient。<br>(4) 测试对Oracle服务器的连接。  </p>\n<h1 id=\"监听器和连接\"><a href=\"#监听器和连接\" class=\"headerlink\" title=\"监听器和连接\"></a>监听器和连接</h1><p>Oracle监听器是一个只运行在服务器上并监听连接请求的服务。Oracle提供一个名为lsnrctl的实用程序来管理监听器进程。以下是监听器如何配合Oracle网络的概述。<br>a. 数据库用监听器记录关于服务、实例及服务处理器的信息<br>b. 客户机与监听器进行初步连接<br>c. 监听器接收和验证客户机连接请求并把此请求交给数据库服务的服务处理器。一旦交付了客户机请求，监听器在该连接中不再起作用。<br>Listener.ora文件默认位置在UNIX系统上为$ORACLE_HOME/network/admin目录，在Windows系统上为$ORACLE_HOME\\network\\admin目录，它包含监听器的配置信息。因为监听器服务只运行在服务器上，因此在客户机上没有listener.ora文件。代码清单11-1给出了一个典型的listener.ora文件。<br>Listener中的所有配置参数都具有默认值，不需要手动配置监听器服务。在服务器上创建了第一个数据库后，监听器服务自动启动，并且将监听器配置文件listener.ora放于默认目录中。新数据库创建后，数据库的网络和服务信息自动添加到监听器的配置文件中。实例启动后，数据库自动向监听器注册，并且监听器开始监听对此数据库的连接请求。<br>代码清单11-1 典型的监听器配置文件<br>代码清单11-1 典型的监听器配置文件   </p>\n<pre><code>#LISTENRE.ORA Network Configuration file \n/u01/app/oracle/product/11.1.0.6.0/db_1/network/admin/listener.ora\nSID_LIST_LISTENER = \n(DESCRIPTION_LIST =\n    (DESCRIPTION = \n          (ADDRESS_LIST = \n            (ADDRESS = (PROTOCOL = IPC)(KEY = EXTPROC4))\n        )\n        (ADDRESS_LIST = \n            (ADDRESS = (PROTOCOL = TCP)(HOST = NTL-ALAPATISAM)(PORT = 1521))\n        )\n    )\n)\nSID_LIST_LISTENER = \n    (SID_LIST = \n    (SID_DESC = \n        (SID_NAME = PLSExtProc)\n        (ORACLE_HOME = /u01/app/oracle/product/11.1.0/db_1)\n        (PROGRAM = extproc)\n    )\n    (SID_DESC = \n        (GLOBAL_DBNAME = remorse.world)\n        (ORACLE_HOME = /u01/app/oracle/product/11.1.0/db_1)\n        (SID_NAME = remorse)\n    )\n    (SID_DESC = \n        (GLOBAL_DBNAME = finance.world)\n        (ORACLE_HOME = /u01/app/oracle/product/11.1.0/db_1)\n        (SID_NAME = finance)\n    )\n)\n</code></pre><h2 id=\"自动服务注册\"><a href=\"#自动服务注册\" class=\"headerlink\" title=\"自动服务注册\"></a>自动服务注册</h2><p>Oracle PMON进程负责向监听器动态服务注册新Oracle数据库服务名，也就是说，在创建新Oracle数据库时，它们将自动向监听器服务注册。PMON进程将在每个新数据库在服务器上创建之后更新listener.ora文件。<br>为自动服务注册，ini.ora文件或SPFILE应该包含如下参数：<br>a. SERVICE_NAMES(如sales.us.oracle.com)<br>b. INSTANCE_NAME(如sales)<br>如果不指定SERVICE_NAMES参数的值，它默认为全局数据库名，全局数据库名是DB_NAME和DB_DOMAIN参数的组合。INSTANCE_NAME参数的默认值为Oracle安装或数据库创建时输入的SID。<br>可使用lsnrctl实用程序查看服务器上监听器的状态，如代码清单11-2所示。相应的输出说明监听器启动了多长时间，监听器服务的配置文件位于何处。它还给出监听器为连接请求而监听的数据库的名称。<br>代码清单11-2 使用lsnrctl实用程序查看监听器的状态<br>$ lsnrctl status  </p>\n<p>在代码清单11-2的Services Summary部分，相应的状态可具有如下的某个值。<br>a. READY：此实例可接受连接<br>b. BLOCKED：此实例不能接受连接<br>c. UNKNOWN：此实例在listener.ora文件中注册而不是通过动态服务注册，因而不知道其状态  </p>\n<h2 id=\"监听器命令\"><a href=\"#监听器命令\" class=\"headerlink\" title=\"监听器命令\"></a>监听器命令</h2><p>在调用lsnrctl实用程序后，除了status命令外还可以执行其他一些重要的命令。例如，service命令允许查看监听器正为连接请求而监控的是什么服务。<br>注解：还可以从Oracle企业管理器的Net Services Administration页面查看监听器服务的状态。<br>代码清单11-2 使用lsnrctl help列出lsnrctl命令<br>$lsnrctl help<br>可以调用lsnrctl实用程序后，使用start命令启动监听器，使用stop命令停业监听器。如果希望从操作系统命令行发布这些命令，可使用lsnrctl start和lsnrctl stop命令执行这两个任务。<br>如果对listener.ora文件做了更改，为使更改起作用的一种方法是重启监听器。另一种安全的方法是重新装载监听信息，包括对监听器配置文件所做的最新更改。Lsnrctl reload命令允许在运行中重新装载监听器，而不用重新启动它。在监听器重装载（甚至是重启）的过程中，当前连接的客户机将继续保持连接，因为监听器已经将连接“交付”给数据库，在客户和数据库服务之间不起作用。<br>注意：我的忠告是，如非绝对有必要，不要修改listener.ora文件，而且对于动态自动服务注册，几乎没有必要修改此文件。不过，有时可能需要修改监听器文件的某些部分，此文件由监听器监控连接请求的所有服务的网络配置信息组成。  </p>\n<h2 id=\"命名和连接\"><a href=\"#命名和连接\" class=\"headerlink\" title=\"命名和连接\"></a>命名和连接</h2><p>在前面连接描述符和连接标识符的例子中，使用sales连接标识符来连接sales服务。连接标识符可以是连接描述符本身，也可以是一个能解析为连接描述符的简单名字(如sales)。一般使用的简单连接标识符称为net service name(网络服务名)。因此前面例子中的sales连接标识符就是一个net service name。<br>因为每次进行连接时都需要提供一下完整的连接描述符非常令人厌烦，使用网络服务名是明智的。但这需要维护网络服务名和连接描述信息之间所有映射的一个中心信息库(central repository)，以便Oracle验证这些网络服务名。因此，在一个用户使用网络服务名sales启动连接进程时，Oracle将搜索中心信息库查找sales的连接描述符。找到连接描述符后，Oracle Net会为指定服务器上的数据库初始化一个连接。<br>Oracle允许几种类型的命名信息库，可用下列4种命名方法访问存储在这些位置中的映射信息。<br>a. 本地命令(local naming )：使用存储在每个客户机上的名为tnsnames.ora的文件连接到数据库服务器。<br>b. 简易连接命名(easy connect naming)：允许连接而无需任何服务名配置。<br>c. 外部命名(external naming)：使用第三方命名服务来解析服务名。<br>d. 目录命令(directory naming)：使用一个集中式的符合LDAP的目录服务器来解析服务名。<br>    不管使用何种命名方法，名字解析过程都是相同的。每种命名法都遵循以下步骤将连接描述符解析为网络服务名：<br>i. 选择命令方法—本地、简易连接、外部命名或目录服务命名<br>ii. 映射连接描述符到服务名；<br>iii. 配置客户机以使用步骤1中选择的命名方法  </p>\n<h2 id=\"本地命名方法\"><a href=\"#本地命名方法\" class=\"headerlink\" title=\"本地命名方法\"></a>本地命名方法</h2><p>本地命令是建立Oracle连接最简单、最容易的方法。使用这种方法，在名为tnsnames.ora的本地化配置文件中存储服务名及其连接描述符。此文件默认存储在$ORACLE_HOME/network/admin目录中。  </p>\n"},{"title":"消息传送基础","date":"2016-05-21T15:43:49.000Z","_content":"\n#二进制串：RAW类型 #\n\n文章摘自《Oracle Database 9i10g11g编程艺术 深入数据库体系结构（第2版）》\n\n    Oracle除了支持文本，还支持二进制数据的存储。前面讨论了CHAR和VARCHAR2类型需要进行字符集转换，而二进制数据不会做这种字符集转换。因此，二进制数据类型不适合存储用户提供的文本，而适于存储加密信息，加密数据不是“文本”，而是原文本的一个二进制表示、包含二进制标记信息的字处理文档，等等。如果数据库不认为某些数据是“文本”(或任意其他基本数据类型，如，数值型、日期型等)，这些数据就应该采用一种二进制数据类型来存储，另外不应该应用字符集转换的数据也要使用二进制数据类型存储。\n\nOracle支持下面3种数据类型来存储二进制数据。\nA.  RAW类型，这是这一节强调的重点，它很适合存储多达2000字节的RAW数据。\nB.  BLOB类型，它支持更大的二进制数据，我们将在12.7节中再做介绍。\nC.  LONG　RAW类型，这是为支持向后兼容性提供的，新应用不应考虑使用这个类型。\n\n二进制RAW类型的语法很简单：\nRAW(<size>)\n例如，以下代码创建了一个每行能存储16字节二进制信息的表：\nSQL> create table t(raw_data raw(16));\nTable created.\nSQL>\nOps$tkyte@ORA11GR2>create table t(raw_data raw(16));\nTable created.\n    从磁盘上的存储来看，RAW类型与VARCHAR2类型很相似。RAW类型是一个变长的二进制串，这说明前面创建的表T可以存储0-16字节的二进制数据。它不会像CHAR类型那样用空格填充。\n    处理RAW数据时，你可能会发现它被隐式地转换为一个VARCHAR2类型，也就是说，诸如SQL*Plus之类的许多工具不会直接显示RAW数据，而是会将其转换为一种十六进制格式来显示，在以下例子中，我们使用SYS_GUID()在表中创建一些二进制数据，SYS_GUID()是一个内置函数，将返回一个全局唯一的16字节RAW串(GUID就代表全局唯一标识符，Globally Unique Identifier)：\n\nSQL> insert into t values(sys_guid());\nRAW_DATA\n--------------------------------\n370798BAA57BEAF0E05001A8653002BE\nSQL>\n\n    在此，你会马上注意到两点。首先，RAW数据看上去就像是一个字符串。SQL*Plus就是以字符串形式获取和打印RAW数据，但是RAW数据在磁盘上并不存储为字符串。SQL*Plus不能在屏幕上打印任意的二进制数据，因为这可能对显示有严重的副作用。要记住，二进制数据可能包含诸如回车或换行等控制字符，还可能是一个Ctrl+G字符，这会导致终端发出”嘟嘟“的叫声。\n    其次，RAW数据看上去远远大于16字节，实际上，在这个例子中，你会看到32个字符。这是因为，每个二进制字节都显示为两个十六进制字符。所存储的RAW数据其实长度就是16字节，可以使用Oracle DUMP函数确认这一点。在此，我转储了这个二进制串的值，并使用了一个可选参数来指定显示各个字节值时应使用哪一种进制。这里使用了基数16，从而能将转储的结果与前面的串进行比较：\n\nSQL> select dump(raw_data,16) from t;\nDUMP(RAW_DATA,16)\n----------------------------------------------\nTyp=23 Len=16: 37,7,98,ba,a5,7b,ea,f0,e0,50,1,a8,65,30,2,be\nSQL>\n\nDUMP显示出，这个二进制串实际上长度为16字节（LEN=16）,另外还逐字节地显示了这个二进制数据。可以看到，这个转储显示与SQL*Plus将RAW数据获取为一个串时所执行的隐式转换是匹配的。\n另一个方向上（插入）也会执行隐式转换：\nSQL> insert into t values( 'abcdef' ); \n1 row created.\n\n    这不会插入串abcdef，而会插入一个3字节的RAW数据，其字节分别是AB、CD、EF，如果用十进制表示则为字节171、205、239。如果试图使用一个包含非法16进制字符的串，就会收到一个错误消息：\n\nSQL> insert into t values( 'abcdefgh' );\ninsert into t values( 'abcdefgh' )\n                            *\nERROR at line 1:\nORA-01465: invalid hex number\nSQL>\n\n    RAW类型可以加索引，还能在谓词中使用，它与其他任何数据类型有同样的功能。不过，必须当心避免不希望的隐式转换，而且必须知道确实会发生隐式转换。\n    在任何情况下我都喜欢使用显式转换，而且推荐这种做法，可以使用以下内置函数来执行这种操作。\nA.  HEXTORAW：将十六进制字符串转换为RAW类型。\nB.  RAWTOHEX：将RAW串转换为十六进制串。\nSQL*Plus将RAW类型获取为一个串时，会隐式地调用RWATOHEX函数，而插入串时会隐式地调用HEXTORAW函数。应该避免隐式转换，而在编写代码时总是使用显式转换，这是一个很好的实践做法。\n所以当前的例子应该写作：\nSQL> select rawtohex(raw_data) from t\nRAWTOHEX(RAW_DATA)\n--------------------------------\n370798BAA57BEAF0E05001A8653002BE\nABCDEF\nSQL> insert into t values( hextoraw('abcdef') );\n1 row created.\n\n","source":"_posts/oracle/RAW类型.md","raw":"---\ntitle: 消息传送基础\ndate: 2016-05-21 23:43:49\ntags: [oracle,数据库]\ncategories: [数据库,oracle]\n---\n\n#二进制串：RAW类型 #\n\n文章摘自《Oracle Database 9i10g11g编程艺术 深入数据库体系结构（第2版）》\n\n    Oracle除了支持文本，还支持二进制数据的存储。前面讨论了CHAR和VARCHAR2类型需要进行字符集转换，而二进制数据不会做这种字符集转换。因此，二进制数据类型不适合存储用户提供的文本，而适于存储加密信息，加密数据不是“文本”，而是原文本的一个二进制表示、包含二进制标记信息的字处理文档，等等。如果数据库不认为某些数据是“文本”(或任意其他基本数据类型，如，数值型、日期型等)，这些数据就应该采用一种二进制数据类型来存储，另外不应该应用字符集转换的数据也要使用二进制数据类型存储。\n\nOracle支持下面3种数据类型来存储二进制数据。\nA.  RAW类型，这是这一节强调的重点，它很适合存储多达2000字节的RAW数据。\nB.  BLOB类型，它支持更大的二进制数据，我们将在12.7节中再做介绍。\nC.  LONG　RAW类型，这是为支持向后兼容性提供的，新应用不应考虑使用这个类型。\n\n二进制RAW类型的语法很简单：\nRAW(<size>)\n例如，以下代码创建了一个每行能存储16字节二进制信息的表：\nSQL> create table t(raw_data raw(16));\nTable created.\nSQL>\nOps$tkyte@ORA11GR2>create table t(raw_data raw(16));\nTable created.\n    从磁盘上的存储来看，RAW类型与VARCHAR2类型很相似。RAW类型是一个变长的二进制串，这说明前面创建的表T可以存储0-16字节的二进制数据。它不会像CHAR类型那样用空格填充。\n    处理RAW数据时，你可能会发现它被隐式地转换为一个VARCHAR2类型，也就是说，诸如SQL*Plus之类的许多工具不会直接显示RAW数据，而是会将其转换为一种十六进制格式来显示，在以下例子中，我们使用SYS_GUID()在表中创建一些二进制数据，SYS_GUID()是一个内置函数，将返回一个全局唯一的16字节RAW串(GUID就代表全局唯一标识符，Globally Unique Identifier)：\n\nSQL> insert into t values(sys_guid());\nRAW_DATA\n--------------------------------\n370798BAA57BEAF0E05001A8653002BE\nSQL>\n\n    在此，你会马上注意到两点。首先，RAW数据看上去就像是一个字符串。SQL*Plus就是以字符串形式获取和打印RAW数据，但是RAW数据在磁盘上并不存储为字符串。SQL*Plus不能在屏幕上打印任意的二进制数据，因为这可能对显示有严重的副作用。要记住，二进制数据可能包含诸如回车或换行等控制字符，还可能是一个Ctrl+G字符，这会导致终端发出”嘟嘟“的叫声。\n    其次，RAW数据看上去远远大于16字节，实际上，在这个例子中，你会看到32个字符。这是因为，每个二进制字节都显示为两个十六进制字符。所存储的RAW数据其实长度就是16字节，可以使用Oracle DUMP函数确认这一点。在此，我转储了这个二进制串的值，并使用了一个可选参数来指定显示各个字节值时应使用哪一种进制。这里使用了基数16，从而能将转储的结果与前面的串进行比较：\n\nSQL> select dump(raw_data,16) from t;\nDUMP(RAW_DATA,16)\n----------------------------------------------\nTyp=23 Len=16: 37,7,98,ba,a5,7b,ea,f0,e0,50,1,a8,65,30,2,be\nSQL>\n\nDUMP显示出，这个二进制串实际上长度为16字节（LEN=16）,另外还逐字节地显示了这个二进制数据。可以看到，这个转储显示与SQL*Plus将RAW数据获取为一个串时所执行的隐式转换是匹配的。\n另一个方向上（插入）也会执行隐式转换：\nSQL> insert into t values( 'abcdef' ); \n1 row created.\n\n    这不会插入串abcdef，而会插入一个3字节的RAW数据，其字节分别是AB、CD、EF，如果用十进制表示则为字节171、205、239。如果试图使用一个包含非法16进制字符的串，就会收到一个错误消息：\n\nSQL> insert into t values( 'abcdefgh' );\ninsert into t values( 'abcdefgh' )\n                            *\nERROR at line 1:\nORA-01465: invalid hex number\nSQL>\n\n    RAW类型可以加索引，还能在谓词中使用，它与其他任何数据类型有同样的功能。不过，必须当心避免不希望的隐式转换，而且必须知道确实会发生隐式转换。\n    在任何情况下我都喜欢使用显式转换，而且推荐这种做法，可以使用以下内置函数来执行这种操作。\nA.  HEXTORAW：将十六进制字符串转换为RAW类型。\nB.  RAWTOHEX：将RAW串转换为十六进制串。\nSQL*Plus将RAW类型获取为一个串时，会隐式地调用RWATOHEX函数，而插入串时会隐式地调用HEXTORAW函数。应该避免隐式转换，而在编写代码时总是使用显式转换，这是一个很好的实践做法。\n所以当前的例子应该写作：\nSQL> select rawtohex(raw_data) from t\nRAWTOHEX(RAW_DATA)\n--------------------------------\n370798BAA57BEAF0E05001A8653002BE\nABCDEF\nSQL> insert into t values( hextoraw('abcdef') );\n1 row created.\n\n","slug":"oracle/RAW类型","published":1,"updated":"2017-08-18T01:44:35.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjh7poltg002bbp7r73iasgk4","content":"<p>#二进制串：RAW类型 #</p>\n<p>文章摘自《Oracle Database 9i10g11g编程艺术 深入数据库体系结构（第2版）》</p>\n<pre><code>Oracle除了支持文本，还支持二进制数据的存储。前面讨论了CHAR和VARCHAR2类型需要进行字符集转换，而二进制数据不会做这种字符集转换。因此，二进制数据类型不适合存储用户提供的文本，而适于存储加密信息，加密数据不是“文本”，而是原文本的一个二进制表示、包含二进制标记信息的字处理文档，等等。如果数据库不认为某些数据是“文本”(或任意其他基本数据类型，如，数值型、日期型等)，这些数据就应该采用一种二进制数据类型来存储，另外不应该应用字符集转换的数据也要使用二进制数据类型存储。\n</code></pre><p>Oracle支持下面3种数据类型来存储二进制数据。<br>A.  RAW类型，这是这一节强调的重点，它很适合存储多达2000字节的RAW数据。<br>B.  BLOB类型，它支持更大的二进制数据，我们将在12.7节中再做介绍。<br>C.  LONG　RAW类型，这是为支持向后兼容性提供的，新应用不应考虑使用这个类型。</p>\n<p>二进制RAW类型的语法很简单：<br>RAW(<size>)<br>例如，以下代码创建了一个每行能存储16字节二进制信息的表：<br>SQL&gt; create table t(raw_data raw(16));<br>Table created.<br>SQL&gt;<br>Ops$tkyte@ORA11GR2&gt;create table t(raw_data raw(16));<br>Table created.<br>    从磁盘上的存储来看，RAW类型与VARCHAR2类型很相似。RAW类型是一个变长的二进制串，这说明前面创建的表T可以存储0-16字节的二进制数据。它不会像CHAR类型那样用空格填充。<br>    处理RAW数据时，你可能会发现它被隐式地转换为一个VARCHAR2类型，也就是说，诸如SQL*Plus之类的许多工具不会直接显示RAW数据，而是会将其转换为一种十六进制格式来显示，在以下例子中，我们使用SYS_GUID()在表中创建一些二进制数据，SYS_GUID()是一个内置函数，将返回一个全局唯一的16字节RAW串(GUID就代表全局唯一标识符，Globally Unique Identifier)：</size></p>\n<p>SQL&gt; insert into t values(sys_guid());</p>\n<h2 id=\"RAW-DATA\"><a href=\"#RAW-DATA\" class=\"headerlink\" title=\"RAW_DATA\"></a>RAW_DATA</h2><p>370798BAA57BEAF0E05001A8653002BE<br>SQL&gt;</p>\n<pre><code>在此，你会马上注意到两点。首先，RAW数据看上去就像是一个字符串。SQL*Plus就是以字符串形式获取和打印RAW数据，但是RAW数据在磁盘上并不存储为字符串。SQL*Plus不能在屏幕上打印任意的二进制数据，因为这可能对显示有严重的副作用。要记住，二进制数据可能包含诸如回车或换行等控制字符，还可能是一个Ctrl+G字符，这会导致终端发出”嘟嘟“的叫声。\n其次，RAW数据看上去远远大于16字节，实际上，在这个例子中，你会看到32个字符。这是因为，每个二进制字节都显示为两个十六进制字符。所存储的RAW数据其实长度就是16字节，可以使用Oracle DUMP函数确认这一点。在此，我转储了这个二进制串的值，并使用了一个可选参数来指定显示各个字节值时应使用哪一种进制。这里使用了基数16，从而能将转储的结果与前面的串进行比较：\n</code></pre><p>SQL&gt; select dump(raw_data,16) from t;</p>\n<h2 id=\"DUMP-RAW-DATA-16\"><a href=\"#DUMP-RAW-DATA-16\" class=\"headerlink\" title=\"DUMP(RAW_DATA,16)\"></a>DUMP(RAW_DATA,16)</h2><p>Typ=23 Len=16: 37,7,98,ba,a5,7b,ea,f0,e0,50,1,a8,65,30,2,be<br>SQL&gt;</p>\n<p>DUMP显示出，这个二进制串实际上长度为16字节（LEN=16）,另外还逐字节地显示了这个二进制数据。可以看到，这个转储显示与SQL*Plus将RAW数据获取为一个串时所执行的隐式转换是匹配的。<br>另一个方向上（插入）也会执行隐式转换：<br>SQL&gt; insert into t values( ‘abcdef’ );<br>1 row created.</p>\n<pre><code>这不会插入串abcdef，而会插入一个3字节的RAW数据，其字节分别是AB、CD、EF，如果用十进制表示则为字节171、205、239。如果试图使用一个包含非法16进制字符的串，就会收到一个错误消息：\n</code></pre><p>SQL&gt; insert into t values( ‘abcdefgh’ );<br>insert into t values( ‘abcdefgh’ )<br>                            *<br>ERROR at line 1:<br>ORA-01465: invalid hex number<br>SQL&gt;</p>\n<pre><code>RAW类型可以加索引，还能在谓词中使用，它与其他任何数据类型有同样的功能。不过，必须当心避免不希望的隐式转换，而且必须知道确实会发生隐式转换。\n在任何情况下我都喜欢使用显式转换，而且推荐这种做法，可以使用以下内置函数来执行这种操作。\n</code></pre><p>A.  HEXTORAW：将十六进制字符串转换为RAW类型。<br>B.  RAWTOHEX：将RAW串转换为十六进制串。<br>SQL*Plus将RAW类型获取为一个串时，会隐式地调用RWATOHEX函数，而插入串时会隐式地调用HEXTORAW函数。应该避免隐式转换，而在编写代码时总是使用显式转换，这是一个很好的实践做法。<br>所以当前的例子应该写作：<br>SQL&gt; select rawtohex(raw_data) from t</p>\n<h2 id=\"RAWTOHEX-RAW-DATA\"><a href=\"#RAWTOHEX-RAW-DATA\" class=\"headerlink\" title=\"RAWTOHEX(RAW_DATA)\"></a>RAWTOHEX(RAW_DATA)</h2><p>370798BAA57BEAF0E05001A8653002BE<br>ABCDEF<br>SQL&gt; insert into t values( hextoraw(‘abcdef’) );<br>1 row created.</p>\n","site":{"data":{}},"excerpt":"","more":"<p>#二进制串：RAW类型 #</p>\n<p>文章摘自《Oracle Database 9i10g11g编程艺术 深入数据库体系结构（第2版）》</p>\n<pre><code>Oracle除了支持文本，还支持二进制数据的存储。前面讨论了CHAR和VARCHAR2类型需要进行字符集转换，而二进制数据不会做这种字符集转换。因此，二进制数据类型不适合存储用户提供的文本，而适于存储加密信息，加密数据不是“文本”，而是原文本的一个二进制表示、包含二进制标记信息的字处理文档，等等。如果数据库不认为某些数据是“文本”(或任意其他基本数据类型，如，数值型、日期型等)，这些数据就应该采用一种二进制数据类型来存储，另外不应该应用字符集转换的数据也要使用二进制数据类型存储。\n</code></pre><p>Oracle支持下面3种数据类型来存储二进制数据。<br>A.  RAW类型，这是这一节强调的重点，它很适合存储多达2000字节的RAW数据。<br>B.  BLOB类型，它支持更大的二进制数据，我们将在12.7节中再做介绍。<br>C.  LONG　RAW类型，这是为支持向后兼容性提供的，新应用不应考虑使用这个类型。</p>\n<p>二进制RAW类型的语法很简单：<br>RAW(<size>)<br>例如，以下代码创建了一个每行能存储16字节二进制信息的表：<br>SQL&gt; create table t(raw_data raw(16));<br>Table created.<br>SQL&gt;<br>Ops$tkyte@ORA11GR2&gt;create table t(raw_data raw(16));<br>Table created.<br>    从磁盘上的存储来看，RAW类型与VARCHAR2类型很相似。RAW类型是一个变长的二进制串，这说明前面创建的表T可以存储0-16字节的二进制数据。它不会像CHAR类型那样用空格填充。<br>    处理RAW数据时，你可能会发现它被隐式地转换为一个VARCHAR2类型，也就是说，诸如SQL*Plus之类的许多工具不会直接显示RAW数据，而是会将其转换为一种十六进制格式来显示，在以下例子中，我们使用SYS_GUID()在表中创建一些二进制数据，SYS_GUID()是一个内置函数，将返回一个全局唯一的16字节RAW串(GUID就代表全局唯一标识符，Globally Unique Identifier)：</size></p>\n<p>SQL&gt; insert into t values(sys_guid());</p>\n<h2 id=\"RAW-DATA\"><a href=\"#RAW-DATA\" class=\"headerlink\" title=\"RAW_DATA\"></a>RAW_DATA</h2><p>370798BAA57BEAF0E05001A8653002BE<br>SQL&gt;</p>\n<pre><code>在此，你会马上注意到两点。首先，RAW数据看上去就像是一个字符串。SQL*Plus就是以字符串形式获取和打印RAW数据，但是RAW数据在磁盘上并不存储为字符串。SQL*Plus不能在屏幕上打印任意的二进制数据，因为这可能对显示有严重的副作用。要记住，二进制数据可能包含诸如回车或换行等控制字符，还可能是一个Ctrl+G字符，这会导致终端发出”嘟嘟“的叫声。\n其次，RAW数据看上去远远大于16字节，实际上，在这个例子中，你会看到32个字符。这是因为，每个二进制字节都显示为两个十六进制字符。所存储的RAW数据其实长度就是16字节，可以使用Oracle DUMP函数确认这一点。在此，我转储了这个二进制串的值，并使用了一个可选参数来指定显示各个字节值时应使用哪一种进制。这里使用了基数16，从而能将转储的结果与前面的串进行比较：\n</code></pre><p>SQL&gt; select dump(raw_data,16) from t;</p>\n<h2 id=\"DUMP-RAW-DATA-16\"><a href=\"#DUMP-RAW-DATA-16\" class=\"headerlink\" title=\"DUMP(RAW_DATA,16)\"></a>DUMP(RAW_DATA,16)</h2><p>Typ=23 Len=16: 37,7,98,ba,a5,7b,ea,f0,e0,50,1,a8,65,30,2,be<br>SQL&gt;</p>\n<p>DUMP显示出，这个二进制串实际上长度为16字节（LEN=16）,另外还逐字节地显示了这个二进制数据。可以看到，这个转储显示与SQL*Plus将RAW数据获取为一个串时所执行的隐式转换是匹配的。<br>另一个方向上（插入）也会执行隐式转换：<br>SQL&gt; insert into t values( ‘abcdef’ );<br>1 row created.</p>\n<pre><code>这不会插入串abcdef，而会插入一个3字节的RAW数据，其字节分别是AB、CD、EF，如果用十进制表示则为字节171、205、239。如果试图使用一个包含非法16进制字符的串，就会收到一个错误消息：\n</code></pre><p>SQL&gt; insert into t values( ‘abcdefgh’ );<br>insert into t values( ‘abcdefgh’ )<br>                            *<br>ERROR at line 1:<br>ORA-01465: invalid hex number<br>SQL&gt;</p>\n<pre><code>RAW类型可以加索引，还能在谓词中使用，它与其他任何数据类型有同样的功能。不过，必须当心避免不希望的隐式转换，而且必须知道确实会发生隐式转换。\n在任何情况下我都喜欢使用显式转换，而且推荐这种做法，可以使用以下内置函数来执行这种操作。\n</code></pre><p>A.  HEXTORAW：将十六进制字符串转换为RAW类型。<br>B.  RAWTOHEX：将RAW串转换为十六进制串。<br>SQL*Plus将RAW类型获取为一个串时，会隐式地调用RWATOHEX函数，而插入串时会隐式地调用HEXTORAW函数。应该避免隐式转换，而在编写代码时总是使用显式转换，这是一个很好的实践做法。<br>所以当前的例子应该写作：<br>SQL&gt; select rawtohex(raw_data) from t</p>\n<h2 id=\"RAWTOHEX-RAW-DATA\"><a href=\"#RAWTOHEX-RAW-DATA\" class=\"headerlink\" title=\"RAWTOHEX(RAW_DATA)\"></a>RAWTOHEX(RAW_DATA)</h2><p>370798BAA57BEAF0E05001A8653002BE<br>ABCDEF<br>SQL&gt; insert into t values( hextoraw(‘abcdef’) );<br>1 row created.</p>\n"},{"title":"Spark体系概述","date":"2017-04-16T15:43:49.000Z","_content":"本文目的是介绍spark框架下的内容，以简要概述方式。\n\n\n\n","source":"_posts/spark/TODO-Spark体系概述.md","raw":"---\ntitle: Spark体系概述\ndate: 2017-04-16 23:43:49\ntags: [大数据,spark]\ncategories: [大数据,spark]\n---\n本文目的是介绍spark框架下的内容，以简要概述方式。\n\n\n\n","slug":"spark/TODO-Spark体系概述","published":1,"updated":"2017-08-18T01:44:35.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjh7polti002ebp7rga5slf59","content":"<p>本文目的是介绍spark框架下的内容，以简要概述方式。</p>\n","site":{"data":{}},"excerpt":"","more":"<p>本文目的是介绍spark框架下的内容，以简要概述方式。</p>\n"},{"title":"Spring源码分析之环境准备","date":"2017-04-18T02:00:00.000Z","_content":"#Spring源码分析之环境准备\n","source":"_posts/spring/Spring源码分析之环境准备.md","raw":"---\ntitle: Spring源码分析之环境准备\ndate: 2017-04-18 10:00:00\ntags: [java,spring]\ncategories: [开源项目,spring]\n---\n#Spring源码分析之环境准备\n","slug":"spring/Spring源码分析之环境准备","published":1,"updated":"2017-08-18T01:44:35.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjh7poltj002fbp7r7su3p7bq","content":"<p>#Spring源码分析之环境准备</p>\n","site":{"data":{}},"excerpt":"","more":"<p>#Spring源码分析之环境准备</p>\n"},{"title":"JasperListener类找不到","date":"2017-08-18T01:59:00.000Z","_content":"#Tomcat java.lang.ClassNotFoundException: org.apache.catalina.core.JasperListener\n##问题描述\nLinux下启动tomcat8.0.45错误，windons下可用。\n\n##原因分析\nClassNotFoundException大致可能有两种情况，一是找不到jar包，二是jar包冲突，不知用哪个。\n\n##解决方案\nJasperListener好像是一个报表支持，目前不需要，可以暂时去掉。\n在tomcat目录 /conf/server.xml里注释如下内容\n<Listener className=\"org.apache.catalina.core.JasperListener\" />   ","source":"_posts/异常/JasperListener类找不到.md","raw":"---\ntitle: JasperListener类找不到\ndate: 2017-08-18 09:59:00\ntags: [异常,tomcat]\ncategories: [异常,tomcat]\n---\n#Tomcat java.lang.ClassNotFoundException: org.apache.catalina.core.JasperListener\n##问题描述\nLinux下启动tomcat8.0.45错误，windons下可用。\n\n##原因分析\nClassNotFoundException大致可能有两种情况，一是找不到jar包，二是jar包冲突，不知用哪个。\n\n##解决方案\nJasperListener好像是一个报表支持，目前不需要，可以暂时去掉。\n在tomcat目录 /conf/server.xml里注释如下内容\n<Listener className=\"org.apache.catalina.core.JasperListener\" />   ","slug":"异常/JasperListener类找不到","published":1,"updated":"2017-08-20T17:36:00.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjh7poltk002ibp7r8k6v71bm","content":"<p>#Tomcat java.lang.ClassNotFoundException: org.apache.catalina.core.JasperListener</p>\n<p>##问题描述<br>Linux下启动tomcat8.0.45错误，windons下可用。</p>\n<p>##原因分析<br>ClassNotFoundException大致可能有两种情况，一是找不到jar包，二是jar包冲突，不知用哪个。</p>\n<p>##解决方案<br>JasperListener好像是一个报表支持，目前不需要，可以暂时去掉。<br>在tomcat目录 /conf/server.xml里注释如下内容</p>\n<listener classname=\"org.apache.catalina.core.JasperListener\">   </listener>","site":{"data":{}},"excerpt":"","more":"<p>#Tomcat java.lang.ClassNotFoundException: org.apache.catalina.core.JasperListener</p>\n<p>##问题描述<br>Linux下启动tomcat8.0.45错误，windons下可用。</p>\n<p>##原因分析<br>ClassNotFoundException大致可能有两种情况，一是找不到jar包，二是jar包冲突，不知用哪个。</p>\n<p>##解决方案<br>JasperListener好像是一个报表支持，目前不需要，可以暂时去掉。<br>在tomcat目录 /conf/server.xml里注释如下内容</p>\n<listener classname=\"org.apache.catalina.core.JasperListener\">   </listener>"},{"title":"Java_heap_space_OutOfMemoryError","date":"2017-08-18T12:30:00.000Z","_content":"#java.lang.OutOfMemoryError: Java heap space\n##问题描述\nTomcat 8.0.45在linux环境下启动错误\n\n##原因分析\nJVM堆是指java程序运行过程中JVM可以调配使用的内存空间,主要用于存放Instance。JVM在启动的时候会自动设置Heap size的值，其初始空间(即-Xms)是物理内存的1/64，最大空间(-Xmx)是物理内存的1/4。可以利用JVM提供的-Xmn -Xms -Xmx等选项可进行设置。Heap size 的大小是Young Generation 和Tenured Generaion 之和。在JVM中如果98％的时间是用于GC且可用的Heap size 不足2％的时候将抛出此异常信息。\n\n##解决方案\n手动设置Heap size\na.如果tomcat是以bat方式启动的，则如下设置：\n修改TOMCAT_HOME/bin/catalina.sh\nJAVA_OPTS=\"-server -Xms1024m -Xmx1024m    -XX:MaxNewSize=256m\"","source":"_posts/异常/Java_heap_space_OutOfMemoryError.md","raw":"---\ntitle: Java_heap_space_OutOfMemoryError\ndate: 2017-08-18 20:30:00\ntags: [异常,tomcat]\ncategories: [异常,tomcat]\n---\n#java.lang.OutOfMemoryError: Java heap space\n##问题描述\nTomcat 8.0.45在linux环境下启动错误\n\n##原因分析\nJVM堆是指java程序运行过程中JVM可以调配使用的内存空间,主要用于存放Instance。JVM在启动的时候会自动设置Heap size的值，其初始空间(即-Xms)是物理内存的1/64，最大空间(-Xmx)是物理内存的1/4。可以利用JVM提供的-Xmn -Xms -Xmx等选项可进行设置。Heap size 的大小是Young Generation 和Tenured Generaion 之和。在JVM中如果98％的时间是用于GC且可用的Heap size 不足2％的时候将抛出此异常信息。\n\n##解决方案\n手动设置Heap size\na.如果tomcat是以bat方式启动的，则如下设置：\n修改TOMCAT_HOME/bin/catalina.sh\nJAVA_OPTS=\"-server -Xms1024m -Xmx1024m    -XX:MaxNewSize=256m\"","slug":"异常/Java_heap_space_OutOfMemoryError","published":1,"updated":"2018-05-15T05:57:06.605Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjh7poltl002kbp7r3qn669vg","content":"<p>#java.lang.OutOfMemoryError: Java heap space</p>\n<p>##问题描述<br>Tomcat 8.0.45在linux环境下启动错误</p>\n<p>##原因分析<br>JVM堆是指java程序运行过程中JVM可以调配使用的内存空间,主要用于存放Instance。JVM在启动的时候会自动设置Heap size的值，其初始空间(即-Xms)是物理内存的1/64，最大空间(-Xmx)是物理内存的1/4。可以利用JVM提供的-Xmn -Xms -Xmx等选项可进行设置。Heap size 的大小是Young Generation 和Tenured Generaion 之和。在JVM中如果98％的时间是用于GC且可用的Heap size 不足2％的时候将抛出此异常信息。</p>\n<p>##解决方案<br>手动设置Heap size<br>a.如果tomcat是以bat方式启动的，则如下设置：<br>修改TOMCAT_HOME/bin/catalina.sh<br>JAVA_OPTS=”-server -Xms1024m -Xmx1024m    -XX:MaxNewSize=256m”</p>\n","site":{"data":{}},"excerpt":"","more":"<p>#java.lang.OutOfMemoryError: Java heap space</p>\n<p>##问题描述<br>Tomcat 8.0.45在linux环境下启动错误</p>\n<p>##原因分析<br>JVM堆是指java程序运行过程中JVM可以调配使用的内存空间,主要用于存放Instance。JVM在启动的时候会自动设置Heap size的值，其初始空间(即-Xms)是物理内存的1/64，最大空间(-Xmx)是物理内存的1/4。可以利用JVM提供的-Xmn -Xms -Xmx等选项可进行设置。Heap size 的大小是Young Generation 和Tenured Generaion 之和。在JVM中如果98％的时间是用于GC且可用的Heap size 不足2％的时候将抛出此异常信息。</p>\n<p>##解决方案<br>手动设置Heap size<br>a.如果tomcat是以bat方式启动的，则如下设置：<br>修改TOMCAT_HOME/bin/catalina.sh<br>JAVA_OPTS=”-server -Xms1024m -Xmx1024m    -XX:MaxNewSize=256m”</p>\n"},{"title":"connection_holder_is_null","date":"2017-08-18T01:55:00.000Z","_content":"#java.sql.SQLException: connection holder is null\n##问题描述\n使用druid连接池时出现的间歇性错误，偶尔会出现。\n\n##原因分析\n连接池可能对连接持有时间有限制，一种保护机制，如果某个连接很长时间不释放，其它应用就没有办法使用这个连接。\n\n##解决方案\n###方案一：\n        延长这个超时时间，默认为300秒（推荐）\n        <!--是否自动回收超时连接-->\n   \t\t<property name=\"removeAbandoned\" value=\"true\" />\n   \t\t<!--延长这个所谓的超时时间-->\n   \t\t<property name=\"removeAbandonedTimeout\" value=\"1800\" />\n   \t\t<!--将当前关闭动作记录到日志-->\n   \t\t<property name=\"logAbandoned\" value=\"true\" />        \n###方案二：\n        关闭这个超时保护\n        /*直接关闭这个   自动回收超时连接*/\n        <property name=\"removeAbandoned\" value=\"false\" />\n","source":"_posts/异常/connection_holder_is_null.md","raw":"---\ntitle: connection_holder_is_null\n\ndate: 2017-08-18 09:55:00\ntags: [异常,tomcat]\ncategories: [异常,tomcat]\n---\n#java.sql.SQLException: connection holder is null\n##问题描述\n使用druid连接池时出现的间歇性错误，偶尔会出现。\n\n##原因分析\n连接池可能对连接持有时间有限制，一种保护机制，如果某个连接很长时间不释放，其它应用就没有办法使用这个连接。\n\n##解决方案\n###方案一：\n        延长这个超时时间，默认为300秒（推荐）\n        <!--是否自动回收超时连接-->\n   \t\t<property name=\"removeAbandoned\" value=\"true\" />\n   \t\t<!--延长这个所谓的超时时间-->\n   \t\t<property name=\"removeAbandonedTimeout\" value=\"1800\" />\n   \t\t<!--将当前关闭动作记录到日志-->\n   \t\t<property name=\"logAbandoned\" value=\"true\" />        \n###方案二：\n        关闭这个超时保护\n        /*直接关闭这个   自动回收超时连接*/\n        <property name=\"removeAbandoned\" value=\"false\" />\n","slug":"异常/connection_holder_is_null","published":1,"updated":"2017-08-20T17:36:00.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjh7poltm002obp7ryahxa1ne","content":"<p>#java.sql.SQLException: connection holder is null</p>\n<p>##问题描述<br>使用druid连接池时出现的间歇性错误，偶尔会出现。</p>\n<p>##原因分析<br>连接池可能对连接持有时间有限制，一种保护机制，如果某个连接很长时间不释放，其它应用就没有办法使用这个连接。</p>\n<p>##解决方案</p>\n<p>###方案一：<br>        延长这个超时时间，默认为300秒（推荐）<br>        <!--是否自动回收超时连接--><br>           <property name=\"removeAbandoned\" value=\"true\"><br>           <!--延长这个所谓的超时时间--><br>           <property name=\"removeAbandonedTimeout\" value=\"1800\"><br>           <!--将当前关闭动作记录到日志--><br>           <property name=\"logAbandoned\" value=\"true\">        </property></property></property></p>\n<p>###方案二：<br>        关闭这个超时保护<br>        /<em>直接关闭这个   自动回收超时连接</em>/<br>        <property name=\"removeAbandoned\" value=\"false\"></property></p>\n","site":{"data":{}},"excerpt":"","more":"<p>#java.sql.SQLException: connection holder is null</p>\n<p>##问题描述<br>使用druid连接池时出现的间歇性错误，偶尔会出现。</p>\n<p>##原因分析<br>连接池可能对连接持有时间有限制，一种保护机制，如果某个连接很长时间不释放，其它应用就没有办法使用这个连接。</p>\n<p>##解决方案</p>\n<p>###方案一：<br>        延长这个超时时间，默认为300秒（推荐）<br>        <!--是否自动回收超时连接--><br>           <property name=\"removeAbandoned\" value=\"true\"><br>           <!--延长这个所谓的超时时间--><br>           <property name=\"removeAbandonedTimeout\" value=\"1800\"><br>           <!--将当前关闭动作记录到日志--><br>           <property name=\"logAbandoned\" value=\"true\">        </property></property></property></p>\n<p>###方案二：<br>        关闭这个超时保护<br>        /<em>直接关闭这个   自动回收超时连接</em>/<br>        <property name=\"removeAbandoned\" value=\"false\"></property></p>\n"},{"title":"permGen_space_OutOfMemoryError","date":"2017-08-18T01:55:00.000Z","_content":"#Tomcat java.lang.OutOfMemoryError: PermGen space\n##问题描述\nTomcat 8.0.45在linux环境下启动错误\n\n##原因分析\nPermGen space是指内存的永久保存区域内存溢出。这块内存主要是JVM来来存放Class和Meta信息的，Class在被Load的时候被放入PermGen space区域，它和存放Instance的Heap区域不同,GC(Garbage Collection)不会在主程序运行期对PermGen space进行清理，所以如果你的APP会LOAD很多CLASS的话，就很可能出现PermGen space错误。这种错误常见在web服务器对JSP进行预编译的时候。如果你的WEB APP下都用了大量的第三方jar, 其大小超过了jvm默认的大小(4M)那么就会产生此错误信息了。\n\n##解决方案\n手动设置MaxPermSize大小,修改TOMCAT_HOME/bin/catalina.sh\nJAVA_OPTS=\"-server -XX:PermSize=256M -XX:MaxPermSize=1024m\"   ","source":"_posts/异常/permGen_space_OutOfMemoryError.md","raw":"---\ntitle: permGen_space_OutOfMemoryError\ndate: 2017-08-18 09:55:00\ntags: [异常,tomcat]\ncategories: [异常,tomcat]\n---\n#Tomcat java.lang.OutOfMemoryError: PermGen space\n##问题描述\nTomcat 8.0.45在linux环境下启动错误\n\n##原因分析\nPermGen space是指内存的永久保存区域内存溢出。这块内存主要是JVM来来存放Class和Meta信息的，Class在被Load的时候被放入PermGen space区域，它和存放Instance的Heap区域不同,GC(Garbage Collection)不会在主程序运行期对PermGen space进行清理，所以如果你的APP会LOAD很多CLASS的话，就很可能出现PermGen space错误。这种错误常见在web服务器对JSP进行预编译的时候。如果你的WEB APP下都用了大量的第三方jar, 其大小超过了jvm默认的大小(4M)那么就会产生此错误信息了。\n\n##解决方案\n手动设置MaxPermSize大小,修改TOMCAT_HOME/bin/catalina.sh\nJAVA_OPTS=\"-server -XX:PermSize=256M -XX:MaxPermSize=1024m\"   ","slug":"异常/permGen_space_OutOfMemoryError","published":1,"updated":"2017-08-20T17:36:00.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjh7poltn002pbp7rsbprdrbd","content":"<p>#Tomcat java.lang.OutOfMemoryError: PermGen space</p>\n<p>##问题描述<br>Tomcat 8.0.45在linux环境下启动错误</p>\n<p>##原因分析<br>PermGen space是指内存的永久保存区域内存溢出。这块内存主要是JVM来来存放Class和Meta信息的，Class在被Load的时候被放入PermGen space区域，它和存放Instance的Heap区域不同,GC(Garbage Collection)不会在主程序运行期对PermGen space进行清理，所以如果你的APP会LOAD很多CLASS的话，就很可能出现PermGen space错误。这种错误常见在web服务器对JSP进行预编译的时候。如果你的WEB APP下都用了大量的第三方jar, 其大小超过了jvm默认的大小(4M)那么就会产生此错误信息了。</p>\n<p>##解决方案<br>手动设置MaxPermSize大小,修改TOMCAT_HOME/bin/catalina.sh<br>JAVA_OPTS=”-server -XX:PermSize=256M -XX:MaxPermSize=1024m”   </p>\n","site":{"data":{}},"excerpt":"","more":"<p>#Tomcat java.lang.OutOfMemoryError: PermGen space</p>\n<p>##问题描述<br>Tomcat 8.0.45在linux环境下启动错误</p>\n<p>##原因分析<br>PermGen space是指内存的永久保存区域内存溢出。这块内存主要是JVM来来存放Class和Meta信息的，Class在被Load的时候被放入PermGen space区域，它和存放Instance的Heap区域不同,GC(Garbage Collection)不会在主程序运行期对PermGen space进行清理，所以如果你的APP会LOAD很多CLASS的话，就很可能出现PermGen space错误。这种错误常见在web服务器对JSP进行预编译的时候。如果你的WEB APP下都用了大量的第三方jar, 其大小超过了jvm默认的大小(4M)那么就会产生此错误信息了。</p>\n<p>##解决方案<br>手动设置MaxPermSize大小,修改TOMCAT_HOME/bin/catalina.sh<br>JAVA_OPTS=”-server -XX:PermSize=256M -XX:MaxPermSize=1024m”   </p>\n"},{"title":"和田市卫浴安装家具安装","date":"2017-06-09T09:28:00.000Z","_content":"#和田市卫浴安装工人\n朱力  \n电话：139 9943 3811\n介绍：从事家具（厨柜、床）、卫浴等工作20余年，目前在和田专业从事安具、厨卫安装、销售服务。\n\n#和田市卫浴家具安装工人\n朱力  \n电话：139 9943 3811\n介绍：从事家具（厨柜、床）、卫浴等工作20余年，目前在和田专业从事安具、厨卫安装、销售服务。\n\n\n# 可以接收什么类型的工作\n1.家具安装，包含床、柜子、书桌等。  \n2.卫浴安装  \n3.品牌家具销售  \n\n\n\n","source":"_posts/服务/和田市卫浴安装家具安装.md","raw":"---\ntitle: 和田市卫浴安装家具安装 \ndate: 2017-06-09 17:28:00\ntags: [服务]\ncategories: [服务]\n---\n#和田市卫浴安装工人\n朱力  \n电话：139 9943 3811\n介绍：从事家具（厨柜、床）、卫浴等工作20余年，目前在和田专业从事安具、厨卫安装、销售服务。\n\n#和田市卫浴家具安装工人\n朱力  \n电话：139 9943 3811\n介绍：从事家具（厨柜、床）、卫浴等工作20余年，目前在和田专业从事安具、厨卫安装、销售服务。\n\n\n# 可以接收什么类型的工作\n1.家具安装，包含床、柜子、书桌等。  \n2.卫浴安装  \n3.品牌家具销售  \n\n\n\n","slug":"服务/和田市卫浴安装家具安装","published":1,"updated":"2017-08-18T01:44:35.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjh7polto002sbp7r2dl44tpe","content":"<p>#和田市卫浴安装工人<br>朱力<br>电话：139 9943 3811<br>介绍：从事家具（厨柜、床）、卫浴等工作20余年，目前在和田专业从事安具、厨卫安装、销售服务。</p>\n<p>#和田市卫浴家具安装工人<br>朱力<br>电话：139 9943 3811<br>介绍：从事家具（厨柜、床）、卫浴等工作20余年，目前在和田专业从事安具、厨卫安装、销售服务。</p>\n<h1 id=\"可以接收什么类型的工作\"><a href=\"#可以接收什么类型的工作\" class=\"headerlink\" title=\"可以接收什么类型的工作\"></a>可以接收什么类型的工作</h1><p>1.家具安装，包含床、柜子、书桌等。<br>2.卫浴安装<br>3.品牌家具销售  </p>\n","site":{"data":{}},"excerpt":"","more":"<p>#和田市卫浴安装工人<br>朱力<br>电话：139 9943 3811<br>介绍：从事家具（厨柜、床）、卫浴等工作20余年，目前在和田专业从事安具、厨卫安装、销售服务。</p>\n<p>#和田市卫浴家具安装工人<br>朱力<br>电话：139 9943 3811<br>介绍：从事家具（厨柜、床）、卫浴等工作20余年，目前在和田专业从事安具、厨卫安装、销售服务。</p>\n<h1 id=\"可以接收什么类型的工作\"><a href=\"#可以接收什么类型的工作\" class=\"headerlink\" title=\"可以接收什么类型的工作\"></a>可以接收什么类型的工作</h1><p>1.家具安装，包含床、柜子、书桌等。<br>2.卫浴安装<br>3.品牌家具销售  </p>\n"},{"title":"TODO-企业安全组","date":"2016-12-31T16:00:00.000Z","_content":"#工作内容指导\n\n\n#安全小组工作范围\n\n\n#关注的领域\n\n\n#分类","source":"_posts/网络安全/企业安全组.md","raw":"---\ntitle: TODO-企业安全组\ndate: 2017-01-01 00:00:00\ntags: [网络安全,安全小组]\ncategories: [网络安全,安全小组]\n---\n#工作内容指导\n\n\n#安全小组工作范围\n\n\n#关注的领域\n\n\n#分类","slug":"网络安全/企业安全组","published":1,"updated":"2017-08-18T01:44:35.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjh7poltp002ubp7r1kahijdn","content":"<p>#工作内容指导</p>\n<p>#安全小组工作范围</p>\n<p>#关注的领域</p>\n<p>#分类</p>\n","site":{"data":{}},"excerpt":"","more":"<p>#工作内容指导</p>\n<p>#安全小组工作范围</p>\n<p>#关注的领域</p>\n<p>#分类</p>\n"},{"title":"安全组第一次会议提出的问题整理","date":"2017-05-16T02:30:00.000Z","_content":"创建安全组是个长期的、不断迭代的过程，目前我们公司系统百废末兴之际，各方面的系统也都在创建初期如果安全控制要求高，势必会影响部分工作进度；当然，从开始就严格控制，也会为后期不必要的重构提供有利条件。根据现有情况，可以优先进行投入少，回报多的工作，慢慢渗透。  \n\n# 事前-防范\n1.制定漏洞管理制度  \n为规范生产网和办公网安全漏洞发现、评估及处理，首先应该制定《漏洞管理制度》对漏洞评级，根据评级做不同和响应处理。    \n漏洞处理流程：发现漏洞->评估漏洞->如果是不可接受的风险对业务下线->修补漏洞->测试验收->上线  \n\n2.敏感数据保护  \n\t1) 银行卡可以显示首末4，手机号可以显示首3末4位，电话可以显示区号和末4位，身份证、邮箱、地址等。  \n\t2) 日志文件里的敏感信息  \n\t3) 用户名密码加密  \n\t4) 数据库防篡改签名  \n\t\n3.访问控制管理  \n\t1) 制定信息授权的策略，及访问权限的管理策略；  \n\t2) 规定每个用户或每组用户的访问控制规则和权力；  \n\t\n4.用户帐号及权限安全。  \n\t1) 最小权限原则  \n\t最小权限是指限定系统中每个用户所必须的最小访问权限的原则，设定账号访问权限，控制用户仅能够访问到工作需要的信息。  \n\t2) 职责分离原则  \n\t职责分离主要是防止单个用户利用其所拥有的多重权限进行舞弊、盗窃或其它的非法行为，或对工作错误和违规活动进行掩盖。  \n\t账号权限管理应按照职责分离的原则，确保不存在权限交叉而形成舞弊的可能  \n\t\n5. 产品安全功能设计规范   \n\t需要对产品的安全功能设计，如身份认证基本策略、用户登录失败/超时处理、账户信息输入防护、接口认证。  \n\t\n6.保密管理  \n     对文档、内容做好密级分类，哪些文件是对内，对外，或机密。  \n\n7.测试规范，  \n\t1) 依据《漏洞管理制度》判定漏洞等级，凡存在高危漏洞，除大领导特批外，禁止上线；低危或中危漏洞，可先上线后排期修复。  \n\t任何系统未经过黑盒和白盒测试，禁止上线，特批和紧急情况除外。   \n\t2) 为避免安全测试人员漏测，需定期对生产进行全面安全测试：  \n\t• 半年内至少执行一次全面渗透测试；  \n\t• 每个季度至少需要一次全面的ACL验证，系统底层漏洞检测；  \n\t\n8.研发规范：  \n\t1) 研发流程规范  \n\t2) 代码规范  \n\t3) 对技术的选型，比如组件、中间容器、中间件使用版本统一及安全、架构评审。  \n\t\n# 事中-应急的规定  \n  1) 建议成立应急指挥小组主要是在事故处理过程中进行信息收集、资源调度和沟通反馈信息。  应急处理流程如：信息收集->初步判断->事故处理->信息通报->事后处理-故障报告。  原则：以尽快恢复业务为第一优先。    \n  2) 故障时间之前做相关系统的上线或系统变更备份，在3分钟内无法定位故障原因的，立即执行回滚变更操作。  \n\n# 事后-总结  \n  1) 做好故障记录，主要包括事故、事故发生时间，事故恢复时间、持续时间，等级、影响产品、影响商户、影响交易、事故发现、事故类型、产生原因等   \n  2) 故障报告改进措施跟进 ，主要是改进措施、目前的完成情况，遗留问题  \n\n# 漏洞发现和处理流程  \n1.注册补天、漏洞盒子、乌云等国内外漏洞平台的企业帐号，针对企业贴平台会第一时间推送最新漏洞并进行安全指导。      \n2.可以在补天、漏洞盒子等平台以企业帐号方式创建安全测试平台，对外的白帽子增加奖励，鼓励大众参与众测。  \n\n# 安全网站参考\n安全资讯  \nhttp://www.freebuf.com/  \n\n乌云 (WooYun)(已停服)  \nhttp://wooyun.org/  \n可通过如下查乌云数据：http://wooyun.tangscan.cn/  \n\n补天  \nhttp://loudong.360.cn/  \n\n漏洞盒子  \nhttps://www.vulbox.com/bounties  \n\n阿里SRC  \nhttps://security.alibaba.com/  \n\n腾讯SRC  \nhttps://security.tencent.com/  \n\n\n\n","source":"_posts/网络安全/安全组第一次会议提出的问题整理.md","raw":"---\ntitle: 安全组第一次会议提出的问题整理\ndate: 2017-05-16 10:30:00\ntags: [网络安全,安全小组]\ncategories: [网络安全,安全小组]\n---\n创建安全组是个长期的、不断迭代的过程，目前我们公司系统百废末兴之际，各方面的系统也都在创建初期如果安全控制要求高，势必会影响部分工作进度；当然，从开始就严格控制，也会为后期不必要的重构提供有利条件。根据现有情况，可以优先进行投入少，回报多的工作，慢慢渗透。  \n\n# 事前-防范\n1.制定漏洞管理制度  \n为规范生产网和办公网安全漏洞发现、评估及处理，首先应该制定《漏洞管理制度》对漏洞评级，根据评级做不同和响应处理。    \n漏洞处理流程：发现漏洞->评估漏洞->如果是不可接受的风险对业务下线->修补漏洞->测试验收->上线  \n\n2.敏感数据保护  \n\t1) 银行卡可以显示首末4，手机号可以显示首3末4位，电话可以显示区号和末4位，身份证、邮箱、地址等。  \n\t2) 日志文件里的敏感信息  \n\t3) 用户名密码加密  \n\t4) 数据库防篡改签名  \n\t\n3.访问控制管理  \n\t1) 制定信息授权的策略，及访问权限的管理策略；  \n\t2) 规定每个用户或每组用户的访问控制规则和权力；  \n\t\n4.用户帐号及权限安全。  \n\t1) 最小权限原则  \n\t最小权限是指限定系统中每个用户所必须的最小访问权限的原则，设定账号访问权限，控制用户仅能够访问到工作需要的信息。  \n\t2) 职责分离原则  \n\t职责分离主要是防止单个用户利用其所拥有的多重权限进行舞弊、盗窃或其它的非法行为，或对工作错误和违规活动进行掩盖。  \n\t账号权限管理应按照职责分离的原则，确保不存在权限交叉而形成舞弊的可能  \n\t\n5. 产品安全功能设计规范   \n\t需要对产品的安全功能设计，如身份认证基本策略、用户登录失败/超时处理、账户信息输入防护、接口认证。  \n\t\n6.保密管理  \n     对文档、内容做好密级分类，哪些文件是对内，对外，或机密。  \n\n7.测试规范，  \n\t1) 依据《漏洞管理制度》判定漏洞等级，凡存在高危漏洞，除大领导特批外，禁止上线；低危或中危漏洞，可先上线后排期修复。  \n\t任何系统未经过黑盒和白盒测试，禁止上线，特批和紧急情况除外。   \n\t2) 为避免安全测试人员漏测，需定期对生产进行全面安全测试：  \n\t• 半年内至少执行一次全面渗透测试；  \n\t• 每个季度至少需要一次全面的ACL验证，系统底层漏洞检测；  \n\t\n8.研发规范：  \n\t1) 研发流程规范  \n\t2) 代码规范  \n\t3) 对技术的选型，比如组件、中间容器、中间件使用版本统一及安全、架构评审。  \n\t\n# 事中-应急的规定  \n  1) 建议成立应急指挥小组主要是在事故处理过程中进行信息收集、资源调度和沟通反馈信息。  应急处理流程如：信息收集->初步判断->事故处理->信息通报->事后处理-故障报告。  原则：以尽快恢复业务为第一优先。    \n  2) 故障时间之前做相关系统的上线或系统变更备份，在3分钟内无法定位故障原因的，立即执行回滚变更操作。  \n\n# 事后-总结  \n  1) 做好故障记录，主要包括事故、事故发生时间，事故恢复时间、持续时间，等级、影响产品、影响商户、影响交易、事故发现、事故类型、产生原因等   \n  2) 故障报告改进措施跟进 ，主要是改进措施、目前的完成情况，遗留问题  \n\n# 漏洞发现和处理流程  \n1.注册补天、漏洞盒子、乌云等国内外漏洞平台的企业帐号，针对企业贴平台会第一时间推送最新漏洞并进行安全指导。      \n2.可以在补天、漏洞盒子等平台以企业帐号方式创建安全测试平台，对外的白帽子增加奖励，鼓励大众参与众测。  \n\n# 安全网站参考\n安全资讯  \nhttp://www.freebuf.com/  \n\n乌云 (WooYun)(已停服)  \nhttp://wooyun.org/  \n可通过如下查乌云数据：http://wooyun.tangscan.cn/  \n\n补天  \nhttp://loudong.360.cn/  \n\n漏洞盒子  \nhttps://www.vulbox.com/bounties  \n\n阿里SRC  \nhttps://security.alibaba.com/  \n\n腾讯SRC  \nhttps://security.tencent.com/  \n\n\n\n","slug":"网络安全/安全组第一次会议提出的问题整理","published":1,"updated":"2017-08-18T01:44:35.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjh7poltq002ybp7r25nikbsl","content":"<p>创建安全组是个长期的、不断迭代的过程，目前我们公司系统百废末兴之际，各方面的系统也都在创建初期如果安全控制要求高，势必会影响部分工作进度；当然，从开始就严格控制，也会为后期不必要的重构提供有利条件。根据现有情况，可以优先进行投入少，回报多的工作，慢慢渗透。  </p>\n<h1 id=\"事前-防范\"><a href=\"#事前-防范\" class=\"headerlink\" title=\"事前-防范\"></a>事前-防范</h1><p>1.制定漏洞管理制度<br>为规范生产网和办公网安全漏洞发现、评估及处理，首先应该制定《漏洞管理制度》对漏洞评级，根据评级做不同和响应处理。<br>漏洞处理流程：发现漏洞-&gt;评估漏洞-&gt;如果是不可接受的风险对业务下线-&gt;修补漏洞-&gt;测试验收-&gt;上线  </p>\n<p>2.敏感数据保护<br>    1) 银行卡可以显示首末4，手机号可以显示首3末4位，电话可以显示区号和末4位，身份证、邮箱、地址等。<br>    2) 日志文件里的敏感信息<br>    3) 用户名密码加密<br>    4) 数据库防篡改签名  </p>\n<p>3.访问控制管理<br>    1) 制定信息授权的策略，及访问权限的管理策略；<br>    2) 规定每个用户或每组用户的访问控制规则和权力；  </p>\n<p>4.用户帐号及权限安全。<br>    1) 最小权限原则<br>    最小权限是指限定系统中每个用户所必须的最小访问权限的原则，设定账号访问权限，控制用户仅能够访问到工作需要的信息。<br>    2) 职责分离原则<br>    职责分离主要是防止单个用户利用其所拥有的多重权限进行舞弊、盗窃或其它的非法行为，或对工作错误和违规活动进行掩盖。<br>    账号权限管理应按照职责分离的原则，确保不存在权限交叉而形成舞弊的可能  </p>\n<ol start=\"5\">\n<li>产品安全功能设计规范<br> 需要对产品的安全功能设计，如身份认证基本策略、用户登录失败/超时处理、账户信息输入防护、接口认证。  </li>\n</ol>\n<p>6.保密管理<br>     对文档、内容做好密级分类，哪些文件是对内，对外，或机密。  </p>\n<p>7.测试规范，<br>    1) 依据《漏洞管理制度》判定漏洞等级，凡存在高危漏洞，除大领导特批外，禁止上线；低危或中危漏洞，可先上线后排期修复。<br>    任何系统未经过黑盒和白盒测试，禁止上线，特批和紧急情况除外。<br>    2) 为避免安全测试人员漏测，需定期对生产进行全面安全测试：<br>    • 半年内至少执行一次全面渗透测试；<br>    • 每个季度至少需要一次全面的ACL验证，系统底层漏洞检测；  </p>\n<p>8.研发规范：<br>    1) 研发流程规范<br>    2) 代码规范<br>    3) 对技术的选型，比如组件、中间容器、中间件使用版本统一及安全、架构评审。  </p>\n<h1 id=\"事中-应急的规定\"><a href=\"#事中-应急的规定\" class=\"headerlink\" title=\"事中-应急的规定\"></a>事中-应急的规定</h1><p>  1) 建议成立应急指挥小组主要是在事故处理过程中进行信息收集、资源调度和沟通反馈信息。  应急处理流程如：信息收集-&gt;初步判断-&gt;事故处理-&gt;信息通报-&gt;事后处理-故障报告。  原则：以尽快恢复业务为第一优先。<br>  2) 故障时间之前做相关系统的上线或系统变更备份，在3分钟内无法定位故障原因的，立即执行回滚变更操作。  </p>\n<h1 id=\"事后-总结\"><a href=\"#事后-总结\" class=\"headerlink\" title=\"事后-总结\"></a>事后-总结</h1><p>  1) 做好故障记录，主要包括事故、事故发生时间，事故恢复时间、持续时间，等级、影响产品、影响商户、影响交易、事故发现、事故类型、产生原因等<br>  2) 故障报告改进措施跟进 ，主要是改进措施、目前的完成情况，遗留问题  </p>\n<h1 id=\"漏洞发现和处理流程\"><a href=\"#漏洞发现和处理流程\" class=\"headerlink\" title=\"漏洞发现和处理流程\"></a>漏洞发现和处理流程</h1><p>1.注册补天、漏洞盒子、乌云等国内外漏洞平台的企业帐号，针对企业贴平台会第一时间推送最新漏洞并进行安全指导。<br>2.可以在补天、漏洞盒子等平台以企业帐号方式创建安全测试平台，对外的白帽子增加奖励，鼓励大众参与众测。  </p>\n<h1 id=\"安全网站参考\"><a href=\"#安全网站参考\" class=\"headerlink\" title=\"安全网站参考\"></a>安全网站参考</h1><p>安全资讯<br><a href=\"http://www.freebuf.com/\" target=\"_blank\" rel=\"noopener\">http://www.freebuf.com/</a>  </p>\n<p>乌云 (WooYun)(已停服)<br><a href=\"http://wooyun.org/\" target=\"_blank\" rel=\"noopener\">http://wooyun.org/</a><br>可通过如下查乌云数据：<a href=\"http://wooyun.tangscan.cn/\" target=\"_blank\" rel=\"noopener\">http://wooyun.tangscan.cn/</a>  </p>\n<p>补天<br><a href=\"http://loudong.360.cn/\" target=\"_blank\" rel=\"noopener\">http://loudong.360.cn/</a>  </p>\n<p>漏洞盒子<br><a href=\"https://www.vulbox.com/bounties\" target=\"_blank\" rel=\"noopener\">https://www.vulbox.com/bounties</a>  </p>\n<p>阿里SRC<br><a href=\"https://security.alibaba.com/\" target=\"_blank\" rel=\"noopener\">https://security.alibaba.com/</a>  </p>\n<p>腾讯SRC<br><a href=\"https://security.tencent.com/\" target=\"_blank\" rel=\"noopener\">https://security.tencent.com/</a>  </p>\n","site":{"data":{}},"excerpt":"","more":"<p>创建安全组是个长期的、不断迭代的过程，目前我们公司系统百废末兴之际，各方面的系统也都在创建初期如果安全控制要求高，势必会影响部分工作进度；当然，从开始就严格控制，也会为后期不必要的重构提供有利条件。根据现有情况，可以优先进行投入少，回报多的工作，慢慢渗透。  </p>\n<h1 id=\"事前-防范\"><a href=\"#事前-防范\" class=\"headerlink\" title=\"事前-防范\"></a>事前-防范</h1><p>1.制定漏洞管理制度<br>为规范生产网和办公网安全漏洞发现、评估及处理，首先应该制定《漏洞管理制度》对漏洞评级，根据评级做不同和响应处理。<br>漏洞处理流程：发现漏洞-&gt;评估漏洞-&gt;如果是不可接受的风险对业务下线-&gt;修补漏洞-&gt;测试验收-&gt;上线  </p>\n<p>2.敏感数据保护<br>    1) 银行卡可以显示首末4，手机号可以显示首3末4位，电话可以显示区号和末4位，身份证、邮箱、地址等。<br>    2) 日志文件里的敏感信息<br>    3) 用户名密码加密<br>    4) 数据库防篡改签名  </p>\n<p>3.访问控制管理<br>    1) 制定信息授权的策略，及访问权限的管理策略；<br>    2) 规定每个用户或每组用户的访问控制规则和权力；  </p>\n<p>4.用户帐号及权限安全。<br>    1) 最小权限原则<br>    最小权限是指限定系统中每个用户所必须的最小访问权限的原则，设定账号访问权限，控制用户仅能够访问到工作需要的信息。<br>    2) 职责分离原则<br>    职责分离主要是防止单个用户利用其所拥有的多重权限进行舞弊、盗窃或其它的非法行为，或对工作错误和违规活动进行掩盖。<br>    账号权限管理应按照职责分离的原则，确保不存在权限交叉而形成舞弊的可能  </p>\n<ol start=\"5\">\n<li>产品安全功能设计规范<br> 需要对产品的安全功能设计，如身份认证基本策略、用户登录失败/超时处理、账户信息输入防护、接口认证。  </li>\n</ol>\n<p>6.保密管理<br>     对文档、内容做好密级分类，哪些文件是对内，对外，或机密。  </p>\n<p>7.测试规范，<br>    1) 依据《漏洞管理制度》判定漏洞等级，凡存在高危漏洞，除大领导特批外，禁止上线；低危或中危漏洞，可先上线后排期修复。<br>    任何系统未经过黑盒和白盒测试，禁止上线，特批和紧急情况除外。<br>    2) 为避免安全测试人员漏测，需定期对生产进行全面安全测试：<br>    • 半年内至少执行一次全面渗透测试；<br>    • 每个季度至少需要一次全面的ACL验证，系统底层漏洞检测；  </p>\n<p>8.研发规范：<br>    1) 研发流程规范<br>    2) 代码规范<br>    3) 对技术的选型，比如组件、中间容器、中间件使用版本统一及安全、架构评审。  </p>\n<h1 id=\"事中-应急的规定\"><a href=\"#事中-应急的规定\" class=\"headerlink\" title=\"事中-应急的规定\"></a>事中-应急的规定</h1><p>  1) 建议成立应急指挥小组主要是在事故处理过程中进行信息收集、资源调度和沟通反馈信息。  应急处理流程如：信息收集-&gt;初步判断-&gt;事故处理-&gt;信息通报-&gt;事后处理-故障报告。  原则：以尽快恢复业务为第一优先。<br>  2) 故障时间之前做相关系统的上线或系统变更备份，在3分钟内无法定位故障原因的，立即执行回滚变更操作。  </p>\n<h1 id=\"事后-总结\"><a href=\"#事后-总结\" class=\"headerlink\" title=\"事后-总结\"></a>事后-总结</h1><p>  1) 做好故障记录，主要包括事故、事故发生时间，事故恢复时间、持续时间，等级、影响产品、影响商户、影响交易、事故发现、事故类型、产生原因等<br>  2) 故障报告改进措施跟进 ，主要是改进措施、目前的完成情况，遗留问题  </p>\n<h1 id=\"漏洞发现和处理流程\"><a href=\"#漏洞发现和处理流程\" class=\"headerlink\" title=\"漏洞发现和处理流程\"></a>漏洞发现和处理流程</h1><p>1.注册补天、漏洞盒子、乌云等国内外漏洞平台的企业帐号，针对企业贴平台会第一时间推送最新漏洞并进行安全指导。<br>2.可以在补天、漏洞盒子等平台以企业帐号方式创建安全测试平台，对外的白帽子增加奖励，鼓励大众参与众测。  </p>\n<h1 id=\"安全网站参考\"><a href=\"#安全网站参考\" class=\"headerlink\" title=\"安全网站参考\"></a>安全网站参考</h1><p>安全资讯<br><a href=\"http://www.freebuf.com/\" target=\"_blank\" rel=\"noopener\">http://www.freebuf.com/</a>  </p>\n<p>乌云 (WooYun)(已停服)<br><a href=\"http://wooyun.org/\" target=\"_blank\" rel=\"noopener\">http://wooyun.org/</a><br>可通过如下查乌云数据：<a href=\"http://wooyun.tangscan.cn/\" target=\"_blank\" rel=\"noopener\">http://wooyun.tangscan.cn/</a>  </p>\n<p>补天<br><a href=\"http://loudong.360.cn/\" target=\"_blank\" rel=\"noopener\">http://loudong.360.cn/</a>  </p>\n<p>漏洞盒子<br><a href=\"https://www.vulbox.com/bounties\" target=\"_blank\" rel=\"noopener\">https://www.vulbox.com/bounties</a>  </p>\n<p>阿里SRC<br><a href=\"https://security.alibaba.com/\" target=\"_blank\" rel=\"noopener\">https://security.alibaba.com/</a>  </p>\n<p>腾讯SRC<br><a href=\"https://security.tencent.com/\" target=\"_blank\" rel=\"noopener\">https://security.tencent.com/</a>  </p>\n"},{"title":"TODO-常用算法概述","date":"2016-12-31T16:00:00.000Z","_content":"#工作内容指导\n\n\n#安全小组工作范围\n\n\n#关注的领域\n\n\n#分类","source":"_posts/算法/常用算法概述.md","raw":"---\ntitle: TODO-常用算法概述\ndate: 2017-01-01 00:00:00\ntags: [软件工程,算法]\ncategories: [软件工程,算法]\n---\n#工作内容指导\n\n\n#安全小组工作范围\n\n\n#关注的领域\n\n\n#分类","slug":"算法/常用算法概述","published":1,"updated":"2017-08-18T01:44:35.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjh7polts002zbp7r8xjwd09g","content":"<p>#工作内容指导</p>\n<p>#安全小组工作范围</p>\n<p>#关注的领域</p>\n<p>#分类</p>\n","site":{"data":{}},"excerpt":"","more":"<p>#工作内容指导</p>\n<p>#安全小组工作范围</p>\n<p>#关注的领域</p>\n<p>#分类</p>\n"},{"title":"TODO-主要设计模式及简要介绍","date":"2017-05-02T05:48:00.000Z","_content":"#java设计模式分类\n\n#设计模式概述","source":"_posts/设计模式/主要设计模式及简要介绍.md","raw":"---\ntitle: TODO-主要设计模式及简要介绍\ndate: 2017-05-02 13:48:00\ntags: [软件工程,设计模式]\ncategories: [软件工程,设计模式]\n---\n#java设计模式分类\n\n#设计模式概述","slug":"设计模式/主要设计模式及简要介绍","published":1,"updated":"2017-08-18T01:44:35.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjh7poltt0032bp7rtru0cebl","content":"<p>#java设计模式分类</p>\n<p>#设计模式概述</p>\n","site":{"data":{}},"excerpt":"","more":"<p>#java设计模式分类</p>\n<p>#设计模式概述</p>\n"},{"title":"Oracle SQL基础知识","date":"2016-05-22T06:43:49.000Z","_content":"## Oracle SQL基本知识##\n### 安装数据库###\n#### 1）安装Oracle常用问题(常用”用户名/密码“规则)：####\n超级管理员：sys /change_on_install\n普通管理员：system/manager\n普通用户：scott/tiger----->默认是被锁定的\n大数据用户：sh/sh\n#### 2）SQL,DDL...####\nSQL：structured query language 结构化查询语言\n1.file(文件)\n\nSQL:DDL DML TCL DQL DCL\nDDL(data definition language 数据定义语言): column(列)--structure\ncreate table (创建表):\n列名 data type(数据类型) width(宽度)\nconstraint (约束)      alter table(修改表结构)           drop table(删除表)\n\nDML(data manipulation language 数据操作语言)\n:row(行)--data\ninsert 增       update 改            delete 删数据,删表里的记录\n\nTCL(transaction control language 事务控制语言)\ncommit(提交)         rollback(回滚)               savepoint(保留点)\n\nDQL(data query language 数据查询语言)\nselect\nDCL(data control language 数据控制语言)\ngrant(授权)  grant to       revoke(回收权限) revoke from \n#### 3）RDBMS关系型数据库管理系统####\nRDBMS(relationship database management system 关系型数据库管理系统) software(软件) --->(create database)database--->login in database (登录数据库系统 )--->用SQL操作table\n\ncreate database 创建空间存储表 (datafile 数据文件)\nlogin in database\n1 远程登录到数据库所在的机器上\n  192.168.0.20 192.168.0.23 192.168.0.26\nshell(终端) telnet 192.168.0.20  (跟操作系统建连接)\nlogin:openlab\npassword:open123\nsunv210% shell提示符,执行操作系统命令\n#### 4） 登录该机器上的数据库系统####\nsunv210% sqlplus (跟数据库建连接)\nEnter user-name: openlab\nEnter password:open123\nSQL>sqlplus openlab/open123\nSQL> 数据库提示符,执行SQL命令\n\n#### 5）登录的是哪个数据库####\necho $ORACLE_SID(环境变量)<---DBA(database administrator 数据库管理员)\n查看ORACLE_SID变量的取值,oracle提供\n通过设置ORACLE_SID变量,sqlplus就知道跟哪个数据库建连接.\nunix平台\n%c shell\n%echo $ORACLE_SID  (tarena)\n%setenv ORACLE_SID hiloo\n%setenv ORACLE_SID tarena\n\n$ b shell\n$ echo $ORACLE_SID  (tarena)\n$ ORACLE_SID=hiloo\n$ export ORACLE_SID\n\nwindows平台\nD:\\>set ORACLE_SID=hiloo (设置环境变量)\nD:\\>set ORACLE_SID (查看环境变量)\nORACLE_SID=hiloo\n##### 数据表信息：##### \ndept(表名) department 部门信息   列名\ndeptno 部门号  dname  部门名称      location 位置(地区)\ncreate table dept_hiloo\n(deptno  number(2), dname char(20),  location char(20));\ninsert into dept_hiloo values (10,'developer','beijing');\ninsert into dept_hiloo values (20,'account','shanghai');\ninsert into dept_hiloo values (30,'sales','guangzhou');\ninsert into dept_hiloo values  ( 40,'operations','tianjin');\ncommit;\ninsert成功后的提示:1 rows inserted\nemp(表名) employee 员工信息    列名\nempno 员工 ename 员工名字  job   职位   salary  月薪   bonus   奖金  \nhiredate  入职日期  mgr   manager 管理者    deptno  部门号\ncreate table emp_hiloo(\nempno number(4),\tename varchar2(20),  job  varchar2(15),  \nsalary number(7,2), bonus number(7,2),  hiredate date,\n mgr number(4),  deptno number(10));\nalter session set nls_date_language='american';\ninsert into emp_hiloo values (1001,'zhangwuji','Manager',10000,2000,'12-MAR-10',1005,10);\ninsert into emp_hiloo values (1001,'zhangwuji','Manager',10000,2000,'12-MAR-10',1005,10);\ninsert into emp_hiloo values (1002,'liucangsong','Analyst',8000,1000, '01-APR-11',1001,10);\ninsert into emp_hiloo values (1003,'liyi','Analyst',9000,1000,'11-APR-10',1001,10);\ninsertinto emp_hiloo values (1004,'guofurong','Programmer',5000,null,'01-JAN-11',1001,10);\ninsertintoemp_hiloo values (1005,'zhangsanfeng','President',15000,null,'15-MAY-08',null,20);\ninsert into emp_hiloo values (1006,'yanxiaoliu','Manager',5000,400,'01-FEB-09',1005,20);\ninsert into emp_hiloo values (1007,'luwushuang','clerk',3000,500,'01-FEB-06',1006,20);\ninsert into emp_hiloo values (1008,'huangrong','Manager',5000,500,'1-MAY-09',1005,30);\ninsert into emp_hiloo values (1009,'weixiaobao','salesman',4000,null,'20-FEB-09',1008,30);\ninsert into emp_hiloo values (1010,'guojing','salesman',4500,500,'10-MAY-09',1008,30);\n报错信息\nORA-00955: name is already used by an existing object(名字已经被一个存在的对象使用)\n错误：ORA-01843:无效的月份（在中文的plsql控制台上月份要写成’10-3月-02’这种形式，必须是一个数字和一个汉语月。也可以把日期改成英文环境，在执行插入前执行alter session set nls_date_language='american';就可以 了。\n\nDQL\nselect(选择)\n源表  结果集\n1 投影操作 select子句实现\n2 选择操作 where子句实现\n3 连接操作\n 1  select ename,salary*12 ann_sal(列别名)\n 2* from emp_hiloo\n\n单引号 表达字符串 ''\n双引号 表达列别名 \"\",别名中包含空格,大小写敏感\n\n\n##### 1）null值的理解##### \n1 null值出现在算术表达式中,结果必为null,null可以看作无穷大.\n2 函数(function) nvl功能空值转换函数\nnvl是函数名,p1,p2是参数,数据类型必须一致,函数本身有返回值\nnvl(p1,p2)  \nnvl函数实现:\nif p1 is null then\n   return p2;\nelse\n   return p1;\nend if;\n\n3 若有多个null值,distinct去重时,结果集保留一个null值.\n4 null = null 不成立 null <> null 不成立\n5 若用in运算符,集合中有null值跟没有null值结果一致的,结果集中不会出现跟null值有关的记录\n  若用not in运算符,集合中有null值,这个结果集不包含记录.no rows selected.\n##### 2）各个子句的功能##### \n1 select后面跟列名,列别名,函数,表达式\n2 select后面的distinct:去重\n3 where子句\n  where 条件表达式 (列名 比较运算符 值)  \n表达式 比较运算符 值(尽量不用,为了性能)\n  where子句中的列为字符类型,放值的位置上不加单引号或加双引号当列名解释,加单引号当字符串解释.\n  where子句中的列为字符类型,表达具体值时注意字符是大小写敏感的.\nSQL提供的四个比较运算符\n肯定形式\n   between and 区间,范围\n   in <=> =any  (= or = )(跟集合里的任意一个值相等就满足条件) 集合 离散值\n   = 单值运算符\n   in =any 多值运算符\n   like 像...一样\n   通配符: %表示0或任意多个字符 _任意一个字符\n   'S' 'S%' 'S_'\n   is null  如何判断一个列的取值是否为空\n否定形式\n= <> != ^=\nbetween and   not between and\nin\tnot in (<> and <>) <=> <>all(跟集合里的所有值都不能相等)\nlike \tnot like\nis null   is not null \n各个子句的执行顺序\nfrom-->where-->select\n##### 3）课堂练习##### \n1 列出每个员工的名字和他的工资\n  select ename,salary from emp_hiloo;\n2 列出每个员工的名字和他的职位\n  select ename,job from emp_hiloo;\n3 列出每个员工的名字和他的年薪\n select ename,salary*12 ann_sal from emp_hiloo;\n4 列出每个员工的名字和他一年的总收入\n  (salary+bonus)*12 (15000+null)*12=null\n  select ename,(salary+nvl(bonus,0))*12 tol_sal\n  from emp_hiloo;\n5 输出结果如下:\n  zhangwuji is in department 10.\n  liucangsong is in department 10.\n  .....\n  guojing is in department 30.\nselect ename||'is in department'||deptno||'.'employee from emp_hiloo;\n什么要加employee呢？Employee是列别名为了显示用的。\n6 列出该公司有哪些职位\n  select distinct(job) from emp_hiloo;\n  select distinct job from emp_hiloo;\n7 列出该公司不同的奖金\n  select distinct bonus from emp_hiloo;\n8 各个部门有哪些不同的职位?\n  select distinct deptno,job from emp_hiloo;\n  去重方式:deptno和job联合唯一.\n  distinct之后和from之前的所有列联合唯一.\ndistinct是保证每一行的唯一性而非某一列的唯一性，所以必须紧跟在select后面。\n所以distinct只能放在select后面，紧跟select不然会报缺失表达式错误。\n9 哪些员工的工资高于5000?\n  select ename,salary from emp_hiloo \n  where salary > 5000; \n10 列出员工工资高于5000的员工的年薪?\n  select ename,salary*12 from emp_hiloo\n  where salary > 5000;\n11 列出员工年薪高于60000的员工的年薪?\n  select ename,salary*12 from emp_hiloo\n  where salary*12> 60000;\n  select ename,salary*12 ann_sal from emp_hiloo\n  where ann_sal > 60000(错误的写法)\n  select ename,salary*12 from emp_hiloo\n  where salary > 5000;\n12 zhangwuji的年薪是多少?\nselect ename,salary*12 from emp_hiloo\nwhere ename='zhangwuji';\n  哪些员工的职位是Manager?\nselect ename,job from emp_hiloo\nwhere job='Manager';\n  哪些员工的职位是clerk?\n  select ename,job from emp_hiloo\n  where job = 'Manager'\n   select ename,job from emp_hiloo\n  where job = 'clerk'(效率高)\n  clerk的大小写不清楚\n  函数:upper(),lower()\n  select ename,job from emp_hiloo\n  where upper(job) = 'CLERK' (通用性好)\n13 员工工资在5000到10000之间的员工的年薪\n   select ename,salary*12\n   from emp_hiloo\n   where salary >= 5000 \n   and   salary <= 10000;\n   select ename,salary*12\n   from emp_hiloo\n   where salary between 5000 and 10000;\n14 哪些员工的工资是5000或10000.\n   select ename,salary\n   from emp_hiloo\n   where salary = 5000\n   or salary = 10000\n   select ename,salary\n   from emp_hiloo\n   where salary in (5000,10000)\n   select ename,salary\n   from emp_hiloo\n   where salary =any (5000,10000)\n15 哪个员工的名字的第二个字符是a.\n   select ename\n   from emp_hiloo\n   where ename like '_a%';\n16 哪个员工的名字的第二个字符是_.\n   select ename\n   from emp_hiloo\n   where ename like '_\\_%' escape '\\';\n   第一个_表示任意一个字符,代表通配符\n   \\_必须连起来看,表示下划线本身,escape定义哪个字符可以定义转义'\\'\n17 哪些员工没有奖金?\n   select ename,bonus\n   from emp_hiloo\n   where bonus is null\n18 哪些员工有奖金?\n   select ename,bonus\n   from emp_hiloo\n   where bonus is not null\n19哪些员工的工资不是5000也不是10000.\n  select ename,salary\n  from emp_hiloo\n  where salary not in (5000,10000);\n  select ename,salary\n  from emp_hiloo\n  where salary <> 5000\n  and salary <> 10000\n\ncreate table emp_hiloo\n( hiredate date）\ninsert into emp_hiloo values (1001,'zhangwuji','Manager',10000,2000,'12-MAR-10',1005,10);\n解决方案：\n\tinsert into emp_hiloo values (1001,'zhangwuji','Manager',10000,2000,'12-3月-10',1005,10);\n##### 更改字段名字(mysql、orcle)：##### \nOracle修改表\nalter table 表名 rename column 原名 to 新名；\nMysql:\nalter table 表名 change column(可写，可不写）原名 新名 字段类型；\n\nORA-00904：“ANN_SAL\":invalid identifier\n无效的标识符\n\n\nindex(索引) view(视图) sequence(顺序号/序列号) function(函数)\nsession altered.会话已更改\nset feed on可以设置一个，显示操作数\nconnet tiger重新建立连接  show user查看当前用户是谁。\nedit 用记事本编辑  /运行。\n###Function (单行、多行)###\n单行函数:表中的一列作为函数的参数,对于每一条记录函数都有一个返回值. \n例如:upper lower nvl\n多行函数：表中的一列作为函数的参数,将记录分组,对于每组数据函数返回一个值. \n例如:avg\n####1）单行函数####\n 根据处理参数的数据类型分为\n  ##### 1）字符函数:upper,lower##### \n   ##### 2）数值函数:##### \n     round 四舍五入\n     round(12.345,2)-->12.35\n     round(12.345,0)=round(12.345)-->12\n     round(12,345,-1)-->10\n     trunc 截取\n     trunc(12.345,2)-->12.34\n     trunc(12.345,0)=trunc(12.345)-->12\n     trunc(12,345,-1)-->10\n  ##### 3) 日期和日期函数##### \n    select sysdate from dual\n    06-SEP-12 DD-MON-RR \n    alter session set\n      nls_date_format = 'yyyy mm dd hh24:mi:ss'\n    session 会话 connection(连接)\n   日期类型的数据是用固定的字节7个字节来存储世纪,年,月,日,时,分,秒. 格式敏感\n   会话级 alter session set nls_date_format\n   语句级 select to_char(c1日期类型用7个字节来表达，日期类型的数据是用固定的字节7个字节来存储世纪，年，月，日，时，分，秒。四位年的前两位代表世纪20，后两位代表当前年12\n如果不想修改sql语句运行的话，就需要在执行该语句之前，使用alter session 命令将nls_date_language修改为american，如下：\nalter session set nls_date_language='american'    --以英语显示日期\n如果不想修改sql语句运行的话，就需要在执行该语句之前，使用alter session 命令将\n\n'01-JAN-08' 系统做了隐式数据类型转换,调用了to_date函数\n'2008-01-01',用户做显式数据类型转换,自己调用\nto_date('2008-01-01','yyyy-mm-dd'),第二个参数是对第一个参数的格式说明.\nto_char的返回类型是字符类型,把date转换成了字符串类型,所以参数的数据类型是date.to_char函数可以获得日期的任何一部分信息,比如年,月,日等.\nselect c1 from ... 系统做了隐式数据类型转换,调用了to_char函数\nselect to_char(c1,.. 用户做显式数据类型转换,自己调用to_char(c1,'yyyy-mm-dd'),第二个参数是对第一个参数的格式说明.\n日期的运算\n   日期可以加减一个数值,单位为天.\n   select sysdate-1,sysdate,sysdate+1 from dual\n两个日期相减\n   add_months 按月加 返回类型是date\n   add_months(sysdate,6)\n   select add_months(hiredate,6) from emp_hiloo\n   add_months(sysdate,-6)\n   months_between()  返回类型是number\n   months_between(sysdate,hiredate) 两个日期之间相差多少个月\nselect months_between(sysdate,hiredate) from emp_hiloo;\n   last_day(sysdate) 本月的最后一天\n##### 4) 转换函数#####    \n两个日期相减转换函数\nto_date  char-->date\nto_char  date-->char , number --> char\nto_number  char-->number\n##### 其他函数##### \ncoalesce 类似nvl(oracle专有)\nnvl(bonus,salary*0.1)\ncoalesce(bonus,salary*0.1,100)。输出所有员工的奖金，如果没有奖金就按工资的10%发放，如果奖金和工资都没有的临时工，就给100元。\n不同的记录处理方式不一样时,用case when.\ncase when 条件表达式 then 返回结果\nelse\n     返回结果\nend\n若没有else,当不匹配条件,表达式的返回值为null.\ncase deptno when 10 then(不建议该语法形式)\ndecode跟case when的功能类似.\ndecode(deptno,10,salary*1.1,\n              20,salary*1.2,\n              salary)\n若没有最后一个参数,函数的返回值为null.\nselect语句\norder by子句\nselect   from    where\norder by\norder by子句是select语句中的最后一个子句.\norder by salary 缺省是升序 asc\norder by salary desc 降序\norder by子句后面可以跟列名,表达式(函数),列别名,在select子句中的位置.\nORDER BY 子句\nORDER BY 语句用于对结果集进行排序。\nORDER BY 语句\nORDER BY 语句用于根据指定的列对结果集进行排序。\nORDER BY 语句默认按照升序对记录进行排序。\n如果您希望按照降序对记录进行排序，可以使用 DESC 关键字。\n原始的表 (用在例子中的)：\nOrders 表:\nCompany\tOrderNumber\nIBM\t3532\nW3School\t2356\nApple\t4698\nW3School\t6953\n实例 1\n以字母顺序显示公司名称：\nSELECT Company, OrderNumber FROM Orders ORDER BY Company\n结果：\nCompany\tOrderNumber\nApple\t4698\nIBM\t3532\nW3School\t6953\nW3School\t2356\n实例 2\n以字母顺序显示公司名称（Company），并以数字顺序显示顺序号（OrderNumber）：\nSELECT Company, OrderNumber FROM Orders ORDER BY Company, OrderNumber\n结果：\nCompany\tOrderNumber\nApple\t4698\nIBM\t3532\nW3School\t2356\nW3School\t6953\n实例 3\n以逆字母顺序显示公司名称：\nSELECT Company, OrderNumber FROM Orders ORDER BY Company DESC\n结果：\nCompany\tOrderNumber\nW3School\t6953\nW3School\t2356\nIBM\t3532\nApple\t4698\n实例 4\n以逆字母顺序显示公司名称，并以数字顺序显示顺序号：\nSELECT Company, OrderNumber FROM Orders ORDER BY Company DESC, OrderNumber ASC\n结果：\nCompany\tOrderNumber\nW3School\t2356\nW3School\t6953\nIBM\t3532\nApple\t4698\n注意：在以上的结果中有两个相等的公司名称 (W3School)。只有这一次，在第一列中有相同的值时，第二列是以升序排列的。如果第一列中有些值为 nulls 时，情况也是这样的。\n\n#### 2) 多行函数(哪两个函数里只能放number)####\navg()\t平均值  函数的参数只能是number\nsum()\t求和\t函数的参数只能是number\ncount()\t计数 函数的参数可以是number date 字符\n        count(*)统计记录,count(bonus)\nmax() 最大值 函数的参数可以是number date 字符\nmin() 最小值 函数的参数可以是number date 字符\n\n组函数的缺省处理方式是处理所有的非空值.\navg(bonus) 所有有奖金的员工的平均值\ncount(bonus) 有奖金的员工个数\n当所有的值都是null,count函数返回0,其他组函数返回null.\n\n#### 3) group by子句####\n若有group by子句,select后面跟组标识和组函数\n组标识指group by后面的内容\nfrom-->where-->group by-->select-->order by\n若没有group by子句,select后面只要有一个是组函数,其余的都得是组函数.\n\n#### having子句####\nselect deptno,round(avg(salary)) davg\nfrom emp_hiloo\ngroup by deptno\nhaving round(avg(salary))> 5000\n\nfrom-->where-->group by-->having-->select-->order by \n#### GROUP BY 语句####\nGROUP BY 语句用于结合合计函数，根据一个或多个列对结果集进行分组。\nSQL GROUP BY 语法\nSELECT column_name, aggregate_function(column_name)\nFROM table_name\nWHERE column_name operator value\nGROUP BY column_name\nSQL GROUP BY 实例\n我们拥有下面这个 \"Orders\" 表：\nO_Id\tOrderDate\tOrderPrice\tCustomer\n1\t2008/12/29\t1000\tBush\n2\t2008/11/23\t1600\tCarter\n3\t2008/10/05\t700\tBush\n4\t2008/09/28\t300\tBush\n5\t2008/08/06\t2000\tAdams\n6\t2008/07/21\t100\tCarter\n现在，我们希望查找每个客户的总金额（总订单）。我们想要使用 GROUP BY 语句对客户进行组合。\n我们使用下列 SQL 语句：\nSELECT Customer,SUM(OrderPrice) FROM Orders\nGROUP BY Customer\n结果集类似这样：\nCustomer\tSUM(OrderPrice)\nBush\t2000\nCarter\t1700\nAdams\t2000\n很棒吧，对不对？\n让我们看一下如果省略 GROUP BY 会出现什么情况：\nSELECT Customer,SUM(OrderPrice) FROM Orders\n结果集类似这样：\nCustomer\tSUM(OrderPrice)\nBush\t5700\nCarter\t5700\nBush\t5700\nBush\t5700\nAdams\t5700\nCarter\t5700\n上面的结果集不是我们需要的。\n那么为什么不能使用上面这条 SELECT 语句呢？解释如下：上面的 SELECT 语句指定了两列（Customer 和 SUM(OrderPrice)）。\"SUM(OrderPrice)\" 返回一个单独的值（\"OrderPrice\" 列的总计），而 \"Customer\" 返回 6 个值（每个值对应 \"Orders\" 表中的每一行）。因此，我们得不到正确的结果。不过，您已经看到了，GROUP BY 语句解决了这个问题。\nGROUP BY 一个以上的列\n我们也可以对一个以上的列应用 GROUP BY 语句，就像这样：\nSELECT Customer,OrderDate,SUM(OrderPrice) FROM Orders\nGROUP BY Customer,OrderDate\n#### 4) where和having比较####\n共同点:都执行在select之前,都有过滤功能\n区别\nwhere执行在having之前\nwhere过滤的是记录,任意列名都可以出现在where子句,单行函数可以用在where子句,组函数不能出现在where子句\nhaving过滤的是组,组标识可以出现在having子句,其他列名不行,组函数用于having子句,单行函数不可以.\n##### HAVING 子句##### \n在 SQL 中增加 HAVING 子句原因是，WHERE 关键字无法与合计函数一起使用。\nSQL HAVING 语法\nSELECT column_name, aggregate_function(column_name)\nFROM table_name\nWHERE column_name operator value\nGROUP BY column_name\nHAVING aggregate_function(column_name) operator value\nSQL HAVING 实例\n我们拥有下面这个 \"Orders\" 表：\nO_Id\tOrderDate\tOrderPrice\tCustomer\n1\t2008/12/29\t1000\tBush\n2\t2008/11/23\t1600\tCarter\n3\t2008/10/05\t700\tBush\n4\t2008/09/28\t300\tBush\n5\t2008/08/06\t2000\tAdams\n6\t2008/07/21\t100\tCarter\n现在，我们希望查找订单总金额少于 2000 的客户。\n我们使用如下 SQL 语句：\nSELECT Customer,SUM(OrderPrice) FROM Orders\nGROUP BY Customer\nHAVING SUM(OrderPrice)<2000\n结果集类似：\nCustomer\tSUM(OrderPrice)\nCarter\t1700\n现在我们希望查找客户 \"Bush\" 或 \"Adams\" 拥有超过 1500 的订单总金额。\n我们在 SQL 语句中增加了一个普通的 WHERE 子句：\nSELECT Customer,SUM(OrderPrice) FROM Orders\nWHERE Customer='Bush' OR Customer='Adams'\nGROUP BY Customer\nHAVING SUM(OrderPrice)>1500\n结果集：\nCustomer\tSUM(OrderPrice)\nBush\t2000\nAdams\t2000\n\n#### 5) DCL#### \nconnect openlab/open123\nselect count(*) from hiloo.emp_hiloo;\n\nconnect hiloo/hiloo123\ngrant select on emp_hiloo to openlab;\n\nconnect openlab/open123\nselect count(*) from hilool.emp_hiloo\n10rows selected\n\nconnect hiloo/hiloo123\nrevoke select on emp_hiloo from openlab;\n\nshow user\nselect count(*) from hiloo.emp_hiloo\n\ncreate synonym emp_hiloo for hiloo.emp_hiloo\n#### 6) 关于null值的讨论####\n1 case when在没有else和decode少一个参数时,返回null.\n2order by bonus,asc升序时null值在最后,desc降序时null在最前.\n3 组函数和null值的关系:1组函数的缺省处理方式是处理所有的非空值.2当所有的值都是null,count函数返回0,其他组函数返回null.\n4若group by的列有null值,所有的null值分在一组.\n课堂练习\n1将每个员工的工资涨12.34567%,用round和trunc分别实现\nselect ename,nvl(trunc(round(salary+salary*0.1234567,2),1),0.0) from emp_hiloo;//自己写的。\n2 将'2008-01-01'插入表中,\n  再将'2008 08 08 08:08:08'插入表中\ninsert into test values\n(to_date('01-JAN-08','DD-MON-RR'));\n\n3找出3月份入职的员工.\nselect ename,hiredate\nfrom emp_hiloo\nwhere to_char(hiredate,'mm') = '03';\nselect ename,hiredate\nfrom emp_hiloo\nwhere to_char(hiredate,'mm') = 3;//可以正常输出winXP下\n'03' = 3  ---> to_number('03') = 3\n字符   数值  缺省系统将字符转成数值\nselect ename,hiredate\nfrom emp_hiloo\nwhere to_char(hiredate,'fmmm') = '03';(错，未选定行，无输出)\n\nselect ename,hiredate\nfrom emp_hiloo\nwhere to_char(hiredate,'fmmm') = '3';(对)\n'03' = '3' (错)\nfm表示去掉前导0或去掉两边的空格.\n4 zhangsanfeng的mgr上显示boss,其他人不变.\nselect ename,empno,\n       nvl(to_char(mgr),'boss') mgr\nfrom emp_hiloo\n函数nvl（“1”，“2”）:如果字符串1是空，就返回字符串”2”\n#### 5十分钟之后####\n select sysdate,sysdate+1/144 from dual;\n解释：Oracle 里面,\n\nsysdate + 1 意思是 当前时间 + 1天\n\nsysdate + 1/24  意思是 当前时间 + 1/24天  也就是1小时后\n\nsysdate+1/144  意思是 当前时间 + 1/144天 （1/24*6）  也就是10分钟后\n 6 若员工是10部门的,工资涨10%,20部门工资涨20%,其他员工工资不变.\nselect ename,salary,\n       case when deptno = 10 then salary*1.1\n            when deptno = 20 then salary*1.2\n       else\n            salary\n       end new_sal\nfrom emp_hiloo;\n\nselect ename,salary, \n       decode(deptno,10,salary*1.1,\n                     20,salary*1.2,\n                     salary) new_sal\nfrom emp_hiloo;\n7 列出每个员工的年薪,按年薪降序排列.\nselect ename,salary*12\nfrom emp_hiloo\norder by salary desc (好)\nselect ename,salary*12\nfrom emp_hiloo\norder by salary*12 desc\nselect ename,salary*12 n_sal\nfrom emp_hiloo\norder by n_sal desc\n\nselect ename,salary*12 n_sal from emp_hiloo order by 2 desc;\nselect salary*12,ename n_sal from emp_hiloo order by 2 asc;\n8 列出员工的名字,部门号以及工资,按部门号从小到大的顺序,同一部门的工资按降序排列.\nselect ename,deptno,salary\nfrom emp_hiloo\norder by deptno,salary desc\n9 列出奖金的平均值,和,个数,最大值,最小值.\nAVG 函数返回数值列的平均值。NULL 值不包括在计算中\nselect avg(bonus),avg(nvl(bonus,0)),\n       sum(bonus), sum(nvl(bonus,0)),\n       count(bonus),count(nvl(bonus,0)),\n       max(bonus),max(nvl(bonus,0)),\n       min(bonus),min(nvl(bonus,0))\nfrom emp_hiloo\n10 各个部门的平均工资\nROUND 函数用于把数值字段舍入为指定的小数位数。\nselect deptno,round(avg(salary))\nfrom emp_hiloo\ngroup by deptno\n11 求10部门的平均工资,只显示平均工资\n   求10部门的平均工资,显示部门号,平均工资\n   select round(avg(salary))\n   from emp_hiloo\n   where deptno = 10\n   group by deptno\n\n   select max(deptno),round(avg(salary))\n   from emp_hiloo\n   where deptno = 10 \n12各个部门不同职位的平均工资\n   select deptno,job,round(avg(salary))\n   from emp_hiloo\n   group by deptno,job\n13 每种奖金有多少人?\n   select bonus,count(empno)\n   from emp_hiloo\n   group by bonus\n14 列出平均工资大于5000的部门的平均工资\n   select deptno,round(avg(salary)) \n   from emp_hiloo\n   group by deptno\n   having round(avg(salary)) > 5000\n15哪些员工的工资是最低的.\n  select ename from emp_hiloo\n  where salary = ( select min(salary)\n                   from emp_hiloo)\n报错信息\nORA-01861: literal does not match format string\n文字值不匹配格式串\nORA-01722: invalid number 无效的数值 to_number\nORA-00937: not a single-group group function 不是一个组函数\nORA-00979: not a GROUP BY expression 不是一个group by表达式 GROUP BY expression指跟在group by后面的东西(列名),称之为组标识\ndetail 细节 summary 聚合\n\n### 查询###\n子查询定义\n在SQL语句中嵌入select语句\ncreate table new_tabname\nas\nselect ename,salary*12 ann_sal from emp_hiloo;\n新表的结构由select后面的项来决定,new_table包含两列ename,ann_sal.\n\n#### 子查询####\n  非关联子查询\n    单列子查询\n    多列子查询\n  关联子查询\n\n##### 子查询执行##### \n非关联子查询\n子查询的表和主查询的表没有建关联\n先执行子查询(只执行一遍),当返回多条记录,系统会将自动去重的结果返回给主查询,再执行主查询.\n\n关联子查询\n子查询的表和主查询的表建关联.所谓建关联指主查询表里的列和子查询表里的列写成一个条件表达式.\n\n先执行主查询,判断表里的记录是否应该放入结果集.过程如下:拿到第一条记录,获得了各个列的值,将需要的列值带入子查询,执行后返回的结果再和主查询表里的列做比较,符合条件,该记录放入结果集,否则过滤掉.依次执行主查询表里的每条记录.子查询执行的次数由主查询表里的记录数决定.\n\n1) exists和not exists\nexists的执行过程\n从主查询表里拿到第一条记录,按子查询里的关联条件在子查询的表里看是否能找到匹配的记录,当找到第一条匹配的记录后,立即返回(即不需要找出所有匹配的记录),exists条件满足,主查询表里的该记录放入结果集.若按子查询里的关联条件将子查询\n表里的记录全部检查一遍后没有一条符合条件的记录,此时也返回, exists 条件不满足,主查询表里的该记录不能放入结果集,被过滤掉.\n\nselect ename from emp_afei o\nwhere exists\n             (select 1 from emp_afei i\n              where o.empno = i.mgr)\n\n##### 非关联子查询的分类##### \n单列子查询\nselect ename,salary\nfrom emp_hiloo \nwhere salary = (select min(salary)\n                from emp_hiloo \n                )\n多列子查询:按键值对比较\nselect ename,salary,deptno\nfrom emp_afei\nwhere (deptno,salary) in\n             (select deptno,round(avg(salary))\n              from emp_afei\n              group by deptno)\n\n2) 课堂练习\n1哪些人是领导?(非关联子查询)\n如果一个员工的empno能出现在mgr里就说明他是领导.\nselect ename\nfrom emp_hiloo\nwhere empno in (select mgr from emp_hiloo)\nselect ename\nfro emp_afei\nwhere empno in (1001,1005,1006,1008,null)\n2 哪些人是员工?\n他的empno绝对不能出现在mgr中,他的empno跟mgr的出现的所有的值不能相等. <>all\nselect ename\nfrom emp_hiloo\nwhere empno not in (select mgr from emp_hiloo)\nselect ename\nfro emp_afei\nwhere empno not in (1001,1005,1006,1008,null)\nselect ename\nfrom emp_hiloo\nwhere empno not in (select mgr from emp_hiloo\n                    where mgr is not null)\n\n3哪些部门的平均工资比30部门的平均工资高?\nselect deptno,round(avg(salary))\nfrom emp_hiloo\ngroup by deptno\nhaving round(avg(salary)) >\n                    (select round(avg(salary))\n                     from emp_hiloo\n                     where deptno = 30)\n4哪些员工的工资比zhangwuji的工资高?\nselect ename,salary\nfrom emp_afei\nwhere salary > (select salary from emp_afei\n                where ename = 'zhangwuji')\nERROR at line 3:\nORA-01427: single-row subquery returns more than one row\n单行子查询返回多条记录\n\n比所有人高 > (select max(salary))\n           >all\n比任意人高 > (select min(salary)\n           >any\n5哪些员工的工资等于本部门的平均工资?\nselect ename,salary,deptno\nfrom emp_afei\nwhere (deptno,salary) in\n             (select deptno,round(avg(salary))\n              from emp_afei\n              group by deptno)\n5哪些员工的工资比本部门的平均工资高?\nselect ename,salary,deptno\nfrom emp_afei o\nwhere salary > (select round(avg(salary))\n                from emp_afei i\n                where i.deptno = o.deptno)\n6哪些人是领导?(关联子查询)\nselect ename from emp_afei o\nwhere exists\n             (select 1 from emp_afei i\n              where o.empno = i.mgr)\n7哪些部门有员工?\nselect deptno,dname\nfrom dept_afei o\nwhere exists (select 1 from emp_afei i\n              where o.deptno = i.deptno)\n\n3) 课外练习day03am\n1 zhangwuji的领导是谁,显示名称?\n2 zangwuji领导谁,显示名称?\n3 列出devoleper部门有哪些职位?\n1) 课外练习day04am答案\n1 zhangwuji的领导是谁,显示名称?\n  select ename from emp_afei\n  where empno in \n\t\t(select mgr from emp_afei\n                 where ename = 'zhangwuji')\n\nzangwuji领导谁,显示名称?\n\n select ename from emp_afei\n where mgr in (select empno from emp_afei\n               where ename = 'zhangwuji')\n\n3 列出developer部门有哪些职位?\n  select distinct job from emp_afei\n  where deptno in \n           (select deptno from dept_afei\n            where dname = 'developer')\n\n2) 非关联子查询               \nexists和not exists\nnot exists的执行过程\n从主查询表里拿到第一条记录,按子查询里的关联条件在子查询的表里看是否能找到匹配的记录,当找到第一条匹配的记录后,立即返回(即不需要找出所有匹配的记录),not exists条件不满足,主查询表里的该记录不能放入结果集,被过滤掉.若按子查询里的关联条件将子查询表里的记录全部检查一遍后没有一条符合条件的记录,返回, not exists 条件满足,主查询表里的该记录放入结果集.\n\n对于exists和not exists,在子查询中找到第一条匹配的记录都会立即返回,exists将主查询表里的记录放入结果集,not exsits将主查询表里的记录过滤掉.\n对于exists和not exists,如果子查询没有返回任何记录,即扫描全部记录后没有一条符合条件的记录,都返回,exists将主查询表里的记录过滤掉,not exists将主查询表里的记录放入结果集. \nnot in ,<> all逻辑上跟not exists等价\nin ,=any逻辑上跟exists等价\n\n查询形式:集合操作\n把结果集作为一个集合,结果集必须是同构的,列的个数及数据类型一致\n\n3) 并集  union(去重)/union all(不去重)\nselect ename,deptno,salary,salary*1.1 new_sal\nfrom emp_afei\nwhere deptno = 10\nunion all\nselect ename,deptno,salary,salary*1.2 new_sal\nfrom emp_afei\nwhere deptno = 20\nunion all\nselect ename,deptno,salary,salary new_sal\nfrom emp_afei\nwhere deptno not in (10,20)\n\ncase when和decode可以实现类似功能.\n\n4) 交集  intersect(去重)\nselect job from emp_afei\nwhere deptno = 10\nintersect\nselect job from emp_afei\nwhere deptno = 20\n10部门和20部门都有的职位是哪些?\n\n5) 差  minus(去重)\nselect deptno from dept_afei\nminus\nselect deptno from emp_afei\n那些部门没有员工.\n\n6) 多表查询\n1) 交叉连接 cross join\nselect e.ename,d.dname\nfrom emp_afei e cross join dept_afei d\n结果集产生\n10*4=40,组合操作,笛卡尔积\n\n2) 内连接 inner join(匹配一个条件)\nselect e.ename,e.deptno,d.deptno,d.dname\nfrom emp_afei e join dept_afei d\nORA-00905: missing keyword(丢失关键字)\n\n如果把结果集的产生看成双层循环,驱动表是外层循环,匹配表是内层循环.\n对于内连接哪张表做驱动表,哪张表做匹配表产生出的结果集是一样的,不同的是性能.\n驱动表在匹配表的匹配情况如下:\n一条记录找到一条匹配\n一条记录找到多条匹配\n一条记录找不到任何匹配.\n内连接的核心是驱动表的记录要出现在结果集中必须在匹配表中能找到匹配的记录,否则该记录被过滤掉.\n\n3) 内连接查询形式\n等值连接 on e.deptno = d.deptno\n两张表有表述同一属性的列,两张表都有deptno列.\n自连接 on e.mgr = m.empno\n同一张表的不同列能写成一个表达式,即同一张表的两条记录之间有关系.通过给表起别名的方式,将同一张表的两条记录之间的关系转化成不同表的两条记录之间的关系.\n4) 外连接\n外连接 outer join(驱动表的记录一个都不能少的出现在结果集里)\nfrom t1 left join t2\non t1.c1 = t2.c2(t1驱动表,t2匹配表)\n外连接结果集=内连接的结果集+t1表中匹配不上的记录和t2表中的null记录的组合\nfrom t1 right join t2\non t1.c1 = t2.c2(t2驱动表,t1匹配表)\n外连接结果集=内连接的结果集+t2表中匹配不上的记录和t1表中的null记录的组合\nfrom t1 full join t2\non t1.c1 = t2.c2\n外连接结果集=内连接的结果集+t1表中匹配不上的记录和t2表中的null记录的组合+t2表中匹配不上的记录和t1表中的null记录的组合\n\n5) 外连接的应用场景\n1 某张表的记录全部出现在结果集中,包括匹配不上的.\nselect e.ename,nvl(m.ename,'Boss')\nfrom emp_afei e left join emp_afei m\non e.mgr = m.empno\n2解决否定问题,匹配不上的记录找出来(跟所有的记录都不匹配.)(not in/not exists)\n外连接 + where 匹配表.主键列 is null\nselect e.ename,d.dname\nfrom emp_afei e right join dept_afei d\non e.deptno = d.deptno\nwhere e.empno is null\n(解决结果集只包含匹配不上的记录.where子句执行在外连接之后)哪些部门没有员工\n\nselect d.dname\nfrom emp_afei e right join  dept_afei d\non e.deptno = d.deptno\nand e.ename = 'zhangwuji'\nwhere e.empno is null\n如果希望在外连接之前过滤匹配表用and子句,如果想在外连接之后通过匹配表里的列过滤外连接的结果集时候用where.\n过滤驱动表统计用where子句过滤.\n\n6) 课内练习\n1 哪些部门没有员工(not exists)\n  select dname from dept_afei o\n  where not exists \n        (select 1 from emp_afei i\n         where o.deptno = i.deptno)\n2 哪些人是员工?(not exists)\n  select ename from emp_afei o\n  where not exists \n            (select 1 from emp_afei i\n             where o.empno = i.mgr)\n他的empno和其他人的mgr相等是不可能存在的.即和所有人的mgr都不相等.\nnot in ,<> all逻辑上跟not exists等价\n3 列出哪些员工在北京地区上班?\n思路:确定表,两张表,匹配问题用inner join-->on(匹配条件)-->(对表是否过滤)\nselect e.ename,e.deptno,d.deptno,d.dname\nfrom emp_afei e join dept_afei d\non e.deptno = d.deptno\nand d.location = 'beijing'\n4zhangwuji在哪个地区上班?\nselect e.ename,d.dname,d.location\nfrom emp_afei e join dept_afei d\non e.deptno = d.deptno\nand e.ename = 'zhangwuji'\n5列出每个部门有哪些职位?部门名称,职位\n select distinct d.dname,e.job\n from emp_afei e join dept_afei d\n on e.deptno = d.deptno\n order by d.dname\n6各个部门的平均工资,列出部门名称,平均工资.\nselect d.dname,round(avg(e.salary)) savg\nfrom emp_afei e join dept_afei d\non e.deptno = d.deptno\ngroup by d.dname\nselect max(d.dname),round(avg(e.salary)) savg\nfrom emp_afei e join dept_afei d\non e.deptno = d.deptno\ngroup by d.deptno\nselect min(deptno),round(avg(salary))\nfrom emp_hiloo\nwhere deptno = 10\n7 列出每个员工的名字和他的领导的名字\nselect e.ename employee,\n       m.ename manager\nfrom emp_afei e join emp_afei m\non e.mgr = m.empno\n结果集是9条.\ne表中有10条记录,其中9条记录找到匹配,zhangsanfeng没匹配\nm表中有10条记录,其中4条记录找到匹配,4条记录是领导,6条记录找不到匹配,他们是员工.\nselect e.ename employee,\n       m.ename manager\nfrom emp_afei e join emp_afei m\non e.mgr = m.empno\nunion all\nselect ename,'Boss'\nfrom emp_afei\nwhere mgr is null\n\nselect e.ename employee,\n       decode(m.ename,e.ename,'Boss',\n                  m.ename)   manager\nfrom emp_afei e join emp_afei m\non nvl(e.mgr,e.empno) = m.empno\n\nselect e.ename,nvl(m.ename,'Boss')\nfrom emp_afei e left join emp_afei m\non e.mgr = m.empno\n10=9+1\n\n8哪些人是领导?\nselect distinct m.ename\nfrom emp_afei e join emp_afei m\non e.mgr = m.empno\n9哪些部门没有员工?\nselect e.ename,d.dname\nfrom emp_afei e right join dept_afei d\non e.deptno = d.deptno\nwhere e.empno is null\n(解决结果集只包含匹配不上的记录.where子句执行在外连接之后)\n11=10+1\n如果部门表里的某条记录的deptno在emp表找不到匹配,在内连接中,它被过滤,\ne表的empno的特性是唯一且非空的(主键约束),居然e.empno is null,说明null是外连接时为了驱动表中那条匹配不上的记录出现在结果集中,在匹配表中模拟的null记录.\n10哪些人是员工,哪些人不是领导?\nselect e.empno,m.ename\nfrom emp_afei e right join emp_afei m\non e.mgr = m.empno\nwhere e.empno is null\n\nfrom emp_afei e right join emp_afei m\n15=9+(10(m表中有10条记录)-4(m表中有4条匹配记录 ))\nfrom emp_afei e left join emp_afei m\n10(结果集)=9+(10(e表中有10条记录)-9(e表中有9条匹配记录))\n11 哪些部门没有叫zhangwuji的?\nselect d.dname\nfrom emp_afei e right join  dept_afei d\non e.deptno = d.deptno\nand e.ename = 'zhangwuji'\nwhere e.empno is null\n\n7) 课外练习(day04)(答案在Day05)\n1zhangwuji的领导是谁?(表连接)\n2zhangwuji领导谁?(表连接)\n3哪些人是领导?(in exists join)\n4哪些部门没有员工?(not in/not exists/outer join)\n5哪些人是员工,哪些人不是领导?(not in/not exists/outer join)\nDay05.txt\nGrade级别\nLowsal最低工资\nHisal最高工资\nCreate table salgrade_hiloo(\nGrade \n)\ncross join  inner join   outer join\ninner join(匹配)\n  等值连接\n  自连接\n  非等值连接\nouter join(匹配+不匹配)\n  等值连接\n\n  自连接\n  非等值连接\n\n所谓非等值连接表示两张表里的列不能写成等值表达式,而是写成between and之类.所以两个表之间有关系是指表里的列可以写成表达式,而不是等值表达式.\nsalgrade\ngrade  级别\nlowsal 最低工资\nhisal  最高工资\n\nfrom后面跟子查询\nemp,各个部门的平均工资dept_avgsal(depnto,avgsal)\nselect e.ename,e.salary,e.deptno\nfrom emp_afei e join\n      (select deptno,round(avg(salary)) avgsal\n       from emp_afei \n       group by deptno) a\non e.deptno = a.deptno\nand e.salary > a.avgsal\n\n各个部门的平均工资,列出部门名称,平均工资\nselect max(d.dname),round(avg(salary))\nfrom emp_afei e join dept_afei d\non e.deptno = d.deptno\ngroup by d.deptno\n\nselect d.dname,a.avgsal\nfrom dept_afei d join\n      (select deptno,round(avg(salary)) avgsal\n       from emp_afei \n       group by deptno) a\non d.deptno = a.depto\n\nDML\ninsert一条记录时,若某些列为null值,有哪些语法实现?\ninsert into tabname values (1,'a',null,sysdate)\ninsert into tabname(c1,c2,c4)\nvalues (1,'a',sysdate)\ninsert语句的两种语法形式?\ninsert into tabname values () insert一条记录\ninsert into tabname\nselect * from tabname1  insert多条记录\n连接图解：\n    \n\n### 数据类型###\n1) 课外练习答案day04\n1zhangwuji的领导是谁?(表连接)\n select m.ename\n from emp_afei e join emp_afei m\n on e.mgr = m.empno\n and m.ename = 'zhangwuji'\n2 zhanghangwuji领导谁?(表连接)\n select e.ename\n from emp_afei e join emp_afei m\n on e.mgr = m.empno\n and m.ename = 'zhangwuji'\n3哪些人是领导?(in exists join)\n select ename from emp_afei\n where empno in (select mgr from emp_afei)\n select ename from emp_afei o\n where exists\n            (select 1 from emp_afei i\n             where o.empno = i.mgr)\n select distinct m.ename\n from emp_afei e join emp_afei m\n on e.mgr = m.empno\n4哪些部门没有员工?(not in/not exists/outer join)\n select dname from dept_afei\n where deptno not in \n               (select deptno from emp_afei)\n select dname from dept_afei o\n where not exists \n             (select 1 from emp_afei i\n              where o.deptno = i.deptno)\n select d.dname\n from emp_afei e right join dept_afei d\n on e.deptno = d.deptno\n where e.empno is null\n5哪些人是员工,哪些人不是领导?(not in/not exists/outer join)\n select ename from emp_afei\n where empno not in (\n               select mgr from emp_afei\n               where mgr is not null)\n select ename from emp_afei o\n where not exists\n             (select 1 from emp_afei i\n              where o.empno = i.mgr)\n select m.ename\n from emp_afei e right join emp_afei m\n on e.mgr = m.empno\n where e.empno is null\ncross join (笛卡尔积)\n\nrownum 伪列,记录号\n若用rownum选择出记录,编号必须从1开始.\n分页问题\n第一页\nselect rownum,ename\nfrom emp_afei\nwhere rownum <= 3;\n第二页\nselect rn,ename\nfrom (\n      select rownum rn,ename\n      from emp_afei\n      where rownum <= 6)\nwhere rn between 4 and 6\n排名问题\n按工资排名的前三条记录\nselect rownum,ename,salary\nfrom emp_hiloo\nwhere rownum <=3\norder by salary desc;(错)\n\nselect rownum,ename,salary\nfrom ( select ename,salary\n       from emp_afei\n       order by salary desc)\nwhere rownum <= 3\n\nupdate语句的中set后面的=是什么含义?where后面的=是什么含义?\nset c1 = null (= 赋值)\nwhere c1 = null (= 等号)\n\nupdate和delete语句中的where子句是什么含义?\n用来确定对表里的哪些记录要进行update或delete操作,没有where子句多表里的所有记录update或delete\nupdate\nset\nwhere c1 = (select ...)\nrename 关键字 17\ncommit\n\n1011 abc 1000 10 'clerk'\nupdate 1001 1000-->2000\ndelete 1011\ncommit\n如何编写和运行一个sql脚本(文本文件)\n1 编辑文件\n在linux环境下已经编写好了test.sql,做一个鼠标右键的copy\n\n在20,23,26机器上,\nvi test.sql\n按a i o进入编辑模式,paste,按esc键,再按:wq!回车\n\n2 运行文件\nsun-server% sqlplus openlab/open123 @test.sql\n@表示运行\nSP2-0310: unable to open file \"test.sql\"在当前目录下没有test.sql文件\nsqlplus openlab/open123 ../test.sql\n\ncd ..\nsun-server% sqlplus openlab/open123 @test.sql\n\nSQL>@test.sql\n\n数据库对象 PL/SQL\ncreate or replace function test\ninsert into test values (1,1)\n            *\nERROR at line 1:\nORA-04044: procedure(存储过程), function(函数), package(包), or type is not allowed here\n\n事务(transaction 交易)\n事务里包含的DML语句\n事务的结束\ncommit 提交,(dml操作的数据入库了)\nrollback 回滚 撤销(DML操作被取消)\nsqlplus正常退出=commit\nDDL语句自动提交\n开始\n上一个事务的结束是下一个事务的开始.\n一致状态\n数据库的数据被事务改变.\noltp online transaction processing联机事务处理系统 高并发系统\n\n事务的隔离级别 read committed(读已经提交了的数据)\n\n\n如果不commit----->commit rollback\n1如果不commit,其他session是看不见你的操作\n2如果不commit,会阻塞操作同一条记录的事务(session),commit才能释放所有DML加的锁.\n3如果不commit,系统做DML操作,会将old data放入rollback segment(回滚段) ,所占用的回滚段资源不释放.\n\nDML系统会自动给表及表里的记录加锁\n表级共享锁\n行级排他锁\n\t表级共享锁 \t行级排他锁\ns1\tok\t\tok\ns2\tok\t\tenqueue wait\ns3\tok\t\tok\n\n执行DDL语句,系统自动加DDL排他锁\nSQL> drop table test purge;\ndrop table test purge\n           *\nERROR at line 1:\nORA-00054: resource busy(资源忙 test表) and acquire with NOWAIT specified (dml wait,ddl nowait 如果加不上锁,报错退出)\n\nDDL语句\n字符类型\nvarchar2,必须带宽度, 按字符串的实际长度存,本身的数据是变化,对空格敏感\nchar,可以不带宽度,缺省宽度是1,按字符串的定义长度存,本身的数据是固定长度的.对空格不敏感\n数值类型\n\nnumber类型\ncreate table test90\n(c1 number,\n c2 number(6),\n c3 number(4,2),\n c4 number(2,4),\n c5 number(3,-3))\n\n四舍五入\nnumber(6) 表示6为整数 999999\nnumber(4,2) 表示小数点后2位,整数位2位 99.99\nnumber(2,4) 表示小数点后4位,能填数字的位数是2位 0.0099\nnumber(3,-3) 999000 999123-->999000 \n                    999511-->报错\n\nuser_tables 是一张系统表,里面记录当前用户所有的表的信息,里面没有记录表的创建日期.\nuser_objects 是一张系统表,里面记录当前用户所有的数据库对象的信息.created的列记录数据库对象(如表)的创建日期.\nuser_tables和user_objects这两张表的关系体现在table_name和object_name都记录的是表名.\n\ndata block 数据块,操作数据的最小逻辑(物理)单元,最少读一个block的数据\n\nHWM high water mark 高水位线,表示曾经插入数据的最高位置\nFTS full table scan 全表扫描,把表里的所有记录读一遍,把HWM之下的所有data block读一遍\n\ntruncate table 释放空间,HWM下移\ndelete 不释放空间,HWM不动\n不适合用delete命令删大表.\n\n课内练习\n1 列出工资级别为3级,5级的员工\n  select e.ename,e.salary,s.grade\n  from emp_afei e join salgrade_afei s\n  on e.salary between s.lowsal and s.hisal\n  and s.grade in (3,5)\n2 列出各个工资级别有多少人?\n  select s.grade,count(e.empno)\n  from emp_afei e join salgrade_afei s\n  on e.salary between s.lowsal and s.hisal\n  group by s.grade\n  order by s.grade\n3 列出各个工资级别有多少人?(包含0级)\n  select s.grade,count(e.empno)\n  from emp_afei e right join salgrade_afei s\n  on e.salary between s.lowsal and s.hisal\n  group by s.grade\n  order by s.grade\n特别注意count不要写*或者s.grade\n\n课外练习day05\n1按工资排名的第4到第6名员工.\n###关键点###\n课外练习day05答案\n\n按工资排名的第4到第6名员工.\nselect rn,ename,salary\nfrom \n    (select rownum rn,ename,salary\n     from (select ename,salary\n           from emp_afei\n           order by salary desc)\n     where rownum <= 6\n    )\nwhere rn >= 4 \n\n####1）事务####\n\n####约束 constraint (安检)####\nprimary key(主键)\nforeign key(外键)\nunique key (唯一键)\nnot null(非空)\ncheck (检查)\n\n主键 (表中不会出现重复记录)\n列级约束\ncreate table test\n(c1 number(2) \n    constraint test_c1_pk primary key,\n c2 number(3))\n\n    constraint test_c1_pk primary key,\n               *\nERROR at line 3:\nORA-02264: name already used by an existing constraint (名字被存在的约束使用了)\n\nSQL> select table_name from user_constraints\n  2  where constraint_name = 'TEST_C1_PK';\n哪张表里有叫TEST_C1_PK这个约束名.\n\nORA-00001: unique constraint (HILOO(用户名) .TEST_C1_PK) violated(冲突)\n\nPK=UK + NN\n\n表级约束\ncreate table test(\nc1 number(2),\nc2 number,\nconstraint test_c1_pk primary key(c1)\n)\n表中有三列c1,c2,c3,c1和c2做成联合主键\ncreate table test(\nc1 number,\nc2 number,\nconstraint test_c1_c2_pk primary key(c1,c2),\nc3 number\n)\n没有constraint关键字,系统用自动起名字sys_c数字.\n\nnot null\ncreate table test\n(c1 number constraint test_c1_pk primary key,\n c2 number not null);\nnot null约束没有表级形式\n\nunique (pk)\n相同点:都要保证唯一性\n区别:uk允许为null,而且可以多个null值,一个表中只能有一个pk约束,可以有多个uk约束.\ncreate table test\n(c1 number constraint test_c1_pk primary key,\n c2 number constraint test_c2_uk unique)\n\ncreate table test(\nc1 number primary key,\nc2 number primary key,\nc3 number unique,\nc4 number unique)  (报错,一张表只能有一个primary key)\n\ncreate table test(\nc1 number constraint test_c1_pk primary key,\nc2 number constraint test_c2_uk unique,\nc3 number constraint test_c3_uk unique,\nc4 number ) \nc2上定义了一个唯一键 c3上定义了一个唯一键\n\ncreate table test(\nc1 number constraint test_c1_pk primary key,\nc2 number,\nc3 number,\nconstraint test_c2_c3_uk unique (c2,c3),\nc4 number)\nc2,c3联合唯一键\n\ncheck\ncreate table test(\nc1 number(3) constraint test_c1_ck\n             check (c1 > 100))\n\ncreate table test(\nc1 number(3),\nconstraint test_c1_ck check (c1 > 100))\n\n外键\nparent table(父表)上定义唯一列(pk/uk)\nchild table(子表)上定义外键列(fk)\n\n1 先create parent table(pk/uk),再create child table(fk)\n2 先insert into parent table,再insert into child table\n3 先delete from child table,再delete from parent table\n4 先drop child table,再drop parent table\n\nreference 引用\ncreate table parent\n(c1 number(3))\n\ncreate table child\n(c1 number(2) constraint child_c1_pk\n              primary key,\n c2 number(3) constraint child_c2_fk\n              references parent(c1))\n\n              references parent(c1))\n                                *\nERROR at line 5:\nORA-02270: no matching unique or primary key for this column-list\n在c1上没有定义uk或pk\n\nalter table parent\nadd constraint parent_c1_pk primary key(c1);\n给c1列增加主键约束\n\ninsert into child values (1,1)\nORA-02291: integrity constraint(完整性约束) (HILOO.CHILD_C2_FK) violated - parent key not found (父键值没发现)\n违反fk约束\n\ninsert into parent values (1);\ninsert into child values (1,1)\n\ndelete from parent where c1 = 1;\nORA-02292: integrity constraint (HILOO.CHILD_C2_FK) violated - child record\nfound(子记录被发现)\n\ndelete from child where c2 = 1;\ndelete from parent where c1 = 1;\n\ndrop table parent purge;\nORA-02449: unique/primary keys in table referenced by foreign keys\n在parent table上的pk/uk正在fk所引用\n\ndrop table child purge;\ndrop table parent purge;\n\ndrop table parent cascade constraints purge;\ncascade constraints 级联约束,child table本身没被删除,只是先把子表上的fk约束删除,再删parent table.\n\n表级约束\ncreate table child\n(c1 number(2) constraint child_c1_pk \n              primary key,\n c2 number(3),\n constraint child_c2_fk foreign key(c2)\n            references parent(c1)\n)\n\n外键约束另外两种定义方法\ncreate table child1\n(c1 number(2) constraint child1_c1_pk\n              primary key,\n c2 number(3) constraint child1_c2_fk\n              references parent(c1)\n              on delete cascade)\non delete cascade :级联删除会影响到对parent table的删除,先delete from child1,再delete from\nparent\n\ndelete from parent where c1 = 1;\ncreate table child2\n(c1 number(2) constraint child2_c1_pk\n              primary key,\n c2 number(3) constraint child2_c2_fk\n              references parent(c1)\n              on delete set null)\n\ndelete from parent where c1 = 1\n等价于以下操作\nSQL> update child2 set c2 = null\n  2  where c2 = 1;\nSQL> delete from parent where c1 = 1;\n\ntable \nDDL(数据类型 约束)\ntransaction (包含一堆DML)\n\n4000\n100 \n1000\n3100\n\n视图(view)\ncreate table test_t1\nas\nselect * from test\nwhere c1 = 1;\ncreate or replace view test_v1\nas\nselect * from test\nwhere c1 = 1;\ndesc test_v1\nselelct * from test_v1\n\ninsert into test values (1,3);\nselect * from test_v1 (1,3)\ninsert into test_v1 values (1,4)\nselect * from test_v1;\nselect * from test;\ninsert into test_v1 values (2,3);\nselect * from test_v1;(没有)\nselect * from test;(2,3)\n\ndrop table test purge;\nselect * from test_v1; \nSQL> desc test_v1\nERROR:\nORA-24372: invalid object for describe\n无法描述无效对象的结构\n\nSQL> select text from user_views\n  2  where view_name = 'TEST_V1';\n\nTEXT\n-----------------------------------------\nselect \"C1\",\"C2\" from test\nwhere c1 = 1\n\nview是一条select语句. select语句中包含的表为源表.通过view对源表做DML操作.\n\nview作用\n1 create view (deptno = 30)\n  grant view to user\n  限定用户查询的数据 子集\n2 简化查询语句\n3 create view beijing\n  as\n  select * from haidian\n  union all\n  select * from xicheng\n...\n  超集\nview的类型\n1 简单view (DML)\n2 复杂view  (不能DML)\n\ncreate or replace view avgscore_v\nselect s.name,a.avgscore\nfrom student s,\n     (select sid,round(avg(score)) avgscore\n      from stu_cour\n      group by sid) a\non s.id = a.sid\n\nview的约束\ncreate or replace view test_ck\nas\nselect * from test\nwhere c1 = 1\nwith check option;\nc1=2,违反where条件,2,3记录insert时报错\n\ncreate or replace view test_ro\nas\nselect * from test\nwhere c1 = 1\nwith read only;\n只读视图\n#### 索引####\ncreate index test_c1_idx\non test(c1);\n对索引不能做desc,select,DML操作\nrowid 代表一条记录的物理位置\n属于哪个数据对象(table)\n属于哪个数据文件的\n属于数据文件的第几个数据块\n属于数据块里的第几条记录\n#### index的结构####\nindex记录rowid\nindex的结构是一棵平衡树,有三类数据块组成,根节点,分支节点,叶子节点,数据块的数据是排序的.根节点和分支节点用于导航,里面记录下一级节点的物理位置以及该节点包含的数据范围.叶子节点里记录的是index entry(索引项),由key值和rowid组成,key值是建索引的列在每条记录上的取值,rowid是记录的物理位置,所有的叶子节点做成双向链表(升序/降序),适用于范围查询.\n用索引查询的路线图,从根节点出发,找相应的分支节点,叶子节点,最后要找到index entry,通过rowid定位\n表里所需要的数据块,避免了全表扫描.\n\n索引为什么提高查询效率,为select语句\n有效地降低了读取数据块的数量.读取数据块,一种从文件里读,物理读 physical read,一种从内存读,逻辑读 logical read /buffer gets\n\n建索引代价\n空间,DML变慢\n\n\n#### 哪些列适合建索引####\n1 经常出现在where子句的列\n2 pk/uk列\n3 经常出现在表连接的列\n4 fk列 parent.pk列 = child.fk列\n5 经常用于group by,order by的列\n7 where c1 is null(全表扫描),索引里不记null值,\n 该列有大量null值,找not null值用索引会快\n\n#### 索引类型####\n非唯一性索引,提高查询效率\n唯一性索引,解决唯一性.等价建唯一性约束\ncreate unique index test_c2_idx\non test(c2);\n\ninsert into test (c2) values (1)\n*\nERROR at line 1:\nORA-00001: unique constraint(HILOO.TEST_C2_IDX ) violated\n\n联合索引\ncreate index test_c1_c2_idx\non test(c1,c2)\nwhere c1 = 1 and c2 = 1\n\nselect ename from emp_hiloo\nwhere salary*12 > 60000\nwhere salary > 5000\n如果salary建索引,where salary > 5000(用),where salary*12 > 60000(不能用)\n\nwhere upper(ename) = 'ZHANGWUJI'\n\nwhere c1 = 100 c1是varchar2类型\nwhere to_number(c1) = 100\n\nwhere ename like 'a%'\nwhere substr(ename,1,1) = 'a'\n\ndeptno not in (20,30)\ndepotno in (10)\n\n#### 函数索引####\ncreate index test_c1_funidx\non test(round(c1));\nwhere round(c1) = 10\n\ncreate index student_name_idx\non student(name);\n\n#### 序列号####\nsequence\n为table里的主键服务,产生主键值\n唯一值产生器\nsequence_name.nextval\n\n为student表的id建sequence\ninsert into student(student_id.nextval...\n为course表的id建sequence\ninsert into course (course_id.nextval...\n\n创建序列如下：\ncreate sequence SEQ_TEST100\nminvalue 1\nmaxvalue 999999999999999999999999999\nstart with 11\nincrement by 1\ncache 10;\n\n函数\ncreate or replace function dept_avgsal\n(p_deptno number) --定义参数,数据类型不能有宽度\nreturn number    --定义函数的返回类型\nis\n  v_salary emp_hiloo.salary%type;     --变量v_salary 的类型跟表emp_hiloo里的salary的类型定义一致\nbegin\n  select round(avg(salary)) into v_salary\n  from emp_hiloo\n  where deptno = p_deptno;    --select当且仅当返回一条记录用select into语法,表示把select语句的执行结果赋值给v_salary\n  return v_salary;       --返回函数值 \nend;\n.不运行,回到SQL>下\n/表示运行\nshow error\nSQL> select dept_avgsal(10) from dual;\n\n\n\n\n\n练习\n用语法实现多对多关系\nstudent\nid pk\nname not null\n\ncourse\nid pk\nname not null\n\nstu_cour\nsid fk -->student(id)\ncid fk -->course(id)\npk(sid,cid)\nscore check [0,100](between and) \n#### 数据库日期比较####\nSql代码：\n1\ttimesten内存数据库比较日期是不是同一天,低效的方法  \n2\tto_char(create_date,'yyyymmdd')=to_char(sysdate NUMTODSINTERVAL(60*60*24,'SECOND'),'yyyymmdd')  \n3\toracle 数据库低效的方法  \n4\tto_char(create_date,'yyyymmdd')=to_char(sysdate-1,'yyyymmdd')   \n5\t2个数据库通用高效的方法  \n6\ttrunc(create_date)=trunc(sysdate)-NUMTODSINTERVAL(1,'DAY')  \n查找数据库里的表，索引等\n支持oracle的模糊查询如select * from user_tables where table_name like '%_PROJECT';查表名以PROJECT结尾的表（注：区别大小写）\n查所有用户的表在all_tables\n主键名称、外键在all_constraints\n索引在all_indexes\n但主键也会成为索引，所以主键也会在all_indexes里面。\n具体需要的字段可以DESC下这几个view，dba登陆的话可以把all换成dba。\n\n查询用户表的索引(非聚集索引):\nselect * from user_indexes\nwhere uniqueness = 'NONUNIQUE'\n\n查询用户表的主键(聚集索引):\nselect * from user_indexes\nwhere uniqueness = 'UNIQUE'\n\n1、\t查找表的所有索引（包括索引名，类型，构成列）：\nselect t.*,i.index_type from user_ind_columns t,user_indexes i where t.index_name = i.index_name and t.table_name = i.table_name and t.table_name = 要查询的表\n2、查找表的主键（包括名称，构成列）：\nselect cu.* from user_cons_columns cu, user_constraints au where cu.constraint_name = au.constraint_name and au.constraint_type = 'P' and au.table_name = 要查询的表\n3、查找表的唯一性约束（包括名称，构成列）：\nselect column_name from user_cons_columns cu, user_constraints au where cu.constraint_name = au.constraint_name and au.constraint_type = 'U' and au.table_name = 要查询的表\n4、查找表的外键（包括名称，引用表的表名和对应的键名，下面是分成多步查询）：\nselect * from user_constraints c where c.constraint_type = 'R' and c.table_name = 要查询的表\n查询外键约束的列名：\nselect * from user_cons_columns cl where cl.constraint_name = 外键名称\n查询引用表的键的列名：\nselect * from user_cons_columns cl where cl.constraint_name = 外键引用表的键名\n5、查询表的所有列及其属性\nselect t.*,c.COMMENTS from user_tab_columns t,user_col_comments c where t.table_name = c.table_name and t.column_name = c.column_name and t.table_name = 要查询的表\n####数据唯一Id：####\n1.\t用Oracle来生成UUID，做法很简单，如下：select sys_guid() from dual;数据类型是 raw(16) 有32个字符。\ncreate table test_guid3(\n    id varchar(50)\n)\nselect * from test_guid3;\ninsert into test_guid3(id) values(sys_guid())\n----------- ----------------------------------------\n       1000 7CD5B7769DF75CEFE034080020825436\n       1100 7CD5B7769DF85CEFE034080020825436\n       1200 7CD5B7769DF95CEFE034080020825436\n       1300 7CD5B7769DFA5CEFE034080020825436\n### 名词###\n\n#### Oracle的方案（Schema）和用户（User）的区别####\n \n从定义中我们可以看出方案（Schema）为数据库对象的集合，为了区分各个集合，我们需要给这个集合起个名字，这些名字就是我们在企业管理器的方案下看到的许多类似用户名的节点，这些类似用户名的节点其实就是一个schema，schema里面包含了各种对象如tables, views, sequences, stored procedures, synonyms, indexes, clusters, and database links。\n \n   一个用户一般对应一个schema,该用户的schema名等于用户名，并作为该用户缺省schema。这也就是我们在企业管理器的方案下看到schema名都为数据库用户名的原因。Oracle数据库中不能新创建一个schema，要想创建一个schema，只能通过创建一个用户的方法解决(Oracle中虽然有create schema语句，但是它并不是用来创建一个schema的)，在创建一个用户的同时为这个用户创建一个与用户名同名的schem并作为该用户的缺省shcema。即schema的个数同user的个数相同，而且schema名字同user名字一一对应并且相同，所有我们可以称schema为user的别名，虽然这样说并不准确，但是更容易理解一些。\n \n   一个用户有一个缺省的schema，其schema名就等于用户名，当然一个用户还可以使用其他的schema。如果我们访问一个表时，没有指明该表属于哪一个schema中的，系统就会自动给我们在表上加上缺省的sheman名。比如我们在访问数据库时，访问scott用户下的emp表，通过select * from emp; 其实，这sql语句的完整写法为select * from scott.emp。在数据库中一个对象的完整名称为schema.object，而不属user.object。类似如果我们在创建对象时不指定该对象的schema，在该对象的schema为用户的缺省schema。这就像一个用户有一个缺省的表空间，但是该用户还可以使用其他的表空间，如果我们在创建对象时不指定表空间，则对象存储在缺省表空间中，要想让对象存储在其他表空间中，我们需要在创建对象时指定该对象的表空间。\n \n   oracle中的schema就是指一个用户下所有对象的集合，schema本身不能理解成一个对象，oracle并没有提供创建schema的语法，schema也并不是在创建user时就创建，而是在该用户下创建第一个对象之后schema也随之产生，只要user下存在对象，schema就一定存在，user下如果不存在对象，schema也不存在；这一点类似于temp tablespace group，另外也可以通过oem来观察，如果创建一个新用户，该用户下如果没有对象则schema不存在，如果创建一个对象则和用户同名的schema也随之产生。\n####Oracle中User与Schema的简单理解####\n技术积累（126）  \n版权声明：本文为博主原创文章，未经博主允许不得转载。\n方案（Schema）为数据库对象的集合，为了区分各个集合，我们需要给这个集合起个名字，这些名字就是我们在企业管理器的方案下看到的许多类似用户名的节点，这些类似用户名的节点其实就是一个schema，schema里面包含了各种对象如tables, views, sequences, stored procedures, synonyms, indexes, clusters, and database links。  一个用户一般对应一个schema,该用户的schema名等于用户名，并作为该用户缺省schema。\nSQL Server中的Schema\nSQL Server中一个用户有一个缺省的schema，其schema名就等于用户名，这也就是我们在企业管理器的方案下看到schema名都为数据库用户名的原因。当然一个用户还可以使用其他的schema。如果我们访问一个表时，没有指明该表属于哪一个schema中的，系统就会自动给我们在表上加上缺省的sheman名。比如我们在访问数据库时，访问scott用户下的emp表，通过select * from emp; 其实，这sql语句的完整写法为select * from scott.emp。在数据库中一个对象的完整名称为schema.object，而不属user.object。类似如果我们在创建对象时不指定该对象的schema，在该对象的schema为用户的缺省schema。这就像一个用户有一个缺省的表空间，但是该用户还可以使用其他的表空间，如果我们在创建对象时不指定表空间，则对象存储在缺省表空间中，要想让对象存储在其他表空间中，我们需要在创建对象时指定该对象的表空间。\n\nOracle中的Schema\nOracle中的schema就是指一个用户下所有对象的集合，schema本身不能理解成一个对象，oracle并没有提供创建schema的语法，schema也并不是在创建user时就创建，而是在该用户下创建第一个对象之后schema也随之产生，只要user下存在对象，schema就一定存在，user下如果不存在对象，schema也不存在；如果创建一个新用户，该用户下如果没有对象则schema不存在，如果创建一个对象则和用户同名的schema也随之产生。实际上在使用上，shcema与user完全一样，没有什么区别，在出现schema名的地方也可以出现user名。\n\nTablspace \n逻辑上用来放objects,，这是个逻辑概念，本质上是一个或者多个数据文件的集合，物理上对应磁盘上的数据文件或者裸设备。\n\n数据文件\n具体存储数据的物理文件，是一个物理概念。一个数据文件只能属于一个表空间，一个表空间可以包含一个或多个数据文件。一个数据库由多个表空间组成，一个表空间只能属于一个数据库。\n\n下边是源自网络的一个形象的比喻\n我们可以把Database看作是一个大仓库，仓库分了很多很多的房间，Schema就是其中的房间，一个Schema代表一个房间，Table可以看作是每个Schema中的床，Table（床）被放入每个房间中，不能放置在房间之外，那岂不是晚上睡觉无家可归了，然后床上可以放置很多物品，就好比 Table上可以放置很多列和行一样，数据库中存储数据的基本单元是Table，现实中每个仓库放置物品的基本单位就是床， User就是每个Schema的主人，（所以Schema包含的是Object，而不是User），user和schema是一一对应的，每个user在没有特别指定下只能使用自己schema（房间）的东西，如果一个user想使用其他schema（房间）的东西，那就要看那个schema（房间）的user（主人）有没有给你这个权限了，或者看这个仓库的老大（DBA）有没有给你这个权限了。换句话说，如果你是某个仓库的主人，那么这个仓库的使用权和仓库中的所有东西都是你的（包括房间），你有完全的操作权，可以扔掉不用的东西从每个房间，也可以放置一些有用的东西到某一个房间，你还可以给每个User分配具体的权限，也就是他到某一个房间能做些什么，是只能看（Read-Only），还是可以像主人一样有所有的控制权（R/W），这个就要看这个User所对应的角色Role了。\n#### oracle的schema的含义####\n在现在做的Kraft Catalyst 项目中，Cransoft其中有一个功能就是schema refresh. 一直不理解schema什么意思，也曾经和同事讨论过，当时同事就给我举过一个例子，下面会详细说的。其实schema是Oracle中的，其他数据库中不知道有没有这个概念。\n首先,可以先看一下schema和user的定义：\nA schema is a collection of database objects (used by a user).\nSchema objects are the logical structures that directly refer to the database’s data.\nA user is a name defined in the database that can connect to and access objects.\nSchemas and users help database administrators manage database security.\n从中我们可以看出,schema为数据库对象的集合，为了区分各个集合，需要给这个集合起个名字，这些名字就是在企业管理器的方案下看到的许多类似用户名的节点，这些类似用户名的节点其实就是一个schema。\nschema里面包含了各种对象如tables, views, sequences, stored procedures, synonyms, indexes, clusters, and database links。\n一个用户一般对应一个schema，该用户的schema名等于用户名，并作为该用户缺省schema。这也就是在企业管理器的方案下看到schema名都为数据库用户名的原因。\nOracle数据库中不能新创建一个schema，要想创建一个schema，只能通过创建一个用户的方法解决(Oracle中虽然有create schema语句，但是它并不是用来创建一个schema的)。在创建一个用户的同时，为这个用户创建一个与用户名同名的schem并作为该用户的缺省 shcema。即schema的个数同user的个数相同，而且schema名字同user名字一一 对应并且相同，所有我们可以称schema为user的别名，虽然这样说并不准确，但是更容易理解一些。\n一个用户有一个缺省的schema，其schema名就等于用户名，当然一个用户还可以使用其他的schema。如果我们访问一个表时，没有指明该表属于 哪一个schema中的，系统就会自动给我们在表上加上缺省的sheman名。比如我们在访问数据库时，访问scott用户下的emp表，通过 select * from emp; 其实，这sql语句的完整写法为select * from scott.emp。在数据库中一个对象的完整名称为schema.object，而不属user.object。类似如果我们在创建对象时不指定该对象 的schema，在该对象的schema为用户的缺省schema。这就像一个用户有一个缺省的表空间，但是该用户还可以使用其他的表空间，如果我们在创 建对象时不指定表空间，则对象存储在缺省表空间中，要想让对象存储在其他表空间中，需要在创建对象时指定该对象的表空间。\n有人举了个很生动的例子，来说明Database、User、Schema、Tables、Col、Row等之间的关系\n“可以把Database看作是一个大仓库，仓库分了很多很多的房间，Schema就是其中的房间，一个Schema代表一个房间，Table可以看作是每个Schema中的床，Table（床）就被放入每个房间中，不能放置在房间之外，那岂不是晚上睡觉无家可归了。\n然后床上可以放置很多物品，就好比Table上可以放置很多列和行一样，数据库中存储数据的基本单元是Table，现实中每个仓库放置物品的基本单位就是床， User就是每个Schema的主人（所以Schema包含的是Object，而不是User）。\n其实User是对应与数据库的（即User是每个对应数据库的主人），既然有操作数据库（仓库）的权利，就肯定有操作数据库中每个Schema（房间）的 权利，就是说每个数据库映射的User有每个Schema（房间）的钥匙，换句话说，如果他是某个仓库的主人，那么这个仓库的使用权和仓库中的所有东西都 是他的（包括房间），他有完全的操作权，可以扔掉不用的东西从每个房间，也可以放置一些有用的东西到某一个房间。还可以给User分配具体的权限，也就是 他到某一个房间能做些什么，是只能看（Read-Only），还是可以像主人一样有所有的控制权（R/W），这个就要看这个User所对应的角色Role 了”\n从定义中我们可以看出schema为数据库对象的集合，为了区分各个集合，我们需要给这个集合起个名字，这些名字就是我们在企业管理器的方案下看到的许多类似用户名的节点，这些类似用户名的节点其实就是一个schema，schema里面包含了各种对象如tables, views, sequences, stored procedures, synonyms, indexes, clusters, and database links。\n一个用户一般对应一个schema,该用户的schema名等于用户名，并作为该用户缺省schema。这也就是我们在企业管理器的方案下看到schema名都为数据库用户名的原因。Oracle数据库中不能新创建一个schema，要想创建一个schema，只能通过创建一个用户的方法解决(Oracle中虽然有create schema语句，但是它并不是用来创建一个schema的)，在创建一个用户的同时为这个用户创建一个与用户名同名的schem并作为该用户的缺省shcema。即schema的个数同user的个数相同，而且schema名字同user名字一一 对应并且相同，所有我们可以称schema为user的别名，虽然这样说并不准确，但是更容易理解一些。\n一个用户有一个缺省的schema，其schema名就等于用户名，当然一个用户还可以使用其他的schema。如果我们访问一个表时，没有指明该表属于哪一个schema中的，系统就会自动给我们在表上加上缺省的sheman名。比如我们在访问数据库时，访问scott用户下的emp表，通过select * from emp; 其实，这sql语句的完整写法为select * from scott.emp。在数据库中一个对象的完整名称为schema.object，而不属user.object。类似如果我们在创建对象时不指定该对象的schema，在该对象的schema为用户的缺省schema。这就像一个用户有一个缺省的表空间，但是该用户还可以使用其他的表空间，如果我们在创建对象时不指定表空间，则对象存储在缺省表空间中，要想让对象存储在其他表空间中，我们需要在创建对象时指定该对象的表空间。\n咳，说了这么多，给大家举个例子，否则，一切枯燥无味！\nSQL> Gruant dba to scott\nSQL> create table test(name char(10));\nTable created.\nSQL> create table system.test(name char(10));\nTable created.\nSQL> insert into test values('scott');\n1 row created.\nSQL> insert into system.test values('system');\n1 row created.\nSQL> commit;\nCommit complete.\nSQL> conn system/manager\nConnected.\nSQL> select * from test;\n\nNAME\n----------\nsystem\nSQL> ALTER SESSION SET CURRENT_SCHEMA = scott; --改变用户缺省schema名\nSession altered.\nSQL> select * from test;\n\nNAME\n----------\nscott\nSQL> select owner ,table_name from dba_tables where table_name=upper('test');\nOWNER TABLE_NAME\n------------------------------ ------------------------------\nSCOTT TEST\nSYSTEM TEST\n--上面这个查询就是我说将schema作为user的别名的依据。实际上在使用上，shcema与user完全一样，没有什么区别，在出现schema名的地方也可以出现user名。\n表空间：\n一个表空间就是一片磁盘区域,他又一个或者多个磁盘文件组成,一个表空间可以容纳许多表、索引或者簇等  \n  每个表空间又一个预制的打一磁盘区域称为初始区间（initial   extent）用完这个区间厚在用下一个，知道用完表空间，这时候需要对表空间进行扩展，增加数据文件或者扩大已经存在的数据文件\n \n \n\ninstance是一大坨内存sga,pga....和后台的进程smon pmon.....组成的一个大的应用。\nschema就是一个用户和他下面的所有对象。。\ntablspace 逻辑上用来放objects.物理上对应磁盘上的数据文件或者裸设备。\n 在Oracle中，结合逻辑存储与物理存储的概念，我们可以这样来理解数据库、表空间、SCHEMA、数据文件这些概念：\n      数据库是一个大圈，里面圈着的是表空间，表空间里面是数据文件，那么schema是什么呢？schema是一个逻辑概念，是一个集合，但schema并不是一个对象，oracle也并没有提供创建schema的语法。\nschema：\n      一般而言，一个用户就对应一个schema,该用户的schema名等于用户名，并作为该用户缺省schema，用户是不能创建schema的，schema在创建用户的时候创建，并可以指定用户的各种表空间（这点与PostgreSQL是不同，PostgreSQL是可以创建schema并指派给某个用户）。当前连接到数据库上的用户创建的所有数据库对象默认都属于这个schema（即在不指明schema的情况下），比如若用户scott连接到数据库，然后create table test(id int not null)创建表，那么这个表被创建在了scott这个schema中；但若这样create kanon.table test(id int not null)的话，这个表被创建在了kanon这个schema中，当然前提是权限允许。\n      创建用户的方法是这样的：\n      create user 用户名 identified by 密码 \n      default tablespace 表空间名 \n      temporary tablespace 表空间名 \n      quota 限额  （建议创建的时候指明表空间名）\n由此来看，schema是一个逻辑概念。\n      但一定要注意一点：schema好像并不是在创建user时就创建的，而是在该用户创建了第一个对象之后才将schema真正创建的，只有user下存在对象，他对应的schema才会存在，如果user下不存在任何对象了，schema也就不存在了；\n \n数据库：\n     在oracle中，数据库是由表空间来组成的，而表空间里面是具体的物理文件---数据文件。我们可以创建数据库并为其指定各种表空间。\n \n表空间：\n     这是个逻辑概念，本质上是一个或者多个数据文件的集合。\n \n数据文件：\n     具体存储数据的物理文件，是一个物理概念。\n     一个数据文件只能属于一个表空间，一个表空间可以包含一个或多个数据文件。一个数据库由多个表空间组成，一个表空间只能属于一个数据库。\n","source":"_posts/oracle/Oracle SQL基础知识.md","raw":"---\ntitle: Oracle SQL基础知识\ndate: 2016-05-22 14:43:49\ntags: [oracle,数据库]\ncategories: [数据库,oracle]\n---\n## Oracle SQL基本知识##\n### 安装数据库###\n#### 1）安装Oracle常用问题(常用”用户名/密码“规则)：####\n超级管理员：sys /change_on_install\n普通管理员：system/manager\n普通用户：scott/tiger----->默认是被锁定的\n大数据用户：sh/sh\n#### 2）SQL,DDL...####\nSQL：structured query language 结构化查询语言\n1.file(文件)\n\nSQL:DDL DML TCL DQL DCL\nDDL(data definition language 数据定义语言): column(列)--structure\ncreate table (创建表):\n列名 data type(数据类型) width(宽度)\nconstraint (约束)      alter table(修改表结构)           drop table(删除表)\n\nDML(data manipulation language 数据操作语言)\n:row(行)--data\ninsert 增       update 改            delete 删数据,删表里的记录\n\nTCL(transaction control language 事务控制语言)\ncommit(提交)         rollback(回滚)               savepoint(保留点)\n\nDQL(data query language 数据查询语言)\nselect\nDCL(data control language 数据控制语言)\ngrant(授权)  grant to       revoke(回收权限) revoke from \n#### 3）RDBMS关系型数据库管理系统####\nRDBMS(relationship database management system 关系型数据库管理系统) software(软件) --->(create database)database--->login in database (登录数据库系统 )--->用SQL操作table\n\ncreate database 创建空间存储表 (datafile 数据文件)\nlogin in database\n1 远程登录到数据库所在的机器上\n  192.168.0.20 192.168.0.23 192.168.0.26\nshell(终端) telnet 192.168.0.20  (跟操作系统建连接)\nlogin:openlab\npassword:open123\nsunv210% shell提示符,执行操作系统命令\n#### 4） 登录该机器上的数据库系统####\nsunv210% sqlplus (跟数据库建连接)\nEnter user-name: openlab\nEnter password:open123\nSQL>sqlplus openlab/open123\nSQL> 数据库提示符,执行SQL命令\n\n#### 5）登录的是哪个数据库####\necho $ORACLE_SID(环境变量)<---DBA(database administrator 数据库管理员)\n查看ORACLE_SID变量的取值,oracle提供\n通过设置ORACLE_SID变量,sqlplus就知道跟哪个数据库建连接.\nunix平台\n%c shell\n%echo $ORACLE_SID  (tarena)\n%setenv ORACLE_SID hiloo\n%setenv ORACLE_SID tarena\n\n$ b shell\n$ echo $ORACLE_SID  (tarena)\n$ ORACLE_SID=hiloo\n$ export ORACLE_SID\n\nwindows平台\nD:\\>set ORACLE_SID=hiloo (设置环境变量)\nD:\\>set ORACLE_SID (查看环境变量)\nORACLE_SID=hiloo\n##### 数据表信息：##### \ndept(表名) department 部门信息   列名\ndeptno 部门号  dname  部门名称      location 位置(地区)\ncreate table dept_hiloo\n(deptno  number(2), dname char(20),  location char(20));\ninsert into dept_hiloo values (10,'developer','beijing');\ninsert into dept_hiloo values (20,'account','shanghai');\ninsert into dept_hiloo values (30,'sales','guangzhou');\ninsert into dept_hiloo values  ( 40,'operations','tianjin');\ncommit;\ninsert成功后的提示:1 rows inserted\nemp(表名) employee 员工信息    列名\nempno 员工 ename 员工名字  job   职位   salary  月薪   bonus   奖金  \nhiredate  入职日期  mgr   manager 管理者    deptno  部门号\ncreate table emp_hiloo(\nempno number(4),\tename varchar2(20),  job  varchar2(15),  \nsalary number(7,2), bonus number(7,2),  hiredate date,\n mgr number(4),  deptno number(10));\nalter session set nls_date_language='american';\ninsert into emp_hiloo values (1001,'zhangwuji','Manager',10000,2000,'12-MAR-10',1005,10);\ninsert into emp_hiloo values (1001,'zhangwuji','Manager',10000,2000,'12-MAR-10',1005,10);\ninsert into emp_hiloo values (1002,'liucangsong','Analyst',8000,1000, '01-APR-11',1001,10);\ninsert into emp_hiloo values (1003,'liyi','Analyst',9000,1000,'11-APR-10',1001,10);\ninsertinto emp_hiloo values (1004,'guofurong','Programmer',5000,null,'01-JAN-11',1001,10);\ninsertintoemp_hiloo values (1005,'zhangsanfeng','President',15000,null,'15-MAY-08',null,20);\ninsert into emp_hiloo values (1006,'yanxiaoliu','Manager',5000,400,'01-FEB-09',1005,20);\ninsert into emp_hiloo values (1007,'luwushuang','clerk',3000,500,'01-FEB-06',1006,20);\ninsert into emp_hiloo values (1008,'huangrong','Manager',5000,500,'1-MAY-09',1005,30);\ninsert into emp_hiloo values (1009,'weixiaobao','salesman',4000,null,'20-FEB-09',1008,30);\ninsert into emp_hiloo values (1010,'guojing','salesman',4500,500,'10-MAY-09',1008,30);\n报错信息\nORA-00955: name is already used by an existing object(名字已经被一个存在的对象使用)\n错误：ORA-01843:无效的月份（在中文的plsql控制台上月份要写成’10-3月-02’这种形式，必须是一个数字和一个汉语月。也可以把日期改成英文环境，在执行插入前执行alter session set nls_date_language='american';就可以 了。\n\nDQL\nselect(选择)\n源表  结果集\n1 投影操作 select子句实现\n2 选择操作 where子句实现\n3 连接操作\n 1  select ename,salary*12 ann_sal(列别名)\n 2* from emp_hiloo\n\n单引号 表达字符串 ''\n双引号 表达列别名 \"\",别名中包含空格,大小写敏感\n\n\n##### 1）null值的理解##### \n1 null值出现在算术表达式中,结果必为null,null可以看作无穷大.\n2 函数(function) nvl功能空值转换函数\nnvl是函数名,p1,p2是参数,数据类型必须一致,函数本身有返回值\nnvl(p1,p2)  \nnvl函数实现:\nif p1 is null then\n   return p2;\nelse\n   return p1;\nend if;\n\n3 若有多个null值,distinct去重时,结果集保留一个null值.\n4 null = null 不成立 null <> null 不成立\n5 若用in运算符,集合中有null值跟没有null值结果一致的,结果集中不会出现跟null值有关的记录\n  若用not in运算符,集合中有null值,这个结果集不包含记录.no rows selected.\n##### 2）各个子句的功能##### \n1 select后面跟列名,列别名,函数,表达式\n2 select后面的distinct:去重\n3 where子句\n  where 条件表达式 (列名 比较运算符 值)  \n表达式 比较运算符 值(尽量不用,为了性能)\n  where子句中的列为字符类型,放值的位置上不加单引号或加双引号当列名解释,加单引号当字符串解释.\n  where子句中的列为字符类型,表达具体值时注意字符是大小写敏感的.\nSQL提供的四个比较运算符\n肯定形式\n   between and 区间,范围\n   in <=> =any  (= or = )(跟集合里的任意一个值相等就满足条件) 集合 离散值\n   = 单值运算符\n   in =any 多值运算符\n   like 像...一样\n   通配符: %表示0或任意多个字符 _任意一个字符\n   'S' 'S%' 'S_'\n   is null  如何判断一个列的取值是否为空\n否定形式\n= <> != ^=\nbetween and   not between and\nin\tnot in (<> and <>) <=> <>all(跟集合里的所有值都不能相等)\nlike \tnot like\nis null   is not null \n各个子句的执行顺序\nfrom-->where-->select\n##### 3）课堂练习##### \n1 列出每个员工的名字和他的工资\n  select ename,salary from emp_hiloo;\n2 列出每个员工的名字和他的职位\n  select ename,job from emp_hiloo;\n3 列出每个员工的名字和他的年薪\n select ename,salary*12 ann_sal from emp_hiloo;\n4 列出每个员工的名字和他一年的总收入\n  (salary+bonus)*12 (15000+null)*12=null\n  select ename,(salary+nvl(bonus,0))*12 tol_sal\n  from emp_hiloo;\n5 输出结果如下:\n  zhangwuji is in department 10.\n  liucangsong is in department 10.\n  .....\n  guojing is in department 30.\nselect ename||'is in department'||deptno||'.'employee from emp_hiloo;\n什么要加employee呢？Employee是列别名为了显示用的。\n6 列出该公司有哪些职位\n  select distinct(job) from emp_hiloo;\n  select distinct job from emp_hiloo;\n7 列出该公司不同的奖金\n  select distinct bonus from emp_hiloo;\n8 各个部门有哪些不同的职位?\n  select distinct deptno,job from emp_hiloo;\n  去重方式:deptno和job联合唯一.\n  distinct之后和from之前的所有列联合唯一.\ndistinct是保证每一行的唯一性而非某一列的唯一性，所以必须紧跟在select后面。\n所以distinct只能放在select后面，紧跟select不然会报缺失表达式错误。\n9 哪些员工的工资高于5000?\n  select ename,salary from emp_hiloo \n  where salary > 5000; \n10 列出员工工资高于5000的员工的年薪?\n  select ename,salary*12 from emp_hiloo\n  where salary > 5000;\n11 列出员工年薪高于60000的员工的年薪?\n  select ename,salary*12 from emp_hiloo\n  where salary*12> 60000;\n  select ename,salary*12 ann_sal from emp_hiloo\n  where ann_sal > 60000(错误的写法)\n  select ename,salary*12 from emp_hiloo\n  where salary > 5000;\n12 zhangwuji的年薪是多少?\nselect ename,salary*12 from emp_hiloo\nwhere ename='zhangwuji';\n  哪些员工的职位是Manager?\nselect ename,job from emp_hiloo\nwhere job='Manager';\n  哪些员工的职位是clerk?\n  select ename,job from emp_hiloo\n  where job = 'Manager'\n   select ename,job from emp_hiloo\n  where job = 'clerk'(效率高)\n  clerk的大小写不清楚\n  函数:upper(),lower()\n  select ename,job from emp_hiloo\n  where upper(job) = 'CLERK' (通用性好)\n13 员工工资在5000到10000之间的员工的年薪\n   select ename,salary*12\n   from emp_hiloo\n   where salary >= 5000 \n   and   salary <= 10000;\n   select ename,salary*12\n   from emp_hiloo\n   where salary between 5000 and 10000;\n14 哪些员工的工资是5000或10000.\n   select ename,salary\n   from emp_hiloo\n   where salary = 5000\n   or salary = 10000\n   select ename,salary\n   from emp_hiloo\n   where salary in (5000,10000)\n   select ename,salary\n   from emp_hiloo\n   where salary =any (5000,10000)\n15 哪个员工的名字的第二个字符是a.\n   select ename\n   from emp_hiloo\n   where ename like '_a%';\n16 哪个员工的名字的第二个字符是_.\n   select ename\n   from emp_hiloo\n   where ename like '_\\_%' escape '\\';\n   第一个_表示任意一个字符,代表通配符\n   \\_必须连起来看,表示下划线本身,escape定义哪个字符可以定义转义'\\'\n17 哪些员工没有奖金?\n   select ename,bonus\n   from emp_hiloo\n   where bonus is null\n18 哪些员工有奖金?\n   select ename,bonus\n   from emp_hiloo\n   where bonus is not null\n19哪些员工的工资不是5000也不是10000.\n  select ename,salary\n  from emp_hiloo\n  where salary not in (5000,10000);\n  select ename,salary\n  from emp_hiloo\n  where salary <> 5000\n  and salary <> 10000\n\ncreate table emp_hiloo\n( hiredate date）\ninsert into emp_hiloo values (1001,'zhangwuji','Manager',10000,2000,'12-MAR-10',1005,10);\n解决方案：\n\tinsert into emp_hiloo values (1001,'zhangwuji','Manager',10000,2000,'12-3月-10',1005,10);\n##### 更改字段名字(mysql、orcle)：##### \nOracle修改表\nalter table 表名 rename column 原名 to 新名；\nMysql:\nalter table 表名 change column(可写，可不写）原名 新名 字段类型；\n\nORA-00904：“ANN_SAL\":invalid identifier\n无效的标识符\n\n\nindex(索引) view(视图) sequence(顺序号/序列号) function(函数)\nsession altered.会话已更改\nset feed on可以设置一个，显示操作数\nconnet tiger重新建立连接  show user查看当前用户是谁。\nedit 用记事本编辑  /运行。\n###Function (单行、多行)###\n单行函数:表中的一列作为函数的参数,对于每一条记录函数都有一个返回值. \n例如:upper lower nvl\n多行函数：表中的一列作为函数的参数,将记录分组,对于每组数据函数返回一个值. \n例如:avg\n####1）单行函数####\n 根据处理参数的数据类型分为\n  ##### 1）字符函数:upper,lower##### \n   ##### 2）数值函数:##### \n     round 四舍五入\n     round(12.345,2)-->12.35\n     round(12.345,0)=round(12.345)-->12\n     round(12,345,-1)-->10\n     trunc 截取\n     trunc(12.345,2)-->12.34\n     trunc(12.345,0)=trunc(12.345)-->12\n     trunc(12,345,-1)-->10\n  ##### 3) 日期和日期函数##### \n    select sysdate from dual\n    06-SEP-12 DD-MON-RR \n    alter session set\n      nls_date_format = 'yyyy mm dd hh24:mi:ss'\n    session 会话 connection(连接)\n   日期类型的数据是用固定的字节7个字节来存储世纪,年,月,日,时,分,秒. 格式敏感\n   会话级 alter session set nls_date_format\n   语句级 select to_char(c1日期类型用7个字节来表达，日期类型的数据是用固定的字节7个字节来存储世纪，年，月，日，时，分，秒。四位年的前两位代表世纪20，后两位代表当前年12\n如果不想修改sql语句运行的话，就需要在执行该语句之前，使用alter session 命令将nls_date_language修改为american，如下：\nalter session set nls_date_language='american'    --以英语显示日期\n如果不想修改sql语句运行的话，就需要在执行该语句之前，使用alter session 命令将\n\n'01-JAN-08' 系统做了隐式数据类型转换,调用了to_date函数\n'2008-01-01',用户做显式数据类型转换,自己调用\nto_date('2008-01-01','yyyy-mm-dd'),第二个参数是对第一个参数的格式说明.\nto_char的返回类型是字符类型,把date转换成了字符串类型,所以参数的数据类型是date.to_char函数可以获得日期的任何一部分信息,比如年,月,日等.\nselect c1 from ... 系统做了隐式数据类型转换,调用了to_char函数\nselect to_char(c1,.. 用户做显式数据类型转换,自己调用to_char(c1,'yyyy-mm-dd'),第二个参数是对第一个参数的格式说明.\n日期的运算\n   日期可以加减一个数值,单位为天.\n   select sysdate-1,sysdate,sysdate+1 from dual\n两个日期相减\n   add_months 按月加 返回类型是date\n   add_months(sysdate,6)\n   select add_months(hiredate,6) from emp_hiloo\n   add_months(sysdate,-6)\n   months_between()  返回类型是number\n   months_between(sysdate,hiredate) 两个日期之间相差多少个月\nselect months_between(sysdate,hiredate) from emp_hiloo;\n   last_day(sysdate) 本月的最后一天\n##### 4) 转换函数#####    \n两个日期相减转换函数\nto_date  char-->date\nto_char  date-->char , number --> char\nto_number  char-->number\n##### 其他函数##### \ncoalesce 类似nvl(oracle专有)\nnvl(bonus,salary*0.1)\ncoalesce(bonus,salary*0.1,100)。输出所有员工的奖金，如果没有奖金就按工资的10%发放，如果奖金和工资都没有的临时工，就给100元。\n不同的记录处理方式不一样时,用case when.\ncase when 条件表达式 then 返回结果\nelse\n     返回结果\nend\n若没有else,当不匹配条件,表达式的返回值为null.\ncase deptno when 10 then(不建议该语法形式)\ndecode跟case when的功能类似.\ndecode(deptno,10,salary*1.1,\n              20,salary*1.2,\n              salary)\n若没有最后一个参数,函数的返回值为null.\nselect语句\norder by子句\nselect   from    where\norder by\norder by子句是select语句中的最后一个子句.\norder by salary 缺省是升序 asc\norder by salary desc 降序\norder by子句后面可以跟列名,表达式(函数),列别名,在select子句中的位置.\nORDER BY 子句\nORDER BY 语句用于对结果集进行排序。\nORDER BY 语句\nORDER BY 语句用于根据指定的列对结果集进行排序。\nORDER BY 语句默认按照升序对记录进行排序。\n如果您希望按照降序对记录进行排序，可以使用 DESC 关键字。\n原始的表 (用在例子中的)：\nOrders 表:\nCompany\tOrderNumber\nIBM\t3532\nW3School\t2356\nApple\t4698\nW3School\t6953\n实例 1\n以字母顺序显示公司名称：\nSELECT Company, OrderNumber FROM Orders ORDER BY Company\n结果：\nCompany\tOrderNumber\nApple\t4698\nIBM\t3532\nW3School\t6953\nW3School\t2356\n实例 2\n以字母顺序显示公司名称（Company），并以数字顺序显示顺序号（OrderNumber）：\nSELECT Company, OrderNumber FROM Orders ORDER BY Company, OrderNumber\n结果：\nCompany\tOrderNumber\nApple\t4698\nIBM\t3532\nW3School\t2356\nW3School\t6953\n实例 3\n以逆字母顺序显示公司名称：\nSELECT Company, OrderNumber FROM Orders ORDER BY Company DESC\n结果：\nCompany\tOrderNumber\nW3School\t6953\nW3School\t2356\nIBM\t3532\nApple\t4698\n实例 4\n以逆字母顺序显示公司名称，并以数字顺序显示顺序号：\nSELECT Company, OrderNumber FROM Orders ORDER BY Company DESC, OrderNumber ASC\n结果：\nCompany\tOrderNumber\nW3School\t2356\nW3School\t6953\nIBM\t3532\nApple\t4698\n注意：在以上的结果中有两个相等的公司名称 (W3School)。只有这一次，在第一列中有相同的值时，第二列是以升序排列的。如果第一列中有些值为 nulls 时，情况也是这样的。\n\n#### 2) 多行函数(哪两个函数里只能放number)####\navg()\t平均值  函数的参数只能是number\nsum()\t求和\t函数的参数只能是number\ncount()\t计数 函数的参数可以是number date 字符\n        count(*)统计记录,count(bonus)\nmax() 最大值 函数的参数可以是number date 字符\nmin() 最小值 函数的参数可以是number date 字符\n\n组函数的缺省处理方式是处理所有的非空值.\navg(bonus) 所有有奖金的员工的平均值\ncount(bonus) 有奖金的员工个数\n当所有的值都是null,count函数返回0,其他组函数返回null.\n\n#### 3) group by子句####\n若有group by子句,select后面跟组标识和组函数\n组标识指group by后面的内容\nfrom-->where-->group by-->select-->order by\n若没有group by子句,select后面只要有一个是组函数,其余的都得是组函数.\n\n#### having子句####\nselect deptno,round(avg(salary)) davg\nfrom emp_hiloo\ngroup by deptno\nhaving round(avg(salary))> 5000\n\nfrom-->where-->group by-->having-->select-->order by \n#### GROUP BY 语句####\nGROUP BY 语句用于结合合计函数，根据一个或多个列对结果集进行分组。\nSQL GROUP BY 语法\nSELECT column_name, aggregate_function(column_name)\nFROM table_name\nWHERE column_name operator value\nGROUP BY column_name\nSQL GROUP BY 实例\n我们拥有下面这个 \"Orders\" 表：\nO_Id\tOrderDate\tOrderPrice\tCustomer\n1\t2008/12/29\t1000\tBush\n2\t2008/11/23\t1600\tCarter\n3\t2008/10/05\t700\tBush\n4\t2008/09/28\t300\tBush\n5\t2008/08/06\t2000\tAdams\n6\t2008/07/21\t100\tCarter\n现在，我们希望查找每个客户的总金额（总订单）。我们想要使用 GROUP BY 语句对客户进行组合。\n我们使用下列 SQL 语句：\nSELECT Customer,SUM(OrderPrice) FROM Orders\nGROUP BY Customer\n结果集类似这样：\nCustomer\tSUM(OrderPrice)\nBush\t2000\nCarter\t1700\nAdams\t2000\n很棒吧，对不对？\n让我们看一下如果省略 GROUP BY 会出现什么情况：\nSELECT Customer,SUM(OrderPrice) FROM Orders\n结果集类似这样：\nCustomer\tSUM(OrderPrice)\nBush\t5700\nCarter\t5700\nBush\t5700\nBush\t5700\nAdams\t5700\nCarter\t5700\n上面的结果集不是我们需要的。\n那么为什么不能使用上面这条 SELECT 语句呢？解释如下：上面的 SELECT 语句指定了两列（Customer 和 SUM(OrderPrice)）。\"SUM(OrderPrice)\" 返回一个单独的值（\"OrderPrice\" 列的总计），而 \"Customer\" 返回 6 个值（每个值对应 \"Orders\" 表中的每一行）。因此，我们得不到正确的结果。不过，您已经看到了，GROUP BY 语句解决了这个问题。\nGROUP BY 一个以上的列\n我们也可以对一个以上的列应用 GROUP BY 语句，就像这样：\nSELECT Customer,OrderDate,SUM(OrderPrice) FROM Orders\nGROUP BY Customer,OrderDate\n#### 4) where和having比较####\n共同点:都执行在select之前,都有过滤功能\n区别\nwhere执行在having之前\nwhere过滤的是记录,任意列名都可以出现在where子句,单行函数可以用在where子句,组函数不能出现在where子句\nhaving过滤的是组,组标识可以出现在having子句,其他列名不行,组函数用于having子句,单行函数不可以.\n##### HAVING 子句##### \n在 SQL 中增加 HAVING 子句原因是，WHERE 关键字无法与合计函数一起使用。\nSQL HAVING 语法\nSELECT column_name, aggregate_function(column_name)\nFROM table_name\nWHERE column_name operator value\nGROUP BY column_name\nHAVING aggregate_function(column_name) operator value\nSQL HAVING 实例\n我们拥有下面这个 \"Orders\" 表：\nO_Id\tOrderDate\tOrderPrice\tCustomer\n1\t2008/12/29\t1000\tBush\n2\t2008/11/23\t1600\tCarter\n3\t2008/10/05\t700\tBush\n4\t2008/09/28\t300\tBush\n5\t2008/08/06\t2000\tAdams\n6\t2008/07/21\t100\tCarter\n现在，我们希望查找订单总金额少于 2000 的客户。\n我们使用如下 SQL 语句：\nSELECT Customer,SUM(OrderPrice) FROM Orders\nGROUP BY Customer\nHAVING SUM(OrderPrice)<2000\n结果集类似：\nCustomer\tSUM(OrderPrice)\nCarter\t1700\n现在我们希望查找客户 \"Bush\" 或 \"Adams\" 拥有超过 1500 的订单总金额。\n我们在 SQL 语句中增加了一个普通的 WHERE 子句：\nSELECT Customer,SUM(OrderPrice) FROM Orders\nWHERE Customer='Bush' OR Customer='Adams'\nGROUP BY Customer\nHAVING SUM(OrderPrice)>1500\n结果集：\nCustomer\tSUM(OrderPrice)\nBush\t2000\nAdams\t2000\n\n#### 5) DCL#### \nconnect openlab/open123\nselect count(*) from hiloo.emp_hiloo;\n\nconnect hiloo/hiloo123\ngrant select on emp_hiloo to openlab;\n\nconnect openlab/open123\nselect count(*) from hilool.emp_hiloo\n10rows selected\n\nconnect hiloo/hiloo123\nrevoke select on emp_hiloo from openlab;\n\nshow user\nselect count(*) from hiloo.emp_hiloo\n\ncreate synonym emp_hiloo for hiloo.emp_hiloo\n#### 6) 关于null值的讨论####\n1 case when在没有else和decode少一个参数时,返回null.\n2order by bonus,asc升序时null值在最后,desc降序时null在最前.\n3 组函数和null值的关系:1组函数的缺省处理方式是处理所有的非空值.2当所有的值都是null,count函数返回0,其他组函数返回null.\n4若group by的列有null值,所有的null值分在一组.\n课堂练习\n1将每个员工的工资涨12.34567%,用round和trunc分别实现\nselect ename,nvl(trunc(round(salary+salary*0.1234567,2),1),0.0) from emp_hiloo;//自己写的。\n2 将'2008-01-01'插入表中,\n  再将'2008 08 08 08:08:08'插入表中\ninsert into test values\n(to_date('01-JAN-08','DD-MON-RR'));\n\n3找出3月份入职的员工.\nselect ename,hiredate\nfrom emp_hiloo\nwhere to_char(hiredate,'mm') = '03';\nselect ename,hiredate\nfrom emp_hiloo\nwhere to_char(hiredate,'mm') = 3;//可以正常输出winXP下\n'03' = 3  ---> to_number('03') = 3\n字符   数值  缺省系统将字符转成数值\nselect ename,hiredate\nfrom emp_hiloo\nwhere to_char(hiredate,'fmmm') = '03';(错，未选定行，无输出)\n\nselect ename,hiredate\nfrom emp_hiloo\nwhere to_char(hiredate,'fmmm') = '3';(对)\n'03' = '3' (错)\nfm表示去掉前导0或去掉两边的空格.\n4 zhangsanfeng的mgr上显示boss,其他人不变.\nselect ename,empno,\n       nvl(to_char(mgr),'boss') mgr\nfrom emp_hiloo\n函数nvl（“1”，“2”）:如果字符串1是空，就返回字符串”2”\n#### 5十分钟之后####\n select sysdate,sysdate+1/144 from dual;\n解释：Oracle 里面,\n\nsysdate + 1 意思是 当前时间 + 1天\n\nsysdate + 1/24  意思是 当前时间 + 1/24天  也就是1小时后\n\nsysdate+1/144  意思是 当前时间 + 1/144天 （1/24*6）  也就是10分钟后\n 6 若员工是10部门的,工资涨10%,20部门工资涨20%,其他员工工资不变.\nselect ename,salary,\n       case when deptno = 10 then salary*1.1\n            when deptno = 20 then salary*1.2\n       else\n            salary\n       end new_sal\nfrom emp_hiloo;\n\nselect ename,salary, \n       decode(deptno,10,salary*1.1,\n                     20,salary*1.2,\n                     salary) new_sal\nfrom emp_hiloo;\n7 列出每个员工的年薪,按年薪降序排列.\nselect ename,salary*12\nfrom emp_hiloo\norder by salary desc (好)\nselect ename,salary*12\nfrom emp_hiloo\norder by salary*12 desc\nselect ename,salary*12 n_sal\nfrom emp_hiloo\norder by n_sal desc\n\nselect ename,salary*12 n_sal from emp_hiloo order by 2 desc;\nselect salary*12,ename n_sal from emp_hiloo order by 2 asc;\n8 列出员工的名字,部门号以及工资,按部门号从小到大的顺序,同一部门的工资按降序排列.\nselect ename,deptno,salary\nfrom emp_hiloo\norder by deptno,salary desc\n9 列出奖金的平均值,和,个数,最大值,最小值.\nAVG 函数返回数值列的平均值。NULL 值不包括在计算中\nselect avg(bonus),avg(nvl(bonus,0)),\n       sum(bonus), sum(nvl(bonus,0)),\n       count(bonus),count(nvl(bonus,0)),\n       max(bonus),max(nvl(bonus,0)),\n       min(bonus),min(nvl(bonus,0))\nfrom emp_hiloo\n10 各个部门的平均工资\nROUND 函数用于把数值字段舍入为指定的小数位数。\nselect deptno,round(avg(salary))\nfrom emp_hiloo\ngroup by deptno\n11 求10部门的平均工资,只显示平均工资\n   求10部门的平均工资,显示部门号,平均工资\n   select round(avg(salary))\n   from emp_hiloo\n   where deptno = 10\n   group by deptno\n\n   select max(deptno),round(avg(salary))\n   from emp_hiloo\n   where deptno = 10 \n12各个部门不同职位的平均工资\n   select deptno,job,round(avg(salary))\n   from emp_hiloo\n   group by deptno,job\n13 每种奖金有多少人?\n   select bonus,count(empno)\n   from emp_hiloo\n   group by bonus\n14 列出平均工资大于5000的部门的平均工资\n   select deptno,round(avg(salary)) \n   from emp_hiloo\n   group by deptno\n   having round(avg(salary)) > 5000\n15哪些员工的工资是最低的.\n  select ename from emp_hiloo\n  where salary = ( select min(salary)\n                   from emp_hiloo)\n报错信息\nORA-01861: literal does not match format string\n文字值不匹配格式串\nORA-01722: invalid number 无效的数值 to_number\nORA-00937: not a single-group group function 不是一个组函数\nORA-00979: not a GROUP BY expression 不是一个group by表达式 GROUP BY expression指跟在group by后面的东西(列名),称之为组标识\ndetail 细节 summary 聚合\n\n### 查询###\n子查询定义\n在SQL语句中嵌入select语句\ncreate table new_tabname\nas\nselect ename,salary*12 ann_sal from emp_hiloo;\n新表的结构由select后面的项来决定,new_table包含两列ename,ann_sal.\n\n#### 子查询####\n  非关联子查询\n    单列子查询\n    多列子查询\n  关联子查询\n\n##### 子查询执行##### \n非关联子查询\n子查询的表和主查询的表没有建关联\n先执行子查询(只执行一遍),当返回多条记录,系统会将自动去重的结果返回给主查询,再执行主查询.\n\n关联子查询\n子查询的表和主查询的表建关联.所谓建关联指主查询表里的列和子查询表里的列写成一个条件表达式.\n\n先执行主查询,判断表里的记录是否应该放入结果集.过程如下:拿到第一条记录,获得了各个列的值,将需要的列值带入子查询,执行后返回的结果再和主查询表里的列做比较,符合条件,该记录放入结果集,否则过滤掉.依次执行主查询表里的每条记录.子查询执行的次数由主查询表里的记录数决定.\n\n1) exists和not exists\nexists的执行过程\n从主查询表里拿到第一条记录,按子查询里的关联条件在子查询的表里看是否能找到匹配的记录,当找到第一条匹配的记录后,立即返回(即不需要找出所有匹配的记录),exists条件满足,主查询表里的该记录放入结果集.若按子查询里的关联条件将子查询\n表里的记录全部检查一遍后没有一条符合条件的记录,此时也返回, exists 条件不满足,主查询表里的该记录不能放入结果集,被过滤掉.\n\nselect ename from emp_afei o\nwhere exists\n             (select 1 from emp_afei i\n              where o.empno = i.mgr)\n\n##### 非关联子查询的分类##### \n单列子查询\nselect ename,salary\nfrom emp_hiloo \nwhere salary = (select min(salary)\n                from emp_hiloo \n                )\n多列子查询:按键值对比较\nselect ename,salary,deptno\nfrom emp_afei\nwhere (deptno,salary) in\n             (select deptno,round(avg(salary))\n              from emp_afei\n              group by deptno)\n\n2) 课堂练习\n1哪些人是领导?(非关联子查询)\n如果一个员工的empno能出现在mgr里就说明他是领导.\nselect ename\nfrom emp_hiloo\nwhere empno in (select mgr from emp_hiloo)\nselect ename\nfro emp_afei\nwhere empno in (1001,1005,1006,1008,null)\n2 哪些人是员工?\n他的empno绝对不能出现在mgr中,他的empno跟mgr的出现的所有的值不能相等. <>all\nselect ename\nfrom emp_hiloo\nwhere empno not in (select mgr from emp_hiloo)\nselect ename\nfro emp_afei\nwhere empno not in (1001,1005,1006,1008,null)\nselect ename\nfrom emp_hiloo\nwhere empno not in (select mgr from emp_hiloo\n                    where mgr is not null)\n\n3哪些部门的平均工资比30部门的平均工资高?\nselect deptno,round(avg(salary))\nfrom emp_hiloo\ngroup by deptno\nhaving round(avg(salary)) >\n                    (select round(avg(salary))\n                     from emp_hiloo\n                     where deptno = 30)\n4哪些员工的工资比zhangwuji的工资高?\nselect ename,salary\nfrom emp_afei\nwhere salary > (select salary from emp_afei\n                where ename = 'zhangwuji')\nERROR at line 3:\nORA-01427: single-row subquery returns more than one row\n单行子查询返回多条记录\n\n比所有人高 > (select max(salary))\n           >all\n比任意人高 > (select min(salary)\n           >any\n5哪些员工的工资等于本部门的平均工资?\nselect ename,salary,deptno\nfrom emp_afei\nwhere (deptno,salary) in\n             (select deptno,round(avg(salary))\n              from emp_afei\n              group by deptno)\n5哪些员工的工资比本部门的平均工资高?\nselect ename,salary,deptno\nfrom emp_afei o\nwhere salary > (select round(avg(salary))\n                from emp_afei i\n                where i.deptno = o.deptno)\n6哪些人是领导?(关联子查询)\nselect ename from emp_afei o\nwhere exists\n             (select 1 from emp_afei i\n              where o.empno = i.mgr)\n7哪些部门有员工?\nselect deptno,dname\nfrom dept_afei o\nwhere exists (select 1 from emp_afei i\n              where o.deptno = i.deptno)\n\n3) 课外练习day03am\n1 zhangwuji的领导是谁,显示名称?\n2 zangwuji领导谁,显示名称?\n3 列出devoleper部门有哪些职位?\n1) 课外练习day04am答案\n1 zhangwuji的领导是谁,显示名称?\n  select ename from emp_afei\n  where empno in \n\t\t(select mgr from emp_afei\n                 where ename = 'zhangwuji')\n\nzangwuji领导谁,显示名称?\n\n select ename from emp_afei\n where mgr in (select empno from emp_afei\n               where ename = 'zhangwuji')\n\n3 列出developer部门有哪些职位?\n  select distinct job from emp_afei\n  where deptno in \n           (select deptno from dept_afei\n            where dname = 'developer')\n\n2) 非关联子查询               \nexists和not exists\nnot exists的执行过程\n从主查询表里拿到第一条记录,按子查询里的关联条件在子查询的表里看是否能找到匹配的记录,当找到第一条匹配的记录后,立即返回(即不需要找出所有匹配的记录),not exists条件不满足,主查询表里的该记录不能放入结果集,被过滤掉.若按子查询里的关联条件将子查询表里的记录全部检查一遍后没有一条符合条件的记录,返回, not exists 条件满足,主查询表里的该记录放入结果集.\n\n对于exists和not exists,在子查询中找到第一条匹配的记录都会立即返回,exists将主查询表里的记录放入结果集,not exsits将主查询表里的记录过滤掉.\n对于exists和not exists,如果子查询没有返回任何记录,即扫描全部记录后没有一条符合条件的记录,都返回,exists将主查询表里的记录过滤掉,not exists将主查询表里的记录放入结果集. \nnot in ,<> all逻辑上跟not exists等价\nin ,=any逻辑上跟exists等价\n\n查询形式:集合操作\n把结果集作为一个集合,结果集必须是同构的,列的个数及数据类型一致\n\n3) 并集  union(去重)/union all(不去重)\nselect ename,deptno,salary,salary*1.1 new_sal\nfrom emp_afei\nwhere deptno = 10\nunion all\nselect ename,deptno,salary,salary*1.2 new_sal\nfrom emp_afei\nwhere deptno = 20\nunion all\nselect ename,deptno,salary,salary new_sal\nfrom emp_afei\nwhere deptno not in (10,20)\n\ncase when和decode可以实现类似功能.\n\n4) 交集  intersect(去重)\nselect job from emp_afei\nwhere deptno = 10\nintersect\nselect job from emp_afei\nwhere deptno = 20\n10部门和20部门都有的职位是哪些?\n\n5) 差  minus(去重)\nselect deptno from dept_afei\nminus\nselect deptno from emp_afei\n那些部门没有员工.\n\n6) 多表查询\n1) 交叉连接 cross join\nselect e.ename,d.dname\nfrom emp_afei e cross join dept_afei d\n结果集产生\n10*4=40,组合操作,笛卡尔积\n\n2) 内连接 inner join(匹配一个条件)\nselect e.ename,e.deptno,d.deptno,d.dname\nfrom emp_afei e join dept_afei d\nORA-00905: missing keyword(丢失关键字)\n\n如果把结果集的产生看成双层循环,驱动表是外层循环,匹配表是内层循环.\n对于内连接哪张表做驱动表,哪张表做匹配表产生出的结果集是一样的,不同的是性能.\n驱动表在匹配表的匹配情况如下:\n一条记录找到一条匹配\n一条记录找到多条匹配\n一条记录找不到任何匹配.\n内连接的核心是驱动表的记录要出现在结果集中必须在匹配表中能找到匹配的记录,否则该记录被过滤掉.\n\n3) 内连接查询形式\n等值连接 on e.deptno = d.deptno\n两张表有表述同一属性的列,两张表都有deptno列.\n自连接 on e.mgr = m.empno\n同一张表的不同列能写成一个表达式,即同一张表的两条记录之间有关系.通过给表起别名的方式,将同一张表的两条记录之间的关系转化成不同表的两条记录之间的关系.\n4) 外连接\n外连接 outer join(驱动表的记录一个都不能少的出现在结果集里)\nfrom t1 left join t2\non t1.c1 = t2.c2(t1驱动表,t2匹配表)\n外连接结果集=内连接的结果集+t1表中匹配不上的记录和t2表中的null记录的组合\nfrom t1 right join t2\non t1.c1 = t2.c2(t2驱动表,t1匹配表)\n外连接结果集=内连接的结果集+t2表中匹配不上的记录和t1表中的null记录的组合\nfrom t1 full join t2\non t1.c1 = t2.c2\n外连接结果集=内连接的结果集+t1表中匹配不上的记录和t2表中的null记录的组合+t2表中匹配不上的记录和t1表中的null记录的组合\n\n5) 外连接的应用场景\n1 某张表的记录全部出现在结果集中,包括匹配不上的.\nselect e.ename,nvl(m.ename,'Boss')\nfrom emp_afei e left join emp_afei m\non e.mgr = m.empno\n2解决否定问题,匹配不上的记录找出来(跟所有的记录都不匹配.)(not in/not exists)\n外连接 + where 匹配表.主键列 is null\nselect e.ename,d.dname\nfrom emp_afei e right join dept_afei d\non e.deptno = d.deptno\nwhere e.empno is null\n(解决结果集只包含匹配不上的记录.where子句执行在外连接之后)哪些部门没有员工\n\nselect d.dname\nfrom emp_afei e right join  dept_afei d\non e.deptno = d.deptno\nand e.ename = 'zhangwuji'\nwhere e.empno is null\n如果希望在外连接之前过滤匹配表用and子句,如果想在外连接之后通过匹配表里的列过滤外连接的结果集时候用where.\n过滤驱动表统计用where子句过滤.\n\n6) 课内练习\n1 哪些部门没有员工(not exists)\n  select dname from dept_afei o\n  where not exists \n        (select 1 from emp_afei i\n         where o.deptno = i.deptno)\n2 哪些人是员工?(not exists)\n  select ename from emp_afei o\n  where not exists \n            (select 1 from emp_afei i\n             where o.empno = i.mgr)\n他的empno和其他人的mgr相等是不可能存在的.即和所有人的mgr都不相等.\nnot in ,<> all逻辑上跟not exists等价\n3 列出哪些员工在北京地区上班?\n思路:确定表,两张表,匹配问题用inner join-->on(匹配条件)-->(对表是否过滤)\nselect e.ename,e.deptno,d.deptno,d.dname\nfrom emp_afei e join dept_afei d\non e.deptno = d.deptno\nand d.location = 'beijing'\n4zhangwuji在哪个地区上班?\nselect e.ename,d.dname,d.location\nfrom emp_afei e join dept_afei d\non e.deptno = d.deptno\nand e.ename = 'zhangwuji'\n5列出每个部门有哪些职位?部门名称,职位\n select distinct d.dname,e.job\n from emp_afei e join dept_afei d\n on e.deptno = d.deptno\n order by d.dname\n6各个部门的平均工资,列出部门名称,平均工资.\nselect d.dname,round(avg(e.salary)) savg\nfrom emp_afei e join dept_afei d\non e.deptno = d.deptno\ngroup by d.dname\nselect max(d.dname),round(avg(e.salary)) savg\nfrom emp_afei e join dept_afei d\non e.deptno = d.deptno\ngroup by d.deptno\nselect min(deptno),round(avg(salary))\nfrom emp_hiloo\nwhere deptno = 10\n7 列出每个员工的名字和他的领导的名字\nselect e.ename employee,\n       m.ename manager\nfrom emp_afei e join emp_afei m\non e.mgr = m.empno\n结果集是9条.\ne表中有10条记录,其中9条记录找到匹配,zhangsanfeng没匹配\nm表中有10条记录,其中4条记录找到匹配,4条记录是领导,6条记录找不到匹配,他们是员工.\nselect e.ename employee,\n       m.ename manager\nfrom emp_afei e join emp_afei m\non e.mgr = m.empno\nunion all\nselect ename,'Boss'\nfrom emp_afei\nwhere mgr is null\n\nselect e.ename employee,\n       decode(m.ename,e.ename,'Boss',\n                  m.ename)   manager\nfrom emp_afei e join emp_afei m\non nvl(e.mgr,e.empno) = m.empno\n\nselect e.ename,nvl(m.ename,'Boss')\nfrom emp_afei e left join emp_afei m\non e.mgr = m.empno\n10=9+1\n\n8哪些人是领导?\nselect distinct m.ename\nfrom emp_afei e join emp_afei m\non e.mgr = m.empno\n9哪些部门没有员工?\nselect e.ename,d.dname\nfrom emp_afei e right join dept_afei d\non e.deptno = d.deptno\nwhere e.empno is null\n(解决结果集只包含匹配不上的记录.where子句执行在外连接之后)\n11=10+1\n如果部门表里的某条记录的deptno在emp表找不到匹配,在内连接中,它被过滤,\ne表的empno的特性是唯一且非空的(主键约束),居然e.empno is null,说明null是外连接时为了驱动表中那条匹配不上的记录出现在结果集中,在匹配表中模拟的null记录.\n10哪些人是员工,哪些人不是领导?\nselect e.empno,m.ename\nfrom emp_afei e right join emp_afei m\non e.mgr = m.empno\nwhere e.empno is null\n\nfrom emp_afei e right join emp_afei m\n15=9+(10(m表中有10条记录)-4(m表中有4条匹配记录 ))\nfrom emp_afei e left join emp_afei m\n10(结果集)=9+(10(e表中有10条记录)-9(e表中有9条匹配记录))\n11 哪些部门没有叫zhangwuji的?\nselect d.dname\nfrom emp_afei e right join  dept_afei d\non e.deptno = d.deptno\nand e.ename = 'zhangwuji'\nwhere e.empno is null\n\n7) 课外练习(day04)(答案在Day05)\n1zhangwuji的领导是谁?(表连接)\n2zhangwuji领导谁?(表连接)\n3哪些人是领导?(in exists join)\n4哪些部门没有员工?(not in/not exists/outer join)\n5哪些人是员工,哪些人不是领导?(not in/not exists/outer join)\nDay05.txt\nGrade级别\nLowsal最低工资\nHisal最高工资\nCreate table salgrade_hiloo(\nGrade \n)\ncross join  inner join   outer join\ninner join(匹配)\n  等值连接\n  自连接\n  非等值连接\nouter join(匹配+不匹配)\n  等值连接\n\n  自连接\n  非等值连接\n\n所谓非等值连接表示两张表里的列不能写成等值表达式,而是写成between and之类.所以两个表之间有关系是指表里的列可以写成表达式,而不是等值表达式.\nsalgrade\ngrade  级别\nlowsal 最低工资\nhisal  最高工资\n\nfrom后面跟子查询\nemp,各个部门的平均工资dept_avgsal(depnto,avgsal)\nselect e.ename,e.salary,e.deptno\nfrom emp_afei e join\n      (select deptno,round(avg(salary)) avgsal\n       from emp_afei \n       group by deptno) a\non e.deptno = a.deptno\nand e.salary > a.avgsal\n\n各个部门的平均工资,列出部门名称,平均工资\nselect max(d.dname),round(avg(salary))\nfrom emp_afei e join dept_afei d\non e.deptno = d.deptno\ngroup by d.deptno\n\nselect d.dname,a.avgsal\nfrom dept_afei d join\n      (select deptno,round(avg(salary)) avgsal\n       from emp_afei \n       group by deptno) a\non d.deptno = a.depto\n\nDML\ninsert一条记录时,若某些列为null值,有哪些语法实现?\ninsert into tabname values (1,'a',null,sysdate)\ninsert into tabname(c1,c2,c4)\nvalues (1,'a',sysdate)\ninsert语句的两种语法形式?\ninsert into tabname values () insert一条记录\ninsert into tabname\nselect * from tabname1  insert多条记录\n连接图解：\n    \n\n### 数据类型###\n1) 课外练习答案day04\n1zhangwuji的领导是谁?(表连接)\n select m.ename\n from emp_afei e join emp_afei m\n on e.mgr = m.empno\n and m.ename = 'zhangwuji'\n2 zhanghangwuji领导谁?(表连接)\n select e.ename\n from emp_afei e join emp_afei m\n on e.mgr = m.empno\n and m.ename = 'zhangwuji'\n3哪些人是领导?(in exists join)\n select ename from emp_afei\n where empno in (select mgr from emp_afei)\n select ename from emp_afei o\n where exists\n            (select 1 from emp_afei i\n             where o.empno = i.mgr)\n select distinct m.ename\n from emp_afei e join emp_afei m\n on e.mgr = m.empno\n4哪些部门没有员工?(not in/not exists/outer join)\n select dname from dept_afei\n where deptno not in \n               (select deptno from emp_afei)\n select dname from dept_afei o\n where not exists \n             (select 1 from emp_afei i\n              where o.deptno = i.deptno)\n select d.dname\n from emp_afei e right join dept_afei d\n on e.deptno = d.deptno\n where e.empno is null\n5哪些人是员工,哪些人不是领导?(not in/not exists/outer join)\n select ename from emp_afei\n where empno not in (\n               select mgr from emp_afei\n               where mgr is not null)\n select ename from emp_afei o\n where not exists\n             (select 1 from emp_afei i\n              where o.empno = i.mgr)\n select m.ename\n from emp_afei e right join emp_afei m\n on e.mgr = m.empno\n where e.empno is null\ncross join (笛卡尔积)\n\nrownum 伪列,记录号\n若用rownum选择出记录,编号必须从1开始.\n分页问题\n第一页\nselect rownum,ename\nfrom emp_afei\nwhere rownum <= 3;\n第二页\nselect rn,ename\nfrom (\n      select rownum rn,ename\n      from emp_afei\n      where rownum <= 6)\nwhere rn between 4 and 6\n排名问题\n按工资排名的前三条记录\nselect rownum,ename,salary\nfrom emp_hiloo\nwhere rownum <=3\norder by salary desc;(错)\n\nselect rownum,ename,salary\nfrom ( select ename,salary\n       from emp_afei\n       order by salary desc)\nwhere rownum <= 3\n\nupdate语句的中set后面的=是什么含义?where后面的=是什么含义?\nset c1 = null (= 赋值)\nwhere c1 = null (= 等号)\n\nupdate和delete语句中的where子句是什么含义?\n用来确定对表里的哪些记录要进行update或delete操作,没有where子句多表里的所有记录update或delete\nupdate\nset\nwhere c1 = (select ...)\nrename 关键字 17\ncommit\n\n1011 abc 1000 10 'clerk'\nupdate 1001 1000-->2000\ndelete 1011\ncommit\n如何编写和运行一个sql脚本(文本文件)\n1 编辑文件\n在linux环境下已经编写好了test.sql,做一个鼠标右键的copy\n\n在20,23,26机器上,\nvi test.sql\n按a i o进入编辑模式,paste,按esc键,再按:wq!回车\n\n2 运行文件\nsun-server% sqlplus openlab/open123 @test.sql\n@表示运行\nSP2-0310: unable to open file \"test.sql\"在当前目录下没有test.sql文件\nsqlplus openlab/open123 ../test.sql\n\ncd ..\nsun-server% sqlplus openlab/open123 @test.sql\n\nSQL>@test.sql\n\n数据库对象 PL/SQL\ncreate or replace function test\ninsert into test values (1,1)\n            *\nERROR at line 1:\nORA-04044: procedure(存储过程), function(函数), package(包), or type is not allowed here\n\n事务(transaction 交易)\n事务里包含的DML语句\n事务的结束\ncommit 提交,(dml操作的数据入库了)\nrollback 回滚 撤销(DML操作被取消)\nsqlplus正常退出=commit\nDDL语句自动提交\n开始\n上一个事务的结束是下一个事务的开始.\n一致状态\n数据库的数据被事务改变.\noltp online transaction processing联机事务处理系统 高并发系统\n\n事务的隔离级别 read committed(读已经提交了的数据)\n\n\n如果不commit----->commit rollback\n1如果不commit,其他session是看不见你的操作\n2如果不commit,会阻塞操作同一条记录的事务(session),commit才能释放所有DML加的锁.\n3如果不commit,系统做DML操作,会将old data放入rollback segment(回滚段) ,所占用的回滚段资源不释放.\n\nDML系统会自动给表及表里的记录加锁\n表级共享锁\n行级排他锁\n\t表级共享锁 \t行级排他锁\ns1\tok\t\tok\ns2\tok\t\tenqueue wait\ns3\tok\t\tok\n\n执行DDL语句,系统自动加DDL排他锁\nSQL> drop table test purge;\ndrop table test purge\n           *\nERROR at line 1:\nORA-00054: resource busy(资源忙 test表) and acquire with NOWAIT specified (dml wait,ddl nowait 如果加不上锁,报错退出)\n\nDDL语句\n字符类型\nvarchar2,必须带宽度, 按字符串的实际长度存,本身的数据是变化,对空格敏感\nchar,可以不带宽度,缺省宽度是1,按字符串的定义长度存,本身的数据是固定长度的.对空格不敏感\n数值类型\n\nnumber类型\ncreate table test90\n(c1 number,\n c2 number(6),\n c3 number(4,2),\n c4 number(2,4),\n c5 number(3,-3))\n\n四舍五入\nnumber(6) 表示6为整数 999999\nnumber(4,2) 表示小数点后2位,整数位2位 99.99\nnumber(2,4) 表示小数点后4位,能填数字的位数是2位 0.0099\nnumber(3,-3) 999000 999123-->999000 \n                    999511-->报错\n\nuser_tables 是一张系统表,里面记录当前用户所有的表的信息,里面没有记录表的创建日期.\nuser_objects 是一张系统表,里面记录当前用户所有的数据库对象的信息.created的列记录数据库对象(如表)的创建日期.\nuser_tables和user_objects这两张表的关系体现在table_name和object_name都记录的是表名.\n\ndata block 数据块,操作数据的最小逻辑(物理)单元,最少读一个block的数据\n\nHWM high water mark 高水位线,表示曾经插入数据的最高位置\nFTS full table scan 全表扫描,把表里的所有记录读一遍,把HWM之下的所有data block读一遍\n\ntruncate table 释放空间,HWM下移\ndelete 不释放空间,HWM不动\n不适合用delete命令删大表.\n\n课内练习\n1 列出工资级别为3级,5级的员工\n  select e.ename,e.salary,s.grade\n  from emp_afei e join salgrade_afei s\n  on e.salary between s.lowsal and s.hisal\n  and s.grade in (3,5)\n2 列出各个工资级别有多少人?\n  select s.grade,count(e.empno)\n  from emp_afei e join salgrade_afei s\n  on e.salary between s.lowsal and s.hisal\n  group by s.grade\n  order by s.grade\n3 列出各个工资级别有多少人?(包含0级)\n  select s.grade,count(e.empno)\n  from emp_afei e right join salgrade_afei s\n  on e.salary between s.lowsal and s.hisal\n  group by s.grade\n  order by s.grade\n特别注意count不要写*或者s.grade\n\n课外练习day05\n1按工资排名的第4到第6名员工.\n###关键点###\n课外练习day05答案\n\n按工资排名的第4到第6名员工.\nselect rn,ename,salary\nfrom \n    (select rownum rn,ename,salary\n     from (select ename,salary\n           from emp_afei\n           order by salary desc)\n     where rownum <= 6\n    )\nwhere rn >= 4 \n\n####1）事务####\n\n####约束 constraint (安检)####\nprimary key(主键)\nforeign key(外键)\nunique key (唯一键)\nnot null(非空)\ncheck (检查)\n\n主键 (表中不会出现重复记录)\n列级约束\ncreate table test\n(c1 number(2) \n    constraint test_c1_pk primary key,\n c2 number(3))\n\n    constraint test_c1_pk primary key,\n               *\nERROR at line 3:\nORA-02264: name already used by an existing constraint (名字被存在的约束使用了)\n\nSQL> select table_name from user_constraints\n  2  where constraint_name = 'TEST_C1_PK';\n哪张表里有叫TEST_C1_PK这个约束名.\n\nORA-00001: unique constraint (HILOO(用户名) .TEST_C1_PK) violated(冲突)\n\nPK=UK + NN\n\n表级约束\ncreate table test(\nc1 number(2),\nc2 number,\nconstraint test_c1_pk primary key(c1)\n)\n表中有三列c1,c2,c3,c1和c2做成联合主键\ncreate table test(\nc1 number,\nc2 number,\nconstraint test_c1_c2_pk primary key(c1,c2),\nc3 number\n)\n没有constraint关键字,系统用自动起名字sys_c数字.\n\nnot null\ncreate table test\n(c1 number constraint test_c1_pk primary key,\n c2 number not null);\nnot null约束没有表级形式\n\nunique (pk)\n相同点:都要保证唯一性\n区别:uk允许为null,而且可以多个null值,一个表中只能有一个pk约束,可以有多个uk约束.\ncreate table test\n(c1 number constraint test_c1_pk primary key,\n c2 number constraint test_c2_uk unique)\n\ncreate table test(\nc1 number primary key,\nc2 number primary key,\nc3 number unique,\nc4 number unique)  (报错,一张表只能有一个primary key)\n\ncreate table test(\nc1 number constraint test_c1_pk primary key,\nc2 number constraint test_c2_uk unique,\nc3 number constraint test_c3_uk unique,\nc4 number ) \nc2上定义了一个唯一键 c3上定义了一个唯一键\n\ncreate table test(\nc1 number constraint test_c1_pk primary key,\nc2 number,\nc3 number,\nconstraint test_c2_c3_uk unique (c2,c3),\nc4 number)\nc2,c3联合唯一键\n\ncheck\ncreate table test(\nc1 number(3) constraint test_c1_ck\n             check (c1 > 100))\n\ncreate table test(\nc1 number(3),\nconstraint test_c1_ck check (c1 > 100))\n\n外键\nparent table(父表)上定义唯一列(pk/uk)\nchild table(子表)上定义外键列(fk)\n\n1 先create parent table(pk/uk),再create child table(fk)\n2 先insert into parent table,再insert into child table\n3 先delete from child table,再delete from parent table\n4 先drop child table,再drop parent table\n\nreference 引用\ncreate table parent\n(c1 number(3))\n\ncreate table child\n(c1 number(2) constraint child_c1_pk\n              primary key,\n c2 number(3) constraint child_c2_fk\n              references parent(c1))\n\n              references parent(c1))\n                                *\nERROR at line 5:\nORA-02270: no matching unique or primary key for this column-list\n在c1上没有定义uk或pk\n\nalter table parent\nadd constraint parent_c1_pk primary key(c1);\n给c1列增加主键约束\n\ninsert into child values (1,1)\nORA-02291: integrity constraint(完整性约束) (HILOO.CHILD_C2_FK) violated - parent key not found (父键值没发现)\n违反fk约束\n\ninsert into parent values (1);\ninsert into child values (1,1)\n\ndelete from parent where c1 = 1;\nORA-02292: integrity constraint (HILOO.CHILD_C2_FK) violated - child record\nfound(子记录被发现)\n\ndelete from child where c2 = 1;\ndelete from parent where c1 = 1;\n\ndrop table parent purge;\nORA-02449: unique/primary keys in table referenced by foreign keys\n在parent table上的pk/uk正在fk所引用\n\ndrop table child purge;\ndrop table parent purge;\n\ndrop table parent cascade constraints purge;\ncascade constraints 级联约束,child table本身没被删除,只是先把子表上的fk约束删除,再删parent table.\n\n表级约束\ncreate table child\n(c1 number(2) constraint child_c1_pk \n              primary key,\n c2 number(3),\n constraint child_c2_fk foreign key(c2)\n            references parent(c1)\n)\n\n外键约束另外两种定义方法\ncreate table child1\n(c1 number(2) constraint child1_c1_pk\n              primary key,\n c2 number(3) constraint child1_c2_fk\n              references parent(c1)\n              on delete cascade)\non delete cascade :级联删除会影响到对parent table的删除,先delete from child1,再delete from\nparent\n\ndelete from parent where c1 = 1;\ncreate table child2\n(c1 number(2) constraint child2_c1_pk\n              primary key,\n c2 number(3) constraint child2_c2_fk\n              references parent(c1)\n              on delete set null)\n\ndelete from parent where c1 = 1\n等价于以下操作\nSQL> update child2 set c2 = null\n  2  where c2 = 1;\nSQL> delete from parent where c1 = 1;\n\ntable \nDDL(数据类型 约束)\ntransaction (包含一堆DML)\n\n4000\n100 \n1000\n3100\n\n视图(view)\ncreate table test_t1\nas\nselect * from test\nwhere c1 = 1;\ncreate or replace view test_v1\nas\nselect * from test\nwhere c1 = 1;\ndesc test_v1\nselelct * from test_v1\n\ninsert into test values (1,3);\nselect * from test_v1 (1,3)\ninsert into test_v1 values (1,4)\nselect * from test_v1;\nselect * from test;\ninsert into test_v1 values (2,3);\nselect * from test_v1;(没有)\nselect * from test;(2,3)\n\ndrop table test purge;\nselect * from test_v1; \nSQL> desc test_v1\nERROR:\nORA-24372: invalid object for describe\n无法描述无效对象的结构\n\nSQL> select text from user_views\n  2  where view_name = 'TEST_V1';\n\nTEXT\n-----------------------------------------\nselect \"C1\",\"C2\" from test\nwhere c1 = 1\n\nview是一条select语句. select语句中包含的表为源表.通过view对源表做DML操作.\n\nview作用\n1 create view (deptno = 30)\n  grant view to user\n  限定用户查询的数据 子集\n2 简化查询语句\n3 create view beijing\n  as\n  select * from haidian\n  union all\n  select * from xicheng\n...\n  超集\nview的类型\n1 简单view (DML)\n2 复杂view  (不能DML)\n\ncreate or replace view avgscore_v\nselect s.name,a.avgscore\nfrom student s,\n     (select sid,round(avg(score)) avgscore\n      from stu_cour\n      group by sid) a\non s.id = a.sid\n\nview的约束\ncreate or replace view test_ck\nas\nselect * from test\nwhere c1 = 1\nwith check option;\nc1=2,违反where条件,2,3记录insert时报错\n\ncreate or replace view test_ro\nas\nselect * from test\nwhere c1 = 1\nwith read only;\n只读视图\n#### 索引####\ncreate index test_c1_idx\non test(c1);\n对索引不能做desc,select,DML操作\nrowid 代表一条记录的物理位置\n属于哪个数据对象(table)\n属于哪个数据文件的\n属于数据文件的第几个数据块\n属于数据块里的第几条记录\n#### index的结构####\nindex记录rowid\nindex的结构是一棵平衡树,有三类数据块组成,根节点,分支节点,叶子节点,数据块的数据是排序的.根节点和分支节点用于导航,里面记录下一级节点的物理位置以及该节点包含的数据范围.叶子节点里记录的是index entry(索引项),由key值和rowid组成,key值是建索引的列在每条记录上的取值,rowid是记录的物理位置,所有的叶子节点做成双向链表(升序/降序),适用于范围查询.\n用索引查询的路线图,从根节点出发,找相应的分支节点,叶子节点,最后要找到index entry,通过rowid定位\n表里所需要的数据块,避免了全表扫描.\n\n索引为什么提高查询效率,为select语句\n有效地降低了读取数据块的数量.读取数据块,一种从文件里读,物理读 physical read,一种从内存读,逻辑读 logical read /buffer gets\n\n建索引代价\n空间,DML变慢\n\n\n#### 哪些列适合建索引####\n1 经常出现在where子句的列\n2 pk/uk列\n3 经常出现在表连接的列\n4 fk列 parent.pk列 = child.fk列\n5 经常用于group by,order by的列\n7 where c1 is null(全表扫描),索引里不记null值,\n 该列有大量null值,找not null值用索引会快\n\n#### 索引类型####\n非唯一性索引,提高查询效率\n唯一性索引,解决唯一性.等价建唯一性约束\ncreate unique index test_c2_idx\non test(c2);\n\ninsert into test (c2) values (1)\n*\nERROR at line 1:\nORA-00001: unique constraint(HILOO.TEST_C2_IDX ) violated\n\n联合索引\ncreate index test_c1_c2_idx\non test(c1,c2)\nwhere c1 = 1 and c2 = 1\n\nselect ename from emp_hiloo\nwhere salary*12 > 60000\nwhere salary > 5000\n如果salary建索引,where salary > 5000(用),where salary*12 > 60000(不能用)\n\nwhere upper(ename) = 'ZHANGWUJI'\n\nwhere c1 = 100 c1是varchar2类型\nwhere to_number(c1) = 100\n\nwhere ename like 'a%'\nwhere substr(ename,1,1) = 'a'\n\ndeptno not in (20,30)\ndepotno in (10)\n\n#### 函数索引####\ncreate index test_c1_funidx\non test(round(c1));\nwhere round(c1) = 10\n\ncreate index student_name_idx\non student(name);\n\n#### 序列号####\nsequence\n为table里的主键服务,产生主键值\n唯一值产生器\nsequence_name.nextval\n\n为student表的id建sequence\ninsert into student(student_id.nextval...\n为course表的id建sequence\ninsert into course (course_id.nextval...\n\n创建序列如下：\ncreate sequence SEQ_TEST100\nminvalue 1\nmaxvalue 999999999999999999999999999\nstart with 11\nincrement by 1\ncache 10;\n\n函数\ncreate or replace function dept_avgsal\n(p_deptno number) --定义参数,数据类型不能有宽度\nreturn number    --定义函数的返回类型\nis\n  v_salary emp_hiloo.salary%type;     --变量v_salary 的类型跟表emp_hiloo里的salary的类型定义一致\nbegin\n  select round(avg(salary)) into v_salary\n  from emp_hiloo\n  where deptno = p_deptno;    --select当且仅当返回一条记录用select into语法,表示把select语句的执行结果赋值给v_salary\n  return v_salary;       --返回函数值 \nend;\n.不运行,回到SQL>下\n/表示运行\nshow error\nSQL> select dept_avgsal(10) from dual;\n\n\n\n\n\n练习\n用语法实现多对多关系\nstudent\nid pk\nname not null\n\ncourse\nid pk\nname not null\n\nstu_cour\nsid fk -->student(id)\ncid fk -->course(id)\npk(sid,cid)\nscore check [0,100](between and) \n#### 数据库日期比较####\nSql代码：\n1\ttimesten内存数据库比较日期是不是同一天,低效的方法  \n2\tto_char(create_date,'yyyymmdd')=to_char(sysdate NUMTODSINTERVAL(60*60*24,'SECOND'),'yyyymmdd')  \n3\toracle 数据库低效的方法  \n4\tto_char(create_date,'yyyymmdd')=to_char(sysdate-1,'yyyymmdd')   \n5\t2个数据库通用高效的方法  \n6\ttrunc(create_date)=trunc(sysdate)-NUMTODSINTERVAL(1,'DAY')  \n查找数据库里的表，索引等\n支持oracle的模糊查询如select * from user_tables where table_name like '%_PROJECT';查表名以PROJECT结尾的表（注：区别大小写）\n查所有用户的表在all_tables\n主键名称、外键在all_constraints\n索引在all_indexes\n但主键也会成为索引，所以主键也会在all_indexes里面。\n具体需要的字段可以DESC下这几个view，dba登陆的话可以把all换成dba。\n\n查询用户表的索引(非聚集索引):\nselect * from user_indexes\nwhere uniqueness = 'NONUNIQUE'\n\n查询用户表的主键(聚集索引):\nselect * from user_indexes\nwhere uniqueness = 'UNIQUE'\n\n1、\t查找表的所有索引（包括索引名，类型，构成列）：\nselect t.*,i.index_type from user_ind_columns t,user_indexes i where t.index_name = i.index_name and t.table_name = i.table_name and t.table_name = 要查询的表\n2、查找表的主键（包括名称，构成列）：\nselect cu.* from user_cons_columns cu, user_constraints au where cu.constraint_name = au.constraint_name and au.constraint_type = 'P' and au.table_name = 要查询的表\n3、查找表的唯一性约束（包括名称，构成列）：\nselect column_name from user_cons_columns cu, user_constraints au where cu.constraint_name = au.constraint_name and au.constraint_type = 'U' and au.table_name = 要查询的表\n4、查找表的外键（包括名称，引用表的表名和对应的键名，下面是分成多步查询）：\nselect * from user_constraints c where c.constraint_type = 'R' and c.table_name = 要查询的表\n查询外键约束的列名：\nselect * from user_cons_columns cl where cl.constraint_name = 外键名称\n查询引用表的键的列名：\nselect * from user_cons_columns cl where cl.constraint_name = 外键引用表的键名\n5、查询表的所有列及其属性\nselect t.*,c.COMMENTS from user_tab_columns t,user_col_comments c where t.table_name = c.table_name and t.column_name = c.column_name and t.table_name = 要查询的表\n####数据唯一Id：####\n1.\t用Oracle来生成UUID，做法很简单，如下：select sys_guid() from dual;数据类型是 raw(16) 有32个字符。\ncreate table test_guid3(\n    id varchar(50)\n)\nselect * from test_guid3;\ninsert into test_guid3(id) values(sys_guid())\n----------- ----------------------------------------\n       1000 7CD5B7769DF75CEFE034080020825436\n       1100 7CD5B7769DF85CEFE034080020825436\n       1200 7CD5B7769DF95CEFE034080020825436\n       1300 7CD5B7769DFA5CEFE034080020825436\n### 名词###\n\n#### Oracle的方案（Schema）和用户（User）的区别####\n \n从定义中我们可以看出方案（Schema）为数据库对象的集合，为了区分各个集合，我们需要给这个集合起个名字，这些名字就是我们在企业管理器的方案下看到的许多类似用户名的节点，这些类似用户名的节点其实就是一个schema，schema里面包含了各种对象如tables, views, sequences, stored procedures, synonyms, indexes, clusters, and database links。\n \n   一个用户一般对应一个schema,该用户的schema名等于用户名，并作为该用户缺省schema。这也就是我们在企业管理器的方案下看到schema名都为数据库用户名的原因。Oracle数据库中不能新创建一个schema，要想创建一个schema，只能通过创建一个用户的方法解决(Oracle中虽然有create schema语句，但是它并不是用来创建一个schema的)，在创建一个用户的同时为这个用户创建一个与用户名同名的schem并作为该用户的缺省shcema。即schema的个数同user的个数相同，而且schema名字同user名字一一对应并且相同，所有我们可以称schema为user的别名，虽然这样说并不准确，但是更容易理解一些。\n \n   一个用户有一个缺省的schema，其schema名就等于用户名，当然一个用户还可以使用其他的schema。如果我们访问一个表时，没有指明该表属于哪一个schema中的，系统就会自动给我们在表上加上缺省的sheman名。比如我们在访问数据库时，访问scott用户下的emp表，通过select * from emp; 其实，这sql语句的完整写法为select * from scott.emp。在数据库中一个对象的完整名称为schema.object，而不属user.object。类似如果我们在创建对象时不指定该对象的schema，在该对象的schema为用户的缺省schema。这就像一个用户有一个缺省的表空间，但是该用户还可以使用其他的表空间，如果我们在创建对象时不指定表空间，则对象存储在缺省表空间中，要想让对象存储在其他表空间中，我们需要在创建对象时指定该对象的表空间。\n \n   oracle中的schema就是指一个用户下所有对象的集合，schema本身不能理解成一个对象，oracle并没有提供创建schema的语法，schema也并不是在创建user时就创建，而是在该用户下创建第一个对象之后schema也随之产生，只要user下存在对象，schema就一定存在，user下如果不存在对象，schema也不存在；这一点类似于temp tablespace group，另外也可以通过oem来观察，如果创建一个新用户，该用户下如果没有对象则schema不存在，如果创建一个对象则和用户同名的schema也随之产生。\n####Oracle中User与Schema的简单理解####\n技术积累（126）  \n版权声明：本文为博主原创文章，未经博主允许不得转载。\n方案（Schema）为数据库对象的集合，为了区分各个集合，我们需要给这个集合起个名字，这些名字就是我们在企业管理器的方案下看到的许多类似用户名的节点，这些类似用户名的节点其实就是一个schema，schema里面包含了各种对象如tables, views, sequences, stored procedures, synonyms, indexes, clusters, and database links。  一个用户一般对应一个schema,该用户的schema名等于用户名，并作为该用户缺省schema。\nSQL Server中的Schema\nSQL Server中一个用户有一个缺省的schema，其schema名就等于用户名，这也就是我们在企业管理器的方案下看到schema名都为数据库用户名的原因。当然一个用户还可以使用其他的schema。如果我们访问一个表时，没有指明该表属于哪一个schema中的，系统就会自动给我们在表上加上缺省的sheman名。比如我们在访问数据库时，访问scott用户下的emp表，通过select * from emp; 其实，这sql语句的完整写法为select * from scott.emp。在数据库中一个对象的完整名称为schema.object，而不属user.object。类似如果我们在创建对象时不指定该对象的schema，在该对象的schema为用户的缺省schema。这就像一个用户有一个缺省的表空间，但是该用户还可以使用其他的表空间，如果我们在创建对象时不指定表空间，则对象存储在缺省表空间中，要想让对象存储在其他表空间中，我们需要在创建对象时指定该对象的表空间。\n\nOracle中的Schema\nOracle中的schema就是指一个用户下所有对象的集合，schema本身不能理解成一个对象，oracle并没有提供创建schema的语法，schema也并不是在创建user时就创建，而是在该用户下创建第一个对象之后schema也随之产生，只要user下存在对象，schema就一定存在，user下如果不存在对象，schema也不存在；如果创建一个新用户，该用户下如果没有对象则schema不存在，如果创建一个对象则和用户同名的schema也随之产生。实际上在使用上，shcema与user完全一样，没有什么区别，在出现schema名的地方也可以出现user名。\n\nTablspace \n逻辑上用来放objects,，这是个逻辑概念，本质上是一个或者多个数据文件的集合，物理上对应磁盘上的数据文件或者裸设备。\n\n数据文件\n具体存储数据的物理文件，是一个物理概念。一个数据文件只能属于一个表空间，一个表空间可以包含一个或多个数据文件。一个数据库由多个表空间组成，一个表空间只能属于一个数据库。\n\n下边是源自网络的一个形象的比喻\n我们可以把Database看作是一个大仓库，仓库分了很多很多的房间，Schema就是其中的房间，一个Schema代表一个房间，Table可以看作是每个Schema中的床，Table（床）被放入每个房间中，不能放置在房间之外，那岂不是晚上睡觉无家可归了，然后床上可以放置很多物品，就好比 Table上可以放置很多列和行一样，数据库中存储数据的基本单元是Table，现实中每个仓库放置物品的基本单位就是床， User就是每个Schema的主人，（所以Schema包含的是Object，而不是User），user和schema是一一对应的，每个user在没有特别指定下只能使用自己schema（房间）的东西，如果一个user想使用其他schema（房间）的东西，那就要看那个schema（房间）的user（主人）有没有给你这个权限了，或者看这个仓库的老大（DBA）有没有给你这个权限了。换句话说，如果你是某个仓库的主人，那么这个仓库的使用权和仓库中的所有东西都是你的（包括房间），你有完全的操作权，可以扔掉不用的东西从每个房间，也可以放置一些有用的东西到某一个房间，你还可以给每个User分配具体的权限，也就是他到某一个房间能做些什么，是只能看（Read-Only），还是可以像主人一样有所有的控制权（R/W），这个就要看这个User所对应的角色Role了。\n#### oracle的schema的含义####\n在现在做的Kraft Catalyst 项目中，Cransoft其中有一个功能就是schema refresh. 一直不理解schema什么意思，也曾经和同事讨论过，当时同事就给我举过一个例子，下面会详细说的。其实schema是Oracle中的，其他数据库中不知道有没有这个概念。\n首先,可以先看一下schema和user的定义：\nA schema is a collection of database objects (used by a user).\nSchema objects are the logical structures that directly refer to the database’s data.\nA user is a name defined in the database that can connect to and access objects.\nSchemas and users help database administrators manage database security.\n从中我们可以看出,schema为数据库对象的集合，为了区分各个集合，需要给这个集合起个名字，这些名字就是在企业管理器的方案下看到的许多类似用户名的节点，这些类似用户名的节点其实就是一个schema。\nschema里面包含了各种对象如tables, views, sequences, stored procedures, synonyms, indexes, clusters, and database links。\n一个用户一般对应一个schema，该用户的schema名等于用户名，并作为该用户缺省schema。这也就是在企业管理器的方案下看到schema名都为数据库用户名的原因。\nOracle数据库中不能新创建一个schema，要想创建一个schema，只能通过创建一个用户的方法解决(Oracle中虽然有create schema语句，但是它并不是用来创建一个schema的)。在创建一个用户的同时，为这个用户创建一个与用户名同名的schem并作为该用户的缺省 shcema。即schema的个数同user的个数相同，而且schema名字同user名字一一 对应并且相同，所有我们可以称schema为user的别名，虽然这样说并不准确，但是更容易理解一些。\n一个用户有一个缺省的schema，其schema名就等于用户名，当然一个用户还可以使用其他的schema。如果我们访问一个表时，没有指明该表属于 哪一个schema中的，系统就会自动给我们在表上加上缺省的sheman名。比如我们在访问数据库时，访问scott用户下的emp表，通过 select * from emp; 其实，这sql语句的完整写法为select * from scott.emp。在数据库中一个对象的完整名称为schema.object，而不属user.object。类似如果我们在创建对象时不指定该对象 的schema，在该对象的schema为用户的缺省schema。这就像一个用户有一个缺省的表空间，但是该用户还可以使用其他的表空间，如果我们在创 建对象时不指定表空间，则对象存储在缺省表空间中，要想让对象存储在其他表空间中，需要在创建对象时指定该对象的表空间。\n有人举了个很生动的例子，来说明Database、User、Schema、Tables、Col、Row等之间的关系\n“可以把Database看作是一个大仓库，仓库分了很多很多的房间，Schema就是其中的房间，一个Schema代表一个房间，Table可以看作是每个Schema中的床，Table（床）就被放入每个房间中，不能放置在房间之外，那岂不是晚上睡觉无家可归了。\n然后床上可以放置很多物品，就好比Table上可以放置很多列和行一样，数据库中存储数据的基本单元是Table，现实中每个仓库放置物品的基本单位就是床， User就是每个Schema的主人（所以Schema包含的是Object，而不是User）。\n其实User是对应与数据库的（即User是每个对应数据库的主人），既然有操作数据库（仓库）的权利，就肯定有操作数据库中每个Schema（房间）的 权利，就是说每个数据库映射的User有每个Schema（房间）的钥匙，换句话说，如果他是某个仓库的主人，那么这个仓库的使用权和仓库中的所有东西都 是他的（包括房间），他有完全的操作权，可以扔掉不用的东西从每个房间，也可以放置一些有用的东西到某一个房间。还可以给User分配具体的权限，也就是 他到某一个房间能做些什么，是只能看（Read-Only），还是可以像主人一样有所有的控制权（R/W），这个就要看这个User所对应的角色Role 了”\n从定义中我们可以看出schema为数据库对象的集合，为了区分各个集合，我们需要给这个集合起个名字，这些名字就是我们在企业管理器的方案下看到的许多类似用户名的节点，这些类似用户名的节点其实就是一个schema，schema里面包含了各种对象如tables, views, sequences, stored procedures, synonyms, indexes, clusters, and database links。\n一个用户一般对应一个schema,该用户的schema名等于用户名，并作为该用户缺省schema。这也就是我们在企业管理器的方案下看到schema名都为数据库用户名的原因。Oracle数据库中不能新创建一个schema，要想创建一个schema，只能通过创建一个用户的方法解决(Oracle中虽然有create schema语句，但是它并不是用来创建一个schema的)，在创建一个用户的同时为这个用户创建一个与用户名同名的schem并作为该用户的缺省shcema。即schema的个数同user的个数相同，而且schema名字同user名字一一 对应并且相同，所有我们可以称schema为user的别名，虽然这样说并不准确，但是更容易理解一些。\n一个用户有一个缺省的schema，其schema名就等于用户名，当然一个用户还可以使用其他的schema。如果我们访问一个表时，没有指明该表属于哪一个schema中的，系统就会自动给我们在表上加上缺省的sheman名。比如我们在访问数据库时，访问scott用户下的emp表，通过select * from emp; 其实，这sql语句的完整写法为select * from scott.emp。在数据库中一个对象的完整名称为schema.object，而不属user.object。类似如果我们在创建对象时不指定该对象的schema，在该对象的schema为用户的缺省schema。这就像一个用户有一个缺省的表空间，但是该用户还可以使用其他的表空间，如果我们在创建对象时不指定表空间，则对象存储在缺省表空间中，要想让对象存储在其他表空间中，我们需要在创建对象时指定该对象的表空间。\n咳，说了这么多，给大家举个例子，否则，一切枯燥无味！\nSQL> Gruant dba to scott\nSQL> create table test(name char(10));\nTable created.\nSQL> create table system.test(name char(10));\nTable created.\nSQL> insert into test values('scott');\n1 row created.\nSQL> insert into system.test values('system');\n1 row created.\nSQL> commit;\nCommit complete.\nSQL> conn system/manager\nConnected.\nSQL> select * from test;\n\nNAME\n----------\nsystem\nSQL> ALTER SESSION SET CURRENT_SCHEMA = scott; --改变用户缺省schema名\nSession altered.\nSQL> select * from test;\n\nNAME\n----------\nscott\nSQL> select owner ,table_name from dba_tables where table_name=upper('test');\nOWNER TABLE_NAME\n------------------------------ ------------------------------\nSCOTT TEST\nSYSTEM TEST\n--上面这个查询就是我说将schema作为user的别名的依据。实际上在使用上，shcema与user完全一样，没有什么区别，在出现schema名的地方也可以出现user名。\n表空间：\n一个表空间就是一片磁盘区域,他又一个或者多个磁盘文件组成,一个表空间可以容纳许多表、索引或者簇等  \n  每个表空间又一个预制的打一磁盘区域称为初始区间（initial   extent）用完这个区间厚在用下一个，知道用完表空间，这时候需要对表空间进行扩展，增加数据文件或者扩大已经存在的数据文件\n \n \n\ninstance是一大坨内存sga,pga....和后台的进程smon pmon.....组成的一个大的应用。\nschema就是一个用户和他下面的所有对象。。\ntablspace 逻辑上用来放objects.物理上对应磁盘上的数据文件或者裸设备。\n 在Oracle中，结合逻辑存储与物理存储的概念，我们可以这样来理解数据库、表空间、SCHEMA、数据文件这些概念：\n      数据库是一个大圈，里面圈着的是表空间，表空间里面是数据文件，那么schema是什么呢？schema是一个逻辑概念，是一个集合，但schema并不是一个对象，oracle也并没有提供创建schema的语法。\nschema：\n      一般而言，一个用户就对应一个schema,该用户的schema名等于用户名，并作为该用户缺省schema，用户是不能创建schema的，schema在创建用户的时候创建，并可以指定用户的各种表空间（这点与PostgreSQL是不同，PostgreSQL是可以创建schema并指派给某个用户）。当前连接到数据库上的用户创建的所有数据库对象默认都属于这个schema（即在不指明schema的情况下），比如若用户scott连接到数据库，然后create table test(id int not null)创建表，那么这个表被创建在了scott这个schema中；但若这样create kanon.table test(id int not null)的话，这个表被创建在了kanon这个schema中，当然前提是权限允许。\n      创建用户的方法是这样的：\n      create user 用户名 identified by 密码 \n      default tablespace 表空间名 \n      temporary tablespace 表空间名 \n      quota 限额  （建议创建的时候指明表空间名）\n由此来看，schema是一个逻辑概念。\n      但一定要注意一点：schema好像并不是在创建user时就创建的，而是在该用户创建了第一个对象之后才将schema真正创建的，只有user下存在对象，他对应的schema才会存在，如果user下不存在任何对象了，schema也就不存在了；\n \n数据库：\n     在oracle中，数据库是由表空间来组成的，而表空间里面是具体的物理文件---数据文件。我们可以创建数据库并为其指定各种表空间。\n \n表空间：\n     这是个逻辑概念，本质上是一个或者多个数据文件的集合。\n \n数据文件：\n     具体存储数据的物理文件，是一个物理概念。\n     一个数据文件只能属于一个表空间，一个表空间可以包含一个或多个数据文件。一个数据库由多个表空间组成，一个表空间只能属于一个数据库。\n","slug":"oracle/Oracle SQL基础知识","published":1,"updated":"2017-08-18T01:44:35.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjh7polwy009nbp7r3lq5s90f","content":"<h2 id=\"Oracle-SQL基本知识\"><a href=\"#Oracle-SQL基本知识\" class=\"headerlink\" title=\"Oracle SQL基本知识\"></a>Oracle SQL基本知识</h2><h3 id=\"安装数据库\"><a href=\"#安装数据库\" class=\"headerlink\" title=\"安装数据库\"></a>安装数据库</h3><h4 id=\"1）安装Oracle常用问题-常用”用户名-密码“规则-：\"><a href=\"#1）安装Oracle常用问题-常用”用户名-密码“规则-：\" class=\"headerlink\" title=\"1）安装Oracle常用问题(常用”用户名/密码“规则)：\"></a>1）安装Oracle常用问题(常用”用户名/密码“规则)：</h4><p>超级管理员：sys /change_on_install<br>普通管理员：system/manager<br>普通用户：scott/tiger—–&gt;默认是被锁定的<br>大数据用户：sh/sh</p>\n<h4 id=\"2）SQL-DDL…\"><a href=\"#2）SQL-DDL…\" class=\"headerlink\" title=\"2）SQL,DDL…\"></a>2）SQL,DDL…</h4><p>SQL：structured query language 结构化查询语言<br>1.file(文件)</p>\n<p>SQL:DDL DML TCL DQL DCL<br>DDL(data definition language 数据定义语言): column(列)–structure<br>create table (创建表):<br>列名 data type(数据类型) width(宽度)<br>constraint (约束)      alter table(修改表结构)           drop table(删除表)</p>\n<p>DML(data manipulation language 数据操作语言)<br>:row(行)–data<br>insert 增       update 改            delete 删数据,删表里的记录</p>\n<p>TCL(transaction control language 事务控制语言)<br>commit(提交)         rollback(回滚)               savepoint(保留点)</p>\n<p>DQL(data query language 数据查询语言)<br>select<br>DCL(data control language 数据控制语言)<br>grant(授权)  grant to       revoke(回收权限) revoke from </p>\n<h4 id=\"3）RDBMS关系型数据库管理系统\"><a href=\"#3）RDBMS关系型数据库管理系统\" class=\"headerlink\" title=\"3）RDBMS关系型数据库管理系统\"></a>3）RDBMS关系型数据库管理系统</h4><p>RDBMS(relationship database management system 关系型数据库管理系统) software(软件) —&gt;(create database)database—&gt;login in database (登录数据库系统 )—&gt;用SQL操作table</p>\n<p>create database 创建空间存储表 (datafile 数据文件)<br>login in database<br>1 远程登录到数据库所在的机器上<br>  192.168.0.20 192.168.0.23 192.168.0.26<br>shell(终端) telnet 192.168.0.20  (跟操作系统建连接)<br>login:openlab<br>password:open123<br>sunv210% shell提示符,执行操作系统命令</p>\n<h4 id=\"4）-登录该机器上的数据库系统\"><a href=\"#4）-登录该机器上的数据库系统\" class=\"headerlink\" title=\"4） 登录该机器上的数据库系统\"></a>4） 登录该机器上的数据库系统</h4><p>sunv210% sqlplus (跟数据库建连接)<br>Enter user-name: openlab<br>Enter password:open123<br>SQL&gt;sqlplus openlab/open123<br>SQL&gt; 数据库提示符,执行SQL命令</p>\n<h4 id=\"5）登录的是哪个数据库\"><a href=\"#5）登录的是哪个数据库\" class=\"headerlink\" title=\"5）登录的是哪个数据库\"></a>5）登录的是哪个数据库</h4><p>echo $ORACLE_SID(环境变量)&lt;—DBA(database administrator 数据库管理员)<br>查看ORACLE_SID变量的取值,oracle提供<br>通过设置ORACLE_SID变量,sqlplus就知道跟哪个数据库建连接.<br>unix平台<br>%c shell<br>%echo $ORACLE_SID  (tarena)<br>%setenv ORACLE_SID hiloo<br>%setenv ORACLE_SID tarena</p>\n<p>$ b shell<br>$ echo $ORACLE_SID  (tarena)<br>$ ORACLE_SID=hiloo<br>$ export ORACLE_SID</p>\n<p>windows平台<br>D:>set ORACLE_SID=hiloo (设置环境变量)<br>D:>set ORACLE_SID (查看环境变量)<br>ORACLE_SID=hiloo</p>\n<h5 id=\"数据表信息：\"><a href=\"#数据表信息：\" class=\"headerlink\" title=\"数据表信息：\"></a>数据表信息：</h5><p>dept(表名) department 部门信息   列名<br>deptno 部门号  dname  部门名称      location 位置(地区)<br>create table dept_hiloo<br>(deptno  number(2), dname char(20),  location char(20));<br>insert into dept_hiloo values (10,’developer’,’beijing’);<br>insert into dept_hiloo values (20,’account’,’shanghai’);<br>insert into dept_hiloo values (30,’sales’,’guangzhou’);<br>insert into dept_hiloo values  ( 40,’operations’,’tianjin’);<br>commit;<br>insert成功后的提示:1 rows inserted<br>emp(表名) employee 员工信息    列名<br>empno 员工 ename 员工名字  job   职位   salary  月薪   bonus   奖金<br>hiredate  入职日期  mgr   manager 管理者    deptno  部门号<br>create table emp_hiloo(<br>empno number(4),    ename varchar2(20),  job  varchar2(15),<br>salary number(7,2), bonus number(7,2),  hiredate date,<br> mgr number(4),  deptno number(10));<br>alter session set nls_date_language=’american’;<br>insert into emp_hiloo values (1001,’zhangwuji’,’Manager’,10000,2000,’12-MAR-10’,1005,10);<br>insert into emp_hiloo values (1001,’zhangwuji’,’Manager’,10000,2000,’12-MAR-10’,1005,10);<br>insert into emp_hiloo values (1002,’liucangsong’,’Analyst’,8000,1000, ‘01-APR-11’,1001,10);<br>insert into emp_hiloo values (1003,’liyi’,’Analyst’,9000,1000,’11-APR-10’,1001,10);<br>insertinto emp_hiloo values (1004,’guofurong’,’Programmer’,5000,null,’01-JAN-11’,1001,10);<br>insertintoemp_hiloo values (1005,’zhangsanfeng’,’President’,15000,null,’15-MAY-08’,null,20);<br>insert into emp_hiloo values (1006,’yanxiaoliu’,’Manager’,5000,400,’01-FEB-09’,1005,20);<br>insert into emp_hiloo values (1007,’luwushuang’,’clerk’,3000,500,’01-FEB-06’,1006,20);<br>insert into emp_hiloo values (1008,’huangrong’,’Manager’,5000,500,’1-MAY-09’,1005,30);<br>insert into emp_hiloo values (1009,’weixiaobao’,’salesman’,4000,null,’20-FEB-09’,1008,30);<br>insert into emp_hiloo values (1010,’guojing’,’salesman’,4500,500,’10-MAY-09’,1008,30);<br>报错信息<br>ORA-00955: name is already used by an existing object(名字已经被一个存在的对象使用)<br>错误：ORA-01843:无效的月份（在中文的plsql控制台上月份要写成’10-3月-02’这种形式，必须是一个数字和一个汉语月。也可以把日期改成英文环境，在执行插入前执行alter session set nls_date_language=’american’;就可以 了。</p>\n<p>DQL<br>select(选择)<br>源表  结果集<br>1 投影操作 select子句实现<br>2 选择操作 where子句实现<br>3 连接操作<br> 1  select ename,salary<em>12 ann_sal(列别名)<br> 2</em> from emp_hiloo</p>\n<p>单引号 表达字符串 ‘’<br>双引号 表达列别名 “”,别名中包含空格,大小写敏感</p>\n<h5 id=\"1）null值的理解\"><a href=\"#1）null值的理解\" class=\"headerlink\" title=\"1）null值的理解\"></a>1）null值的理解</h5><p>1 null值出现在算术表达式中,结果必为null,null可以看作无穷大.<br>2 函数(function) nvl功能空值转换函数<br>nvl是函数名,p1,p2是参数,数据类型必须一致,函数本身有返回值<br>nvl(p1,p2)<br>nvl函数实现:<br>if p1 is null then<br>   return p2;<br>else<br>   return p1;<br>end if;</p>\n<p>3 若有多个null值,distinct去重时,结果集保留一个null值.<br>4 null = null 不成立 null &lt;&gt; null 不成立<br>5 若用in运算符,集合中有null值跟没有null值结果一致的,结果集中不会出现跟null值有关的记录<br>  若用not in运算符,集合中有null值,这个结果集不包含记录.no rows selected.</p>\n<h5 id=\"2）各个子句的功能\"><a href=\"#2）各个子句的功能\" class=\"headerlink\" title=\"2）各个子句的功能\"></a>2）各个子句的功能</h5><p>1 select后面跟列名,列别名,函数,表达式<br>2 select后面的distinct:去重<br>3 where子句<br>  where 条件表达式 (列名 比较运算符 值)<br>表达式 比较运算符 值(尽量不用,为了性能)<br>  where子句中的列为字符类型,放值的位置上不加单引号或加双引号当列名解释,加单引号当字符串解释.<br>  where子句中的列为字符类型,表达具体值时注意字符是大小写敏感的.<br>SQL提供的四个比较运算符<br>肯定形式<br>   between and 区间,范围<br>   in &lt;=&gt; =any  (= or = )(跟集合里的任意一个值相等就满足条件) 集合 离散值<br>   = 单值运算符<br>   in =any 多值运算符<br>   like 像…一样<br>   通配符: %表示0或任意多个字符 <em>任意一个字符<br>   ‘S’ ‘S%’ ‘S</em>‘<br>   is null  如何判断一个列的取值是否为空<br>否定形式<br>= &lt;&gt; != ^=<br>between and   not between and<br>in    not in (&lt;&gt; and &lt;&gt;) &lt;=&gt; &lt;&gt;all(跟集合里的所有值都不能相等)<br>like     not like<br>is null   is not null<br>各个子句的执行顺序<br>from–&gt;where–&gt;select</p>\n<h5 id=\"3）课堂练习\"><a href=\"#3）课堂练习\" class=\"headerlink\" title=\"3）课堂练习\"></a>3）课堂练习</h5><p>1 列出每个员工的名字和他的工资<br>  select ename,salary from emp_hiloo;<br>2 列出每个员工的名字和他的职位<br>  select ename,job from emp_hiloo;<br>3 列出每个员工的名字和他的年薪<br> select ename,salary<em>12 ann_sal from emp_hiloo;<br>4 列出每个员工的名字和他一年的总收入<br>  (salary+bonus)</em>12 (15000+null)<em>12=null<br>  select ename,(salary+nvl(bonus,0))</em>12 tol_sal<br>  from emp_hiloo;<br>5 输出结果如下:<br>  zhangwuji is in department 10.<br>  liucangsong is in department 10.<br>  …..<br>  guojing is in department 30.<br>select ename||’is in department’||deptno||’.’employee from emp_hiloo;<br>什么要加employee呢？Employee是列别名为了显示用的。<br>6 列出该公司有哪些职位<br>  select distinct(job) from emp_hiloo;<br>  select distinct job from emp_hiloo;<br>7 列出该公司不同的奖金<br>  select distinct bonus from emp_hiloo;<br>8 各个部门有哪些不同的职位?<br>  select distinct deptno,job from emp_hiloo;<br>  去重方式:deptno和job联合唯一.<br>  distinct之后和from之前的所有列联合唯一.<br>distinct是保证每一行的唯一性而非某一列的唯一性，所以必须紧跟在select后面。<br>所以distinct只能放在select后面，紧跟select不然会报缺失表达式错误。<br>9 哪些员工的工资高于5000?<br>  select ename,salary from emp_hiloo<br>  where salary &gt; 5000;<br>10 列出员工工资高于5000的员工的年薪?<br>  select ename,salary<em>12 from emp_hiloo<br>  where salary &gt; 5000;<br>11 列出员工年薪高于60000的员工的年薪?<br>  select ename,salary</em>12 from emp_hiloo<br>  where salary<em>12&gt; 60000;<br>  select ename,salary</em>12 ann_sal from emp_hiloo<br>  where ann_sal &gt; 60000(错误的写法)<br>  select ename,salary<em>12 from emp_hiloo<br>  where salary &gt; 5000;<br>12 zhangwuji的年薪是多少?<br>select ename,salary</em>12 from emp_hiloo<br>where ename=’zhangwuji’;<br>  哪些员工的职位是Manager?<br>select ename,job from emp_hiloo<br>where job=’Manager’;<br>  哪些员工的职位是clerk?<br>  select ename,job from emp_hiloo<br>  where job = ‘Manager’<br>   select ename,job from emp_hiloo<br>  where job = ‘clerk’(效率高)<br>  clerk的大小写不清楚<br>  函数:upper(),lower()<br>  select ename,job from emp_hiloo<br>  where upper(job) = ‘CLERK’ (通用性好)<br>13 员工工资在5000到10000之间的员工的年薪<br>   select ename,salary<em>12<br>   from emp_hiloo<br>   where salary &gt;= 5000<br>   and   salary &lt;= 10000;<br>   select ename,salary</em>12<br>   from emp_hiloo<br>   where salary between 5000 and 10000;<br>14 哪些员工的工资是5000或10000.<br>   select ename,salary<br>   from emp_hiloo<br>   where salary = 5000<br>   or salary = 10000<br>   select ename,salary<br>   from emp_hiloo<br>   where salary in (5000,10000)<br>   select ename,salary<br>   from emp_hiloo<br>   where salary =any (5000,10000)<br>15 哪个员工的名字的第二个字符是a.<br>   select ename<br>   from emp_hiloo<br>   where ename like ‘<em>a%’;<br>16 哪个员工的名字的第二个字符是</em>.<br>   select ename<br>   from emp_hiloo<br>   where ename like ‘__%’ escape ‘\\’;<br>   第一个<em>表示任意一个字符,代表通配符<br>   \\</em>必须连起来看,表示下划线本身,escape定义哪个字符可以定义转义’\\’<br>17 哪些员工没有奖金?<br>   select ename,bonus<br>   from emp_hiloo<br>   where bonus is null<br>18 哪些员工有奖金?<br>   select ename,bonus<br>   from emp_hiloo<br>   where bonus is not null<br>19哪些员工的工资不是5000也不是10000.<br>  select ename,salary<br>  from emp_hiloo<br>  where salary not in (5000,10000);<br>  select ename,salary<br>  from emp_hiloo<br>  where salary &lt;&gt; 5000<br>  and salary &lt;&gt; 10000</p>\n<p>create table emp_hiloo<br>( hiredate date）<br>insert into emp_hiloo values (1001,’zhangwuji’,’Manager’,10000,2000,’12-MAR-10’,1005,10);<br>解决方案：<br>    insert into emp_hiloo values (1001,’zhangwuji’,’Manager’,10000,2000,’12-3月-10’,1005,10);</p>\n<h5 id=\"更改字段名字-mysql、orcle-：\"><a href=\"#更改字段名字-mysql、orcle-：\" class=\"headerlink\" title=\"更改字段名字(mysql、orcle)：\"></a>更改字段名字(mysql、orcle)：</h5><p>Oracle修改表<br>alter table 表名 rename column 原名 to 新名；<br>Mysql:<br>alter table 表名 change column(可写，可不写）原名 新名 字段类型；</p>\n<p>ORA-00904：“ANN_SAL”:invalid identifier<br>无效的标识符</p>\n<p>index(索引) view(视图) sequence(顺序号/序列号) function(函数)<br>session altered.会话已更改<br>set feed on可以设置一个，显示操作数<br>connet tiger重新建立连接  show user查看当前用户是谁。<br>edit 用记事本编辑  /运行。</p>\n<p>###Function (单行、多行)###<br>单行函数:表中的一列作为函数的参数,对于每一条记录函数都有一个返回值.<br>例如:upper lower nvl<br>多行函数：表中的一列作为函数的参数,将记录分组,对于每组数据函数返回一个值.<br>例如:avg</p>\n<p>####1）单行函数####<br> 根据处理参数的数据类型分为</p>\n<h5 id=\"1）字符函数-upper-lower\"><a href=\"#1）字符函数-upper-lower\" class=\"headerlink\" title=\"1）字符函数:upper,lower\"></a>1）字符函数:upper,lower</h5><h5 id=\"2）数值函数\"><a href=\"#2）数值函数\" class=\"headerlink\" title=\"2）数值函数:\"></a>2）数值函数:</h5><pre><code>round 四舍五入\nround(12.345,2)--&gt;12.35\nround(12.345,0)=round(12.345)--&gt;12\nround(12,345,-1)--&gt;10\ntrunc 截取\ntrunc(12.345,2)--&gt;12.34\ntrunc(12.345,0)=trunc(12.345)--&gt;12\ntrunc(12,345,-1)--&gt;10\n</code></pre><h5 id=\"3-日期和日期函数\"><a href=\"#3-日期和日期函数\" class=\"headerlink\" title=\"3) 日期和日期函数\"></a>3) 日期和日期函数</h5><pre><code>select sysdate from dual\n06-SEP-12 DD-MON-RR \nalter session set\n  nls_date_format = &apos;yyyy mm dd hh24:mi:ss&apos;\nsession 会话 connection(连接)\n</code></pre><p>   日期类型的数据是用固定的字节7个字节来存储世纪,年,月,日,时,分,秒. 格式敏感<br>   会话级 alter session set nls_date_format<br>   语句级 select to_char(c1日期类型用7个字节来表达，日期类型的数据是用固定的字节7个字节来存储世纪，年，月，日，时，分，秒。四位年的前两位代表世纪20，后两位代表当前年12<br>如果不想修改sql语句运行的话，就需要在执行该语句之前，使用alter session 命令将nls_date_language修改为american，如下：<br>alter session set nls_date_language=’american’    –以英语显示日期<br>如果不想修改sql语句运行的话，就需要在执行该语句之前，使用alter session 命令将</p>\n<p>‘01-JAN-08’ 系统做了隐式数据类型转换,调用了to_date函数<br>‘2008-01-01’,用户做显式数据类型转换,自己调用<br>to_date(‘2008-01-01’,’yyyy-mm-dd’),第二个参数是对第一个参数的格式说明.<br>to_char的返回类型是字符类型,把date转换成了字符串类型,所以参数的数据类型是date.to_char函数可以获得日期的任何一部分信息,比如年,月,日等.<br>select c1 from … 系统做了隐式数据类型转换,调用了to_char函数<br>select to_char(c1,.. 用户做显式数据类型转换,自己调用to_char(c1,’yyyy-mm-dd’),第二个参数是对第一个参数的格式说明.<br>日期的运算<br>   日期可以加减一个数值,单位为天.<br>   select sysdate-1,sysdate,sysdate+1 from dual<br>两个日期相减<br>   add_months 按月加 返回类型是date<br>   add_months(sysdate,6)<br>   select add_months(hiredate,6) from emp_hiloo<br>   add_months(sysdate,-6)<br>   months_between()  返回类型是number<br>   months_between(sysdate,hiredate) 两个日期之间相差多少个月<br>select months_between(sysdate,hiredate) from emp_hiloo;<br>   last_day(sysdate) 本月的最后一天</p>\n<h5 id=\"4-转换函数\"><a href=\"#4-转换函数\" class=\"headerlink\" title=\"4) 转换函数\"></a>4) 转换函数</h5><p>两个日期相减转换函数<br>to_date  char–&gt;date<br>to_char  date–&gt;char , number –&gt; char<br>to_number  char–&gt;number</p>\n<h5 id=\"其他函数\"><a href=\"#其他函数\" class=\"headerlink\" title=\"其他函数\"></a>其他函数</h5><p>coalesce 类似nvl(oracle专有)<br>nvl(bonus,salary<em>0.1)<br>coalesce(bonus,salary</em>0.1,100)。输出所有员工的奖金，如果没有奖金就按工资的10%发放，如果奖金和工资都没有的临时工，就给100元。<br>不同的记录处理方式不一样时,用case when.<br>case when 条件表达式 then 返回结果<br>else<br>     返回结果<br>end<br>若没有else,当不匹配条件,表达式的返回值为null.<br>case deptno when 10 then(不建议该语法形式)<br>decode跟case when的功能类似.<br>decode(deptno,10,salary<em>1.1,<br>              20,salary</em>1.2,<br>              salary)<br>若没有最后一个参数,函数的返回值为null.<br>select语句<br>order by子句<br>select   from    where<br>order by<br>order by子句是select语句中的最后一个子句.<br>order by salary 缺省是升序 asc<br>order by salary desc 降序<br>order by子句后面可以跟列名,表达式(函数),列别名,在select子句中的位置.<br>ORDER BY 子句<br>ORDER BY 语句用于对结果集进行排序。<br>ORDER BY 语句<br>ORDER BY 语句用于根据指定的列对结果集进行排序。<br>ORDER BY 语句默认按照升序对记录进行排序。<br>如果您希望按照降序对记录进行排序，可以使用 DESC 关键字。<br>原始的表 (用在例子中的)：<br>Orders 表:<br>Company    OrderNumber<br>IBM    3532<br>W3School    2356<br>Apple    4698<br>W3School    6953<br>实例 1<br>以字母顺序显示公司名称：<br>SELECT Company, OrderNumber FROM Orders ORDER BY Company<br>结果：<br>Company    OrderNumber<br>Apple    4698<br>IBM    3532<br>W3School    6953<br>W3School    2356<br>实例 2<br>以字母顺序显示公司名称（Company），并以数字顺序显示顺序号（OrderNumber）：<br>SELECT Company, OrderNumber FROM Orders ORDER BY Company, OrderNumber<br>结果：<br>Company    OrderNumber<br>Apple    4698<br>IBM    3532<br>W3School    2356<br>W3School    6953<br>实例 3<br>以逆字母顺序显示公司名称：<br>SELECT Company, OrderNumber FROM Orders ORDER BY Company DESC<br>结果：<br>Company    OrderNumber<br>W3School    6953<br>W3School    2356<br>IBM    3532<br>Apple    4698<br>实例 4<br>以逆字母顺序显示公司名称，并以数字顺序显示顺序号：<br>SELECT Company, OrderNumber FROM Orders ORDER BY Company DESC, OrderNumber ASC<br>结果：<br>Company    OrderNumber<br>W3School    2356<br>W3School    6953<br>IBM    3532<br>Apple    4698<br>注意：在以上的结果中有两个相等的公司名称 (W3School)。只有这一次，在第一列中有相同的值时，第二列是以升序排列的。如果第一列中有些值为 nulls 时，情况也是这样的。</p>\n<h4 id=\"2-多行函数-哪两个函数里只能放number\"><a href=\"#2-多行函数-哪两个函数里只能放number\" class=\"headerlink\" title=\"2) 多行函数(哪两个函数里只能放number)\"></a>2) 多行函数(哪两个函数里只能放number)</h4><p>avg()    平均值  函数的参数只能是number<br>sum()    求和    函数的参数只能是number<br>count()    计数 函数的参数可以是number date 字符<br>        count(*)统计记录,count(bonus)<br>max() 最大值 函数的参数可以是number date 字符<br>min() 最小值 函数的参数可以是number date 字符</p>\n<p>组函数的缺省处理方式是处理所有的非空值.<br>avg(bonus) 所有有奖金的员工的平均值<br>count(bonus) 有奖金的员工个数<br>当所有的值都是null,count函数返回0,其他组函数返回null.</p>\n<h4 id=\"3-group-by子句\"><a href=\"#3-group-by子句\" class=\"headerlink\" title=\"3) group by子句\"></a>3) group by子句</h4><p>若有group by子句,select后面跟组标识和组函数<br>组标识指group by后面的内容<br>from–&gt;where–&gt;group by–&gt;select–&gt;order by<br>若没有group by子句,select后面只要有一个是组函数,其余的都得是组函数.</p>\n<h4 id=\"having子句\"><a href=\"#having子句\" class=\"headerlink\" title=\"having子句\"></a>having子句</h4><p>select deptno,round(avg(salary)) davg<br>from emp_hiloo<br>group by deptno<br>having round(avg(salary))&gt; 5000</p>\n<p>from–&gt;where–&gt;group by–&gt;having–&gt;select–&gt;order by </p>\n<h4 id=\"GROUP-BY-语句\"><a href=\"#GROUP-BY-语句\" class=\"headerlink\" title=\"GROUP BY 语句\"></a>GROUP BY 语句</h4><p>GROUP BY 语句用于结合合计函数，根据一个或多个列对结果集进行分组。<br>SQL GROUP BY 语法<br>SELECT column_name, aggregate_function(column_name)<br>FROM table_name<br>WHERE column_name operator value<br>GROUP BY column_name<br>SQL GROUP BY 实例<br>我们拥有下面这个 “Orders” 表：<br>O_Id    OrderDate    OrderPrice    Customer<br>1    2008/12/29    1000    Bush<br>2    2008/11/23    1600    Carter<br>3    2008/10/05    700    Bush<br>4    2008/09/28    300    Bush<br>5    2008/08/06    2000    Adams<br>6    2008/07/21    100    Carter<br>现在，我们希望查找每个客户的总金额（总订单）。我们想要使用 GROUP BY 语句对客户进行组合。<br>我们使用下列 SQL 语句：<br>SELECT Customer,SUM(OrderPrice) FROM Orders<br>GROUP BY Customer<br>结果集类似这样：<br>Customer    SUM(OrderPrice)<br>Bush    2000<br>Carter    1700<br>Adams    2000<br>很棒吧，对不对？<br>让我们看一下如果省略 GROUP BY 会出现什么情况：<br>SELECT Customer,SUM(OrderPrice) FROM Orders<br>结果集类似这样：<br>Customer    SUM(OrderPrice)<br>Bush    5700<br>Carter    5700<br>Bush    5700<br>Bush    5700<br>Adams    5700<br>Carter    5700<br>上面的结果集不是我们需要的。<br>那么为什么不能使用上面这条 SELECT 语句呢？解释如下：上面的 SELECT 语句指定了两列（Customer 和 SUM(OrderPrice)）。”SUM(OrderPrice)” 返回一个单独的值（”OrderPrice” 列的总计），而 “Customer” 返回 6 个值（每个值对应 “Orders” 表中的每一行）。因此，我们得不到正确的结果。不过，您已经看到了，GROUP BY 语句解决了这个问题。<br>GROUP BY 一个以上的列<br>我们也可以对一个以上的列应用 GROUP BY 语句，就像这样：<br>SELECT Customer,OrderDate,SUM(OrderPrice) FROM Orders<br>GROUP BY Customer,OrderDate</p>\n<h4 id=\"4-where和having比较\"><a href=\"#4-where和having比较\" class=\"headerlink\" title=\"4) where和having比较\"></a>4) where和having比较</h4><p>共同点:都执行在select之前,都有过滤功能<br>区别<br>where执行在having之前<br>where过滤的是记录,任意列名都可以出现在where子句,单行函数可以用在where子句,组函数不能出现在where子句<br>having过滤的是组,组标识可以出现在having子句,其他列名不行,组函数用于having子句,单行函数不可以.</p>\n<h5 id=\"HAVING-子句\"><a href=\"#HAVING-子句\" class=\"headerlink\" title=\"HAVING 子句\"></a>HAVING 子句</h5><p>在 SQL 中增加 HAVING 子句原因是，WHERE 关键字无法与合计函数一起使用。<br>SQL HAVING 语法<br>SELECT column_name, aggregate_function(column_name)<br>FROM table_name<br>WHERE column_name operator value<br>GROUP BY column_name<br>HAVING aggregate_function(column_name) operator value<br>SQL HAVING 实例<br>我们拥有下面这个 “Orders” 表：<br>O_Id    OrderDate    OrderPrice    Customer<br>1    2008/12/29    1000    Bush<br>2    2008/11/23    1600    Carter<br>3    2008/10/05    700    Bush<br>4    2008/09/28    300    Bush<br>5    2008/08/06    2000    Adams<br>6    2008/07/21    100    Carter<br>现在，我们希望查找订单总金额少于 2000 的客户。<br>我们使用如下 SQL 语句：<br>SELECT Customer,SUM(OrderPrice) FROM Orders<br>GROUP BY Customer<br>HAVING SUM(OrderPrice)<2000 1500=\"\" 1700=\"\" 结果集类似：=\"\" customer=\"\" sum(orderprice)=\"\" carter=\"\" 现在我们希望查找客户=\"\" \"bush\"=\"\" 或=\"\" \"adams\"=\"\" 拥有超过=\"\" 的订单总金额。=\"\" 我们在=\"\" sql=\"\" 语句中增加了一个普通的=\"\" where=\"\" 子句：=\"\" select=\"\" customer,sum(orderprice)=\"\" from=\"\" orders=\"\" or=\"\" group=\"\" by=\"\" having=\"\">1500<br>结果集：<br>Customer    SUM(OrderPrice)<br>Bush    2000<br>Adams    2000</2000></p>\n<h4 id=\"5-DCL\"><a href=\"#5-DCL\" class=\"headerlink\" title=\"5) DCL\"></a>5) DCL</h4><p>connect openlab/open123<br>select count(*) from hiloo.emp_hiloo;</p>\n<p>connect hiloo/hiloo123<br>grant select on emp_hiloo to openlab;</p>\n<p>connect openlab/open123<br>select count(*) from hilool.emp_hiloo<br>10rows selected</p>\n<p>connect hiloo/hiloo123<br>revoke select on emp_hiloo from openlab;</p>\n<p>show user<br>select count(*) from hiloo.emp_hiloo</p>\n<p>create synonym emp_hiloo for hiloo.emp_hiloo</p>\n<h4 id=\"6-关于null值的讨论\"><a href=\"#6-关于null值的讨论\" class=\"headerlink\" title=\"6) 关于null值的讨论\"></a>6) 关于null值的讨论</h4><p>1 case when在没有else和decode少一个参数时,返回null.<br>2order by bonus,asc升序时null值在最后,desc降序时null在最前.<br>3 组函数和null值的关系:1组函数的缺省处理方式是处理所有的非空值.2当所有的值都是null,count函数返回0,其他组函数返回null.<br>4若group by的列有null值,所有的null值分在一组.<br>课堂练习<br>1将每个员工的工资涨12.34567%,用round和trunc分别实现<br>select ename,nvl(trunc(round(salary+salary*0.1234567,2),1),0.0) from emp_hiloo;//自己写的。<br>2 将’2008-01-01’插入表中,<br>  再将’2008 08 08 08:08:08’插入表中<br>insert into test values<br>(to_date(‘01-JAN-08’,’DD-MON-RR’));</p>\n<p>3找出3月份入职的员工.<br>select ename,hiredate<br>from emp_hiloo<br>where to_char(hiredate,’mm’) = ‘03’;<br>select ename,hiredate<br>from emp_hiloo<br>where to_char(hiredate,’mm’) = 3;//可以正常输出winXP下<br>‘03’ = 3  —&gt; to_number(‘03’) = 3<br>字符   数值  缺省系统将字符转成数值<br>select ename,hiredate<br>from emp_hiloo<br>where to_char(hiredate,’fmmm’) = ‘03’;(错，未选定行，无输出)</p>\n<p>select ename,hiredate<br>from emp_hiloo<br>where to_char(hiredate,’fmmm’) = ‘3’;(对)<br>‘03’ = ‘3’ (错)<br>fm表示去掉前导0或去掉两边的空格.<br>4 zhangsanfeng的mgr上显示boss,其他人不变.<br>select ename,empno,<br>       nvl(to_char(mgr),’boss’) mgr<br>from emp_hiloo<br>函数nvl（“1”，“2”）:如果字符串1是空，就返回字符串”2”</p>\n<h4 id=\"5十分钟之后\"><a href=\"#5十分钟之后\" class=\"headerlink\" title=\"5十分钟之后\"></a>5十分钟之后</h4><p> select sysdate,sysdate+1/144 from dual;<br>解释：Oracle 里面,</p>\n<p>sysdate + 1 意思是 当前时间 + 1天</p>\n<p>sysdate + 1/24  意思是 当前时间 + 1/24天  也就是1小时后</p>\n<p>sysdate+1/144  意思是 当前时间 + 1/144天 （1/24<em>6）  也就是10分钟后<br> 6 若员工是10部门的,工资涨10%,20部门工资涨20%,其他员工工资不变.<br>select ename,salary,<br>       case when deptno = 10 then salary</em>1.1<br>            when deptno = 20 then salary*1.2<br>       else<br>            salary<br>       end new_sal<br>from emp_hiloo;</p>\n<p>select ename,salary,<br>       decode(deptno,10,salary<em>1.1,<br>                     20,salary</em>1.2,<br>                     salary) new_sal<br>from emp_hiloo;<br>7 列出每个员工的年薪,按年薪降序排列.<br>select ename,salary<em>12<br>from emp_hiloo<br>order by salary desc (好)<br>select ename,salary</em>12<br>from emp_hiloo<br>order by salary<em>12 desc<br>select ename,salary</em>12 n_sal<br>from emp_hiloo<br>order by n_sal desc</p>\n<p>select ename,salary<em>12 n_sal from emp_hiloo order by 2 desc;<br>select salary</em>12,ename n_sal from emp_hiloo order by 2 asc;<br>8 列出员工的名字,部门号以及工资,按部门号从小到大的顺序,同一部门的工资按降序排列.<br>select ename,deptno,salary<br>from emp_hiloo<br>order by deptno,salary desc<br>9 列出奖金的平均值,和,个数,最大值,最小值.<br>AVG 函数返回数值列的平均值。NULL 值不包括在计算中<br>select avg(bonus),avg(nvl(bonus,0)),<br>       sum(bonus), sum(nvl(bonus,0)),<br>       count(bonus),count(nvl(bonus,0)),<br>       max(bonus),max(nvl(bonus,0)),<br>       min(bonus),min(nvl(bonus,0))<br>from emp_hiloo<br>10 各个部门的平均工资<br>ROUND 函数用于把数值字段舍入为指定的小数位数。<br>select deptno,round(avg(salary))<br>from emp_hiloo<br>group by deptno<br>11 求10部门的平均工资,只显示平均工资<br>   求10部门的平均工资,显示部门号,平均工资<br>   select round(avg(salary))<br>   from emp_hiloo<br>   where deptno = 10<br>   group by deptno</p>\n<p>   select max(deptno),round(avg(salary))<br>   from emp_hiloo<br>   where deptno = 10<br>12各个部门不同职位的平均工资<br>   select deptno,job,round(avg(salary))<br>   from emp_hiloo<br>   group by deptno,job<br>13 每种奖金有多少人?<br>   select bonus,count(empno)<br>   from emp_hiloo<br>   group by bonus<br>14 列出平均工资大于5000的部门的平均工资<br>   select deptno,round(avg(salary))<br>   from emp_hiloo<br>   group by deptno<br>   having round(avg(salary)) &gt; 5000<br>15哪些员工的工资是最低的.<br>  select ename from emp_hiloo<br>  where salary = ( select min(salary)<br>                   from emp_hiloo)<br>报错信息<br>ORA-01861: literal does not match format string<br>文字值不匹配格式串<br>ORA-01722: invalid number 无效的数值 to_number<br>ORA-00937: not a single-group group function 不是一个组函数<br>ORA-00979: not a GROUP BY expression 不是一个group by表达式 GROUP BY expression指跟在group by后面的东西(列名),称之为组标识<br>detail 细节 summary 聚合</p>\n<h3 id=\"查询\"><a href=\"#查询\" class=\"headerlink\" title=\"查询\"></a>查询</h3><p>子查询定义<br>在SQL语句中嵌入select语句<br>create table new_tabname<br>as<br>select ename,salary*12 ann_sal from emp_hiloo;<br>新表的结构由select后面的项来决定,new_table包含两列ename,ann_sal.</p>\n<h4 id=\"子查询\"><a href=\"#子查询\" class=\"headerlink\" title=\"子查询\"></a>子查询</h4><p>  非关联子查询<br>    单列子查询<br>    多列子查询<br>  关联子查询</p>\n<h5 id=\"子查询执行\"><a href=\"#子查询执行\" class=\"headerlink\" title=\"子查询执行\"></a>子查询执行</h5><p>非关联子查询<br>子查询的表和主查询的表没有建关联<br>先执行子查询(只执行一遍),当返回多条记录,系统会将自动去重的结果返回给主查询,再执行主查询.</p>\n<p>关联子查询<br>子查询的表和主查询的表建关联.所谓建关联指主查询表里的列和子查询表里的列写成一个条件表达式.</p>\n<p>先执行主查询,判断表里的记录是否应该放入结果集.过程如下:拿到第一条记录,获得了各个列的值,将需要的列值带入子查询,执行后返回的结果再和主查询表里的列做比较,符合条件,该记录放入结果集,否则过滤掉.依次执行主查询表里的每条记录.子查询执行的次数由主查询表里的记录数决定.</p>\n<p>1) exists和not exists<br>exists的执行过程<br>从主查询表里拿到第一条记录,按子查询里的关联条件在子查询的表里看是否能找到匹配的记录,当找到第一条匹配的记录后,立即返回(即不需要找出所有匹配的记录),exists条件满足,主查询表里的该记录放入结果集.若按子查询里的关联条件将子查询<br>表里的记录全部检查一遍后没有一条符合条件的记录,此时也返回, exists 条件不满足,主查询表里的该记录不能放入结果集,被过滤掉.</p>\n<p>select ename from emp_afei o<br>where exists<br>             (select 1 from emp_afei i<br>              where o.empno = i.mgr)</p>\n<h5 id=\"非关联子查询的分类\"><a href=\"#非关联子查询的分类\" class=\"headerlink\" title=\"非关联子查询的分类\"></a>非关联子查询的分类</h5><p>单列子查询<br>select ename,salary<br>from emp_hiloo<br>where salary = (select min(salary)<br>                from emp_hiloo<br>                )<br>多列子查询:按键值对比较<br>select ename,salary,deptno<br>from emp_afei<br>where (deptno,salary) in<br>             (select deptno,round(avg(salary))<br>              from emp_afei<br>              group by deptno)</p>\n<p>2) 课堂练习<br>1哪些人是领导?(非关联子查询)<br>如果一个员工的empno能出现在mgr里就说明他是领导.<br>select ename<br>from emp_hiloo<br>where empno in (select mgr from emp_hiloo)<br>select ename<br>fro emp_afei<br>where empno in (1001,1005,1006,1008,null)<br>2 哪些人是员工?<br>他的empno绝对不能出现在mgr中,他的empno跟mgr的出现的所有的值不能相等. &lt;&gt;all<br>select ename<br>from emp_hiloo<br>where empno not in (select mgr from emp_hiloo)<br>select ename<br>fro emp_afei<br>where empno not in (1001,1005,1006,1008,null)<br>select ename<br>from emp_hiloo<br>where empno not in (select mgr from emp_hiloo<br>                    where mgr is not null)</p>\n<p>3哪些部门的平均工资比30部门的平均工资高?<br>select deptno,round(avg(salary))<br>from emp_hiloo<br>group by deptno<br>having round(avg(salary)) &gt;<br>                    (select round(avg(salary))<br>                     from emp_hiloo<br>                     where deptno = 30)<br>4哪些员工的工资比zhangwuji的工资高?<br>select ename,salary<br>from emp_afei<br>where salary &gt; (select salary from emp_afei<br>                where ename = ‘zhangwuji’)<br>ERROR at line 3:<br>ORA-01427: single-row subquery returns more than one row<br>单行子查询返回多条记录</p>\n<p>比所有人高 &gt; (select max(salary))<br>           &gt;all<br>比任意人高 &gt; (select min(salary)<br>           &gt;any<br>5哪些员工的工资等于本部门的平均工资?<br>select ename,salary,deptno<br>from emp_afei<br>where (deptno,salary) in<br>             (select deptno,round(avg(salary))<br>              from emp_afei<br>              group by deptno)<br>5哪些员工的工资比本部门的平均工资高?<br>select ename,salary,deptno<br>from emp_afei o<br>where salary &gt; (select round(avg(salary))<br>                from emp_afei i<br>                where i.deptno = o.deptno)<br>6哪些人是领导?(关联子查询)<br>select ename from emp_afei o<br>where exists<br>             (select 1 from emp_afei i<br>              where o.empno = i.mgr)<br>7哪些部门有员工?<br>select deptno,dname<br>from dept_afei o<br>where exists (select 1 from emp_afei i<br>              where o.deptno = i.deptno)</p>\n<p>3) 课外练习day03am<br>1 zhangwuji的领导是谁,显示名称?<br>2 zangwuji领导谁,显示名称?<br>3 列出devoleper部门有哪些职位?<br>1) 课外练习day04am答案<br>1 zhangwuji的领导是谁,显示名称?<br>  select ename from emp_afei<br>  where empno in<br>        (select mgr from emp_afei<br>                 where ename = ‘zhangwuji’)</p>\n<p>zangwuji领导谁,显示名称?</p>\n<p> select ename from emp_afei<br> where mgr in (select empno from emp_afei<br>               where ename = ‘zhangwuji’)</p>\n<p>3 列出developer部门有哪些职位?<br>  select distinct job from emp_afei<br>  where deptno in<br>           (select deptno from dept_afei<br>            where dname = ‘developer’)</p>\n<p>2) 非关联子查询<br>exists和not exists<br>not exists的执行过程<br>从主查询表里拿到第一条记录,按子查询里的关联条件在子查询的表里看是否能找到匹配的记录,当找到第一条匹配的记录后,立即返回(即不需要找出所有匹配的记录),not exists条件不满足,主查询表里的该记录不能放入结果集,被过滤掉.若按子查询里的关联条件将子查询表里的记录全部检查一遍后没有一条符合条件的记录,返回, not exists 条件满足,主查询表里的该记录放入结果集.</p>\n<p>对于exists和not exists,在子查询中找到第一条匹配的记录都会立即返回,exists将主查询表里的记录放入结果集,not exsits将主查询表里的记录过滤掉.<br>对于exists和not exists,如果子查询没有返回任何记录,即扫描全部记录后没有一条符合条件的记录,都返回,exists将主查询表里的记录过滤掉,not exists将主查询表里的记录放入结果集.<br>not in ,&lt;&gt; all逻辑上跟not exists等价<br>in ,=any逻辑上跟exists等价</p>\n<p>查询形式:集合操作<br>把结果集作为一个集合,结果集必须是同构的,列的个数及数据类型一致</p>\n<p>3) 并集  union(去重)/union all(不去重)<br>select ename,deptno,salary,salary<em>1.1 new_sal<br>from emp_afei<br>where deptno = 10<br>union all<br>select ename,deptno,salary,salary</em>1.2 new_sal<br>from emp_afei<br>where deptno = 20<br>union all<br>select ename,deptno,salary,salary new_sal<br>from emp_afei<br>where deptno not in (10,20)</p>\n<p>case when和decode可以实现类似功能.</p>\n<p>4) 交集  intersect(去重)<br>select job from emp_afei<br>where deptno = 10<br>intersect<br>select job from emp_afei<br>where deptno = 20<br>10部门和20部门都有的职位是哪些?</p>\n<p>5) 差  minus(去重)<br>select deptno from dept_afei<br>minus<br>select deptno from emp_afei<br>那些部门没有员工.</p>\n<p>6) 多表查询<br>1) 交叉连接 cross join<br>select e.ename,d.dname<br>from emp_afei e cross join dept_afei d<br>结果集产生<br>10*4=40,组合操作,笛卡尔积</p>\n<p>2) 内连接 inner join(匹配一个条件)<br>select e.ename,e.deptno,d.deptno,d.dname<br>from emp_afei e join dept_afei d<br>ORA-00905: missing keyword(丢失关键字)</p>\n<p>如果把结果集的产生看成双层循环,驱动表是外层循环,匹配表是内层循环.<br>对于内连接哪张表做驱动表,哪张表做匹配表产生出的结果集是一样的,不同的是性能.<br>驱动表在匹配表的匹配情况如下:<br>一条记录找到一条匹配<br>一条记录找到多条匹配<br>一条记录找不到任何匹配.<br>内连接的核心是驱动表的记录要出现在结果集中必须在匹配表中能找到匹配的记录,否则该记录被过滤掉.</p>\n<p>3) 内连接查询形式<br>等值连接 on e.deptno = d.deptno<br>两张表有表述同一属性的列,两张表都有deptno列.<br>自连接 on e.mgr = m.empno<br>同一张表的不同列能写成一个表达式,即同一张表的两条记录之间有关系.通过给表起别名的方式,将同一张表的两条记录之间的关系转化成不同表的两条记录之间的关系.<br>4) 外连接<br>外连接 outer join(驱动表的记录一个都不能少的出现在结果集里)<br>from t1 left join t2<br>on t1.c1 = t2.c2(t1驱动表,t2匹配表)<br>外连接结果集=内连接的结果集+t1表中匹配不上的记录和t2表中的null记录的组合<br>from t1 right join t2<br>on t1.c1 = t2.c2(t2驱动表,t1匹配表)<br>外连接结果集=内连接的结果集+t2表中匹配不上的记录和t1表中的null记录的组合<br>from t1 full join t2<br>on t1.c1 = t2.c2<br>外连接结果集=内连接的结果集+t1表中匹配不上的记录和t2表中的null记录的组合+t2表中匹配不上的记录和t1表中的null记录的组合</p>\n<p>5) 外连接的应用场景<br>1 某张表的记录全部出现在结果集中,包括匹配不上的.<br>select e.ename,nvl(m.ename,’Boss’)<br>from emp_afei e left join emp_afei m<br>on e.mgr = m.empno<br>2解决否定问题,匹配不上的记录找出来(跟所有的记录都不匹配.)(not in/not exists)<br>外连接 + where 匹配表.主键列 is null<br>select e.ename,d.dname<br>from emp_afei e right join dept_afei d<br>on e.deptno = d.deptno<br>where e.empno is null<br>(解决结果集只包含匹配不上的记录.where子句执行在外连接之后)哪些部门没有员工</p>\n<p>select d.dname<br>from emp_afei e right join  dept_afei d<br>on e.deptno = d.deptno<br>and e.ename = ‘zhangwuji’<br>where e.empno is null<br>如果希望在外连接之前过滤匹配表用and子句,如果想在外连接之后通过匹配表里的列过滤外连接的结果集时候用where.<br>过滤驱动表统计用where子句过滤.</p>\n<p>6) 课内练习<br>1 哪些部门没有员工(not exists)<br>  select dname from dept_afei o<br>  where not exists<br>        (select 1 from emp_afei i<br>         where o.deptno = i.deptno)<br>2 哪些人是员工?(not exists)<br>  select ename from emp_afei o<br>  where not exists<br>            (select 1 from emp_afei i<br>             where o.empno = i.mgr)<br>他的empno和其他人的mgr相等是不可能存在的.即和所有人的mgr都不相等.<br>not in ,&lt;&gt; all逻辑上跟not exists等价<br>3 列出哪些员工在北京地区上班?<br>思路:确定表,两张表,匹配问题用inner join–&gt;on(匹配条件)–&gt;(对表是否过滤)<br>select e.ename,e.deptno,d.deptno,d.dname<br>from emp_afei e join dept_afei d<br>on e.deptno = d.deptno<br>and d.location = ‘beijing’<br>4zhangwuji在哪个地区上班?<br>select e.ename,d.dname,d.location<br>from emp_afei e join dept_afei d<br>on e.deptno = d.deptno<br>and e.ename = ‘zhangwuji’<br>5列出每个部门有哪些职位?部门名称,职位<br> select distinct d.dname,e.job<br> from emp_afei e join dept_afei d<br> on e.deptno = d.deptno<br> order by d.dname<br>6各个部门的平均工资,列出部门名称,平均工资.<br>select d.dname,round(avg(e.salary)) savg<br>from emp_afei e join dept_afei d<br>on e.deptno = d.deptno<br>group by d.dname<br>select max(d.dname),round(avg(e.salary)) savg<br>from emp_afei e join dept_afei d<br>on e.deptno = d.deptno<br>group by d.deptno<br>select min(deptno),round(avg(salary))<br>from emp_hiloo<br>where deptno = 10<br>7 列出每个员工的名字和他的领导的名字<br>select e.ename employee,<br>       m.ename manager<br>from emp_afei e join emp_afei m<br>on e.mgr = m.empno<br>结果集是9条.<br>e表中有10条记录,其中9条记录找到匹配,zhangsanfeng没匹配<br>m表中有10条记录,其中4条记录找到匹配,4条记录是领导,6条记录找不到匹配,他们是员工.<br>select e.ename employee,<br>       m.ename manager<br>from emp_afei e join emp_afei m<br>on e.mgr = m.empno<br>union all<br>select ename,’Boss’<br>from emp_afei<br>where mgr is null</p>\n<p>select e.ename employee,<br>       decode(m.ename,e.ename,’Boss’,<br>                  m.ename)   manager<br>from emp_afei e join emp_afei m<br>on nvl(e.mgr,e.empno) = m.empno</p>\n<p>select e.ename,nvl(m.ename,’Boss’)<br>from emp_afei e left join emp_afei m<br>on e.mgr = m.empno<br>10=9+1</p>\n<p>8哪些人是领导?<br>select distinct m.ename<br>from emp_afei e join emp_afei m<br>on e.mgr = m.empno<br>9哪些部门没有员工?<br>select e.ename,d.dname<br>from emp_afei e right join dept_afei d<br>on e.deptno = d.deptno<br>where e.empno is null<br>(解决结果集只包含匹配不上的记录.where子句执行在外连接之后)<br>11=10+1<br>如果部门表里的某条记录的deptno在emp表找不到匹配,在内连接中,它被过滤,<br>e表的empno的特性是唯一且非空的(主键约束),居然e.empno is null,说明null是外连接时为了驱动表中那条匹配不上的记录出现在结果集中,在匹配表中模拟的null记录.<br>10哪些人是员工,哪些人不是领导?<br>select e.empno,m.ename<br>from emp_afei e right join emp_afei m<br>on e.mgr = m.empno<br>where e.empno is null</p>\n<p>from emp_afei e right join emp_afei m<br>15=9+(10(m表中有10条记录)-4(m表中有4条匹配记录 ))<br>from emp_afei e left join emp_afei m<br>10(结果集)=9+(10(e表中有10条记录)-9(e表中有9条匹配记录))<br>11 哪些部门没有叫zhangwuji的?<br>select d.dname<br>from emp_afei e right join  dept_afei d<br>on e.deptno = d.deptno<br>and e.ename = ‘zhangwuji’<br>where e.empno is null</p>\n<p>7) 课外练习(day04)(答案在Day05)<br>1zhangwuji的领导是谁?(表连接)<br>2zhangwuji领导谁?(表连接)<br>3哪些人是领导?(in exists join)<br>4哪些部门没有员工?(not in/not exists/outer join)<br>5哪些人是员工,哪些人不是领导?(not in/not exists/outer join)<br>Day05.txt<br>Grade级别<br>Lowsal最低工资<br>Hisal最高工资<br>Create table salgrade_hiloo(<br>Grade<br>)<br>cross join  inner join   outer join<br>inner join(匹配)<br>  等值连接<br>  自连接<br>  非等值连接<br>outer join(匹配+不匹配)<br>  等值连接</p>\n<p>  自连接<br>  非等值连接</p>\n<p>所谓非等值连接表示两张表里的列不能写成等值表达式,而是写成between and之类.所以两个表之间有关系是指表里的列可以写成表达式,而不是等值表达式.<br>salgrade<br>grade  级别<br>lowsal 最低工资<br>hisal  最高工资</p>\n<p>from后面跟子查询<br>emp,各个部门的平均工资dept_avgsal(depnto,avgsal)<br>select e.ename,e.salary,e.deptno<br>from emp_afei e join<br>      (select deptno,round(avg(salary)) avgsal<br>       from emp_afei<br>       group by deptno) a<br>on e.deptno = a.deptno<br>and e.salary &gt; a.avgsal</p>\n<p>各个部门的平均工资,列出部门名称,平均工资<br>select max(d.dname),round(avg(salary))<br>from emp_afei e join dept_afei d<br>on e.deptno = d.deptno<br>group by d.deptno</p>\n<p>select d.dname,a.avgsal<br>from dept_afei d join<br>      (select deptno,round(avg(salary)) avgsal<br>       from emp_afei<br>       group by deptno) a<br>on d.deptno = a.depto</p>\n<p>DML<br>insert一条记录时,若某些列为null值,有哪些语法实现?<br>insert into tabname values (1,’a’,null,sysdate)<br>insert into tabname(c1,c2,c4)<br>values (1,’a’,sysdate)<br>insert语句的两种语法形式?<br>insert into tabname values () insert一条记录<br>insert into tabname<br>select * from tabname1  insert多条记录<br>连接图解：</p>\n<h3 id=\"数据类型\"><a href=\"#数据类型\" class=\"headerlink\" title=\"数据类型\"></a>数据类型</h3><p>1) 课外练习答案day04<br>1zhangwuji的领导是谁?(表连接)<br> select m.ename<br> from emp_afei e join emp_afei m<br> on e.mgr = m.empno<br> and m.ename = ‘zhangwuji’<br>2 zhanghangwuji领导谁?(表连接)<br> select e.ename<br> from emp_afei e join emp_afei m<br> on e.mgr = m.empno<br> and m.ename = ‘zhangwuji’<br>3哪些人是领导?(in exists join)<br> select ename from emp_afei<br> where empno in (select mgr from emp_afei)<br> select ename from emp_afei o<br> where exists<br>            (select 1 from emp_afei i<br>             where o.empno = i.mgr)<br> select distinct m.ename<br> from emp_afei e join emp_afei m<br> on e.mgr = m.empno<br>4哪些部门没有员工?(not in/not exists/outer join)<br> select dname from dept_afei<br> where deptno not in<br>               (select deptno from emp_afei)<br> select dname from dept_afei o<br> where not exists<br>             (select 1 from emp_afei i<br>              where o.deptno = i.deptno)<br> select d.dname<br> from emp_afei e right join dept_afei d<br> on e.deptno = d.deptno<br> where e.empno is null<br>5哪些人是员工,哪些人不是领导?(not in/not exists/outer join)<br> select ename from emp_afei<br> where empno not in (<br>               select mgr from emp_afei<br>               where mgr is not null)<br> select ename from emp_afei o<br> where not exists<br>             (select 1 from emp_afei i<br>              where o.empno = i.mgr)<br> select m.ename<br> from emp_afei e right join emp_afei m<br> on e.mgr = m.empno<br> where e.empno is null<br>cross join (笛卡尔积)</p>\n<p>rownum 伪列,记录号<br>若用rownum选择出记录,编号必须从1开始.<br>分页问题<br>第一页<br>select rownum,ename<br>from emp_afei<br>where rownum &lt;= 3;<br>第二页<br>select rn,ename<br>from (<br>      select rownum rn,ename<br>      from emp_afei<br>      where rownum &lt;= 6)<br>where rn between 4 and 6<br>排名问题<br>按工资排名的前三条记录<br>select rownum,ename,salary<br>from emp_hiloo<br>where rownum &lt;=3<br>order by salary desc;(错)</p>\n<p>select rownum,ename,salary<br>from ( select ename,salary<br>       from emp_afei<br>       order by salary desc)<br>where rownum &lt;= 3</p>\n<p>update语句的中set后面的=是什么含义?where后面的=是什么含义?<br>set c1 = null (= 赋值)<br>where c1 = null (= 等号)</p>\n<p>update和delete语句中的where子句是什么含义?<br>用来确定对表里的哪些记录要进行update或delete操作,没有where子句多表里的所有记录update或delete<br>update<br>set<br>where c1 = (select …)<br>rename 关键字 17<br>commit</p>\n<p>1011 abc 1000 10 ‘clerk’<br>update 1001 1000–&gt;2000<br>delete 1011<br>commit<br>如何编写和运行一个sql脚本(文本文件)<br>1 编辑文件<br>在linux环境下已经编写好了test.sql,做一个鼠标右键的copy</p>\n<p>在20,23,26机器上,<br>vi test.sql<br>按a i o进入编辑模式,paste,按esc键,再按:wq!回车</p>\n<p>2 运行文件<br>sun-server% sqlplus openlab/open123 @test.sql<br>@表示运行<br>SP2-0310: unable to open file “test.sql”在当前目录下没有test.sql文件<br>sqlplus openlab/open123 ../test.sql</p>\n<p>cd ..<br>sun-server% sqlplus openlab/open123 @test.sql</p>\n<p>SQL&gt;@test.sql</p>\n<p>数据库对象 PL/SQL<br>create or replace function test<br>insert into test values (1,1)<br>            *<br>ERROR at line 1:<br>ORA-04044: procedure(存储过程), function(函数), package(包), or type is not allowed here</p>\n<p>事务(transaction 交易)<br>事务里包含的DML语句<br>事务的结束<br>commit 提交,(dml操作的数据入库了)<br>rollback 回滚 撤销(DML操作被取消)<br>sqlplus正常退出=commit<br>DDL语句自动提交<br>开始<br>上一个事务的结束是下一个事务的开始.<br>一致状态<br>数据库的数据被事务改变.<br>oltp online transaction processing联机事务处理系统 高并发系统</p>\n<p>事务的隔离级别 read committed(读已经提交了的数据)</p>\n<p>如果不commit—–&gt;commit rollback<br>1如果不commit,其他session是看不见你的操作<br>2如果不commit,会阻塞操作同一条记录的事务(session),commit才能释放所有DML加的锁.<br>3如果不commit,系统做DML操作,会将old data放入rollback segment(回滚段) ,所占用的回滚段资源不释放.</p>\n<p>DML系统会自动给表及表里的记录加锁<br>表级共享锁<br>行级排他锁<br>    表级共享锁     行级排他锁<br>s1    ok        ok<br>s2    ok        enqueue wait<br>s3    ok        ok</p>\n<p>执行DDL语句,系统自动加DDL排他锁<br>SQL&gt; drop table test purge;<br>drop table test purge<br>           *<br>ERROR at line 1:<br>ORA-00054: resource busy(资源忙 test表) and acquire with NOWAIT specified (dml wait,ddl nowait 如果加不上锁,报错退出)</p>\n<p>DDL语句<br>字符类型<br>varchar2,必须带宽度, 按字符串的实际长度存,本身的数据是变化,对空格敏感<br>char,可以不带宽度,缺省宽度是1,按字符串的定义长度存,本身的数据是固定长度的.对空格不敏感<br>数值类型</p>\n<p>number类型<br>create table test90<br>(c1 number,<br> c2 number(6),<br> c3 number(4,2),<br> c4 number(2,4),<br> c5 number(3,-3))</p>\n<p>四舍五入<br>number(6) 表示6为整数 999999<br>number(4,2) 表示小数点后2位,整数位2位 99.99<br>number(2,4) 表示小数点后4位,能填数字的位数是2位 0.0099<br>number(3,-3) 999000 999123–&gt;999000<br>                    999511–&gt;报错</p>\n<p>user_tables 是一张系统表,里面记录当前用户所有的表的信息,里面没有记录表的创建日期.<br>user_objects 是一张系统表,里面记录当前用户所有的数据库对象的信息.created的列记录数据库对象(如表)的创建日期.<br>user_tables和user_objects这两张表的关系体现在table_name和object_name都记录的是表名.</p>\n<p>data block 数据块,操作数据的最小逻辑(物理)单元,最少读一个block的数据</p>\n<p>HWM high water mark 高水位线,表示曾经插入数据的最高位置<br>FTS full table scan 全表扫描,把表里的所有记录读一遍,把HWM之下的所有data block读一遍</p>\n<p>truncate table 释放空间,HWM下移<br>delete 不释放空间,HWM不动<br>不适合用delete命令删大表.</p>\n<p>课内练习<br>1 列出工资级别为3级,5级的员工<br>  select e.ename,e.salary,s.grade<br>  from emp_afei e join salgrade_afei s<br>  on e.salary between s.lowsal and s.hisal<br>  and s.grade in (3,5)<br>2 列出各个工资级别有多少人?<br>  select s.grade,count(e.empno)<br>  from emp_afei e join salgrade_afei s<br>  on e.salary between s.lowsal and s.hisal<br>  group by s.grade<br>  order by s.grade<br>3 列出各个工资级别有多少人?(包含0级)<br>  select s.grade,count(e.empno)<br>  from emp_afei e right join salgrade_afei s<br>  on e.salary between s.lowsal and s.hisal<br>  group by s.grade<br>  order by s.grade<br>特别注意count不要写*或者s.grade</p>\n<p>课外练习day05<br>1按工资排名的第4到第6名员工.</p>\n<p>###关键点###<br>课外练习day05答案</p>\n<p>按工资排名的第4到第6名员工.<br>select rn,ename,salary<br>from<br>    (select rownum rn,ename,salary<br>     from (select ename,salary<br>           from emp_afei<br>           order by salary desc)<br>     where rownum &lt;= 6<br>    )<br>where rn &gt;= 4 </p>\n<p>####1）事务####</p>\n<p>####约束 constraint (安检)####<br>primary key(主键)<br>foreign key(外键)<br>unique key (唯一键)<br>not null(非空)<br>check (检查)</p>\n<p>主键 (表中不会出现重复记录)<br>列级约束<br>create table test<br>(c1 number(2)<br>    constraint test_c1_pk primary key,<br> c2 number(3))</p>\n<pre><code>constraint test_c1_pk primary key,\n           *\n</code></pre><p>ERROR at line 3:<br>ORA-02264: name already used by an existing constraint (名字被存在的约束使用了)</p>\n<p>SQL&gt; select table_name from user_constraints<br>  2  where constraint_name = ‘TEST_C1_PK’;<br>哪张表里有叫TEST_C1_PK这个约束名.</p>\n<p>ORA-00001: unique constraint (HILOO(用户名) .TEST_C1_PK) violated(冲突)</p>\n<p>PK=UK + NN</p>\n<p>表级约束<br>create table test(<br>c1 number(2),<br>c2 number,<br>constraint test_c1_pk primary key(c1)<br>)<br>表中有三列c1,c2,c3,c1和c2做成联合主键<br>create table test(<br>c1 number,<br>c2 number,<br>constraint test_c1_c2_pk primary key(c1,c2),<br>c3 number<br>)<br>没有constraint关键字,系统用自动起名字sys_c数字.</p>\n<p>not null<br>create table test<br>(c1 number constraint test_c1_pk primary key,<br> c2 number not null);<br>not null约束没有表级形式</p>\n<p>unique (pk)<br>相同点:都要保证唯一性<br>区别:uk允许为null,而且可以多个null值,一个表中只能有一个pk约束,可以有多个uk约束.<br>create table test<br>(c1 number constraint test_c1_pk primary key,<br> c2 number constraint test_c2_uk unique)</p>\n<p>create table test(<br>c1 number primary key,<br>c2 number primary key,<br>c3 number unique,<br>c4 number unique)  (报错,一张表只能有一个primary key)</p>\n<p>create table test(<br>c1 number constraint test_c1_pk primary key,<br>c2 number constraint test_c2_uk unique,<br>c3 number constraint test_c3_uk unique,<br>c4 number )<br>c2上定义了一个唯一键 c3上定义了一个唯一键</p>\n<p>create table test(<br>c1 number constraint test_c1_pk primary key,<br>c2 number,<br>c3 number,<br>constraint test_c2_c3_uk unique (c2,c3),<br>c4 number)<br>c2,c3联合唯一键</p>\n<p>check<br>create table test(<br>c1 number(3) constraint test_c1_ck<br>             check (c1 &gt; 100))</p>\n<p>create table test(<br>c1 number(3),<br>constraint test_c1_ck check (c1 &gt; 100))</p>\n<p>外键<br>parent table(父表)上定义唯一列(pk/uk)<br>child table(子表)上定义外键列(fk)</p>\n<p>1 先create parent table(pk/uk),再create child table(fk)<br>2 先insert into parent table,再insert into child table<br>3 先delete from child table,再delete from parent table<br>4 先drop child table,再drop parent table</p>\n<p>reference 引用<br>create table parent<br>(c1 number(3))</p>\n<p>create table child<br>(c1 number(2) constraint child_c1_pk<br>              primary key,<br> c2 number(3) constraint child_c2_fk<br>              references parent(c1))</p>\n<pre><code>references parent(c1))\n                  *\n</code></pre><p>ERROR at line 5:<br>ORA-02270: no matching unique or primary key for this column-list<br>在c1上没有定义uk或pk</p>\n<p>alter table parent<br>add constraint parent_c1_pk primary key(c1);<br>给c1列增加主键约束</p>\n<p>insert into child values (1,1)<br>ORA-02291: integrity constraint(完整性约束) (HILOO.CHILD_C2_FK) violated - parent key not found (父键值没发现)<br>违反fk约束</p>\n<p>insert into parent values (1);<br>insert into child values (1,1)</p>\n<p>delete from parent where c1 = 1;<br>ORA-02292: integrity constraint (HILOO.CHILD_C2_FK) violated - child record<br>found(子记录被发现)</p>\n<p>delete from child where c2 = 1;<br>delete from parent where c1 = 1;</p>\n<p>drop table parent purge;<br>ORA-02449: unique/primary keys in table referenced by foreign keys<br>在parent table上的pk/uk正在fk所引用</p>\n<p>drop table child purge;<br>drop table parent purge;</p>\n<p>drop table parent cascade constraints purge;<br>cascade constraints 级联约束,child table本身没被删除,只是先把子表上的fk约束删除,再删parent table.</p>\n<p>表级约束<br>create table child<br>(c1 number(2) constraint child_c1_pk<br>              primary key,<br> c2 number(3),<br> constraint child_c2_fk foreign key(c2)<br>            references parent(c1)<br>)</p>\n<p>外键约束另外两种定义方法<br>create table child1<br>(c1 number(2) constraint child1_c1_pk<br>              primary key,<br> c2 number(3) constraint child1_c2_fk<br>              references parent(c1)<br>              on delete cascade)<br>on delete cascade :级联删除会影响到对parent table的删除,先delete from child1,再delete from<br>parent</p>\n<p>delete from parent where c1 = 1;<br>create table child2<br>(c1 number(2) constraint child2_c1_pk<br>              primary key,<br> c2 number(3) constraint child2_c2_fk<br>              references parent(c1)<br>              on delete set null)</p>\n<p>delete from parent where c1 = 1<br>等价于以下操作<br>SQL&gt; update child2 set c2 = null<br>  2  where c2 = 1;<br>SQL&gt; delete from parent where c1 = 1;</p>\n<p>table<br>DDL(数据类型 约束)<br>transaction (包含一堆DML)</p>\n<p>4000<br>100<br>1000<br>3100</p>\n<p>视图(view)<br>create table test_t1<br>as<br>select <em> from test<br>where c1 = 1;<br>create or replace view test_v1<br>as<br>select </em> from test<br>where c1 = 1;<br>desc test_v1<br>selelct * from test_v1</p>\n<p>insert into test values (1,3);<br>select <em> from test_v1 (1,3)<br>insert into test_v1 values (1,4)<br>select </em> from test_v1;<br>select <em> from test;<br>insert into test_v1 values (2,3);<br>select </em> from test_v1;(没有)<br>select * from test;(2,3)</p>\n<p>drop table test purge;<br>select * from test_v1;<br>SQL&gt; desc test_v1<br>ERROR:<br>ORA-24372: invalid object for describe<br>无法描述无效对象的结构</p>\n<p>SQL&gt; select text from user_views<br>  2  where view_name = ‘TEST_V1’;</p>\n<h2 id=\"TEXT\"><a href=\"#TEXT\" class=\"headerlink\" title=\"TEXT\"></a>TEXT</h2><p>select “C1”,”C2” from test<br>where c1 = 1</p>\n<p>view是一条select语句. select语句中包含的表为源表.通过view对源表做DML操作.</p>\n<p>view作用<br>1 create view (deptno = 30)<br>  grant view to user<br>  限定用户查询的数据 子集<br>2 简化查询语句<br>3 create view beijing<br>  as<br>  select <em> from haidian<br>  union all<br>  select </em> from xicheng<br>…<br>  超集<br>view的类型<br>1 简单view (DML)<br>2 复杂view  (不能DML)</p>\n<p>create or replace view avgscore_v<br>select s.name,a.avgscore<br>from student s,<br>     (select sid,round(avg(score)) avgscore<br>      from stu_cour<br>      group by sid) a<br>on s.id = a.sid</p>\n<p>view的约束<br>create or replace view test_ck<br>as<br>select * from test<br>where c1 = 1<br>with check option;<br>c1=2,违反where条件,2,3记录insert时报错</p>\n<p>create or replace view test_ro<br>as<br>select * from test<br>where c1 = 1<br>with read only;<br>只读视图</p>\n<h4 id=\"索引\"><a href=\"#索引\" class=\"headerlink\" title=\"索引\"></a>索引</h4><p>create index test_c1_idx<br>on test(c1);<br>对索引不能做desc,select,DML操作<br>rowid 代表一条记录的物理位置<br>属于哪个数据对象(table)<br>属于哪个数据文件的<br>属于数据文件的第几个数据块<br>属于数据块里的第几条记录</p>\n<h4 id=\"index的结构\"><a href=\"#index的结构\" class=\"headerlink\" title=\"index的结构\"></a>index的结构</h4><p>index记录rowid<br>index的结构是一棵平衡树,有三类数据块组成,根节点,分支节点,叶子节点,数据块的数据是排序的.根节点和分支节点用于导航,里面记录下一级节点的物理位置以及该节点包含的数据范围.叶子节点里记录的是index entry(索引项),由key值和rowid组成,key值是建索引的列在每条记录上的取值,rowid是记录的物理位置,所有的叶子节点做成双向链表(升序/降序),适用于范围查询.<br>用索引查询的路线图,从根节点出发,找相应的分支节点,叶子节点,最后要找到index entry,通过rowid定位<br>表里所需要的数据块,避免了全表扫描.</p>\n<p>索引为什么提高查询效率,为select语句<br>有效地降低了读取数据块的数量.读取数据块,一种从文件里读,物理读 physical read,一种从内存读,逻辑读 logical read /buffer gets</p>\n<p>建索引代价<br>空间,DML变慢</p>\n<h4 id=\"哪些列适合建索引\"><a href=\"#哪些列适合建索引\" class=\"headerlink\" title=\"哪些列适合建索引\"></a>哪些列适合建索引</h4><p>1 经常出现在where子句的列<br>2 pk/uk列<br>3 经常出现在表连接的列<br>4 fk列 parent.pk列 = child.fk列<br>5 经常用于group by,order by的列<br>7 where c1 is null(全表扫描),索引里不记null值,<br> 该列有大量null值,找not null值用索引会快</p>\n<h4 id=\"索引类型\"><a href=\"#索引类型\" class=\"headerlink\" title=\"索引类型\"></a>索引类型</h4><p>非唯一性索引,提高查询效率<br>唯一性索引,解决唯一性.等价建唯一性约束<br>create unique index test_c2_idx<br>on test(c2);</p>\n<p>insert into test (c2) values (1)<br>*<br>ERROR at line 1:<br>ORA-00001: unique constraint(HILOO.TEST_C2_IDX ) violated</p>\n<p>联合索引<br>create index test_c1_c2_idx<br>on test(c1,c2)<br>where c1 = 1 and c2 = 1</p>\n<p>select ename from emp_hiloo<br>where salary<em>12 &gt; 60000<br>where salary &gt; 5000<br>如果salary建索引,where salary &gt; 5000(用),where salary</em>12 &gt; 60000(不能用)</p>\n<p>where upper(ename) = ‘ZHANGWUJI’</p>\n<p>where c1 = 100 c1是varchar2类型<br>where to_number(c1) = 100</p>\n<p>where ename like ‘a%’<br>where substr(ename,1,1) = ‘a’</p>\n<p>deptno not in (20,30)<br>depotno in (10)</p>\n<h4 id=\"函数索引\"><a href=\"#函数索引\" class=\"headerlink\" title=\"函数索引\"></a>函数索引</h4><p>create index test_c1_funidx<br>on test(round(c1));<br>where round(c1) = 10</p>\n<p>create index student_name_idx<br>on student(name);</p>\n<h4 id=\"序列号\"><a href=\"#序列号\" class=\"headerlink\" title=\"序列号\"></a>序列号</h4><p>sequence<br>为table里的主键服务,产生主键值<br>唯一值产生器<br>sequence_name.nextval</p>\n<p>为student表的id建sequence<br>insert into student(student_id.nextval…<br>为course表的id建sequence<br>insert into course (course_id.nextval…</p>\n<p>创建序列如下：<br>create sequence SEQ_TEST100<br>minvalue 1<br>maxvalue 999999999999999999999999999<br>start with 11<br>increment by 1<br>cache 10;</p>\n<p>函数<br>create or replace function dept_avgsal<br>(p_deptno number) –定义参数,数据类型不能有宽度<br>return number    –定义函数的返回类型<br>is<br>  v_salary emp_hiloo.salary%type;     –变量v_salary 的类型跟表emp_hiloo里的salary的类型定义一致<br>begin<br>  select round(avg(salary)) into v_salary<br>  from emp_hiloo<br>  where deptno = p_deptno;    –select当且仅当返回一条记录用select into语法,表示把select语句的执行结果赋值给v_salary<br>  return v_salary;       –返回函数值<br>end;<br>.不运行,回到SQL&gt;下<br>/表示运行<br>show error<br>SQL&gt; select dept_avgsal(10) from dual;</p>\n<p>练习<br>用语法实现多对多关系<br>student<br>id pk<br>name not null</p>\n<p>course<br>id pk<br>name not null</p>\n<p>stu_cour<br>sid fk –&gt;student(id)<br>cid fk –&gt;course(id)<br>pk(sid,cid)<br>score check <a href=\"between and\">0,100</a> </p>\n<h4 id=\"数据库日期比较\"><a href=\"#数据库日期比较\" class=\"headerlink\" title=\"数据库日期比较\"></a>数据库日期比较</h4><p>Sql代码：<br>1    timesten内存数据库比较日期是不是同一天,低效的方法<br>2    to_char(create_date,’yyyymmdd’)=to_char(sysdate NUMTODSINTERVAL(60<em>60</em>24,’SECOND’),’yyyymmdd’)<br>3    oracle 数据库低效的方法<br>4    to_char(create_date,’yyyymmdd’)=to_char(sysdate-1,’yyyymmdd’)<br>5    2个数据库通用高效的方法<br>6    trunc(create_date)=trunc(sysdate)-NUMTODSINTERVAL(1,’DAY’)<br>查找数据库里的表，索引等<br>支持oracle的模糊查询如select * from user_tables where table_name like ‘%_PROJECT’;查表名以PROJECT结尾的表（注：区别大小写）<br>查所有用户的表在all_tables<br>主键名称、外键在all_constraints<br>索引在all_indexes<br>但主键也会成为索引，所以主键也会在all_indexes里面。<br>具体需要的字段可以DESC下这几个view，dba登陆的话可以把all换成dba。</p>\n<p>查询用户表的索引(非聚集索引):<br>select * from user_indexes<br>where uniqueness = ‘NONUNIQUE’</p>\n<p>查询用户表的主键(聚集索引):<br>select * from user_indexes<br>where uniqueness = ‘UNIQUE’</p>\n<p>1、    查找表的所有索引（包括索引名，类型，构成列）：<br>select t.<em>,i.index_type from user_ind_columns t,user_indexes i where t.index_name = i.index_name and t.table_name = i.table_name and t.table_name = 要查询的表<br>2、查找表的主键（包括名称，构成列）：<br>select cu.</em> from user_cons_columns cu, user_constraints au where cu.constraint_name = au.constraint_name and au.constraint_type = ‘P’ and au.table_name = 要查询的表<br>3、查找表的唯一性约束（包括名称，构成列）：<br>select column_name from user_cons_columns cu, user_constraints au where cu.constraint_name = au.constraint_name and au.constraint_type = ‘U’ and au.table_name = 要查询的表<br>4、查找表的外键（包括名称，引用表的表名和对应的键名，下面是分成多步查询）：<br>select <em> from user_constraints c where c.constraint_type = ‘R’ and c.table_name = 要查询的表<br>查询外键约束的列名：<br>select </em> from user_cons_columns cl where cl.constraint_name = 外键名称<br>查询引用表的键的列名：<br>select <em> from user_cons_columns cl where cl.constraint_name = 外键引用表的键名<br>5、查询表的所有列及其属性<br>select t.</em>,c.COMMENTS from user_tab_columns t,user_col_comments c where t.table_name = c.table_name and t.column_name = c.column_name and t.table_name = 要查询的表</p>\n<p>####数据唯一Id：####</p>\n<ol>\n<li>用Oracle来生成UUID，做法很简单，如下：select sys_guid() from dual;数据类型是 raw(16) 有32个字符。<br>create table test_guid3(<br>id varchar(50)<br>)<br>select * from test_guid3;<br>insert into test_guid3(id) values(sys_guid())</li>\n</ol>\n<hr>\n<pre><code>1000 7CD5B7769DF75CEFE034080020825436\n1100 7CD5B7769DF85CEFE034080020825436\n1200 7CD5B7769DF95CEFE034080020825436\n1300 7CD5B7769DFA5CEFE034080020825436\n</code></pre><h3 id=\"名词\"><a href=\"#名词\" class=\"headerlink\" title=\"名词\"></a>名词</h3><h4 id=\"Oracle的方案（Schema）和用户（User）的区别\"><a href=\"#Oracle的方案（Schema）和用户（User）的区别\" class=\"headerlink\" title=\"Oracle的方案（Schema）和用户（User）的区别\"></a>Oracle的方案（Schema）和用户（User）的区别</h4><p>从定义中我们可以看出方案（Schema）为数据库对象的集合，为了区分各个集合，我们需要给这个集合起个名字，这些名字就是我们在企业管理器的方案下看到的许多类似用户名的节点，这些类似用户名的节点其实就是一个schema，schema里面包含了各种对象如tables, views, sequences, stored procedures, synonyms, indexes, clusters, and database links。</p>\n<p>   一个用户一般对应一个schema,该用户的schema名等于用户名，并作为该用户缺省schema。这也就是我们在企业管理器的方案下看到schema名都为数据库用户名的原因。Oracle数据库中不能新创建一个schema，要想创建一个schema，只能通过创建一个用户的方法解决(Oracle中虽然有create schema语句，但是它并不是用来创建一个schema的)，在创建一个用户的同时为这个用户创建一个与用户名同名的schem并作为该用户的缺省shcema。即schema的个数同user的个数相同，而且schema名字同user名字一一对应并且相同，所有我们可以称schema为user的别名，虽然这样说并不准确，但是更容易理解一些。</p>\n<p>   一个用户有一个缺省的schema，其schema名就等于用户名，当然一个用户还可以使用其他的schema。如果我们访问一个表时，没有指明该表属于哪一个schema中的，系统就会自动给我们在表上加上缺省的sheman名。比如我们在访问数据库时，访问scott用户下的emp表，通过select <em> from emp; 其实，这sql语句的完整写法为select </em> from scott.emp。在数据库中一个对象的完整名称为schema.object，而不属user.object。类似如果我们在创建对象时不指定该对象的schema，在该对象的schema为用户的缺省schema。这就像一个用户有一个缺省的表空间，但是该用户还可以使用其他的表空间，如果我们在创建对象时不指定表空间，则对象存储在缺省表空间中，要想让对象存储在其他表空间中，我们需要在创建对象时指定该对象的表空间。</p>\n<p>   oracle中的schema就是指一个用户下所有对象的集合，schema本身不能理解成一个对象，oracle并没有提供创建schema的语法，schema也并不是在创建user时就创建，而是在该用户下创建第一个对象之后schema也随之产生，只要user下存在对象，schema就一定存在，user下如果不存在对象，schema也不存在；这一点类似于temp tablespace group，另外也可以通过oem来观察，如果创建一个新用户，该用户下如果没有对象则schema不存在，如果创建一个对象则和用户同名的schema也随之产生。</p>\n<p>####Oracle中User与Schema的简单理解####<br>技术积累（126）<br>版权声明：本文为博主原创文章，未经博主允许不得转载。<br>方案（Schema）为数据库对象的集合，为了区分各个集合，我们需要给这个集合起个名字，这些名字就是我们在企业管理器的方案下看到的许多类似用户名的节点，这些类似用户名的节点其实就是一个schema，schema里面包含了各种对象如tables, views, sequences, stored procedures, synonyms, indexes, clusters, and database links。  一个用户一般对应一个schema,该用户的schema名等于用户名，并作为该用户缺省schema。<br>SQL Server中的Schema<br>SQL Server中一个用户有一个缺省的schema，其schema名就等于用户名，这也就是我们在企业管理器的方案下看到schema名都为数据库用户名的原因。当然一个用户还可以使用其他的schema。如果我们访问一个表时，没有指明该表属于哪一个schema中的，系统就会自动给我们在表上加上缺省的sheman名。比如我们在访问数据库时，访问scott用户下的emp表，通过select <em> from emp; 其实，这sql语句的完整写法为select </em> from scott.emp。在数据库中一个对象的完整名称为schema.object，而不属user.object。类似如果我们在创建对象时不指定该对象的schema，在该对象的schema为用户的缺省schema。这就像一个用户有一个缺省的表空间，但是该用户还可以使用其他的表空间，如果我们在创建对象时不指定表空间，则对象存储在缺省表空间中，要想让对象存储在其他表空间中，我们需要在创建对象时指定该对象的表空间。</p>\n<p>Oracle中的Schema<br>Oracle中的schema就是指一个用户下所有对象的集合，schema本身不能理解成一个对象，oracle并没有提供创建schema的语法，schema也并不是在创建user时就创建，而是在该用户下创建第一个对象之后schema也随之产生，只要user下存在对象，schema就一定存在，user下如果不存在对象，schema也不存在；如果创建一个新用户，该用户下如果没有对象则schema不存在，如果创建一个对象则和用户同名的schema也随之产生。实际上在使用上，shcema与user完全一样，没有什么区别，在出现schema名的地方也可以出现user名。</p>\n<p>Tablspace<br>逻辑上用来放objects,，这是个逻辑概念，本质上是一个或者多个数据文件的集合，物理上对应磁盘上的数据文件或者裸设备。</p>\n<p>数据文件<br>具体存储数据的物理文件，是一个物理概念。一个数据文件只能属于一个表空间，一个表空间可以包含一个或多个数据文件。一个数据库由多个表空间组成，一个表空间只能属于一个数据库。</p>\n<p>下边是源自网络的一个形象的比喻<br>我们可以把Database看作是一个大仓库，仓库分了很多很多的房间，Schema就是其中的房间，一个Schema代表一个房间，Table可以看作是每个Schema中的床，Table（床）被放入每个房间中，不能放置在房间之外，那岂不是晚上睡觉无家可归了，然后床上可以放置很多物品，就好比 Table上可以放置很多列和行一样，数据库中存储数据的基本单元是Table，现实中每个仓库放置物品的基本单位就是床， User就是每个Schema的主人，（所以Schema包含的是Object，而不是User），user和schema是一一对应的，每个user在没有特别指定下只能使用自己schema（房间）的东西，如果一个user想使用其他schema（房间）的东西，那就要看那个schema（房间）的user（主人）有没有给你这个权限了，或者看这个仓库的老大（DBA）有没有给你这个权限了。换句话说，如果你是某个仓库的主人，那么这个仓库的使用权和仓库中的所有东西都是你的（包括房间），你有完全的操作权，可以扔掉不用的东西从每个房间，也可以放置一些有用的东西到某一个房间，你还可以给每个User分配具体的权限，也就是他到某一个房间能做些什么，是只能看（Read-Only），还是可以像主人一样有所有的控制权（R/W），这个就要看这个User所对应的角色Role了。</p>\n<h4 id=\"oracle的schema的含义\"><a href=\"#oracle的schema的含义\" class=\"headerlink\" title=\"oracle的schema的含义\"></a>oracle的schema的含义</h4><p>在现在做的Kraft Catalyst 项目中，Cransoft其中有一个功能就是schema refresh. 一直不理解schema什么意思，也曾经和同事讨论过，当时同事就给我举过一个例子，下面会详细说的。其实schema是Oracle中的，其他数据库中不知道有没有这个概念。<br>首先,可以先看一下schema和user的定义：<br>A schema is a collection of database objects (used by a user).<br>Schema objects are the logical structures that directly refer to the database’s data.<br>A user is a name defined in the database that can connect to and access objects.<br>Schemas and users help database administrators manage database security.<br>从中我们可以看出,schema为数据库对象的集合，为了区分各个集合，需要给这个集合起个名字，这些名字就是在企业管理器的方案下看到的许多类似用户名的节点，这些类似用户名的节点其实就是一个schema。<br>schema里面包含了各种对象如tables, views, sequences, stored procedures, synonyms, indexes, clusters, and database links。<br>一个用户一般对应一个schema，该用户的schema名等于用户名，并作为该用户缺省schema。这也就是在企业管理器的方案下看到schema名都为数据库用户名的原因。<br>Oracle数据库中不能新创建一个schema，要想创建一个schema，只能通过创建一个用户的方法解决(Oracle中虽然有create schema语句，但是它并不是用来创建一个schema的)。在创建一个用户的同时，为这个用户创建一个与用户名同名的schem并作为该用户的缺省 shcema。即schema的个数同user的个数相同，而且schema名字同user名字一一 对应并且相同，所有我们可以称schema为user的别名，虽然这样说并不准确，但是更容易理解一些。<br>一个用户有一个缺省的schema，其schema名就等于用户名，当然一个用户还可以使用其他的schema。如果我们访问一个表时，没有指明该表属于 哪一个schema中的，系统就会自动给我们在表上加上缺省的sheman名。比如我们在访问数据库时，访问scott用户下的emp表，通过 select <em> from emp; 其实，这sql语句的完整写法为select </em> from scott.emp。在数据库中一个对象的完整名称为schema.object，而不属user.object。类似如果我们在创建对象时不指定该对象 的schema，在该对象的schema为用户的缺省schema。这就像一个用户有一个缺省的表空间，但是该用户还可以使用其他的表空间，如果我们在创 建对象时不指定表空间，则对象存储在缺省表空间中，要想让对象存储在其他表空间中，需要在创建对象时指定该对象的表空间。<br>有人举了个很生动的例子，来说明Database、User、Schema、Tables、Col、Row等之间的关系<br>“可以把Database看作是一个大仓库，仓库分了很多很多的房间，Schema就是其中的房间，一个Schema代表一个房间，Table可以看作是每个Schema中的床，Table（床）就被放入每个房间中，不能放置在房间之外，那岂不是晚上睡觉无家可归了。<br>然后床上可以放置很多物品，就好比Table上可以放置很多列和行一样，数据库中存储数据的基本单元是Table，现实中每个仓库放置物品的基本单位就是床， User就是每个Schema的主人（所以Schema包含的是Object，而不是User）。<br>其实User是对应与数据库的（即User是每个对应数据库的主人），既然有操作数据库（仓库）的权利，就肯定有操作数据库中每个Schema（房间）的 权利，就是说每个数据库映射的User有每个Schema（房间）的钥匙，换句话说，如果他是某个仓库的主人，那么这个仓库的使用权和仓库中的所有东西都 是他的（包括房间），他有完全的操作权，可以扔掉不用的东西从每个房间，也可以放置一些有用的东西到某一个房间。还可以给User分配具体的权限，也就是 他到某一个房间能做些什么，是只能看（Read-Only），还是可以像主人一样有所有的控制权（R/W），这个就要看这个User所对应的角色Role 了”<br>从定义中我们可以看出schema为数据库对象的集合，为了区分各个集合，我们需要给这个集合起个名字，这些名字就是我们在企业管理器的方案下看到的许多类似用户名的节点，这些类似用户名的节点其实就是一个schema，schema里面包含了各种对象如tables, views, sequences, stored procedures, synonyms, indexes, clusters, and database links。<br>一个用户一般对应一个schema,该用户的schema名等于用户名，并作为该用户缺省schema。这也就是我们在企业管理器的方案下看到schema名都为数据库用户名的原因。Oracle数据库中不能新创建一个schema，要想创建一个schema，只能通过创建一个用户的方法解决(Oracle中虽然有create schema语句，但是它并不是用来创建一个schema的)，在创建一个用户的同时为这个用户创建一个与用户名同名的schem并作为该用户的缺省shcema。即schema的个数同user的个数相同，而且schema名字同user名字一一 对应并且相同，所有我们可以称schema为user的别名，虽然这样说并不准确，但是更容易理解一些。<br>一个用户有一个缺省的schema，其schema名就等于用户名，当然一个用户还可以使用其他的schema。如果我们访问一个表时，没有指明该表属于哪一个schema中的，系统就会自动给我们在表上加上缺省的sheman名。比如我们在访问数据库时，访问scott用户下的emp表，通过select <em> from emp; 其实，这sql语句的完整写法为select </em> from scott.emp。在数据库中一个对象的完整名称为schema.object，而不属user.object。类似如果我们在创建对象时不指定该对象的schema，在该对象的schema为用户的缺省schema。这就像一个用户有一个缺省的表空间，但是该用户还可以使用其他的表空间，如果我们在创建对象时不指定表空间，则对象存储在缺省表空间中，要想让对象存储在其他表空间中，我们需要在创建对象时指定该对象的表空间。<br>咳，说了这么多，给大家举个例子，否则，一切枯燥无味！<br>SQL&gt; Gruant dba to scott<br>SQL&gt; create table test(name char(10));<br>Table created.<br>SQL&gt; create table system.test(name char(10));<br>Table created.<br>SQL&gt; insert into test values(‘scott’);<br>1 row created.<br>SQL&gt; insert into system.test values(‘system’);<br>1 row created.<br>SQL&gt; commit;<br>Commit complete.<br>SQL&gt; conn system/manager<br>Connected.<br>SQL&gt; select * from test;</p>\n<h2 id=\"NAME\"><a href=\"#NAME\" class=\"headerlink\" title=\"NAME\"></a>NAME</h2><p>system<br>SQL&gt; ALTER SESSION SET CURRENT_SCHEMA = scott; –改变用户缺省schema名<br>Session altered.<br>SQL&gt; select * from test;</p>\n<h2 id=\"NAME-1\"><a href=\"#NAME-1\" class=\"headerlink\" title=\"NAME\"></a>NAME</h2><p>scott<br>SQL&gt; select owner ,table_name from dba_tables where table_name=upper(‘test’);<br>OWNER TABLE_NAME</p>\n<hr>\n<p>SCOTT TEST<br>SYSTEM TEST<br>–上面这个查询就是我说将schema作为user的别名的依据。实际上在使用上，shcema与user完全一样，没有什么区别，在出现schema名的地方也可以出现user名。<br>表空间：<br>一个表空间就是一片磁盘区域,他又一个或者多个磁盘文件组成,一个表空间可以容纳许多表、索引或者簇等<br>  每个表空间又一个预制的打一磁盘区域称为初始区间（initial   extent）用完这个区间厚在用下一个，知道用完表空间，这时候需要对表空间进行扩展，增加数据文件或者扩大已经存在的数据文件</p>\n<p>instance是一大坨内存sga,pga….和后台的进程smon pmon…..组成的一个大的应用。<br>schema就是一个用户和他下面的所有对象。。<br>tablspace 逻辑上用来放objects.物理上对应磁盘上的数据文件或者裸设备。<br> 在Oracle中，结合逻辑存储与物理存储的概念，我们可以这样来理解数据库、表空间、SCHEMA、数据文件这些概念：<br>      数据库是一个大圈，里面圈着的是表空间，表空间里面是数据文件，那么schema是什么呢？schema是一个逻辑概念，是一个集合，但schema并不是一个对象，oracle也并没有提供创建schema的语法。<br>schema：<br>      一般而言，一个用户就对应一个schema,该用户的schema名等于用户名，并作为该用户缺省schema，用户是不能创建schema的，schema在创建用户的时候创建，并可以指定用户的各种表空间（这点与PostgreSQL是不同，PostgreSQL是可以创建schema并指派给某个用户）。当前连接到数据库上的用户创建的所有数据库对象默认都属于这个schema（即在不指明schema的情况下），比如若用户scott连接到数据库，然后create table test(id int not null)创建表，那么这个表被创建在了scott这个schema中；但若这样create kanon.table test(id int not null)的话，这个表被创建在了kanon这个schema中，当然前提是权限允许。<br>      创建用户的方法是这样的：<br>      create user 用户名 identified by 密码<br>      default tablespace 表空间名<br>      temporary tablespace 表空间名<br>      quota 限额  （建议创建的时候指明表空间名）<br>由此来看，schema是一个逻辑概念。<br>      但一定要注意一点：schema好像并不是在创建user时就创建的，而是在该用户创建了第一个对象之后才将schema真正创建的，只有user下存在对象，他对应的schema才会存在，如果user下不存在任何对象了，schema也就不存在了；</p>\n<p>数据库：<br>     在oracle中，数据库是由表空间来组成的，而表空间里面是具体的物理文件—数据文件。我们可以创建数据库并为其指定各种表空间。</p>\n<p>表空间：<br>     这是个逻辑概念，本质上是一个或者多个数据文件的集合。</p>\n<p>数据文件：<br>     具体存储数据的物理文件，是一个物理概念。<br>     一个数据文件只能属于一个表空间，一个表空间可以包含一个或多个数据文件。一个数据库由多个表空间组成，一个表空间只能属于一个数据库。</p>\n","site":{"data":{}},"excerpt":"","more":"<h2 id=\"Oracle-SQL基本知识\"><a href=\"#Oracle-SQL基本知识\" class=\"headerlink\" title=\"Oracle SQL基本知识\"></a>Oracle SQL基本知识</h2><h3 id=\"安装数据库\"><a href=\"#安装数据库\" class=\"headerlink\" title=\"安装数据库\"></a>安装数据库</h3><h4 id=\"1）安装Oracle常用问题-常用”用户名-密码“规则-：\"><a href=\"#1）安装Oracle常用问题-常用”用户名-密码“规则-：\" class=\"headerlink\" title=\"1）安装Oracle常用问题(常用”用户名/密码“规则)：\"></a>1）安装Oracle常用问题(常用”用户名/密码“规则)：</h4><p>超级管理员：sys /change_on_install<br>普通管理员：system/manager<br>普通用户：scott/tiger—–&gt;默认是被锁定的<br>大数据用户：sh/sh</p>\n<h4 id=\"2）SQL-DDL…\"><a href=\"#2）SQL-DDL…\" class=\"headerlink\" title=\"2）SQL,DDL…\"></a>2）SQL,DDL…</h4><p>SQL：structured query language 结构化查询语言<br>1.file(文件)</p>\n<p>SQL:DDL DML TCL DQL DCL<br>DDL(data definition language 数据定义语言): column(列)–structure<br>create table (创建表):<br>列名 data type(数据类型) width(宽度)<br>constraint (约束)      alter table(修改表结构)           drop table(删除表)</p>\n<p>DML(data manipulation language 数据操作语言)<br>:row(行)–data<br>insert 增       update 改            delete 删数据,删表里的记录</p>\n<p>TCL(transaction control language 事务控制语言)<br>commit(提交)         rollback(回滚)               savepoint(保留点)</p>\n<p>DQL(data query language 数据查询语言)<br>select<br>DCL(data control language 数据控制语言)<br>grant(授权)  grant to       revoke(回收权限) revoke from </p>\n<h4 id=\"3）RDBMS关系型数据库管理系统\"><a href=\"#3）RDBMS关系型数据库管理系统\" class=\"headerlink\" title=\"3）RDBMS关系型数据库管理系统\"></a>3）RDBMS关系型数据库管理系统</h4><p>RDBMS(relationship database management system 关系型数据库管理系统) software(软件) —&gt;(create database)database—&gt;login in database (登录数据库系统 )—&gt;用SQL操作table</p>\n<p>create database 创建空间存储表 (datafile 数据文件)<br>login in database<br>1 远程登录到数据库所在的机器上<br>  192.168.0.20 192.168.0.23 192.168.0.26<br>shell(终端) telnet 192.168.0.20  (跟操作系统建连接)<br>login:openlab<br>password:open123<br>sunv210% shell提示符,执行操作系统命令</p>\n<h4 id=\"4）-登录该机器上的数据库系统\"><a href=\"#4）-登录该机器上的数据库系统\" class=\"headerlink\" title=\"4） 登录该机器上的数据库系统\"></a>4） 登录该机器上的数据库系统</h4><p>sunv210% sqlplus (跟数据库建连接)<br>Enter user-name: openlab<br>Enter password:open123<br>SQL&gt;sqlplus openlab/open123<br>SQL&gt; 数据库提示符,执行SQL命令</p>\n<h4 id=\"5）登录的是哪个数据库\"><a href=\"#5）登录的是哪个数据库\" class=\"headerlink\" title=\"5）登录的是哪个数据库\"></a>5）登录的是哪个数据库</h4><p>echo $ORACLE_SID(环境变量)&lt;—DBA(database administrator 数据库管理员)<br>查看ORACLE_SID变量的取值,oracle提供<br>通过设置ORACLE_SID变量,sqlplus就知道跟哪个数据库建连接.<br>unix平台<br>%c shell<br>%echo $ORACLE_SID  (tarena)<br>%setenv ORACLE_SID hiloo<br>%setenv ORACLE_SID tarena</p>\n<p>$ b shell<br>$ echo $ORACLE_SID  (tarena)<br>$ ORACLE_SID=hiloo<br>$ export ORACLE_SID</p>\n<p>windows平台<br>D:>set ORACLE_SID=hiloo (设置环境变量)<br>D:>set ORACLE_SID (查看环境变量)<br>ORACLE_SID=hiloo</p>\n<h5 id=\"数据表信息：\"><a href=\"#数据表信息：\" class=\"headerlink\" title=\"数据表信息：\"></a>数据表信息：</h5><p>dept(表名) department 部门信息   列名<br>deptno 部门号  dname  部门名称      location 位置(地区)<br>create table dept_hiloo<br>(deptno  number(2), dname char(20),  location char(20));<br>insert into dept_hiloo values (10,’developer’,’beijing’);<br>insert into dept_hiloo values (20,’account’,’shanghai’);<br>insert into dept_hiloo values (30,’sales’,’guangzhou’);<br>insert into dept_hiloo values  ( 40,’operations’,’tianjin’);<br>commit;<br>insert成功后的提示:1 rows inserted<br>emp(表名) employee 员工信息    列名<br>empno 员工 ename 员工名字  job   职位   salary  月薪   bonus   奖金<br>hiredate  入职日期  mgr   manager 管理者    deptno  部门号<br>create table emp_hiloo(<br>empno number(4),    ename varchar2(20),  job  varchar2(15),<br>salary number(7,2), bonus number(7,2),  hiredate date,<br> mgr number(4),  deptno number(10));<br>alter session set nls_date_language=’american’;<br>insert into emp_hiloo values (1001,’zhangwuji’,’Manager’,10000,2000,’12-MAR-10’,1005,10);<br>insert into emp_hiloo values (1001,’zhangwuji’,’Manager’,10000,2000,’12-MAR-10’,1005,10);<br>insert into emp_hiloo values (1002,’liucangsong’,’Analyst’,8000,1000, ‘01-APR-11’,1001,10);<br>insert into emp_hiloo values (1003,’liyi’,’Analyst’,9000,1000,’11-APR-10’,1001,10);<br>insertinto emp_hiloo values (1004,’guofurong’,’Programmer’,5000,null,’01-JAN-11’,1001,10);<br>insertintoemp_hiloo values (1005,’zhangsanfeng’,’President’,15000,null,’15-MAY-08’,null,20);<br>insert into emp_hiloo values (1006,’yanxiaoliu’,’Manager’,5000,400,’01-FEB-09’,1005,20);<br>insert into emp_hiloo values (1007,’luwushuang’,’clerk’,3000,500,’01-FEB-06’,1006,20);<br>insert into emp_hiloo values (1008,’huangrong’,’Manager’,5000,500,’1-MAY-09’,1005,30);<br>insert into emp_hiloo values (1009,’weixiaobao’,’salesman’,4000,null,’20-FEB-09’,1008,30);<br>insert into emp_hiloo values (1010,’guojing’,’salesman’,4500,500,’10-MAY-09’,1008,30);<br>报错信息<br>ORA-00955: name is already used by an existing object(名字已经被一个存在的对象使用)<br>错误：ORA-01843:无效的月份（在中文的plsql控制台上月份要写成’10-3月-02’这种形式，必须是一个数字和一个汉语月。也可以把日期改成英文环境，在执行插入前执行alter session set nls_date_language=’american’;就可以 了。</p>\n<p>DQL<br>select(选择)<br>源表  结果集<br>1 投影操作 select子句实现<br>2 选择操作 where子句实现<br>3 连接操作<br> 1  select ename,salary<em>12 ann_sal(列别名)<br> 2</em> from emp_hiloo</p>\n<p>单引号 表达字符串 ‘’<br>双引号 表达列别名 “”,别名中包含空格,大小写敏感</p>\n<h5 id=\"1）null值的理解\"><a href=\"#1）null值的理解\" class=\"headerlink\" title=\"1）null值的理解\"></a>1）null值的理解</h5><p>1 null值出现在算术表达式中,结果必为null,null可以看作无穷大.<br>2 函数(function) nvl功能空值转换函数<br>nvl是函数名,p1,p2是参数,数据类型必须一致,函数本身有返回值<br>nvl(p1,p2)<br>nvl函数实现:<br>if p1 is null then<br>   return p2;<br>else<br>   return p1;<br>end if;</p>\n<p>3 若有多个null值,distinct去重时,结果集保留一个null值.<br>4 null = null 不成立 null &lt;&gt; null 不成立<br>5 若用in运算符,集合中有null值跟没有null值结果一致的,结果集中不会出现跟null值有关的记录<br>  若用not in运算符,集合中有null值,这个结果集不包含记录.no rows selected.</p>\n<h5 id=\"2）各个子句的功能\"><a href=\"#2）各个子句的功能\" class=\"headerlink\" title=\"2）各个子句的功能\"></a>2）各个子句的功能</h5><p>1 select后面跟列名,列别名,函数,表达式<br>2 select后面的distinct:去重<br>3 where子句<br>  where 条件表达式 (列名 比较运算符 值)<br>表达式 比较运算符 值(尽量不用,为了性能)<br>  where子句中的列为字符类型,放值的位置上不加单引号或加双引号当列名解释,加单引号当字符串解释.<br>  where子句中的列为字符类型,表达具体值时注意字符是大小写敏感的.<br>SQL提供的四个比较运算符<br>肯定形式<br>   between and 区间,范围<br>   in &lt;=&gt; =any  (= or = )(跟集合里的任意一个值相等就满足条件) 集合 离散值<br>   = 单值运算符<br>   in =any 多值运算符<br>   like 像…一样<br>   通配符: %表示0或任意多个字符 <em>任意一个字符<br>   ‘S’ ‘S%’ ‘S</em>‘<br>   is null  如何判断一个列的取值是否为空<br>否定形式<br>= &lt;&gt; != ^=<br>between and   not between and<br>in    not in (&lt;&gt; and &lt;&gt;) &lt;=&gt; &lt;&gt;all(跟集合里的所有值都不能相等)<br>like     not like<br>is null   is not null<br>各个子句的执行顺序<br>from–&gt;where–&gt;select</p>\n<h5 id=\"3）课堂练习\"><a href=\"#3）课堂练习\" class=\"headerlink\" title=\"3）课堂练习\"></a>3）课堂练习</h5><p>1 列出每个员工的名字和他的工资<br>  select ename,salary from emp_hiloo;<br>2 列出每个员工的名字和他的职位<br>  select ename,job from emp_hiloo;<br>3 列出每个员工的名字和他的年薪<br> select ename,salary<em>12 ann_sal from emp_hiloo;<br>4 列出每个员工的名字和他一年的总收入<br>  (salary+bonus)</em>12 (15000+null)<em>12=null<br>  select ename,(salary+nvl(bonus,0))</em>12 tol_sal<br>  from emp_hiloo;<br>5 输出结果如下:<br>  zhangwuji is in department 10.<br>  liucangsong is in department 10.<br>  …..<br>  guojing is in department 30.<br>select ename||’is in department’||deptno||’.’employee from emp_hiloo;<br>什么要加employee呢？Employee是列别名为了显示用的。<br>6 列出该公司有哪些职位<br>  select distinct(job) from emp_hiloo;<br>  select distinct job from emp_hiloo;<br>7 列出该公司不同的奖金<br>  select distinct bonus from emp_hiloo;<br>8 各个部门有哪些不同的职位?<br>  select distinct deptno,job from emp_hiloo;<br>  去重方式:deptno和job联合唯一.<br>  distinct之后和from之前的所有列联合唯一.<br>distinct是保证每一行的唯一性而非某一列的唯一性，所以必须紧跟在select后面。<br>所以distinct只能放在select后面，紧跟select不然会报缺失表达式错误。<br>9 哪些员工的工资高于5000?<br>  select ename,salary from emp_hiloo<br>  where salary &gt; 5000;<br>10 列出员工工资高于5000的员工的年薪?<br>  select ename,salary<em>12 from emp_hiloo<br>  where salary &gt; 5000;<br>11 列出员工年薪高于60000的员工的年薪?<br>  select ename,salary</em>12 from emp_hiloo<br>  where salary<em>12&gt; 60000;<br>  select ename,salary</em>12 ann_sal from emp_hiloo<br>  where ann_sal &gt; 60000(错误的写法)<br>  select ename,salary<em>12 from emp_hiloo<br>  where salary &gt; 5000;<br>12 zhangwuji的年薪是多少?<br>select ename,salary</em>12 from emp_hiloo<br>where ename=’zhangwuji’;<br>  哪些员工的职位是Manager?<br>select ename,job from emp_hiloo<br>where job=’Manager’;<br>  哪些员工的职位是clerk?<br>  select ename,job from emp_hiloo<br>  where job = ‘Manager’<br>   select ename,job from emp_hiloo<br>  where job = ‘clerk’(效率高)<br>  clerk的大小写不清楚<br>  函数:upper(),lower()<br>  select ename,job from emp_hiloo<br>  where upper(job) = ‘CLERK’ (通用性好)<br>13 员工工资在5000到10000之间的员工的年薪<br>   select ename,salary<em>12<br>   from emp_hiloo<br>   where salary &gt;= 5000<br>   and   salary &lt;= 10000;<br>   select ename,salary</em>12<br>   from emp_hiloo<br>   where salary between 5000 and 10000;<br>14 哪些员工的工资是5000或10000.<br>   select ename,salary<br>   from emp_hiloo<br>   where salary = 5000<br>   or salary = 10000<br>   select ename,salary<br>   from emp_hiloo<br>   where salary in (5000,10000)<br>   select ename,salary<br>   from emp_hiloo<br>   where salary =any (5000,10000)<br>15 哪个员工的名字的第二个字符是a.<br>   select ename<br>   from emp_hiloo<br>   where ename like ‘<em>a%’;<br>16 哪个员工的名字的第二个字符是</em>.<br>   select ename<br>   from emp_hiloo<br>   where ename like ‘__%’ escape ‘\\’;<br>   第一个<em>表示任意一个字符,代表通配符<br>   \\</em>必须连起来看,表示下划线本身,escape定义哪个字符可以定义转义’\\’<br>17 哪些员工没有奖金?<br>   select ename,bonus<br>   from emp_hiloo<br>   where bonus is null<br>18 哪些员工有奖金?<br>   select ename,bonus<br>   from emp_hiloo<br>   where bonus is not null<br>19哪些员工的工资不是5000也不是10000.<br>  select ename,salary<br>  from emp_hiloo<br>  where salary not in (5000,10000);<br>  select ename,salary<br>  from emp_hiloo<br>  where salary &lt;&gt; 5000<br>  and salary &lt;&gt; 10000</p>\n<p>create table emp_hiloo<br>( hiredate date）<br>insert into emp_hiloo values (1001,’zhangwuji’,’Manager’,10000,2000,’12-MAR-10’,1005,10);<br>解决方案：<br>    insert into emp_hiloo values (1001,’zhangwuji’,’Manager’,10000,2000,’12-3月-10’,1005,10);</p>\n<h5 id=\"更改字段名字-mysql、orcle-：\"><a href=\"#更改字段名字-mysql、orcle-：\" class=\"headerlink\" title=\"更改字段名字(mysql、orcle)：\"></a>更改字段名字(mysql、orcle)：</h5><p>Oracle修改表<br>alter table 表名 rename column 原名 to 新名；<br>Mysql:<br>alter table 表名 change column(可写，可不写）原名 新名 字段类型；</p>\n<p>ORA-00904：“ANN_SAL”:invalid identifier<br>无效的标识符</p>\n<p>index(索引) view(视图) sequence(顺序号/序列号) function(函数)<br>session altered.会话已更改<br>set feed on可以设置一个，显示操作数<br>connet tiger重新建立连接  show user查看当前用户是谁。<br>edit 用记事本编辑  /运行。</p>\n<p>###Function (单行、多行)###<br>单行函数:表中的一列作为函数的参数,对于每一条记录函数都有一个返回值.<br>例如:upper lower nvl<br>多行函数：表中的一列作为函数的参数,将记录分组,对于每组数据函数返回一个值.<br>例如:avg</p>\n<p>####1）单行函数####<br> 根据处理参数的数据类型分为</p>\n<h5 id=\"1）字符函数-upper-lower\"><a href=\"#1）字符函数-upper-lower\" class=\"headerlink\" title=\"1）字符函数:upper,lower\"></a>1）字符函数:upper,lower</h5><h5 id=\"2）数值函数\"><a href=\"#2）数值函数\" class=\"headerlink\" title=\"2）数值函数:\"></a>2）数值函数:</h5><pre><code>round 四舍五入\nround(12.345,2)--&gt;12.35\nround(12.345,0)=round(12.345)--&gt;12\nround(12,345,-1)--&gt;10\ntrunc 截取\ntrunc(12.345,2)--&gt;12.34\ntrunc(12.345,0)=trunc(12.345)--&gt;12\ntrunc(12,345,-1)--&gt;10\n</code></pre><h5 id=\"3-日期和日期函数\"><a href=\"#3-日期和日期函数\" class=\"headerlink\" title=\"3) 日期和日期函数\"></a>3) 日期和日期函数</h5><pre><code>select sysdate from dual\n06-SEP-12 DD-MON-RR \nalter session set\n  nls_date_format = &apos;yyyy mm dd hh24:mi:ss&apos;\nsession 会话 connection(连接)\n</code></pre><p>   日期类型的数据是用固定的字节7个字节来存储世纪,年,月,日,时,分,秒. 格式敏感<br>   会话级 alter session set nls_date_format<br>   语句级 select to_char(c1日期类型用7个字节来表达，日期类型的数据是用固定的字节7个字节来存储世纪，年，月，日，时，分，秒。四位年的前两位代表世纪20，后两位代表当前年12<br>如果不想修改sql语句运行的话，就需要在执行该语句之前，使用alter session 命令将nls_date_language修改为american，如下：<br>alter session set nls_date_language=’american’    –以英语显示日期<br>如果不想修改sql语句运行的话，就需要在执行该语句之前，使用alter session 命令将</p>\n<p>‘01-JAN-08’ 系统做了隐式数据类型转换,调用了to_date函数<br>‘2008-01-01’,用户做显式数据类型转换,自己调用<br>to_date(‘2008-01-01’,’yyyy-mm-dd’),第二个参数是对第一个参数的格式说明.<br>to_char的返回类型是字符类型,把date转换成了字符串类型,所以参数的数据类型是date.to_char函数可以获得日期的任何一部分信息,比如年,月,日等.<br>select c1 from … 系统做了隐式数据类型转换,调用了to_char函数<br>select to_char(c1,.. 用户做显式数据类型转换,自己调用to_char(c1,’yyyy-mm-dd’),第二个参数是对第一个参数的格式说明.<br>日期的运算<br>   日期可以加减一个数值,单位为天.<br>   select sysdate-1,sysdate,sysdate+1 from dual<br>两个日期相减<br>   add_months 按月加 返回类型是date<br>   add_months(sysdate,6)<br>   select add_months(hiredate,6) from emp_hiloo<br>   add_months(sysdate,-6)<br>   months_between()  返回类型是number<br>   months_between(sysdate,hiredate) 两个日期之间相差多少个月<br>select months_between(sysdate,hiredate) from emp_hiloo;<br>   last_day(sysdate) 本月的最后一天</p>\n<h5 id=\"4-转换函数\"><a href=\"#4-转换函数\" class=\"headerlink\" title=\"4) 转换函数\"></a>4) 转换函数</h5><p>两个日期相减转换函数<br>to_date  char–&gt;date<br>to_char  date–&gt;char , number –&gt; char<br>to_number  char–&gt;number</p>\n<h5 id=\"其他函数\"><a href=\"#其他函数\" class=\"headerlink\" title=\"其他函数\"></a>其他函数</h5><p>coalesce 类似nvl(oracle专有)<br>nvl(bonus,salary<em>0.1)<br>coalesce(bonus,salary</em>0.1,100)。输出所有员工的奖金，如果没有奖金就按工资的10%发放，如果奖金和工资都没有的临时工，就给100元。<br>不同的记录处理方式不一样时,用case when.<br>case when 条件表达式 then 返回结果<br>else<br>     返回结果<br>end<br>若没有else,当不匹配条件,表达式的返回值为null.<br>case deptno when 10 then(不建议该语法形式)<br>decode跟case when的功能类似.<br>decode(deptno,10,salary<em>1.1,<br>              20,salary</em>1.2,<br>              salary)<br>若没有最后一个参数,函数的返回值为null.<br>select语句<br>order by子句<br>select   from    where<br>order by<br>order by子句是select语句中的最后一个子句.<br>order by salary 缺省是升序 asc<br>order by salary desc 降序<br>order by子句后面可以跟列名,表达式(函数),列别名,在select子句中的位置.<br>ORDER BY 子句<br>ORDER BY 语句用于对结果集进行排序。<br>ORDER BY 语句<br>ORDER BY 语句用于根据指定的列对结果集进行排序。<br>ORDER BY 语句默认按照升序对记录进行排序。<br>如果您希望按照降序对记录进行排序，可以使用 DESC 关键字。<br>原始的表 (用在例子中的)：<br>Orders 表:<br>Company    OrderNumber<br>IBM    3532<br>W3School    2356<br>Apple    4698<br>W3School    6953<br>实例 1<br>以字母顺序显示公司名称：<br>SELECT Company, OrderNumber FROM Orders ORDER BY Company<br>结果：<br>Company    OrderNumber<br>Apple    4698<br>IBM    3532<br>W3School    6953<br>W3School    2356<br>实例 2<br>以字母顺序显示公司名称（Company），并以数字顺序显示顺序号（OrderNumber）：<br>SELECT Company, OrderNumber FROM Orders ORDER BY Company, OrderNumber<br>结果：<br>Company    OrderNumber<br>Apple    4698<br>IBM    3532<br>W3School    2356<br>W3School    6953<br>实例 3<br>以逆字母顺序显示公司名称：<br>SELECT Company, OrderNumber FROM Orders ORDER BY Company DESC<br>结果：<br>Company    OrderNumber<br>W3School    6953<br>W3School    2356<br>IBM    3532<br>Apple    4698<br>实例 4<br>以逆字母顺序显示公司名称，并以数字顺序显示顺序号：<br>SELECT Company, OrderNumber FROM Orders ORDER BY Company DESC, OrderNumber ASC<br>结果：<br>Company    OrderNumber<br>W3School    2356<br>W3School    6953<br>IBM    3532<br>Apple    4698<br>注意：在以上的结果中有两个相等的公司名称 (W3School)。只有这一次，在第一列中有相同的值时，第二列是以升序排列的。如果第一列中有些值为 nulls 时，情况也是这样的。</p>\n<h4 id=\"2-多行函数-哪两个函数里只能放number\"><a href=\"#2-多行函数-哪两个函数里只能放number\" class=\"headerlink\" title=\"2) 多行函数(哪两个函数里只能放number)\"></a>2) 多行函数(哪两个函数里只能放number)</h4><p>avg()    平均值  函数的参数只能是number<br>sum()    求和    函数的参数只能是number<br>count()    计数 函数的参数可以是number date 字符<br>        count(*)统计记录,count(bonus)<br>max() 最大值 函数的参数可以是number date 字符<br>min() 最小值 函数的参数可以是number date 字符</p>\n<p>组函数的缺省处理方式是处理所有的非空值.<br>avg(bonus) 所有有奖金的员工的平均值<br>count(bonus) 有奖金的员工个数<br>当所有的值都是null,count函数返回0,其他组函数返回null.</p>\n<h4 id=\"3-group-by子句\"><a href=\"#3-group-by子句\" class=\"headerlink\" title=\"3) group by子句\"></a>3) group by子句</h4><p>若有group by子句,select后面跟组标识和组函数<br>组标识指group by后面的内容<br>from–&gt;where–&gt;group by–&gt;select–&gt;order by<br>若没有group by子句,select后面只要有一个是组函数,其余的都得是组函数.</p>\n<h4 id=\"having子句\"><a href=\"#having子句\" class=\"headerlink\" title=\"having子句\"></a>having子句</h4><p>select deptno,round(avg(salary)) davg<br>from emp_hiloo<br>group by deptno<br>having round(avg(salary))&gt; 5000</p>\n<p>from–&gt;where–&gt;group by–&gt;having–&gt;select–&gt;order by </p>\n<h4 id=\"GROUP-BY-语句\"><a href=\"#GROUP-BY-语句\" class=\"headerlink\" title=\"GROUP BY 语句\"></a>GROUP BY 语句</h4><p>GROUP BY 语句用于结合合计函数，根据一个或多个列对结果集进行分组。<br>SQL GROUP BY 语法<br>SELECT column_name, aggregate_function(column_name)<br>FROM table_name<br>WHERE column_name operator value<br>GROUP BY column_name<br>SQL GROUP BY 实例<br>我们拥有下面这个 “Orders” 表：<br>O_Id    OrderDate    OrderPrice    Customer<br>1    2008/12/29    1000    Bush<br>2    2008/11/23    1600    Carter<br>3    2008/10/05    700    Bush<br>4    2008/09/28    300    Bush<br>5    2008/08/06    2000    Adams<br>6    2008/07/21    100    Carter<br>现在，我们希望查找每个客户的总金额（总订单）。我们想要使用 GROUP BY 语句对客户进行组合。<br>我们使用下列 SQL 语句：<br>SELECT Customer,SUM(OrderPrice) FROM Orders<br>GROUP BY Customer<br>结果集类似这样：<br>Customer    SUM(OrderPrice)<br>Bush    2000<br>Carter    1700<br>Adams    2000<br>很棒吧，对不对？<br>让我们看一下如果省略 GROUP BY 会出现什么情况：<br>SELECT Customer,SUM(OrderPrice) FROM Orders<br>结果集类似这样：<br>Customer    SUM(OrderPrice)<br>Bush    5700<br>Carter    5700<br>Bush    5700<br>Bush    5700<br>Adams    5700<br>Carter    5700<br>上面的结果集不是我们需要的。<br>那么为什么不能使用上面这条 SELECT 语句呢？解释如下：上面的 SELECT 语句指定了两列（Customer 和 SUM(OrderPrice)）。”SUM(OrderPrice)” 返回一个单独的值（”OrderPrice” 列的总计），而 “Customer” 返回 6 个值（每个值对应 “Orders” 表中的每一行）。因此，我们得不到正确的结果。不过，您已经看到了，GROUP BY 语句解决了这个问题。<br>GROUP BY 一个以上的列<br>我们也可以对一个以上的列应用 GROUP BY 语句，就像这样：<br>SELECT Customer,OrderDate,SUM(OrderPrice) FROM Orders<br>GROUP BY Customer,OrderDate</p>\n<h4 id=\"4-where和having比较\"><a href=\"#4-where和having比较\" class=\"headerlink\" title=\"4) where和having比较\"></a>4) where和having比较</h4><p>共同点:都执行在select之前,都有过滤功能<br>区别<br>where执行在having之前<br>where过滤的是记录,任意列名都可以出现在where子句,单行函数可以用在where子句,组函数不能出现在where子句<br>having过滤的是组,组标识可以出现在having子句,其他列名不行,组函数用于having子句,单行函数不可以.</p>\n<h5 id=\"HAVING-子句\"><a href=\"#HAVING-子句\" class=\"headerlink\" title=\"HAVING 子句\"></a>HAVING 子句</h5><p>在 SQL 中增加 HAVING 子句原因是，WHERE 关键字无法与合计函数一起使用。<br>SQL HAVING 语法<br>SELECT column_name, aggregate_function(column_name)<br>FROM table_name<br>WHERE column_name operator value<br>GROUP BY column_name<br>HAVING aggregate_function(column_name) operator value<br>SQL HAVING 实例<br>我们拥有下面这个 “Orders” 表：<br>O_Id    OrderDate    OrderPrice    Customer<br>1    2008/12/29    1000    Bush<br>2    2008/11/23    1600    Carter<br>3    2008/10/05    700    Bush<br>4    2008/09/28    300    Bush<br>5    2008/08/06    2000    Adams<br>6    2008/07/21    100    Carter<br>现在，我们希望查找订单总金额少于 2000 的客户。<br>我们使用如下 SQL 语句：<br>SELECT Customer,SUM(OrderPrice) FROM Orders<br>GROUP BY Customer<br>HAVING SUM(OrderPrice)<2000 1500=\"\" 1700=\"\" 结果集类似：=\"\" customer=\"\" sum(orderprice)=\"\" carter=\"\" 现在我们希望查找客户=\"\" \"bush\"=\"\" 或=\"\" \"adams\"=\"\" 拥有超过=\"\" 的订单总金额。=\"\" 我们在=\"\" sql=\"\" 语句中增加了一个普通的=\"\" where=\"\" 子句：=\"\" select=\"\" customer,sum(orderprice)=\"\" from=\"\" orders=\"\" or=\"\" group=\"\" by=\"\" having=\"\">1500<br>结果集：<br>Customer    SUM(OrderPrice)<br>Bush    2000<br>Adams    2000</2000></p>\n<h4 id=\"5-DCL\"><a href=\"#5-DCL\" class=\"headerlink\" title=\"5) DCL\"></a>5) DCL</h4><p>connect openlab/open123<br>select count(*) from hiloo.emp_hiloo;</p>\n<p>connect hiloo/hiloo123<br>grant select on emp_hiloo to openlab;</p>\n<p>connect openlab/open123<br>select count(*) from hilool.emp_hiloo<br>10rows selected</p>\n<p>connect hiloo/hiloo123<br>revoke select on emp_hiloo from openlab;</p>\n<p>show user<br>select count(*) from hiloo.emp_hiloo</p>\n<p>create synonym emp_hiloo for hiloo.emp_hiloo</p>\n<h4 id=\"6-关于null值的讨论\"><a href=\"#6-关于null值的讨论\" class=\"headerlink\" title=\"6) 关于null值的讨论\"></a>6) 关于null值的讨论</h4><p>1 case when在没有else和decode少一个参数时,返回null.<br>2order by bonus,asc升序时null值在最后,desc降序时null在最前.<br>3 组函数和null值的关系:1组函数的缺省处理方式是处理所有的非空值.2当所有的值都是null,count函数返回0,其他组函数返回null.<br>4若group by的列有null值,所有的null值分在一组.<br>课堂练习<br>1将每个员工的工资涨12.34567%,用round和trunc分别实现<br>select ename,nvl(trunc(round(salary+salary*0.1234567,2),1),0.0) from emp_hiloo;//自己写的。<br>2 将’2008-01-01’插入表中,<br>  再将’2008 08 08 08:08:08’插入表中<br>insert into test values<br>(to_date(‘01-JAN-08’,’DD-MON-RR’));</p>\n<p>3找出3月份入职的员工.<br>select ename,hiredate<br>from emp_hiloo<br>where to_char(hiredate,’mm’) = ‘03’;<br>select ename,hiredate<br>from emp_hiloo<br>where to_char(hiredate,’mm’) = 3;//可以正常输出winXP下<br>‘03’ = 3  —&gt; to_number(‘03’) = 3<br>字符   数值  缺省系统将字符转成数值<br>select ename,hiredate<br>from emp_hiloo<br>where to_char(hiredate,’fmmm’) = ‘03’;(错，未选定行，无输出)</p>\n<p>select ename,hiredate<br>from emp_hiloo<br>where to_char(hiredate,’fmmm’) = ‘3’;(对)<br>‘03’ = ‘3’ (错)<br>fm表示去掉前导0或去掉两边的空格.<br>4 zhangsanfeng的mgr上显示boss,其他人不变.<br>select ename,empno,<br>       nvl(to_char(mgr),’boss’) mgr<br>from emp_hiloo<br>函数nvl（“1”，“2”）:如果字符串1是空，就返回字符串”2”</p>\n<h4 id=\"5十分钟之后\"><a href=\"#5十分钟之后\" class=\"headerlink\" title=\"5十分钟之后\"></a>5十分钟之后</h4><p> select sysdate,sysdate+1/144 from dual;<br>解释：Oracle 里面,</p>\n<p>sysdate + 1 意思是 当前时间 + 1天</p>\n<p>sysdate + 1/24  意思是 当前时间 + 1/24天  也就是1小时后</p>\n<p>sysdate+1/144  意思是 当前时间 + 1/144天 （1/24<em>6）  也就是10分钟后<br> 6 若员工是10部门的,工资涨10%,20部门工资涨20%,其他员工工资不变.<br>select ename,salary,<br>       case when deptno = 10 then salary</em>1.1<br>            when deptno = 20 then salary*1.2<br>       else<br>            salary<br>       end new_sal<br>from emp_hiloo;</p>\n<p>select ename,salary,<br>       decode(deptno,10,salary<em>1.1,<br>                     20,salary</em>1.2,<br>                     salary) new_sal<br>from emp_hiloo;<br>7 列出每个员工的年薪,按年薪降序排列.<br>select ename,salary<em>12<br>from emp_hiloo<br>order by salary desc (好)<br>select ename,salary</em>12<br>from emp_hiloo<br>order by salary<em>12 desc<br>select ename,salary</em>12 n_sal<br>from emp_hiloo<br>order by n_sal desc</p>\n<p>select ename,salary<em>12 n_sal from emp_hiloo order by 2 desc;<br>select salary</em>12,ename n_sal from emp_hiloo order by 2 asc;<br>8 列出员工的名字,部门号以及工资,按部门号从小到大的顺序,同一部门的工资按降序排列.<br>select ename,deptno,salary<br>from emp_hiloo<br>order by deptno,salary desc<br>9 列出奖金的平均值,和,个数,最大值,最小值.<br>AVG 函数返回数值列的平均值。NULL 值不包括在计算中<br>select avg(bonus),avg(nvl(bonus,0)),<br>       sum(bonus), sum(nvl(bonus,0)),<br>       count(bonus),count(nvl(bonus,0)),<br>       max(bonus),max(nvl(bonus,0)),<br>       min(bonus),min(nvl(bonus,0))<br>from emp_hiloo<br>10 各个部门的平均工资<br>ROUND 函数用于把数值字段舍入为指定的小数位数。<br>select deptno,round(avg(salary))<br>from emp_hiloo<br>group by deptno<br>11 求10部门的平均工资,只显示平均工资<br>   求10部门的平均工资,显示部门号,平均工资<br>   select round(avg(salary))<br>   from emp_hiloo<br>   where deptno = 10<br>   group by deptno</p>\n<p>   select max(deptno),round(avg(salary))<br>   from emp_hiloo<br>   where deptno = 10<br>12各个部门不同职位的平均工资<br>   select deptno,job,round(avg(salary))<br>   from emp_hiloo<br>   group by deptno,job<br>13 每种奖金有多少人?<br>   select bonus,count(empno)<br>   from emp_hiloo<br>   group by bonus<br>14 列出平均工资大于5000的部门的平均工资<br>   select deptno,round(avg(salary))<br>   from emp_hiloo<br>   group by deptno<br>   having round(avg(salary)) &gt; 5000<br>15哪些员工的工资是最低的.<br>  select ename from emp_hiloo<br>  where salary = ( select min(salary)<br>                   from emp_hiloo)<br>报错信息<br>ORA-01861: literal does not match format string<br>文字值不匹配格式串<br>ORA-01722: invalid number 无效的数值 to_number<br>ORA-00937: not a single-group group function 不是一个组函数<br>ORA-00979: not a GROUP BY expression 不是一个group by表达式 GROUP BY expression指跟在group by后面的东西(列名),称之为组标识<br>detail 细节 summary 聚合</p>\n<h3 id=\"查询\"><a href=\"#查询\" class=\"headerlink\" title=\"查询\"></a>查询</h3><p>子查询定义<br>在SQL语句中嵌入select语句<br>create table new_tabname<br>as<br>select ename,salary*12 ann_sal from emp_hiloo;<br>新表的结构由select后面的项来决定,new_table包含两列ename,ann_sal.</p>\n<h4 id=\"子查询\"><a href=\"#子查询\" class=\"headerlink\" title=\"子查询\"></a>子查询</h4><p>  非关联子查询<br>    单列子查询<br>    多列子查询<br>  关联子查询</p>\n<h5 id=\"子查询执行\"><a href=\"#子查询执行\" class=\"headerlink\" title=\"子查询执行\"></a>子查询执行</h5><p>非关联子查询<br>子查询的表和主查询的表没有建关联<br>先执行子查询(只执行一遍),当返回多条记录,系统会将自动去重的结果返回给主查询,再执行主查询.</p>\n<p>关联子查询<br>子查询的表和主查询的表建关联.所谓建关联指主查询表里的列和子查询表里的列写成一个条件表达式.</p>\n<p>先执行主查询,判断表里的记录是否应该放入结果集.过程如下:拿到第一条记录,获得了各个列的值,将需要的列值带入子查询,执行后返回的结果再和主查询表里的列做比较,符合条件,该记录放入结果集,否则过滤掉.依次执行主查询表里的每条记录.子查询执行的次数由主查询表里的记录数决定.</p>\n<p>1) exists和not exists<br>exists的执行过程<br>从主查询表里拿到第一条记录,按子查询里的关联条件在子查询的表里看是否能找到匹配的记录,当找到第一条匹配的记录后,立即返回(即不需要找出所有匹配的记录),exists条件满足,主查询表里的该记录放入结果集.若按子查询里的关联条件将子查询<br>表里的记录全部检查一遍后没有一条符合条件的记录,此时也返回, exists 条件不满足,主查询表里的该记录不能放入结果集,被过滤掉.</p>\n<p>select ename from emp_afei o<br>where exists<br>             (select 1 from emp_afei i<br>              where o.empno = i.mgr)</p>\n<h5 id=\"非关联子查询的分类\"><a href=\"#非关联子查询的分类\" class=\"headerlink\" title=\"非关联子查询的分类\"></a>非关联子查询的分类</h5><p>单列子查询<br>select ename,salary<br>from emp_hiloo<br>where salary = (select min(salary)<br>                from emp_hiloo<br>                )<br>多列子查询:按键值对比较<br>select ename,salary,deptno<br>from emp_afei<br>where (deptno,salary) in<br>             (select deptno,round(avg(salary))<br>              from emp_afei<br>              group by deptno)</p>\n<p>2) 课堂练习<br>1哪些人是领导?(非关联子查询)<br>如果一个员工的empno能出现在mgr里就说明他是领导.<br>select ename<br>from emp_hiloo<br>where empno in (select mgr from emp_hiloo)<br>select ename<br>fro emp_afei<br>where empno in (1001,1005,1006,1008,null)<br>2 哪些人是员工?<br>他的empno绝对不能出现在mgr中,他的empno跟mgr的出现的所有的值不能相等. &lt;&gt;all<br>select ename<br>from emp_hiloo<br>where empno not in (select mgr from emp_hiloo)<br>select ename<br>fro emp_afei<br>where empno not in (1001,1005,1006,1008,null)<br>select ename<br>from emp_hiloo<br>where empno not in (select mgr from emp_hiloo<br>                    where mgr is not null)</p>\n<p>3哪些部门的平均工资比30部门的平均工资高?<br>select deptno,round(avg(salary))<br>from emp_hiloo<br>group by deptno<br>having round(avg(salary)) &gt;<br>                    (select round(avg(salary))<br>                     from emp_hiloo<br>                     where deptno = 30)<br>4哪些员工的工资比zhangwuji的工资高?<br>select ename,salary<br>from emp_afei<br>where salary &gt; (select salary from emp_afei<br>                where ename = ‘zhangwuji’)<br>ERROR at line 3:<br>ORA-01427: single-row subquery returns more than one row<br>单行子查询返回多条记录</p>\n<p>比所有人高 &gt; (select max(salary))<br>           &gt;all<br>比任意人高 &gt; (select min(salary)<br>           &gt;any<br>5哪些员工的工资等于本部门的平均工资?<br>select ename,salary,deptno<br>from emp_afei<br>where (deptno,salary) in<br>             (select deptno,round(avg(salary))<br>              from emp_afei<br>              group by deptno)<br>5哪些员工的工资比本部门的平均工资高?<br>select ename,salary,deptno<br>from emp_afei o<br>where salary &gt; (select round(avg(salary))<br>                from emp_afei i<br>                where i.deptno = o.deptno)<br>6哪些人是领导?(关联子查询)<br>select ename from emp_afei o<br>where exists<br>             (select 1 from emp_afei i<br>              where o.empno = i.mgr)<br>7哪些部门有员工?<br>select deptno,dname<br>from dept_afei o<br>where exists (select 1 from emp_afei i<br>              where o.deptno = i.deptno)</p>\n<p>3) 课外练习day03am<br>1 zhangwuji的领导是谁,显示名称?<br>2 zangwuji领导谁,显示名称?<br>3 列出devoleper部门有哪些职位?<br>1) 课外练习day04am答案<br>1 zhangwuji的领导是谁,显示名称?<br>  select ename from emp_afei<br>  where empno in<br>        (select mgr from emp_afei<br>                 where ename = ‘zhangwuji’)</p>\n<p>zangwuji领导谁,显示名称?</p>\n<p> select ename from emp_afei<br> where mgr in (select empno from emp_afei<br>               where ename = ‘zhangwuji’)</p>\n<p>3 列出developer部门有哪些职位?<br>  select distinct job from emp_afei<br>  where deptno in<br>           (select deptno from dept_afei<br>            where dname = ‘developer’)</p>\n<p>2) 非关联子查询<br>exists和not exists<br>not exists的执行过程<br>从主查询表里拿到第一条记录,按子查询里的关联条件在子查询的表里看是否能找到匹配的记录,当找到第一条匹配的记录后,立即返回(即不需要找出所有匹配的记录),not exists条件不满足,主查询表里的该记录不能放入结果集,被过滤掉.若按子查询里的关联条件将子查询表里的记录全部检查一遍后没有一条符合条件的记录,返回, not exists 条件满足,主查询表里的该记录放入结果集.</p>\n<p>对于exists和not exists,在子查询中找到第一条匹配的记录都会立即返回,exists将主查询表里的记录放入结果集,not exsits将主查询表里的记录过滤掉.<br>对于exists和not exists,如果子查询没有返回任何记录,即扫描全部记录后没有一条符合条件的记录,都返回,exists将主查询表里的记录过滤掉,not exists将主查询表里的记录放入结果集.<br>not in ,&lt;&gt; all逻辑上跟not exists等价<br>in ,=any逻辑上跟exists等价</p>\n<p>查询形式:集合操作<br>把结果集作为一个集合,结果集必须是同构的,列的个数及数据类型一致</p>\n<p>3) 并集  union(去重)/union all(不去重)<br>select ename,deptno,salary,salary<em>1.1 new_sal<br>from emp_afei<br>where deptno = 10<br>union all<br>select ename,deptno,salary,salary</em>1.2 new_sal<br>from emp_afei<br>where deptno = 20<br>union all<br>select ename,deptno,salary,salary new_sal<br>from emp_afei<br>where deptno not in (10,20)</p>\n<p>case when和decode可以实现类似功能.</p>\n<p>4) 交集  intersect(去重)<br>select job from emp_afei<br>where deptno = 10<br>intersect<br>select job from emp_afei<br>where deptno = 20<br>10部门和20部门都有的职位是哪些?</p>\n<p>5) 差  minus(去重)<br>select deptno from dept_afei<br>minus<br>select deptno from emp_afei<br>那些部门没有员工.</p>\n<p>6) 多表查询<br>1) 交叉连接 cross join<br>select e.ename,d.dname<br>from emp_afei e cross join dept_afei d<br>结果集产生<br>10*4=40,组合操作,笛卡尔积</p>\n<p>2) 内连接 inner join(匹配一个条件)<br>select e.ename,e.deptno,d.deptno,d.dname<br>from emp_afei e join dept_afei d<br>ORA-00905: missing keyword(丢失关键字)</p>\n<p>如果把结果集的产生看成双层循环,驱动表是外层循环,匹配表是内层循环.<br>对于内连接哪张表做驱动表,哪张表做匹配表产生出的结果集是一样的,不同的是性能.<br>驱动表在匹配表的匹配情况如下:<br>一条记录找到一条匹配<br>一条记录找到多条匹配<br>一条记录找不到任何匹配.<br>内连接的核心是驱动表的记录要出现在结果集中必须在匹配表中能找到匹配的记录,否则该记录被过滤掉.</p>\n<p>3) 内连接查询形式<br>等值连接 on e.deptno = d.deptno<br>两张表有表述同一属性的列,两张表都有deptno列.<br>自连接 on e.mgr = m.empno<br>同一张表的不同列能写成一个表达式,即同一张表的两条记录之间有关系.通过给表起别名的方式,将同一张表的两条记录之间的关系转化成不同表的两条记录之间的关系.<br>4) 外连接<br>外连接 outer join(驱动表的记录一个都不能少的出现在结果集里)<br>from t1 left join t2<br>on t1.c1 = t2.c2(t1驱动表,t2匹配表)<br>外连接结果集=内连接的结果集+t1表中匹配不上的记录和t2表中的null记录的组合<br>from t1 right join t2<br>on t1.c1 = t2.c2(t2驱动表,t1匹配表)<br>外连接结果集=内连接的结果集+t2表中匹配不上的记录和t1表中的null记录的组合<br>from t1 full join t2<br>on t1.c1 = t2.c2<br>外连接结果集=内连接的结果集+t1表中匹配不上的记录和t2表中的null记录的组合+t2表中匹配不上的记录和t1表中的null记录的组合</p>\n<p>5) 外连接的应用场景<br>1 某张表的记录全部出现在结果集中,包括匹配不上的.<br>select e.ename,nvl(m.ename,’Boss’)<br>from emp_afei e left join emp_afei m<br>on e.mgr = m.empno<br>2解决否定问题,匹配不上的记录找出来(跟所有的记录都不匹配.)(not in/not exists)<br>外连接 + where 匹配表.主键列 is null<br>select e.ename,d.dname<br>from emp_afei e right join dept_afei d<br>on e.deptno = d.deptno<br>where e.empno is null<br>(解决结果集只包含匹配不上的记录.where子句执行在外连接之后)哪些部门没有员工</p>\n<p>select d.dname<br>from emp_afei e right join  dept_afei d<br>on e.deptno = d.deptno<br>and e.ename = ‘zhangwuji’<br>where e.empno is null<br>如果希望在外连接之前过滤匹配表用and子句,如果想在外连接之后通过匹配表里的列过滤外连接的结果集时候用where.<br>过滤驱动表统计用where子句过滤.</p>\n<p>6) 课内练习<br>1 哪些部门没有员工(not exists)<br>  select dname from dept_afei o<br>  where not exists<br>        (select 1 from emp_afei i<br>         where o.deptno = i.deptno)<br>2 哪些人是员工?(not exists)<br>  select ename from emp_afei o<br>  where not exists<br>            (select 1 from emp_afei i<br>             where o.empno = i.mgr)<br>他的empno和其他人的mgr相等是不可能存在的.即和所有人的mgr都不相等.<br>not in ,&lt;&gt; all逻辑上跟not exists等价<br>3 列出哪些员工在北京地区上班?<br>思路:确定表,两张表,匹配问题用inner join–&gt;on(匹配条件)–&gt;(对表是否过滤)<br>select e.ename,e.deptno,d.deptno,d.dname<br>from emp_afei e join dept_afei d<br>on e.deptno = d.deptno<br>and d.location = ‘beijing’<br>4zhangwuji在哪个地区上班?<br>select e.ename,d.dname,d.location<br>from emp_afei e join dept_afei d<br>on e.deptno = d.deptno<br>and e.ename = ‘zhangwuji’<br>5列出每个部门有哪些职位?部门名称,职位<br> select distinct d.dname,e.job<br> from emp_afei e join dept_afei d<br> on e.deptno = d.deptno<br> order by d.dname<br>6各个部门的平均工资,列出部门名称,平均工资.<br>select d.dname,round(avg(e.salary)) savg<br>from emp_afei e join dept_afei d<br>on e.deptno = d.deptno<br>group by d.dname<br>select max(d.dname),round(avg(e.salary)) savg<br>from emp_afei e join dept_afei d<br>on e.deptno = d.deptno<br>group by d.deptno<br>select min(deptno),round(avg(salary))<br>from emp_hiloo<br>where deptno = 10<br>7 列出每个员工的名字和他的领导的名字<br>select e.ename employee,<br>       m.ename manager<br>from emp_afei e join emp_afei m<br>on e.mgr = m.empno<br>结果集是9条.<br>e表中有10条记录,其中9条记录找到匹配,zhangsanfeng没匹配<br>m表中有10条记录,其中4条记录找到匹配,4条记录是领导,6条记录找不到匹配,他们是员工.<br>select e.ename employee,<br>       m.ename manager<br>from emp_afei e join emp_afei m<br>on e.mgr = m.empno<br>union all<br>select ename,’Boss’<br>from emp_afei<br>where mgr is null</p>\n<p>select e.ename employee,<br>       decode(m.ename,e.ename,’Boss’,<br>                  m.ename)   manager<br>from emp_afei e join emp_afei m<br>on nvl(e.mgr,e.empno) = m.empno</p>\n<p>select e.ename,nvl(m.ename,’Boss’)<br>from emp_afei e left join emp_afei m<br>on e.mgr = m.empno<br>10=9+1</p>\n<p>8哪些人是领导?<br>select distinct m.ename<br>from emp_afei e join emp_afei m<br>on e.mgr = m.empno<br>9哪些部门没有员工?<br>select e.ename,d.dname<br>from emp_afei e right join dept_afei d<br>on e.deptno = d.deptno<br>where e.empno is null<br>(解决结果集只包含匹配不上的记录.where子句执行在外连接之后)<br>11=10+1<br>如果部门表里的某条记录的deptno在emp表找不到匹配,在内连接中,它被过滤,<br>e表的empno的特性是唯一且非空的(主键约束),居然e.empno is null,说明null是外连接时为了驱动表中那条匹配不上的记录出现在结果集中,在匹配表中模拟的null记录.<br>10哪些人是员工,哪些人不是领导?<br>select e.empno,m.ename<br>from emp_afei e right join emp_afei m<br>on e.mgr = m.empno<br>where e.empno is null</p>\n<p>from emp_afei e right join emp_afei m<br>15=9+(10(m表中有10条记录)-4(m表中有4条匹配记录 ))<br>from emp_afei e left join emp_afei m<br>10(结果集)=9+(10(e表中有10条记录)-9(e表中有9条匹配记录))<br>11 哪些部门没有叫zhangwuji的?<br>select d.dname<br>from emp_afei e right join  dept_afei d<br>on e.deptno = d.deptno<br>and e.ename = ‘zhangwuji’<br>where e.empno is null</p>\n<p>7) 课外练习(day04)(答案在Day05)<br>1zhangwuji的领导是谁?(表连接)<br>2zhangwuji领导谁?(表连接)<br>3哪些人是领导?(in exists join)<br>4哪些部门没有员工?(not in/not exists/outer join)<br>5哪些人是员工,哪些人不是领导?(not in/not exists/outer join)<br>Day05.txt<br>Grade级别<br>Lowsal最低工资<br>Hisal最高工资<br>Create table salgrade_hiloo(<br>Grade<br>)<br>cross join  inner join   outer join<br>inner join(匹配)<br>  等值连接<br>  自连接<br>  非等值连接<br>outer join(匹配+不匹配)<br>  等值连接</p>\n<p>  自连接<br>  非等值连接</p>\n<p>所谓非等值连接表示两张表里的列不能写成等值表达式,而是写成between and之类.所以两个表之间有关系是指表里的列可以写成表达式,而不是等值表达式.<br>salgrade<br>grade  级别<br>lowsal 最低工资<br>hisal  最高工资</p>\n<p>from后面跟子查询<br>emp,各个部门的平均工资dept_avgsal(depnto,avgsal)<br>select e.ename,e.salary,e.deptno<br>from emp_afei e join<br>      (select deptno,round(avg(salary)) avgsal<br>       from emp_afei<br>       group by deptno) a<br>on e.deptno = a.deptno<br>and e.salary &gt; a.avgsal</p>\n<p>各个部门的平均工资,列出部门名称,平均工资<br>select max(d.dname),round(avg(salary))<br>from emp_afei e join dept_afei d<br>on e.deptno = d.deptno<br>group by d.deptno</p>\n<p>select d.dname,a.avgsal<br>from dept_afei d join<br>      (select deptno,round(avg(salary)) avgsal<br>       from emp_afei<br>       group by deptno) a<br>on d.deptno = a.depto</p>\n<p>DML<br>insert一条记录时,若某些列为null值,有哪些语法实现?<br>insert into tabname values (1,’a’,null,sysdate)<br>insert into tabname(c1,c2,c4)<br>values (1,’a’,sysdate)<br>insert语句的两种语法形式?<br>insert into tabname values () insert一条记录<br>insert into tabname<br>select * from tabname1  insert多条记录<br>连接图解：</p>\n<h3 id=\"数据类型\"><a href=\"#数据类型\" class=\"headerlink\" title=\"数据类型\"></a>数据类型</h3><p>1) 课外练习答案day04<br>1zhangwuji的领导是谁?(表连接)<br> select m.ename<br> from emp_afei e join emp_afei m<br> on e.mgr = m.empno<br> and m.ename = ‘zhangwuji’<br>2 zhanghangwuji领导谁?(表连接)<br> select e.ename<br> from emp_afei e join emp_afei m<br> on e.mgr = m.empno<br> and m.ename = ‘zhangwuji’<br>3哪些人是领导?(in exists join)<br> select ename from emp_afei<br> where empno in (select mgr from emp_afei)<br> select ename from emp_afei o<br> where exists<br>            (select 1 from emp_afei i<br>             where o.empno = i.mgr)<br> select distinct m.ename<br> from emp_afei e join emp_afei m<br> on e.mgr = m.empno<br>4哪些部门没有员工?(not in/not exists/outer join)<br> select dname from dept_afei<br> where deptno not in<br>               (select deptno from emp_afei)<br> select dname from dept_afei o<br> where not exists<br>             (select 1 from emp_afei i<br>              where o.deptno = i.deptno)<br> select d.dname<br> from emp_afei e right join dept_afei d<br> on e.deptno = d.deptno<br> where e.empno is null<br>5哪些人是员工,哪些人不是领导?(not in/not exists/outer join)<br> select ename from emp_afei<br> where empno not in (<br>               select mgr from emp_afei<br>               where mgr is not null)<br> select ename from emp_afei o<br> where not exists<br>             (select 1 from emp_afei i<br>              where o.empno = i.mgr)<br> select m.ename<br> from emp_afei e right join emp_afei m<br> on e.mgr = m.empno<br> where e.empno is null<br>cross join (笛卡尔积)</p>\n<p>rownum 伪列,记录号<br>若用rownum选择出记录,编号必须从1开始.<br>分页问题<br>第一页<br>select rownum,ename<br>from emp_afei<br>where rownum &lt;= 3;<br>第二页<br>select rn,ename<br>from (<br>      select rownum rn,ename<br>      from emp_afei<br>      where rownum &lt;= 6)<br>where rn between 4 and 6<br>排名问题<br>按工资排名的前三条记录<br>select rownum,ename,salary<br>from emp_hiloo<br>where rownum &lt;=3<br>order by salary desc;(错)</p>\n<p>select rownum,ename,salary<br>from ( select ename,salary<br>       from emp_afei<br>       order by salary desc)<br>where rownum &lt;= 3</p>\n<p>update语句的中set后面的=是什么含义?where后面的=是什么含义?<br>set c1 = null (= 赋值)<br>where c1 = null (= 等号)</p>\n<p>update和delete语句中的where子句是什么含义?<br>用来确定对表里的哪些记录要进行update或delete操作,没有where子句多表里的所有记录update或delete<br>update<br>set<br>where c1 = (select …)<br>rename 关键字 17<br>commit</p>\n<p>1011 abc 1000 10 ‘clerk’<br>update 1001 1000–&gt;2000<br>delete 1011<br>commit<br>如何编写和运行一个sql脚本(文本文件)<br>1 编辑文件<br>在linux环境下已经编写好了test.sql,做一个鼠标右键的copy</p>\n<p>在20,23,26机器上,<br>vi test.sql<br>按a i o进入编辑模式,paste,按esc键,再按:wq!回车</p>\n<p>2 运行文件<br>sun-server% sqlplus openlab/open123 @test.sql<br>@表示运行<br>SP2-0310: unable to open file “test.sql”在当前目录下没有test.sql文件<br>sqlplus openlab/open123 ../test.sql</p>\n<p>cd ..<br>sun-server% sqlplus openlab/open123 @test.sql</p>\n<p>SQL&gt;@test.sql</p>\n<p>数据库对象 PL/SQL<br>create or replace function test<br>insert into test values (1,1)<br>            *<br>ERROR at line 1:<br>ORA-04044: procedure(存储过程), function(函数), package(包), or type is not allowed here</p>\n<p>事务(transaction 交易)<br>事务里包含的DML语句<br>事务的结束<br>commit 提交,(dml操作的数据入库了)<br>rollback 回滚 撤销(DML操作被取消)<br>sqlplus正常退出=commit<br>DDL语句自动提交<br>开始<br>上一个事务的结束是下一个事务的开始.<br>一致状态<br>数据库的数据被事务改变.<br>oltp online transaction processing联机事务处理系统 高并发系统</p>\n<p>事务的隔离级别 read committed(读已经提交了的数据)</p>\n<p>如果不commit—–&gt;commit rollback<br>1如果不commit,其他session是看不见你的操作<br>2如果不commit,会阻塞操作同一条记录的事务(session),commit才能释放所有DML加的锁.<br>3如果不commit,系统做DML操作,会将old data放入rollback segment(回滚段) ,所占用的回滚段资源不释放.</p>\n<p>DML系统会自动给表及表里的记录加锁<br>表级共享锁<br>行级排他锁<br>    表级共享锁     行级排他锁<br>s1    ok        ok<br>s2    ok        enqueue wait<br>s3    ok        ok</p>\n<p>执行DDL语句,系统自动加DDL排他锁<br>SQL&gt; drop table test purge;<br>drop table test purge<br>           *<br>ERROR at line 1:<br>ORA-00054: resource busy(资源忙 test表) and acquire with NOWAIT specified (dml wait,ddl nowait 如果加不上锁,报错退出)</p>\n<p>DDL语句<br>字符类型<br>varchar2,必须带宽度, 按字符串的实际长度存,本身的数据是变化,对空格敏感<br>char,可以不带宽度,缺省宽度是1,按字符串的定义长度存,本身的数据是固定长度的.对空格不敏感<br>数值类型</p>\n<p>number类型<br>create table test90<br>(c1 number,<br> c2 number(6),<br> c3 number(4,2),<br> c4 number(2,4),<br> c5 number(3,-3))</p>\n<p>四舍五入<br>number(6) 表示6为整数 999999<br>number(4,2) 表示小数点后2位,整数位2位 99.99<br>number(2,4) 表示小数点后4位,能填数字的位数是2位 0.0099<br>number(3,-3) 999000 999123–&gt;999000<br>                    999511–&gt;报错</p>\n<p>user_tables 是一张系统表,里面记录当前用户所有的表的信息,里面没有记录表的创建日期.<br>user_objects 是一张系统表,里面记录当前用户所有的数据库对象的信息.created的列记录数据库对象(如表)的创建日期.<br>user_tables和user_objects这两张表的关系体现在table_name和object_name都记录的是表名.</p>\n<p>data block 数据块,操作数据的最小逻辑(物理)单元,最少读一个block的数据</p>\n<p>HWM high water mark 高水位线,表示曾经插入数据的最高位置<br>FTS full table scan 全表扫描,把表里的所有记录读一遍,把HWM之下的所有data block读一遍</p>\n<p>truncate table 释放空间,HWM下移<br>delete 不释放空间,HWM不动<br>不适合用delete命令删大表.</p>\n<p>课内练习<br>1 列出工资级别为3级,5级的员工<br>  select e.ename,e.salary,s.grade<br>  from emp_afei e join salgrade_afei s<br>  on e.salary between s.lowsal and s.hisal<br>  and s.grade in (3,5)<br>2 列出各个工资级别有多少人?<br>  select s.grade,count(e.empno)<br>  from emp_afei e join salgrade_afei s<br>  on e.salary between s.lowsal and s.hisal<br>  group by s.grade<br>  order by s.grade<br>3 列出各个工资级别有多少人?(包含0级)<br>  select s.grade,count(e.empno)<br>  from emp_afei e right join salgrade_afei s<br>  on e.salary between s.lowsal and s.hisal<br>  group by s.grade<br>  order by s.grade<br>特别注意count不要写*或者s.grade</p>\n<p>课外练习day05<br>1按工资排名的第4到第6名员工.</p>\n<p>###关键点###<br>课外练习day05答案</p>\n<p>按工资排名的第4到第6名员工.<br>select rn,ename,salary<br>from<br>    (select rownum rn,ename,salary<br>     from (select ename,salary<br>           from emp_afei<br>           order by salary desc)<br>     where rownum &lt;= 6<br>    )<br>where rn &gt;= 4 </p>\n<p>####1）事务####</p>\n<p>####约束 constraint (安检)####<br>primary key(主键)<br>foreign key(外键)<br>unique key (唯一键)<br>not null(非空)<br>check (检查)</p>\n<p>主键 (表中不会出现重复记录)<br>列级约束<br>create table test<br>(c1 number(2)<br>    constraint test_c1_pk primary key,<br> c2 number(3))</p>\n<pre><code>constraint test_c1_pk primary key,\n           *\n</code></pre><p>ERROR at line 3:<br>ORA-02264: name already used by an existing constraint (名字被存在的约束使用了)</p>\n<p>SQL&gt; select table_name from user_constraints<br>  2  where constraint_name = ‘TEST_C1_PK’;<br>哪张表里有叫TEST_C1_PK这个约束名.</p>\n<p>ORA-00001: unique constraint (HILOO(用户名) .TEST_C1_PK) violated(冲突)</p>\n<p>PK=UK + NN</p>\n<p>表级约束<br>create table test(<br>c1 number(2),<br>c2 number,<br>constraint test_c1_pk primary key(c1)<br>)<br>表中有三列c1,c2,c3,c1和c2做成联合主键<br>create table test(<br>c1 number,<br>c2 number,<br>constraint test_c1_c2_pk primary key(c1,c2),<br>c3 number<br>)<br>没有constraint关键字,系统用自动起名字sys_c数字.</p>\n<p>not null<br>create table test<br>(c1 number constraint test_c1_pk primary key,<br> c2 number not null);<br>not null约束没有表级形式</p>\n<p>unique (pk)<br>相同点:都要保证唯一性<br>区别:uk允许为null,而且可以多个null值,一个表中只能有一个pk约束,可以有多个uk约束.<br>create table test<br>(c1 number constraint test_c1_pk primary key,<br> c2 number constraint test_c2_uk unique)</p>\n<p>create table test(<br>c1 number primary key,<br>c2 number primary key,<br>c3 number unique,<br>c4 number unique)  (报错,一张表只能有一个primary key)</p>\n<p>create table test(<br>c1 number constraint test_c1_pk primary key,<br>c2 number constraint test_c2_uk unique,<br>c3 number constraint test_c3_uk unique,<br>c4 number )<br>c2上定义了一个唯一键 c3上定义了一个唯一键</p>\n<p>create table test(<br>c1 number constraint test_c1_pk primary key,<br>c2 number,<br>c3 number,<br>constraint test_c2_c3_uk unique (c2,c3),<br>c4 number)<br>c2,c3联合唯一键</p>\n<p>check<br>create table test(<br>c1 number(3) constraint test_c1_ck<br>             check (c1 &gt; 100))</p>\n<p>create table test(<br>c1 number(3),<br>constraint test_c1_ck check (c1 &gt; 100))</p>\n<p>外键<br>parent table(父表)上定义唯一列(pk/uk)<br>child table(子表)上定义外键列(fk)</p>\n<p>1 先create parent table(pk/uk),再create child table(fk)<br>2 先insert into parent table,再insert into child table<br>3 先delete from child table,再delete from parent table<br>4 先drop child table,再drop parent table</p>\n<p>reference 引用<br>create table parent<br>(c1 number(3))</p>\n<p>create table child<br>(c1 number(2) constraint child_c1_pk<br>              primary key,<br> c2 number(3) constraint child_c2_fk<br>              references parent(c1))</p>\n<pre><code>references parent(c1))\n                  *\n</code></pre><p>ERROR at line 5:<br>ORA-02270: no matching unique or primary key for this column-list<br>在c1上没有定义uk或pk</p>\n<p>alter table parent<br>add constraint parent_c1_pk primary key(c1);<br>给c1列增加主键约束</p>\n<p>insert into child values (1,1)<br>ORA-02291: integrity constraint(完整性约束) (HILOO.CHILD_C2_FK) violated - parent key not found (父键值没发现)<br>违反fk约束</p>\n<p>insert into parent values (1);<br>insert into child values (1,1)</p>\n<p>delete from parent where c1 = 1;<br>ORA-02292: integrity constraint (HILOO.CHILD_C2_FK) violated - child record<br>found(子记录被发现)</p>\n<p>delete from child where c2 = 1;<br>delete from parent where c1 = 1;</p>\n<p>drop table parent purge;<br>ORA-02449: unique/primary keys in table referenced by foreign keys<br>在parent table上的pk/uk正在fk所引用</p>\n<p>drop table child purge;<br>drop table parent purge;</p>\n<p>drop table parent cascade constraints purge;<br>cascade constraints 级联约束,child table本身没被删除,只是先把子表上的fk约束删除,再删parent table.</p>\n<p>表级约束<br>create table child<br>(c1 number(2) constraint child_c1_pk<br>              primary key,<br> c2 number(3),<br> constraint child_c2_fk foreign key(c2)<br>            references parent(c1)<br>)</p>\n<p>外键约束另外两种定义方法<br>create table child1<br>(c1 number(2) constraint child1_c1_pk<br>              primary key,<br> c2 number(3) constraint child1_c2_fk<br>              references parent(c1)<br>              on delete cascade)<br>on delete cascade :级联删除会影响到对parent table的删除,先delete from child1,再delete from<br>parent</p>\n<p>delete from parent where c1 = 1;<br>create table child2<br>(c1 number(2) constraint child2_c1_pk<br>              primary key,<br> c2 number(3) constraint child2_c2_fk<br>              references parent(c1)<br>              on delete set null)</p>\n<p>delete from parent where c1 = 1<br>等价于以下操作<br>SQL&gt; update child2 set c2 = null<br>  2  where c2 = 1;<br>SQL&gt; delete from parent where c1 = 1;</p>\n<p>table<br>DDL(数据类型 约束)<br>transaction (包含一堆DML)</p>\n<p>4000<br>100<br>1000<br>3100</p>\n<p>视图(view)<br>create table test_t1<br>as<br>select <em> from test<br>where c1 = 1;<br>create or replace view test_v1<br>as<br>select </em> from test<br>where c1 = 1;<br>desc test_v1<br>selelct * from test_v1</p>\n<p>insert into test values (1,3);<br>select <em> from test_v1 (1,3)<br>insert into test_v1 values (1,4)<br>select </em> from test_v1;<br>select <em> from test;<br>insert into test_v1 values (2,3);<br>select </em> from test_v1;(没有)<br>select * from test;(2,3)</p>\n<p>drop table test purge;<br>select * from test_v1;<br>SQL&gt; desc test_v1<br>ERROR:<br>ORA-24372: invalid object for describe<br>无法描述无效对象的结构</p>\n<p>SQL&gt; select text from user_views<br>  2  where view_name = ‘TEST_V1’;</p>\n<h2 id=\"TEXT\"><a href=\"#TEXT\" class=\"headerlink\" title=\"TEXT\"></a>TEXT</h2><p>select “C1”,”C2” from test<br>where c1 = 1</p>\n<p>view是一条select语句. select语句中包含的表为源表.通过view对源表做DML操作.</p>\n<p>view作用<br>1 create view (deptno = 30)<br>  grant view to user<br>  限定用户查询的数据 子集<br>2 简化查询语句<br>3 create view beijing<br>  as<br>  select <em> from haidian<br>  union all<br>  select </em> from xicheng<br>…<br>  超集<br>view的类型<br>1 简单view (DML)<br>2 复杂view  (不能DML)</p>\n<p>create or replace view avgscore_v<br>select s.name,a.avgscore<br>from student s,<br>     (select sid,round(avg(score)) avgscore<br>      from stu_cour<br>      group by sid) a<br>on s.id = a.sid</p>\n<p>view的约束<br>create or replace view test_ck<br>as<br>select * from test<br>where c1 = 1<br>with check option;<br>c1=2,违反where条件,2,3记录insert时报错</p>\n<p>create or replace view test_ro<br>as<br>select * from test<br>where c1 = 1<br>with read only;<br>只读视图</p>\n<h4 id=\"索引\"><a href=\"#索引\" class=\"headerlink\" title=\"索引\"></a>索引</h4><p>create index test_c1_idx<br>on test(c1);<br>对索引不能做desc,select,DML操作<br>rowid 代表一条记录的物理位置<br>属于哪个数据对象(table)<br>属于哪个数据文件的<br>属于数据文件的第几个数据块<br>属于数据块里的第几条记录</p>\n<h4 id=\"index的结构\"><a href=\"#index的结构\" class=\"headerlink\" title=\"index的结构\"></a>index的结构</h4><p>index记录rowid<br>index的结构是一棵平衡树,有三类数据块组成,根节点,分支节点,叶子节点,数据块的数据是排序的.根节点和分支节点用于导航,里面记录下一级节点的物理位置以及该节点包含的数据范围.叶子节点里记录的是index entry(索引项),由key值和rowid组成,key值是建索引的列在每条记录上的取值,rowid是记录的物理位置,所有的叶子节点做成双向链表(升序/降序),适用于范围查询.<br>用索引查询的路线图,从根节点出发,找相应的分支节点,叶子节点,最后要找到index entry,通过rowid定位<br>表里所需要的数据块,避免了全表扫描.</p>\n<p>索引为什么提高查询效率,为select语句<br>有效地降低了读取数据块的数量.读取数据块,一种从文件里读,物理读 physical read,一种从内存读,逻辑读 logical read /buffer gets</p>\n<p>建索引代价<br>空间,DML变慢</p>\n<h4 id=\"哪些列适合建索引\"><a href=\"#哪些列适合建索引\" class=\"headerlink\" title=\"哪些列适合建索引\"></a>哪些列适合建索引</h4><p>1 经常出现在where子句的列<br>2 pk/uk列<br>3 经常出现在表连接的列<br>4 fk列 parent.pk列 = child.fk列<br>5 经常用于group by,order by的列<br>7 where c1 is null(全表扫描),索引里不记null值,<br> 该列有大量null值,找not null值用索引会快</p>\n<h4 id=\"索引类型\"><a href=\"#索引类型\" class=\"headerlink\" title=\"索引类型\"></a>索引类型</h4><p>非唯一性索引,提高查询效率<br>唯一性索引,解决唯一性.等价建唯一性约束<br>create unique index test_c2_idx<br>on test(c2);</p>\n<p>insert into test (c2) values (1)<br>*<br>ERROR at line 1:<br>ORA-00001: unique constraint(HILOO.TEST_C2_IDX ) violated</p>\n<p>联合索引<br>create index test_c1_c2_idx<br>on test(c1,c2)<br>where c1 = 1 and c2 = 1</p>\n<p>select ename from emp_hiloo<br>where salary<em>12 &gt; 60000<br>where salary &gt; 5000<br>如果salary建索引,where salary &gt; 5000(用),where salary</em>12 &gt; 60000(不能用)</p>\n<p>where upper(ename) = ‘ZHANGWUJI’</p>\n<p>where c1 = 100 c1是varchar2类型<br>where to_number(c1) = 100</p>\n<p>where ename like ‘a%’<br>where substr(ename,1,1) = ‘a’</p>\n<p>deptno not in (20,30)<br>depotno in (10)</p>\n<h4 id=\"函数索引\"><a href=\"#函数索引\" class=\"headerlink\" title=\"函数索引\"></a>函数索引</h4><p>create index test_c1_funidx<br>on test(round(c1));<br>where round(c1) = 10</p>\n<p>create index student_name_idx<br>on student(name);</p>\n<h4 id=\"序列号\"><a href=\"#序列号\" class=\"headerlink\" title=\"序列号\"></a>序列号</h4><p>sequence<br>为table里的主键服务,产生主键值<br>唯一值产生器<br>sequence_name.nextval</p>\n<p>为student表的id建sequence<br>insert into student(student_id.nextval…<br>为course表的id建sequence<br>insert into course (course_id.nextval…</p>\n<p>创建序列如下：<br>create sequence SEQ_TEST100<br>minvalue 1<br>maxvalue 999999999999999999999999999<br>start with 11<br>increment by 1<br>cache 10;</p>\n<p>函数<br>create or replace function dept_avgsal<br>(p_deptno number) –定义参数,数据类型不能有宽度<br>return number    –定义函数的返回类型<br>is<br>  v_salary emp_hiloo.salary%type;     –变量v_salary 的类型跟表emp_hiloo里的salary的类型定义一致<br>begin<br>  select round(avg(salary)) into v_salary<br>  from emp_hiloo<br>  where deptno = p_deptno;    –select当且仅当返回一条记录用select into语法,表示把select语句的执行结果赋值给v_salary<br>  return v_salary;       –返回函数值<br>end;<br>.不运行,回到SQL&gt;下<br>/表示运行<br>show error<br>SQL&gt; select dept_avgsal(10) from dual;</p>\n<p>练习<br>用语法实现多对多关系<br>student<br>id pk<br>name not null</p>\n<p>course<br>id pk<br>name not null</p>\n<p>stu_cour<br>sid fk –&gt;student(id)<br>cid fk –&gt;course(id)<br>pk(sid,cid)<br>score check <a href=\"between and\">0,100</a> </p>\n<h4 id=\"数据库日期比较\"><a href=\"#数据库日期比较\" class=\"headerlink\" title=\"数据库日期比较\"></a>数据库日期比较</h4><p>Sql代码：<br>1    timesten内存数据库比较日期是不是同一天,低效的方法<br>2    to_char(create_date,’yyyymmdd’)=to_char(sysdate NUMTODSINTERVAL(60<em>60</em>24,’SECOND’),’yyyymmdd’)<br>3    oracle 数据库低效的方法<br>4    to_char(create_date,’yyyymmdd’)=to_char(sysdate-1,’yyyymmdd’)<br>5    2个数据库通用高效的方法<br>6    trunc(create_date)=trunc(sysdate)-NUMTODSINTERVAL(1,’DAY’)<br>查找数据库里的表，索引等<br>支持oracle的模糊查询如select * from user_tables where table_name like ‘%_PROJECT’;查表名以PROJECT结尾的表（注：区别大小写）<br>查所有用户的表在all_tables<br>主键名称、外键在all_constraints<br>索引在all_indexes<br>但主键也会成为索引，所以主键也会在all_indexes里面。<br>具体需要的字段可以DESC下这几个view，dba登陆的话可以把all换成dba。</p>\n<p>查询用户表的索引(非聚集索引):<br>select * from user_indexes<br>where uniqueness = ‘NONUNIQUE’</p>\n<p>查询用户表的主键(聚集索引):<br>select * from user_indexes<br>where uniqueness = ‘UNIQUE’</p>\n<p>1、    查找表的所有索引（包括索引名，类型，构成列）：<br>select t.<em>,i.index_type from user_ind_columns t,user_indexes i where t.index_name = i.index_name and t.table_name = i.table_name and t.table_name = 要查询的表<br>2、查找表的主键（包括名称，构成列）：<br>select cu.</em> from user_cons_columns cu, user_constraints au where cu.constraint_name = au.constraint_name and au.constraint_type = ‘P’ and au.table_name = 要查询的表<br>3、查找表的唯一性约束（包括名称，构成列）：<br>select column_name from user_cons_columns cu, user_constraints au where cu.constraint_name = au.constraint_name and au.constraint_type = ‘U’ and au.table_name = 要查询的表<br>4、查找表的外键（包括名称，引用表的表名和对应的键名，下面是分成多步查询）：<br>select <em> from user_constraints c where c.constraint_type = ‘R’ and c.table_name = 要查询的表<br>查询外键约束的列名：<br>select </em> from user_cons_columns cl where cl.constraint_name = 外键名称<br>查询引用表的键的列名：<br>select <em> from user_cons_columns cl where cl.constraint_name = 外键引用表的键名<br>5、查询表的所有列及其属性<br>select t.</em>,c.COMMENTS from user_tab_columns t,user_col_comments c where t.table_name = c.table_name and t.column_name = c.column_name and t.table_name = 要查询的表</p>\n<p>####数据唯一Id：####</p>\n<ol>\n<li>用Oracle来生成UUID，做法很简单，如下：select sys_guid() from dual;数据类型是 raw(16) 有32个字符。<br>create table test_guid3(<br>id varchar(50)<br>)<br>select * from test_guid3;<br>insert into test_guid3(id) values(sys_guid())</li>\n</ol>\n<hr>\n<pre><code>1000 7CD5B7769DF75CEFE034080020825436\n1100 7CD5B7769DF85CEFE034080020825436\n1200 7CD5B7769DF95CEFE034080020825436\n1300 7CD5B7769DFA5CEFE034080020825436\n</code></pre><h3 id=\"名词\"><a href=\"#名词\" class=\"headerlink\" title=\"名词\"></a>名词</h3><h4 id=\"Oracle的方案（Schema）和用户（User）的区别\"><a href=\"#Oracle的方案（Schema）和用户（User）的区别\" class=\"headerlink\" title=\"Oracle的方案（Schema）和用户（User）的区别\"></a>Oracle的方案（Schema）和用户（User）的区别</h4><p>从定义中我们可以看出方案（Schema）为数据库对象的集合，为了区分各个集合，我们需要给这个集合起个名字，这些名字就是我们在企业管理器的方案下看到的许多类似用户名的节点，这些类似用户名的节点其实就是一个schema，schema里面包含了各种对象如tables, views, sequences, stored procedures, synonyms, indexes, clusters, and database links。</p>\n<p>   一个用户一般对应一个schema,该用户的schema名等于用户名，并作为该用户缺省schema。这也就是我们在企业管理器的方案下看到schema名都为数据库用户名的原因。Oracle数据库中不能新创建一个schema，要想创建一个schema，只能通过创建一个用户的方法解决(Oracle中虽然有create schema语句，但是它并不是用来创建一个schema的)，在创建一个用户的同时为这个用户创建一个与用户名同名的schem并作为该用户的缺省shcema。即schema的个数同user的个数相同，而且schema名字同user名字一一对应并且相同，所有我们可以称schema为user的别名，虽然这样说并不准确，但是更容易理解一些。</p>\n<p>   一个用户有一个缺省的schema，其schema名就等于用户名，当然一个用户还可以使用其他的schema。如果我们访问一个表时，没有指明该表属于哪一个schema中的，系统就会自动给我们在表上加上缺省的sheman名。比如我们在访问数据库时，访问scott用户下的emp表，通过select <em> from emp; 其实，这sql语句的完整写法为select </em> from scott.emp。在数据库中一个对象的完整名称为schema.object，而不属user.object。类似如果我们在创建对象时不指定该对象的schema，在该对象的schema为用户的缺省schema。这就像一个用户有一个缺省的表空间，但是该用户还可以使用其他的表空间，如果我们在创建对象时不指定表空间，则对象存储在缺省表空间中，要想让对象存储在其他表空间中，我们需要在创建对象时指定该对象的表空间。</p>\n<p>   oracle中的schema就是指一个用户下所有对象的集合，schema本身不能理解成一个对象，oracle并没有提供创建schema的语法，schema也并不是在创建user时就创建，而是在该用户下创建第一个对象之后schema也随之产生，只要user下存在对象，schema就一定存在，user下如果不存在对象，schema也不存在；这一点类似于temp tablespace group，另外也可以通过oem来观察，如果创建一个新用户，该用户下如果没有对象则schema不存在，如果创建一个对象则和用户同名的schema也随之产生。</p>\n<p>####Oracle中User与Schema的简单理解####<br>技术积累（126）<br>版权声明：本文为博主原创文章，未经博主允许不得转载。<br>方案（Schema）为数据库对象的集合，为了区分各个集合，我们需要给这个集合起个名字，这些名字就是我们在企业管理器的方案下看到的许多类似用户名的节点，这些类似用户名的节点其实就是一个schema，schema里面包含了各种对象如tables, views, sequences, stored procedures, synonyms, indexes, clusters, and database links。  一个用户一般对应一个schema,该用户的schema名等于用户名，并作为该用户缺省schema。<br>SQL Server中的Schema<br>SQL Server中一个用户有一个缺省的schema，其schema名就等于用户名，这也就是我们在企业管理器的方案下看到schema名都为数据库用户名的原因。当然一个用户还可以使用其他的schema。如果我们访问一个表时，没有指明该表属于哪一个schema中的，系统就会自动给我们在表上加上缺省的sheman名。比如我们在访问数据库时，访问scott用户下的emp表，通过select <em> from emp; 其实，这sql语句的完整写法为select </em> from scott.emp。在数据库中一个对象的完整名称为schema.object，而不属user.object。类似如果我们在创建对象时不指定该对象的schema，在该对象的schema为用户的缺省schema。这就像一个用户有一个缺省的表空间，但是该用户还可以使用其他的表空间，如果我们在创建对象时不指定表空间，则对象存储在缺省表空间中，要想让对象存储在其他表空间中，我们需要在创建对象时指定该对象的表空间。</p>\n<p>Oracle中的Schema<br>Oracle中的schema就是指一个用户下所有对象的集合，schema本身不能理解成一个对象，oracle并没有提供创建schema的语法，schema也并不是在创建user时就创建，而是在该用户下创建第一个对象之后schema也随之产生，只要user下存在对象，schema就一定存在，user下如果不存在对象，schema也不存在；如果创建一个新用户，该用户下如果没有对象则schema不存在，如果创建一个对象则和用户同名的schema也随之产生。实际上在使用上，shcema与user完全一样，没有什么区别，在出现schema名的地方也可以出现user名。</p>\n<p>Tablspace<br>逻辑上用来放objects,，这是个逻辑概念，本质上是一个或者多个数据文件的集合，物理上对应磁盘上的数据文件或者裸设备。</p>\n<p>数据文件<br>具体存储数据的物理文件，是一个物理概念。一个数据文件只能属于一个表空间，一个表空间可以包含一个或多个数据文件。一个数据库由多个表空间组成，一个表空间只能属于一个数据库。</p>\n<p>下边是源自网络的一个形象的比喻<br>我们可以把Database看作是一个大仓库，仓库分了很多很多的房间，Schema就是其中的房间，一个Schema代表一个房间，Table可以看作是每个Schema中的床，Table（床）被放入每个房间中，不能放置在房间之外，那岂不是晚上睡觉无家可归了，然后床上可以放置很多物品，就好比 Table上可以放置很多列和行一样，数据库中存储数据的基本单元是Table，现实中每个仓库放置物品的基本单位就是床， User就是每个Schema的主人，（所以Schema包含的是Object，而不是User），user和schema是一一对应的，每个user在没有特别指定下只能使用自己schema（房间）的东西，如果一个user想使用其他schema（房间）的东西，那就要看那个schema（房间）的user（主人）有没有给你这个权限了，或者看这个仓库的老大（DBA）有没有给你这个权限了。换句话说，如果你是某个仓库的主人，那么这个仓库的使用权和仓库中的所有东西都是你的（包括房间），你有完全的操作权，可以扔掉不用的东西从每个房间，也可以放置一些有用的东西到某一个房间，你还可以给每个User分配具体的权限，也就是他到某一个房间能做些什么，是只能看（Read-Only），还是可以像主人一样有所有的控制权（R/W），这个就要看这个User所对应的角色Role了。</p>\n<h4 id=\"oracle的schema的含义\"><a href=\"#oracle的schema的含义\" class=\"headerlink\" title=\"oracle的schema的含义\"></a>oracle的schema的含义</h4><p>在现在做的Kraft Catalyst 项目中，Cransoft其中有一个功能就是schema refresh. 一直不理解schema什么意思，也曾经和同事讨论过，当时同事就给我举过一个例子，下面会详细说的。其实schema是Oracle中的，其他数据库中不知道有没有这个概念。<br>首先,可以先看一下schema和user的定义：<br>A schema is a collection of database objects (used by a user).<br>Schema objects are the logical structures that directly refer to the database’s data.<br>A user is a name defined in the database that can connect to and access objects.<br>Schemas and users help database administrators manage database security.<br>从中我们可以看出,schema为数据库对象的集合，为了区分各个集合，需要给这个集合起个名字，这些名字就是在企业管理器的方案下看到的许多类似用户名的节点，这些类似用户名的节点其实就是一个schema。<br>schema里面包含了各种对象如tables, views, sequences, stored procedures, synonyms, indexes, clusters, and database links。<br>一个用户一般对应一个schema，该用户的schema名等于用户名，并作为该用户缺省schema。这也就是在企业管理器的方案下看到schema名都为数据库用户名的原因。<br>Oracle数据库中不能新创建一个schema，要想创建一个schema，只能通过创建一个用户的方法解决(Oracle中虽然有create schema语句，但是它并不是用来创建一个schema的)。在创建一个用户的同时，为这个用户创建一个与用户名同名的schem并作为该用户的缺省 shcema。即schema的个数同user的个数相同，而且schema名字同user名字一一 对应并且相同，所有我们可以称schema为user的别名，虽然这样说并不准确，但是更容易理解一些。<br>一个用户有一个缺省的schema，其schema名就等于用户名，当然一个用户还可以使用其他的schema。如果我们访问一个表时，没有指明该表属于 哪一个schema中的，系统就会自动给我们在表上加上缺省的sheman名。比如我们在访问数据库时，访问scott用户下的emp表，通过 select <em> from emp; 其实，这sql语句的完整写法为select </em> from scott.emp。在数据库中一个对象的完整名称为schema.object，而不属user.object。类似如果我们在创建对象时不指定该对象 的schema，在该对象的schema为用户的缺省schema。这就像一个用户有一个缺省的表空间，但是该用户还可以使用其他的表空间，如果我们在创 建对象时不指定表空间，则对象存储在缺省表空间中，要想让对象存储在其他表空间中，需要在创建对象时指定该对象的表空间。<br>有人举了个很生动的例子，来说明Database、User、Schema、Tables、Col、Row等之间的关系<br>“可以把Database看作是一个大仓库，仓库分了很多很多的房间，Schema就是其中的房间，一个Schema代表一个房间，Table可以看作是每个Schema中的床，Table（床）就被放入每个房间中，不能放置在房间之外，那岂不是晚上睡觉无家可归了。<br>然后床上可以放置很多物品，就好比Table上可以放置很多列和行一样，数据库中存储数据的基本单元是Table，现实中每个仓库放置物品的基本单位就是床， User就是每个Schema的主人（所以Schema包含的是Object，而不是User）。<br>其实User是对应与数据库的（即User是每个对应数据库的主人），既然有操作数据库（仓库）的权利，就肯定有操作数据库中每个Schema（房间）的 权利，就是说每个数据库映射的User有每个Schema（房间）的钥匙，换句话说，如果他是某个仓库的主人，那么这个仓库的使用权和仓库中的所有东西都 是他的（包括房间），他有完全的操作权，可以扔掉不用的东西从每个房间，也可以放置一些有用的东西到某一个房间。还可以给User分配具体的权限，也就是 他到某一个房间能做些什么，是只能看（Read-Only），还是可以像主人一样有所有的控制权（R/W），这个就要看这个User所对应的角色Role 了”<br>从定义中我们可以看出schema为数据库对象的集合，为了区分各个集合，我们需要给这个集合起个名字，这些名字就是我们在企业管理器的方案下看到的许多类似用户名的节点，这些类似用户名的节点其实就是一个schema，schema里面包含了各种对象如tables, views, sequences, stored procedures, synonyms, indexes, clusters, and database links。<br>一个用户一般对应一个schema,该用户的schema名等于用户名，并作为该用户缺省schema。这也就是我们在企业管理器的方案下看到schema名都为数据库用户名的原因。Oracle数据库中不能新创建一个schema，要想创建一个schema，只能通过创建一个用户的方法解决(Oracle中虽然有create schema语句，但是它并不是用来创建一个schema的)，在创建一个用户的同时为这个用户创建一个与用户名同名的schem并作为该用户的缺省shcema。即schema的个数同user的个数相同，而且schema名字同user名字一一 对应并且相同，所有我们可以称schema为user的别名，虽然这样说并不准确，但是更容易理解一些。<br>一个用户有一个缺省的schema，其schema名就等于用户名，当然一个用户还可以使用其他的schema。如果我们访问一个表时，没有指明该表属于哪一个schema中的，系统就会自动给我们在表上加上缺省的sheman名。比如我们在访问数据库时，访问scott用户下的emp表，通过select <em> from emp; 其实，这sql语句的完整写法为select </em> from scott.emp。在数据库中一个对象的完整名称为schema.object，而不属user.object。类似如果我们在创建对象时不指定该对象的schema，在该对象的schema为用户的缺省schema。这就像一个用户有一个缺省的表空间，但是该用户还可以使用其他的表空间，如果我们在创建对象时不指定表空间，则对象存储在缺省表空间中，要想让对象存储在其他表空间中，我们需要在创建对象时指定该对象的表空间。<br>咳，说了这么多，给大家举个例子，否则，一切枯燥无味！<br>SQL&gt; Gruant dba to scott<br>SQL&gt; create table test(name char(10));<br>Table created.<br>SQL&gt; create table system.test(name char(10));<br>Table created.<br>SQL&gt; insert into test values(‘scott’);<br>1 row created.<br>SQL&gt; insert into system.test values(‘system’);<br>1 row created.<br>SQL&gt; commit;<br>Commit complete.<br>SQL&gt; conn system/manager<br>Connected.<br>SQL&gt; select * from test;</p>\n<h2 id=\"NAME\"><a href=\"#NAME\" class=\"headerlink\" title=\"NAME\"></a>NAME</h2><p>system<br>SQL&gt; ALTER SESSION SET CURRENT_SCHEMA = scott; –改变用户缺省schema名<br>Session altered.<br>SQL&gt; select * from test;</p>\n<h2 id=\"NAME-1\"><a href=\"#NAME-1\" class=\"headerlink\" title=\"NAME\"></a>NAME</h2><p>scott<br>SQL&gt; select owner ,table_name from dba_tables where table_name=upper(‘test’);<br>OWNER TABLE_NAME</p>\n<hr>\n<p>SCOTT TEST<br>SYSTEM TEST<br>–上面这个查询就是我说将schema作为user的别名的依据。实际上在使用上，shcema与user完全一样，没有什么区别，在出现schema名的地方也可以出现user名。<br>表空间：<br>一个表空间就是一片磁盘区域,他又一个或者多个磁盘文件组成,一个表空间可以容纳许多表、索引或者簇等<br>  每个表空间又一个预制的打一磁盘区域称为初始区间（initial   extent）用完这个区间厚在用下一个，知道用完表空间，这时候需要对表空间进行扩展，增加数据文件或者扩大已经存在的数据文件</p>\n<p>instance是一大坨内存sga,pga….和后台的进程smon pmon…..组成的一个大的应用。<br>schema就是一个用户和他下面的所有对象。。<br>tablspace 逻辑上用来放objects.物理上对应磁盘上的数据文件或者裸设备。<br> 在Oracle中，结合逻辑存储与物理存储的概念，我们可以这样来理解数据库、表空间、SCHEMA、数据文件这些概念：<br>      数据库是一个大圈，里面圈着的是表空间，表空间里面是数据文件，那么schema是什么呢？schema是一个逻辑概念，是一个集合，但schema并不是一个对象，oracle也并没有提供创建schema的语法。<br>schema：<br>      一般而言，一个用户就对应一个schema,该用户的schema名等于用户名，并作为该用户缺省schema，用户是不能创建schema的，schema在创建用户的时候创建，并可以指定用户的各种表空间（这点与PostgreSQL是不同，PostgreSQL是可以创建schema并指派给某个用户）。当前连接到数据库上的用户创建的所有数据库对象默认都属于这个schema（即在不指明schema的情况下），比如若用户scott连接到数据库，然后create table test(id int not null)创建表，那么这个表被创建在了scott这个schema中；但若这样create kanon.table test(id int not null)的话，这个表被创建在了kanon这个schema中，当然前提是权限允许。<br>      创建用户的方法是这样的：<br>      create user 用户名 identified by 密码<br>      default tablespace 表空间名<br>      temporary tablespace 表空间名<br>      quota 限额  （建议创建的时候指明表空间名）<br>由此来看，schema是一个逻辑概念。<br>      但一定要注意一点：schema好像并不是在创建user时就创建的，而是在该用户创建了第一个对象之后才将schema真正创建的，只有user下存在对象，他对应的schema才会存在，如果user下不存在任何对象了，schema也就不存在了；</p>\n<p>数据库：<br>     在oracle中，数据库是由表空间来组成的，而表空间里面是具体的物理文件—数据文件。我们可以创建数据库并为其指定各种表空间。</p>\n<p>表空间：<br>     这是个逻辑概念，本质上是一个或者多个数据文件的集合。</p>\n<p>数据文件：<br>     具体存储数据的物理文件，是一个物理概念。<br>     一个数据文件只能属于一个表空间，一个表空间可以包含一个或多个数据文件。一个数据库由多个表空间组成，一个表空间只能属于一个数据库。</p>\n"},{"title":"emacs入门","date":"2017-05-04T08:00:00.000Z","_content":"#入门实践\n\n\n","source":"_posts/tools/emacs/TODO-emacs入门.md","raw":"---\ntitle: emacs入门\ndate: 2017-05-04 16:00:00\ntags: [tools.emacs]\ncategories: [tools,emacs]\n---\n#入门实践\n\n\n","slug":"tools/emacs/TODO-emacs入门","published":1,"updated":"2017-08-18T01:44:35.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjh7polxb009sbp7rqgdfzm21","content":"<p>#入门实践</p>\n","site":{"data":{}},"excerpt":"","more":"<p>#入门实践</p>\n"},{"title":"vim入门实践","date":"2017-05-04T08:00:00.000Z","_content":"#入门实践\n\n\n","source":"_posts/tools/vim/TODO-vim入门实践.md","raw":"---\ntitle: vim入门实践\ndate: 2017-05-04 16:00:00\ntags: [tools.vim]\ncategories: [tools,vim]\n---\n#入门实践\n\n\n","slug":"tools/vim/TODO-vim入门实践","published":1,"updated":"2017-08-18T01:44:35.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjh7polxc009tbp7r3ojwtzxm","content":"<p>#入门实践</p>\n","site":{"data":{}},"excerpt":"","more":"<p>#入门实践</p>\n"},{"title":"angular.js入门之HelloWorld","date":"2017-04-27T07:00:00.000Z","_content":"\n\n\n","source":"_posts/前端/angular.js/TODO-angular.js入门之HelloWorld.md","raw":"---\ntitle: angular.js入门之HelloWorld\ndate: 2017-04-27 15:00:00\ntags: [angular.js]\ncategories: [前端,angular.js]\n---\n\n\n\n","slug":"前端/angular.js/TODO-angular.js入门之HelloWorld","published":1,"updated":"2017-08-18T01:44:35.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjh7polxd009wbp7rij99d2t7","content":"","site":{"data":{}},"excerpt":"","more":""},{"title":"react.js入门之HelloWorld","date":"2017-04-27T07:00:00.000Z","_content":"\n\n\n","source":"_posts/前端/react.js/TODO-react.js入门之HelloWorld.md","raw":"---\ntitle: react.js入门之HelloWorld\ndate: 2017-04-27 15:00:00\ntags: [react.js]\ncategories: [前端,react.js]\n---\n\n\n\n","slug":"前端/react.js/TODO-react.js入门之HelloWorld","published":1,"updated":"2017-08-18T01:44:35.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjh7polxe009xbp7rg2e8fj8q","content":"","site":{"data":{}},"excerpt":"","more":""},{"title":"vue.js入门之HelloWorld","date":"2017-04-27T07:00:00.000Z","_content":"\n\n\n","source":"_posts/前端/vue.js/TODO-vue.js入门之HelloWorld.md","raw":"---\ntitle: vue.js入门之HelloWorld\ndate: 2017-04-27 15:00:00\ntags: [angular.js]\ncategories: [前端,angular.js]\n---\n\n\n\n","slug":"前端/vue.js/TODO-vue.js入门之HelloWorld","published":1,"updated":"2017-08-18T01:44:35.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjh7polxf009ybp7r99st5a6e","content":"","site":{"data":{}},"excerpt":"","more":""}],"PostAsset":[],"PostCategory":[{"post_id":"cjh7poll30002bp7rj3ei2anb","category_id":"cjh7poll90005bp7rxt1nwozv","_id":"cjh7polld0009bp7ra5m626n5"},{"post_id":"cjh7pols7000fbp7rfu1fboid","category_id":"cjh7pols6000cbp7ryeuun3lv","_id":"cjh7polsg000mbp7r17ggxz9z"},{"post_id":"cjh7pols2000abp7runsnvvva","category_id":"cjh7pols6000cbp7ryeuun3lv","_id":"cjh7polsu0012bp7rl5smv6ve"},{"post_id":"cjh7pols2000abp7runsnvvva","category_id":"cjh7polsj000sbp7r4yjwomkw","_id":"cjh7polsw0017bp7racq03spx"},{"post_id":"cjh7pols8000gbp7rx7qi8t2t","category_id":"cjh7pols6000cbp7ryeuun3lv","_id":"cjh7polsy0019bp7rwcvzy97z"},{"post_id":"cjh7pols8000gbp7rx7qi8t2t","category_id":"cjh7polsm000xbp7rjq3k79t1","_id":"cjh7polt0001ebp7rao1zmplt"},{"post_id":"cjh7pols3000bbp7r95m8b3q2","category_id":"cjh7pols6000cbp7ryeuun3lv","_id":"cjh7polt6001pbp7rn04fyz3d"},{"post_id":"cjh7pols3000bbp7r95m8b3q2","category_id":"cjh7polsy001bbp7r3sd6urtf","_id":"cjh7polt7001sbp7r7apm94a0"},{"post_id":"cjh7pols6000ebp7rkfdhe3ml","category_id":"cjh7pols6000cbp7ryeuun3lv","_id":"cjh7poltf0025bp7rdyqc6m6w"},{"post_id":"cjh7pols6000ebp7rkfdhe3ml","category_id":"cjh7polsy001bbp7r3sd6urtf","_id":"cjh7poltg0029bp7r5x38wtt1"},{"post_id":"cjh7polsd000kbp7ri19gu1bp","category_id":"cjh7polsv0014bp7rpz8qoyr4","_id":"cjh7poltv0039bp7rmef1r9to"},{"post_id":"cjh7polsd000kbp7ri19gu1bp","category_id":"cjh7polts0030bp7rkfrj3tfx","_id":"cjh7poltv003bbp7r87zbwnoq"},{"post_id":"cjh7polsf000lbp7rw3z937z8","category_id":"cjh7polsv0014bp7rpz8qoyr4","_id":"cjh7polty003pbp7rxk86t74v"},{"post_id":"cjh7polsf000lbp7rw3z937z8","category_id":"cjh7poltx003hbp7rlgw4l1su","_id":"cjh7poltz003rbp7rk91xo8ki"},{"post_id":"cjh7polsh000pbp7rbz2tamhy","category_id":"cjh7polsv0014bp7rpz8qoyr4","_id":"cjh7polu10040bp7rhyd4rvy0"},{"post_id":"cjh7polsh000pbp7rbz2tamhy","category_id":"cjh7poltx003hbp7rlgw4l1su","_id":"cjh7polu20043bp7rp0dkwps5"},{"post_id":"cjh7polsi000rbp7r7343x8p1","category_id":"cjh7poltc001zbp7r2d4y9l04","_id":"cjh7polu6004hbp7ruppwvwsk"},{"post_id":"cjh7polsi000rbp7r7343x8p1","category_id":"cjh7polu4004abp7rq7o4qa7e","_id":"cjh7polu6004ibp7ryb86stb9"},{"post_id":"cjh7polsk000vbp7ri727gj7n","category_id":"cjh7poltc001zbp7r2d4y9l04","_id":"cjh7polu8004rbp7rw18bra6g"},{"post_id":"cjh7polsk000vbp7ri727gj7n","category_id":"cjh7polu4004abp7rq7o4qa7e","_id":"cjh7polu8004sbp7rp9gi4t10"},{"post_id":"cjh7polsl000wbp7rhwe4qgjz","category_id":"cjh7poltc001zbp7r2d4y9l04","_id":"cjh7polu90051bp7rkbacfy70"},{"post_id":"cjh7polsl000wbp7rhwe4qgjz","category_id":"cjh7polu4004abp7rq7o4qa7e","_id":"cjh7polua0054bp7rkxtendga"},{"post_id":"cjh7poltj002fbp7r7su3p7bq","category_id":"cjh7poltc001zbp7r2d4y9l04","_id":"cjh7polub0057bp7r68b1wsa9"},{"post_id":"cjh7poltj002fbp7r7su3p7bq","category_id":"cjh7polu9004ybp7r9cfd8gvb","_id":"cjh7poluc0059bp7rho0mzbt1"},{"post_id":"cjh7polsn000zbp7ro6e59uhf","category_id":"cjh7poltc001zbp7r2d4y9l04","_id":"cjh7polue005gbp7rpqjwtmg2"},{"post_id":"cjh7polsn000zbp7ro6e59uhf","category_id":"cjh7polu4004abp7rq7o4qa7e","_id":"cjh7polue005hbp7r3qiyppjv"},{"post_id":"cjh7polso0011bp7rgneerpd7","category_id":"cjh7poltm002lbp7ryqy9w02p","_id":"cjh7poluh005qbp7ri2hqgako"},{"post_id":"cjh7polso0011bp7rgneerpd7","category_id":"cjh7poluf005jbp7r9fyat32p","_id":"cjh7poluh005sbp7r8mypowiz"},{"post_id":"cjh7polto002sbp7r2dl44tpe","category_id":"cjh7polug005pbp7r2vdv9rv6","_id":"cjh7polui005xbp7r7cx1a0ji"},{"post_id":"cjh7polsv0016bp7rpelqoglo","category_id":"cjh7polto002qbp7r7juajtly","_id":"cjh7poluj0065bp7rgvki1dj8"},{"post_id":"cjh7polsv0016bp7rpelqoglo","category_id":"cjh7polui005ybp7ro7cuycse","_id":"cjh7poluj0067bp7rurfejwen"},{"post_id":"cjh7polsx0018bp7r1yp7smox","category_id":"cjh7poltm002lbp7ryqy9w02p","_id":"cjh7polun006hbp7rnb86qclj"},{"post_id":"cjh7polsx0018bp7r1yp7smox","category_id":"cjh7poluk006abp7rybp7ua5v","_id":"cjh7polun006ibp7rtwes9e3l"},{"post_id":"cjh7polsz001cbp7rw4ewou1m","category_id":"cjh7polto002qbp7r7juajtly","_id":"cjh7polup006obp7rizt7s48e"},{"post_id":"cjh7polsz001cbp7rw4ewou1m","category_id":"cjh7polun006gbp7r2r837lh2","_id":"cjh7polup006rbp7ri82uvcky"},{"post_id":"cjh7polt0001fbp7rdui2a9xy","category_id":"cjh7polto002qbp7r7juajtly","_id":"cjh7poluq006tbp7r8upxz5ku"},{"post_id":"cjh7polt0001fbp7rdui2a9xy","category_id":"cjh7poluo006lbp7r1lo9247x","_id":"cjh7polur006xbp7risbw1fhd"},{"post_id":"cjh7polt1001ibp7ri7gid1ig","category_id":"cjh7poltm002lbp7ryqy9w02p","_id":"cjh7polur006ybp7rtobvuuzj"},{"post_id":"cjh7polt1001ibp7ri7gid1ig","category_id":"cjh7polup006pbp7rrjozbhfp","_id":"cjh7polus0071bp7rnqlimrkp"},{"post_id":"cjh7polt3001lbp7r344ba31i","category_id":"cjh7poltm002lbp7ryqy9w02p","_id":"cjh7polus0074bp7rkkewh63p"},{"post_id":"cjh7polt3001lbp7r344ba31i","category_id":"cjh7poluq006ubp7rx6wuq1a1","_id":"cjh7polut0077bp7rv7nkttqy"},{"post_id":"cjh7polt4001nbp7rgk74wma7","category_id":"cjh7poltm002lbp7ryqy9w02p","_id":"cjh7poluu007bbp7rze1mc7el"},{"post_id":"cjh7polt4001nbp7rgk74wma7","category_id":"cjh7polur006zbp7ris9z2spt","_id":"cjh7poluu007dbp7r4y2k1w3z"},{"post_id":"cjh7polt6001rbp7racatjt8y","category_id":"cjh7poltm002lbp7ryqy9w02p","_id":"cjh7poluu007ebp7rxuiey5nk"},{"post_id":"cjh7polt6001rbp7racatjt8y","category_id":"cjh7poluf005jbp7r9fyat32p","_id":"cjh7poluv007hbp7ryz54605n"},{"post_id":"cjh7polt8001tbp7rr2bj806d","category_id":"cjh7poltm002lbp7ryqy9w02p","_id":"cjh7poluv007jbp7rti0nwkm1"},{"post_id":"cjh7polt8001tbp7rr2bj806d","category_id":"cjh7polut007abp7rrczaotlm","_id":"cjh7poluw007nbp7rtvz49qg8"},{"post_id":"cjh7polta001wbp7rg1x5bu2p","category_id":"cjh7poltm002lbp7ryqy9w02p","_id":"cjh7poluw007obp7rf7lqyitv"},{"post_id":"cjh7polta001wbp7rg1x5bu2p","category_id":"cjh7poluu007fbp7r74k3czoq","_id":"cjh7poluy007sbp7ri8ue9ypt"},{"post_id":"cjh7poltb001ybp7rbl0w3p14","category_id":"cjh7poltm002lbp7ryqy9w02p","_id":"cjh7poluy007vbp7rlk6wja4i"},{"post_id":"cjh7poltb001ybp7rbl0w3p14","category_id":"cjh7poluw007lbp7rwn922k3h","_id":"cjh7poluz007xbp7rxsrpp663"},{"post_id":"cjh7poltd0022bp7rdjgwtez3","category_id":"cjh7polu30047bp7r5njiq3wc","_id":"cjh7polv10080bp7rk52ke8ye"},{"post_id":"cjh7poltd0022bp7rdjgwtez3","category_id":"cjh7polux007qbp7rcx3n5bri","_id":"cjh7polv20082bp7rlgo5ecg1"},{"post_id":"cjh7polte0024bp7risqf1beq","category_id":"cjh7polu30047bp7r5njiq3wc","_id":"cjh7polv30086bp7rvylgs8q1"},{"post_id":"cjh7polte0024bp7risqf1beq","category_id":"cjh7polux007qbp7rcx3n5bri","_id":"cjh7polv40087bp7rg9no2m8r"},{"post_id":"cjh7poltf0028bp7rdbmfbb06","category_id":"cjh7polu30047bp7r5njiq3wc","_id":"cjh7polv5008abp7rm7burctr"},{"post_id":"cjh7poltf0028bp7rdbmfbb06","category_id":"cjh7polux007qbp7rcx3n5bri","_id":"cjh7polv5008cbp7rzj0u53ub"},{"post_id":"cjh7poltg002bbp7r73iasgk4","category_id":"cjh7polu30047bp7r5njiq3wc","_id":"cjh7polv5008fbp7rgukn3hfy"},{"post_id":"cjh7poltg002bbp7r73iasgk4","category_id":"cjh7polux007qbp7rcx3n5bri","_id":"cjh7polv6008ibp7rnprc2m8g"},{"post_id":"cjh7polti002ebp7rga5slf59","category_id":"cjh7poltm002lbp7ryqy9w02p","_id":"cjh7polv6008kbp7rvqw1ej6g"},{"post_id":"cjh7polti002ebp7rga5slf59","category_id":"cjh7polv40089bp7r5san0ok9","_id":"cjh7polv7008mbp7rh7buk0b3"},{"post_id":"cjh7poltk002ibp7r8k6v71bm","category_id":"cjh7polua0052bp7rbqbdjo6s","_id":"cjh7polv7008pbp7ro7c8y3ah"},{"post_id":"cjh7poltk002ibp7r8k6v71bm","category_id":"cjh7polv5008gbp7ralzji6q5","_id":"cjh7polv8008rbp7rmwaxchal"},{"post_id":"cjh7poltl002kbp7r3qn669vg","category_id":"cjh7polua0052bp7rbqbdjo6s","_id":"cjh7polv9008vbp7rrxlx2jgr"},{"post_id":"cjh7poltl002kbp7r3qn669vg","category_id":"cjh7polv5008gbp7ralzji6q5","_id":"cjh7polv9008wbp7rtnocyec0"},{"post_id":"cjh7poltm002obp7ryahxa1ne","category_id":"cjh7polua0052bp7rbqbdjo6s","_id":"cjh7polva008zbp7rx922rh9a"},{"post_id":"cjh7poltm002obp7ryahxa1ne","category_id":"cjh7polv5008gbp7ralzji6q5","_id":"cjh7polva0091bp7rx07l5yye"},{"post_id":"cjh7poltn002pbp7rsbprdrbd","category_id":"cjh7polua0052bp7rbqbdjo6s","_id":"cjh7polvb0093bp7rwgpjejbn"},{"post_id":"cjh7poltn002pbp7rsbprdrbd","category_id":"cjh7polv5008gbp7ralzji6q5","_id":"cjh7polvc0096bp7rtnjtpyxn"},{"post_id":"cjh7poltp002ubp7r1kahijdn","category_id":"cjh7poluh005vbp7rk20sasre","_id":"cjh7polvc0098bp7rmta3brx3"},{"post_id":"cjh7poltp002ubp7r1kahijdn","category_id":"cjh7polva008ybp7rd44wfp73","_id":"cjh7polvd009bbp7rcn4epeq6"},{"post_id":"cjh7poltq002ybp7r25nikbsl","category_id":"cjh7poluh005vbp7rk20sasre","_id":"cjh7polvd009dbp7rfn2pgtcn"},{"post_id":"cjh7poltq002ybp7r25nikbsl","category_id":"cjh7polva008ybp7rd44wfp73","_id":"cjh7polve009gbp7rtpmpg0id"},{"post_id":"cjh7polts002zbp7r8xjwd09g","category_id":"cjh7poluj0064bp7rpvo1wr6f","_id":"cjh7polvf009hbp7r3htlfdr4"},{"post_id":"cjh7polts002zbp7r8xjwd09g","category_id":"cjh7polvc0097bp7r8s8r17xv","_id":"cjh7polvf009jbp7rsduwsoi7"},{"post_id":"cjh7poltt0032bp7rtru0cebl","category_id":"cjh7poluj0064bp7rpvo1wr6f","_id":"cjh7polvf009lbp7rwy7w6ywk"},{"post_id":"cjh7poltt0032bp7rtru0cebl","category_id":"cjh7polvd009fbp7r0misroi6","_id":"cjh7polvf009mbp7rvdg2ad1j"},{"post_id":"cjh7polwy009nbp7r3lq5s90f","category_id":"cjh7polu30047bp7r5njiq3wc","_id":"cjh7polx0009qbp7r539pueri"},{"post_id":"cjh7polwy009nbp7r3lq5s90f","category_id":"cjh7polux007qbp7rcx3n5bri","_id":"cjh7polx0009rbp7r9gpslp1t"},{"post_id":"cjh7polxb009sbp7rqgdfzm21","category_id":"cjh7polxd009ubp7r1x1tlpqf","_id":"cjh7polxk00aebp7rdr098sau"},{"post_id":"cjh7polxb009sbp7rqgdfzm21","category_id":"cjh7polxi00a8bp7re4uubhui","_id":"cjh7polxk00afbp7rh6m3hv9b"},{"post_id":"cjh7polxc009tbp7r3ojwtzxm","category_id":"cjh7polxd009ubp7r1x1tlpqf","_id":"cjh7polxk00ahbp7r2hiey5do"},{"post_id":"cjh7polxc009tbp7r3ojwtzxm","category_id":"cjh7polxk00adbp7r41y11fto","_id":"cjh7polxk00ajbp7rh70b8pf9"},{"post_id":"cjh7polxd009wbp7rij99d2t7","category_id":"cjh7polxh00a2bp7ryyr7x6ek","_id":"cjh7polxl00akbp7rn7hvmnm0"},{"post_id":"cjh7polxd009wbp7rij99d2t7","category_id":"cjh7polxk00agbp7rg8sbvikn","_id":"cjh7polxl00ambp7r8lhosn3k"},{"post_id":"cjh7polxe009xbp7rg2e8fj8q","category_id":"cjh7polxh00a2bp7ryyr7x6ek","_id":"cjh7polxl00anbp7rcpmmxt71"},{"post_id":"cjh7polxe009xbp7rg2e8fj8q","category_id":"cjh7polxk00aibp7rrhqfnfm0","_id":"cjh7polxl00aobp7rnpu87x5i"},{"post_id":"cjh7polxf009ybp7r99st5a6e","category_id":"cjh7polxh00a2bp7ryyr7x6ek","_id":"cjh7polxl00apbp7rbevejsvb"},{"post_id":"cjh7polxf009ybp7r99st5a6e","category_id":"cjh7polxk00agbp7rg8sbvikn","_id":"cjh7polxl00aqbp7ro8y6lyml"}],"PostTag":[{"post_id":"cjh7poll30002bp7rj3ei2anb","tag_id":"cjh7polla0006bp7rl59yhv5w","_id":"cjh7pollc0008bp7rstalhe4y"},{"post_id":"cjh7pols7000fbp7rfu1fboid","tag_id":"cjh7pols6000dbp7r564f39kp","_id":"cjh7polsd000jbp7rxui3w475"},{"post_id":"cjh7pols2000abp7runsnvvva","tag_id":"cjh7pols6000dbp7r564f39kp","_id":"cjh7polsi000qbp7rpe0foajy"},{"post_id":"cjh7pols2000abp7runsnvvva","tag_id":"cjh7polsa000ibp7rnycp5efu","_id":"cjh7polsj000tbp7rsklynlky"},{"post_id":"cjh7pols3000bbp7r95m8b3q2","tag_id":"cjh7pols6000dbp7r564f39kp","_id":"cjh7polso0010bp7rmwprdciz"},{"post_id":"cjh7pols3000bbp7r95m8b3q2","tag_id":"cjh7polsk000ubp7rvhmw9xy2","_id":"cjh7polsv0013bp7rfq83x3b1"},{"post_id":"cjh7pols6000ebp7rkfdhe3ml","tag_id":"cjh7pols6000dbp7r564f39kp","_id":"cjh7polt0001dbp7ryaltiahl"},{"post_id":"cjh7pols6000ebp7rkfdhe3ml","tag_id":"cjh7polsk000ubp7rvhmw9xy2","_id":"cjh7polt1001gbp7rwupcc55u"},{"post_id":"cjh7pols8000gbp7rx7qi8t2t","tag_id":"cjh7pols6000dbp7r564f39kp","_id":"cjh7polt3001kbp7rh6eu68u0"},{"post_id":"cjh7pols8000gbp7rx7qi8t2t","tag_id":"cjh7polsy001abp7r8glwe4ii","_id":"cjh7polt4001mbp7rnpb1liax"},{"post_id":"cjh7polsd000kbp7ri19gu1bp","tag_id":"cjh7polt1001hbp7replga4ct","_id":"cjh7poltb001xbp7r7vmw1ms0"},{"post_id":"cjh7polsd000kbp7ri19gu1bp","tag_id":"cjh7polt6001qbp7r810qrd2q","_id":"cjh7poltc0020bp7rnafk73pc"},{"post_id":"cjh7polsf000lbp7rw3z937z8","tag_id":"cjh7polt1001hbp7replga4ct","_id":"cjh7polte0023bp7r9fmq71u7"},{"post_id":"cjh7polsh000pbp7rbz2tamhy","tag_id":"cjh7polt1001hbp7replga4ct","_id":"cjh7poltg002abp7rlbyp7knd"},{"post_id":"cjh7polsi000rbp7r7343x8p1","tag_id":"cjh7poltf0027bp7riw6zvdfp","_id":"cjh7poltl002jbp7r6b92pnm8"},{"post_id":"cjh7polsi000rbp7r7343x8p1","tag_id":"cjh7polth002dbp7ry8ks7qye","_id":"cjh7poltm002mbp7riz4ig5lx"},{"post_id":"cjh7polsk000vbp7ri727gj7n","tag_id":"cjh7poltf0027bp7riw6zvdfp","_id":"cjh7poltp002tbp7r0vsb1h92"},{"post_id":"cjh7polsk000vbp7ri727gj7n","tag_id":"cjh7polth002dbp7ry8ks7qye","_id":"cjh7poltq002vbp7rdmcybl0b"},{"post_id":"cjh7polsl000wbp7rhwe4qgjz","tag_id":"cjh7poltf0027bp7riw6zvdfp","_id":"cjh7poltu0033bp7rlut5cw1d"},{"post_id":"cjh7polsl000wbp7rhwe4qgjz","tag_id":"cjh7polth002dbp7ry8ks7qye","_id":"cjh7poltu0034bp7r96ciyenq"},{"post_id":"cjh7polsn000zbp7ro6e59uhf","tag_id":"cjh7poltf0027bp7riw6zvdfp","_id":"cjh7poltv003abp7r9yycp9qc"},{"post_id":"cjh7polsn000zbp7ro6e59uhf","tag_id":"cjh7polth002dbp7ry8ks7qye","_id":"cjh7poltv003cbp7rweg00h9m"},{"post_id":"cjh7polso0011bp7rgneerpd7","tag_id":"cjh7poltv0037bp7rdipio9rh","_id":"cjh7poltx003jbp7r4wb3mtda"},{"post_id":"cjh7polso0011bp7rgneerpd7","tag_id":"cjh7poltw003ebp7r8bzcry1n","_id":"cjh7polty003lbp7rl68s5jfr"},{"post_id":"cjh7polso0011bp7rgneerpd7","tag_id":"cjh7poltw003gbp7rka90b3pu","_id":"cjh7polty003nbp7rk1x1eo17"},{"post_id":"cjh7polsv0016bp7rpelqoglo","tag_id":"cjh7poltx003ibp7rmwrk9lfq","_id":"cjh7poltz003sbp7rqrju9hvt"},{"post_id":"cjh7polsv0016bp7rpelqoglo","tag_id":"cjh7polty003mbp7rt69u9hgs","_id":"cjh7polu0003ubp7rgyhbhx1t"},{"post_id":"cjh7polsx0018bp7r1yp7smox","tag_id":"cjh7poltv0037bp7rdipio9rh","_id":"cjh7polu1003ybp7rqgsybq9c"},{"post_id":"cjh7polsx0018bp7r1yp7smox","tag_id":"cjh7poltw003gbp7rka90b3pu","_id":"cjh7polu20041bp7rsqnmfim4"},{"post_id":"cjh7polsz001cbp7rw4ewou1m","tag_id":"cjh7poltx003ibp7rmwrk9lfq","_id":"cjh7polu30046bp7rvgir0duo"},{"post_id":"cjh7polsz001cbp7rw4ewou1m","tag_id":"cjh7polu20042bp7rl92laciz","_id":"cjh7polu40048bp7ryt97xvn9"},{"post_id":"cjh7polt0001fbp7rdui2a9xy","tag_id":"cjh7poltx003ibp7rmwrk9lfq","_id":"cjh7polu5004cbp7r7vl8e7x6"},{"post_id":"cjh7polt0001fbp7rdui2a9xy","tag_id":"cjh7polu40049bp7rb1fwllyb","_id":"cjh7polu5004ebp7rg479auqc"},{"post_id":"cjh7polt1001ibp7ri7gid1ig","tag_id":"cjh7poltv0037bp7rdipio9rh","_id":"cjh7polu6004kbp7rde77ijrp"},{"post_id":"cjh7polt1001ibp7ri7gid1ig","tag_id":"cjh7polu5004fbp7royvhws22","_id":"cjh7polu7004mbp7rh181g94k"},{"post_id":"cjh7polt3001lbp7r344ba31i","tag_id":"cjh7poltv0037bp7rdipio9rh","_id":"cjh7polu9004vbp7rfrrokede"},{"post_id":"cjh7polt3001lbp7r344ba31i","tag_id":"cjh7polu7004nbp7rj0byrrq7","_id":"cjh7polu9004wbp7r6m8a6aom"},{"post_id":"cjh7polt3001lbp7r344ba31i","tag_id":"cjh7polu7004pbp7rl5gmudaf","_id":"cjh7polu9004zbp7rb1vkpdkh"},{"post_id":"cjh7polt4001nbp7rgk74wma7","tag_id":"cjh7poltv0037bp7rdipio9rh","_id":"cjh7polua0053bp7rhm42c2mh"},{"post_id":"cjh7polt4001nbp7rgk74wma7","tag_id":"cjh7polu9004xbp7r0vmpp4j6","_id":"cjh7polua0055bp7r8z6b005u"},{"post_id":"cjh7polt6001rbp7racatjt8y","tag_id":"cjh7poltv0037bp7rdipio9rh","_id":"cjh7polud005cbp7rz9ct579h"},{"post_id":"cjh7polt6001rbp7racatjt8y","tag_id":"cjh7poltw003gbp7rka90b3pu","_id":"cjh7polud005dbp7r9azrpgu3"},{"post_id":"cjh7polt8001tbp7rr2bj806d","tag_id":"cjh7poltv0037bp7rdipio9rh","_id":"cjh7poluf005kbp7rthff7w0e"},{"post_id":"cjh7polt8001tbp7rr2bj806d","tag_id":"cjh7polue005ebp7rrmvatzcz","_id":"cjh7poluf005lbp7rzoz01de9"},{"post_id":"cjh7polta001wbp7rg1x5bu2p","tag_id":"cjh7poltv0037bp7rdipio9rh","_id":"cjh7poluh005rbp7rcclz8ys4"},{"post_id":"cjh7polta001wbp7rg1x5bu2p","tag_id":"cjh7polug005mbp7r9t5st3gq","_id":"cjh7poluh005tbp7rv5774ogg"},{"post_id":"cjh7poltb001ybp7rbl0w3p14","tag_id":"cjh7poltv0037bp7rdipio9rh","_id":"cjh7polui005zbp7rceulbzih"},{"post_id":"cjh7poltb001ybp7rbl0w3p14","tag_id":"cjh7poluh005ubp7rp6bb8wuk","_id":"cjh7polui0060bp7r8cx45svz"},{"post_id":"cjh7poltd0022bp7rdjgwtez3","tag_id":"cjh7poluh005wbp7rk6tr6gtj","_id":"cjh7poluj0066bp7refrcy8bu"},{"post_id":"cjh7poltd0022bp7rdjgwtez3","tag_id":"cjh7polui0061bp7r7y7pp7sv","_id":"cjh7poluj0068bp7rlewtcdjn"},{"post_id":"cjh7polte0024bp7risqf1beq","tag_id":"cjh7poluh005wbp7rk6tr6gtj","_id":"cjh7polum006cbp7r9a6uzu1r"},{"post_id":"cjh7polte0024bp7risqf1beq","tag_id":"cjh7polui0061bp7r7y7pp7sv","_id":"cjh7polum006ebp7r4b06tlo5"},{"post_id":"cjh7poltf0028bp7rdbmfbb06","tag_id":"cjh7poluh005wbp7rk6tr6gtj","_id":"cjh7poluo006kbp7r4vgoa6fc"},{"post_id":"cjh7poltf0028bp7rdbmfbb06","tag_id":"cjh7polui0061bp7r7y7pp7sv","_id":"cjh7poluo006mbp7r04gjd31v"},{"post_id":"cjh7poltg002bbp7r73iasgk4","tag_id":"cjh7poluh005wbp7rk6tr6gtj","_id":"cjh7poluq006sbp7rrcz22qvb"},{"post_id":"cjh7poltg002bbp7r73iasgk4","tag_id":"cjh7polui0061bp7r7y7pp7sv","_id":"cjh7poluq006vbp7rdeiq2zzh"},{"post_id":"cjh7polti002ebp7rga5slf59","tag_id":"cjh7poltv0037bp7rdipio9rh","_id":"cjh7polus0072bp7rd1qstfic"},{"post_id":"cjh7polti002ebp7rga5slf59","tag_id":"cjh7poltw003ebp7r8bzcry1n","_id":"cjh7polus0073bp7rp4npmdqj"},{"post_id":"cjh7poltj002fbp7r7su3p7bq","tag_id":"cjh7pols6000dbp7r564f39kp","_id":"cjh7polut0078bp7ro9vf5v94"},{"post_id":"cjh7poltj002fbp7r7su3p7bq","tag_id":"cjh7polur0070bp7rrkwvjnmo","_id":"cjh7polut0079bp7rjfpkdnp1"},{"post_id":"cjh7poltk002ibp7r8k6v71bm","tag_id":"cjh7polus0076bp7rt1okg23x","_id":"cjh7poluv007ibp7rwlyn0dvx"},{"post_id":"cjh7poltk002ibp7r8k6v71bm","tag_id":"cjh7poluu007cbp7rbwsp8dpk","_id":"cjh7poluv007kbp7r56fif2ua"},{"post_id":"cjh7poltl002kbp7r3qn669vg","tag_id":"cjh7polus0076bp7rt1okg23x","_id":"cjh7poluy007rbp7rmibf8h2f"},{"post_id":"cjh7poltl002kbp7r3qn669vg","tag_id":"cjh7poluu007cbp7rbwsp8dpk","_id":"cjh7poluy007tbp7raqqdg9k6"},{"post_id":"cjh7poltm002obp7ryahxa1ne","tag_id":"cjh7polus0076bp7rt1okg23x","_id":"cjh7polv10081bp7r79fxkyhk"},{"post_id":"cjh7poltm002obp7ryahxa1ne","tag_id":"cjh7poluu007cbp7rbwsp8dpk","_id":"cjh7polv20083bp7r70czvhdq"},{"post_id":"cjh7poltn002pbp7rsbprdrbd","tag_id":"cjh7polus0076bp7rt1okg23x","_id":"cjh7polv5008bbp7r9bmxa3jl"},{"post_id":"cjh7poltn002pbp7rsbprdrbd","tag_id":"cjh7poluu007cbp7rbwsp8dpk","_id":"cjh7polv5008dbp7ry32jpndz"},{"post_id":"cjh7polto002sbp7r2dl44tpe","tag_id":"cjh7polv40088bp7r4c9fc671","_id":"cjh7polv6008hbp7rl84fpkgi"},{"post_id":"cjh7poltp002ubp7r1kahijdn","tag_id":"cjh7polv5008ebp7rcymttzyp","_id":"cjh7polv8008qbp7rc60ndk69"},{"post_id":"cjh7poltp002ubp7r1kahijdn","tag_id":"cjh7polv6008jbp7rzh9oo2qh","_id":"cjh7polv8008sbp7rx46gf3cn"},{"post_id":"cjh7poltq002ybp7r25nikbsl","tag_id":"cjh7polv5008ebp7rcymttzyp","_id":"cjh7polva0090bp7rukcvhrls"},{"post_id":"cjh7poltq002ybp7r25nikbsl","tag_id":"cjh7polv6008jbp7rzh9oo2qh","_id":"cjh7polvb0092bp7rud7jogxd"},{"post_id":"cjh7polts002zbp7r8xjwd09g","tag_id":"cjh7polv9008xbp7rhl2ffk6i","_id":"cjh7polvd009abp7r4j12d4ec"},{"post_id":"cjh7polts002zbp7r8xjwd09g","tag_id":"cjh7polvb0094bp7rtgcgvjrf","_id":"cjh7polvd009cbp7rxrnhk586"},{"post_id":"cjh7poltt0032bp7rtru0cebl","tag_id":"cjh7polv9008xbp7rhl2ffk6i","_id":"cjh7polvf009ibp7r3tofiqi4"},{"post_id":"cjh7poltt0032bp7rtru0cebl","tag_id":"cjh7polvd009ebp7rpgqfxi7m","_id":"cjh7polvf009kbp7r7j5vz8o0"},{"post_id":"cjh7polwy009nbp7r3lq5s90f","tag_id":"cjh7poluh005wbp7rk6tr6gtj","_id":"cjh7polx0009obp7r8g7foxrq"},{"post_id":"cjh7polwy009nbp7r3lq5s90f","tag_id":"cjh7polui0061bp7r7y7pp7sv","_id":"cjh7polx0009pbp7recxr16xg"},{"post_id":"cjh7polxb009sbp7rqgdfzm21","tag_id":"cjh7polxd009vbp7r23g62zax","_id":"cjh7polxh00a1bp7r5ykq3bp5"},{"post_id":"cjh7polxc009tbp7r3ojwtzxm","tag_id":"cjh7polxg00a0bp7r3eilcbk1","_id":"cjh7polxi00a4bp7ryt8d3jk1"},{"post_id":"cjh7polxd009wbp7rij99d2t7","tag_id":"cjh7polxh00a3bp7rkq2tvojz","_id":"cjh7polxi00a7bp7rw083dpbo"},{"post_id":"cjh7polxe009xbp7rg2e8fj8q","tag_id":"cjh7polxi00a6bp7rfy4xy2go","_id":"cjh7polxj00aabp7rlfxa777j"},{"post_id":"cjh7polxf009ybp7r99st5a6e","tag_id":"cjh7polxh00a3bp7rkq2tvojz","_id":"cjh7polxj00acbp7rc8udjs4h"}],"Tag":[{"name":"markdown","_id":"cjh7polla0006bp7rl59yhv5w"},{"name":"java","_id":"cjh7pols6000dbp7r564f39kp"},{"name":"jdk源码","_id":"cjh7polsa000ibp7rnycp5efu"},{"name":"java基础","_id":"cjh7polsk000ubp7rvhmw9xy2"},{"name":"jms","_id":"cjh7polsy001abp7r8glwe4ii"},{"name":"linux","_id":"cjh7polt1001hbp7replga4ct"},{"name":"网络","_id":"cjh7polt6001qbp7r810qrd2q"},{"name":"开源项目","_id":"cjh7poltf0027bp7riw6zvdfp"},{"name":"kettle","_id":"cjh7polth002dbp7ry8ks7qye"},{"name":"大数据","_id":"cjh7poltv0037bp7rdipio9rh"},{"name":"spark","_id":"cjh7poltw003ebp7r8bzcry1n"},{"name":"hadoop","_id":"cjh7poltw003gbp7rka90b3pu"},{"name":"分布式","_id":"cjh7poltx003ibp7rmwrk9lfq"},{"name":"hbase","_id":"cjh7polty003mbp7rt69u9hgs"},{"name":"分布式存储","_id":"cjh7polu20042bp7rl92laciz"},{"name":"分布式计算","_id":"cjh7polu40049bp7rb1fwllyb"},{"name":"实时分析","_id":"cjh7polu5004fbp7royvhws22"},{"name":"数据仓库","_id":"cjh7polu7004nbp7rj0byrrq7"},{"name":"数据采集","_id":"cjh7polu7004pbp7rl5gmudaf"},{"name":"流式计算","_id":"cjh7polu9004xbp7r0vmpp4j6"},{"name":"hive","_id":"cjh7polue005ebp7rrmvatzcz"},{"name":"kylin","_id":"cjh7polug005mbp7r9t5st3gq"},{"name":"数据分析","_id":"cjh7poluh005ubp7rp6bb8wuk"},{"name":"oracle","_id":"cjh7poluh005wbp7rk6tr6gtj"},{"name":"数据库","_id":"cjh7polui0061bp7r7y7pp7sv"},{"name":"spring","_id":"cjh7polur0070bp7rrkwvjnmo"},{"name":"异常","_id":"cjh7polus0076bp7rt1okg23x"},{"name":"tomcat","_id":"cjh7poluu007cbp7rbwsp8dpk"},{"name":"服务","_id":"cjh7polv40088bp7r4c9fc671"},{"name":"网络安全","_id":"cjh7polv5008ebp7rcymttzyp"},{"name":"安全小组","_id":"cjh7polv6008jbp7rzh9oo2qh"},{"name":"软件工程","_id":"cjh7polv9008xbp7rhl2ffk6i"},{"name":"算法","_id":"cjh7polvb0094bp7rtgcgvjrf"},{"name":"设计模式","_id":"cjh7polvd009ebp7rpgqfxi7m"},{"name":"tools.emacs","_id":"cjh7polxd009vbp7r23g62zax"},{"name":"tools.vim","_id":"cjh7polxg00a0bp7r3eilcbk1"},{"name":"angular.js","_id":"cjh7polxh00a3bp7rkq2tvojz"},{"name":"react.js","_id":"cjh7polxi00a6bp7rfy4xy2go"}]}}